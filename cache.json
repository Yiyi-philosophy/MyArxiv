{"2024-03-13T00:00:00Z":{"Distributed, Parallel, and Cluster Computing":[{"id":"http://arxiv.org/abs/2403.07410v2","updated":"2024-03-13T07:03:56Z","published":"2024-03-12T08:38:09Z","title":"Polylog-Competitive Deterministic Local Routing and Scheduling","summary":"  This paper addresses point-to-point packet routing in undirected networks,\nwhich is the most important communication primitive in most networks. The main\nresult proves the existence of routing tables that guarantee a\npolylog-competitive completion-time $\\textbf{deterministically}$: in any\nundirected network, it is possible to give each node simple stateless\ndeterministic local forwarding rules, such that, any adversarially chosen set\nof packets are delivered as fast as possible, up to polylog factors.\n  All previous routing strategies crucially required randomization for both\nroute selection and packet scheduling.\n  The core technical contribution of this paper is a new local packet\nscheduling result of independent interest. This scheduling strategy integrates\nwell with recent sparse semi-oblivious path selection strategies. Such\nstrategies deterministically select not one but several candidate paths for\neach packet and require a global coordinator to select a single good path from\nthose candidates for each packet. Another challenge is that, even if a single\npath is selected for each packet, no strategy for scheduling packets along\nlow-congestion paths that is both local and deterministic is known. Our novel\nscheduling strategy utilizes the fact that every semi-oblivious routing\nstrategy uses only a small (polynomial) subset of candidate routes. It\novercomes the issue of global coordination by furthermore being provably robust\nto adversarial noise. This avoids the issue of having to choose a single path\nper packet because congestion caused by ineffective candidate paths can be\ntreated as noise.\n  Our results imply the first deterministic universally-optimal algorithms in\nthe distributed supported-CONGEST model for many important global distributed\ntasks, including computing minimum spanning trees, approximate shortest paths,\nand part-wise aggregates.\n","authors":["Bernhard Haeupler","Shyamal Patel","Antti Roeyskoe","Cliff Stein","Goran Zuzic"],"pdf_url":"https://arxiv.org/pdf/2403.07410v2.pdf","comment":"To appear at STOC 2024"},{"id":"http://arxiv.org/abs/2403.02619v2","updated":"2024-03-13T07:19:06Z","published":"2024-03-05T03:18:43Z","title":"Training Machine Learning models at the Edge: A Survey","summary":"  Edge Computing (EC) has gained significant traction in recent years,\npromising enhanced efficiency by integrating Artificial Intelligence (AI)\ncapabilities at the edge. While the focus has primarily been on the deployment\nand inference of Machine Learning (ML) models at the edge, the training aspect\nremains less explored. This survey delves into Edge Learning (EL), specifically\nthe optimization of ML model training at the edge. The objective is to\ncomprehensively explore diverse approaches and methodologies in EL, synthesize\nexisting knowledge, identify challenges, and highlight future trends. Utilizing\nScopus' advanced search, relevant literature on EL was identified, revealing a\nconcentration of research efforts in distributed learning methods, particularly\nFederated Learning (FL). This survey further provides a guideline for comparing\ntechniques used to optimize ML for edge learning, along with an exploration of\ndifferent frameworks, libraries, and simulation tools available for EL. In\ndoing so, the paper contributes to a holistic understanding of the current\nlandscape and future directions in the intersection of edge computing and\nmachine learning, paving the way for informed comparisons between optimization\nmethods and techniques designed for edge learning.\n","authors":["Aymen Rayane Khouas","Mohamed Reda Bouadjenek","Hakim Hacid","Sunil Aryal"],"pdf_url":"https://arxiv.org/pdf/2403.02619v2.pdf","comment":"30 pages, 7 figures, submitted to IEEE Communications Surveys &\n  Tutorials"},{"id":"http://arxiv.org/abs/2403.05692v2","updated":"2024-03-13T15:39:45Z","published":"2024-03-08T22:03:21Z","title":"Privacy-Preserving Sharing of Data Analytics Runtime Metrics for\n  Performance Modeling","summary":"  Performance modeling for large-scale data analytics workloads can improve the\nefficiency of cluster resource allocations and job scheduling. However, the\nperformance of these workloads is influenced by numerous factors, such as job\ninputs and the assigned cluster resources. As a result, performance models\nrequire significant amounts of training data. This data can be obtained by\nexchanging runtime metrics between collaborating organizations. Yet, not all\norganizations may be inclined to publicly disclose such metadata.\n  We present a privacy-preserving approach for sharing runtime metrics based on\ndifferential privacy and data synthesis. Our evaluation on performance data\nfrom 736 Spark job executions indicates that fully anonymized training data\nlargely maintains performance prediction accuracy, particularly when there is\nminimal original data available. With 30 or fewer available original data\nsamples, the use of synthetic training data resulted only in a one percent\nreduction in performance model accuracy on average.\n","authors":["Jonathan Will","Dominik Scheinert","Jan Bode","Cedric Kring","Seraphin Zunzer","Lauritz Thamsen"],"pdf_url":"https://arxiv.org/pdf/2403.05692v2.pdf","comment":"4 pages, 4 figures, presented at the WOSP-C workshop at ICPE 2024"},{"id":"http://arxiv.org/abs/2305.07377v2","updated":"2024-03-13T15:28:25Z","published":"2023-05-12T11:04:56Z","title":"On a Voter Model with Context-Dependent Opinion Adoption","summary":"  Opinion diffusion is a crucial phenomenon in social networks, often\nunderlying the way in which a collective of agents develops a consensus on\nrelevant decisions. The voter model is a well-known theoretical model to study\nopinion spreading in social networks and structured populations. Its simplest\nversion assumes that an updating agent will adopt the opinion of a neighboring\nagent chosen at random. The model allows us to study, for example, the\nprobability that a certain opinion will fixate into a consensus opinion, as\nwell as the expected time it takes for a consensus opinion to emerge.\n  Standard voter models are oblivious to the opinions held by the agents\ninvolved in the opinion adoption process. We propose and study a\ncontext-dependent opinion spreading process on an arbitrary social graph, in\nwhich the probability that an agent abandons opinion $a$ in favor of opinion\n$b$ depends on both $a$ and $b$. We discuss the relations of the model with\nexisting voter models and then derive theoretical results for both the fixation\nprobability and the expected consensus time for two opinions, for both the\nsynchronous and the asynchronous update models.\n","authors":["Luca Becchetti","Vincenzo Bonifaci","Emilio Cruciani","Francesco Pasquale"],"pdf_url":"https://arxiv.org/pdf/2305.07377v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.00257v2","updated":"2024-03-13T14:28:03Z","published":"2023-11-01T03:14:48Z","title":"AMSP: Reducing Communication Overhead of ZeRO for Efficient LLM Training","summary":"  Training large language models (LLMs) encounters challenges in GPU memory\nconsumption due to the high memory requirements of model states. The widely\nused Zero Redundancy Optimizer (ZeRO) addresses this issue through strategic\nsharding but introduces communication challenges at scale. To tackle this\nproblem, we propose AMSP, a system designed to optimize ZeRO for scalable LLM\ntraining. AMSP incorporates three flexible sharding strategies: Full-Replica,\nFull-Sharding, and Partial-Sharding, and allows each component within the model\nstates (Parameters, Gradients, Optimizer States) to independently choose a\nsharding strategy as well as the device mesh. We conduct a thorough analysis of\ncommunication costs, formulating an optimization problem to discover the\noptimal sharding strategy. Additionally, AMSP optimizes distributed LLM\ntraining by efficiently overlapping communication with computation. Evaluations\ndemonstrate up to 52\\% Model FLOPs Utilization (MFU) when training the\nLLaMA-based model on 1024 GPUs, resulting in a 1.56 times improvement in\ntraining throughput compared to newly proposed systems like MiCS and ZeRO++.\n","authors":["Qiaoling Chen","Qinghao Hu","Guoteng Wang","Yingtong Xiong","Ting Huang","Xun Chen","Yang Gao","Hang Yan","Yonggang Wen","Tianwei Zhang","Peng Sun"],"pdf_url":"https://arxiv.org/pdf/2311.00257v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08515v1","updated":"2024-03-13T13:24:24Z","published":"2024-03-13T13:24:24Z","title":"Plotinus: A Satellite Internet Digital Twin System","summary":"  The development of integrated space-air-ground network (SAGIN) requires\nsophisticated satellite Internet emulation tools that can handle complex,\ndynamic topologies and offer in-depth analysis. Existing emulation platforms\nstruggle with challenges like the need for detailed implementation across all\nnetwork layers, real-time response times, and the ability to scale. Plotinus, a\nnew digital twin system based on microservices for satellite Internet\nemulation, aims to solve these problems. It features a modular design, allowing\nfor easy replacement of the physical layer to emulate different aerial vehicles\nand analyze channel interference. It also enables the replacement of path\ncomputation methods to simplify testing and deploying algorithms. In\nparticular, Plotinus allows for real-time emulation with live network traffic,\nenhancing the realism of network models. Evaluation result shows that\nPlotinus's effective emulation of dynamic satellite networks with real-world\ndevices. Its adaptability for various communication models and algorithm\ntesting highlights Plotinus's role as a vital tool for developing and analyzing\nSAGIN systems, offering a scalable, real-time response, and flexible digital\ntwin system.\n","authors":["Yue Gao","Kun Qiu","Zhe Chen","Wenjun Zhu","Qi Zhang","Handong Luo","Quanwei Lin","Ziheng Yang","Wenhao Liu"],"pdf_url":"https://arxiv.org/pdf/2403.08515v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08444v1","updated":"2024-03-13T11:56:10Z","published":"2024-03-13T11:56:10Z","title":"COSTREAM: Learned Cost Models for Operator Placement in Edge-Cloud\n  Environments","summary":"  In this work, we present COSTREAM, a novel learned cost model for Distributed\nStream Processing Systems that provides accurate predictions of the execution\ncosts of a streaming query in an edge-cloud environment. The cost model can be\nused to find an initial placement of operators across heterogeneous hardware,\nwhich is particularly important in these environments. In our evaluation, we\ndemonstrate that COSTREAM can produce highly accurate cost estimates for the\ninitial operator placement and even generalize to unseen placements, queries,\nand hardware. When using COSTREAM to optimize the placements of streaming\noperators, a median speed-up of around 21x can be achieved compared to\nbaselines.\n","authors":["Roman Heinrich","Carsten Binnig","Harald Kornmayer","Manisha Luthra"],"pdf_url":"https://arxiv.org/pdf/2403.08444v1.pdf","comment":"This paper has been accepted by IEEE ICDE 2024"},{"id":"http://arxiv.org/abs/2403.08374v1","updated":"2024-03-13T09:38:22Z","published":"2024-03-13T09:38:22Z","title":"Error-Free Near-Optimal Validated Agreement","summary":"  Byzantine agreement enables n processes to agree on a common L-bit value,\ndespite t > 0 arbitrary failures. A long line of work has been dedicated to\nimproving the worst-case bit complexity of Byzantine agreement in synchrony.\nThis has culminated in COOL, an error-free (deterministically secure against a\ncomputationally unbounded adversary) algorithm that achieves a near-optimal bit\ncomplexity of O(nL + n^2 log n). COOL satisfies strong validity: if all correct\nprocesses propose the same value, only that value can be decided. Thus,\nwhenever correct processes do not a priori agree, COOL might decide on\n\"bottom\", thus limiting its application in today's state machine replication\n(SMR) and blockchain protocols. In this work, we focus on the aforementioned\nlimitation. Can we design an error-free near-optimal Byzantine agreement\nalgorithm applicable in today's SMR and blockchain protocols? Can we design an\nerror-free near-optimal agreement algorithm with external validity (a.k.a.\nvalidated agreement) stipulating that only values valid according to a\npredetermined predicate can be decided?\n  This paper answers the question affirmatively. Namely, we present EXT, an\nerror-free synchronous Byzantine agreement algorithm that satisfies external\n(along with strong) validity while exchanging O(n log n L + n^2 log n) bits in\nthe worst case. Importantly, EXT is optimally resilient (tolerates t < n / 3\nfailures) and terminates in optimal O(n) rounds. Perhaps surprisingly, we\nconstruct EXT by exploiting existing concepts: (1) the recursive framework\nproposed by Berman, Garay and Perry and Coan and Welch and recently restated by\nMomose and Ren, (2) the aforementioned COOL algorithm introduced by Chen, and\n(3) the data dissemination primitive introduced by Das, Xiang and Ren.\n","authors":["Pierre Civit","Muhammad Ayaz Dzulfikar","Seth Gilbert","Rachid Guerraoui","Jovan Komatovic","Manuel Vidigueira","Igor Zablotchi"],"pdf_url":"https://arxiv.org/pdf/2403.08374v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.03628v3","updated":"2024-03-13T08:57:29Z","published":"2023-09-07T10:50:32Z","title":"OSMOSIS: Enabling Multi-Tenancy in Datacenter SmartNICs","summary":"  Multi-tenancy is essential for unleashing SmartNIC's potential in\ndatacenters. Our systematic analysis in this work shows that existing on-path\nSmartNICs have resource multiplexing limitations. For example, existing\nsolutions lack multi-tenancy capabilities such as performance isolation and QoS\nprovisioning for compute and IO resources. Compared to standard NIC data paths\nwith a well-defined set of offloaded functions, unpredictable execution times\nof SmartNIC kernels make conventional approaches for multi-tenancy and QoS\ninsufficient. We fill this gap with OSMOSIS, a SmartNICs resource manager\nco-design. OSMOSIS extends existing OS mechanisms to enable dynamic hardware\nresource multiplexing of the on-path packet processing data plane. We integrate\nOSMOSIS within an open-source RISC-V-based 400Gbit/s SmartNIC. Our performance\nresults demonstrate that OSMOSIS fully supports multi-tenancy and enables\nbroader adoption of SmartNICs in datacenters with low overhead.\n","authors":["Mikhail Khalilov","Marcin Chrapek","Siyuan Shen","Alessandro Vezzu","Thomas Benz","Salvatore Di Girolamo","Timo Schneider","Daniele De Sensi","Luca Benini","Torsten Hoefler"],"pdf_url":"https://arxiv.org/pdf/2309.03628v3.pdf","comment":"12 pages, 14 figures, 103 references"},{"id":"http://arxiv.org/abs/2403.08245v1","updated":"2024-03-13T05:00:23Z","published":"2024-03-13T05:00:23Z","title":"Scattered Mixture-of-Experts Implementation","summary":"  We present ScatterMoE, an implementation of Sparse Mixture-of-Experts (SMoE)\non GPUs. ScatterMoE builds upon existing implementations, and overcoming some\nof the limitations to improve inference and training speed, and memory\nfootprint. This implementation achieves this by avoiding padding and making\nexcessive copies of the input. We introduce ParallelLinear, the main component\nwe use to build our implementation and the various kernels used to speed up the\noperation. We benchmark our implementation against Megablocks, and show that it\nenables a higher throughput and lower memory footprint. We also show how\nParallelLinear enables extension of the Mixture-of-Experts concept by\ndemonstrating with an implementation of Mixture of Attention.\n","authors":["Shawn Tan","Yikang Shen","Rameswar Panda","Aaron Courville"],"pdf_url":"https://arxiv.org/pdf/2403.08245v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08940v1","updated":"2024-03-13T20:16:16Z","published":"2024-03-13T20:16:16Z","title":"A Virtual Environment for Collaborative Inspection in Additive\n  Manufacturing","summary":"  Additive manufacturing (AM) techniques have been used to enhance the design\nand fabrication of complex components for various applications in the medical,\naerospace, energy, and consumer products industries. A defining feature for\nmany AM parts is the complex internal geometry enabled by the printing process.\nHowever, inspecting these internal structures requires volumetric imaging,\ni.e., X-ray CT, leading to the well-known challenge of visualizing complex 3D\ngeometries using 2D desktop interfaces. Furthermore, existing tools are limited\nto single-user systems making it difficult to jointly discuss or share findings\nwith a larger team, i.e., the designers, manufacturing experts, and evaluation\nteam. In this work, we present a collaborative virtual reality (VR) for the\nexploration and inspection of AM parts. Geographically separated experts can\nvirtually inspect and jointly discuss data. It also supports VR and non-VR\nusers, who can be spectators in the VR environment. Various features for data\nexploration and inspection are developed and enhanced via real-time\nsynchronization. We followed usability and interface verification guidelines\nusing Nielsen's heuristics approach. Furthermore, we conducted exploratory and\nsemi-structured interviews with domain experts to collect qualitative feedback.\nResults reveal potential benefits, applicability, and current limitations. The\nproposed collaborative VR environment provides a new basis and opens new\nresearch directions for virtual inspection and team collaboration in AM\nsettings.\n","authors":["Vuthea Chheang","Brian Thomas Weston","Robert William Cerda","Brian Au","Brian Giera","Peer-Timo Bremer","Haichao Miao"],"pdf_url":"https://arxiv.org/pdf/2403.08940v1.pdf","comment":"Conditionally Accepted - CHI LBW 2024"},{"id":"http://arxiv.org/abs/2403.08900v1","updated":"2024-03-13T18:45:15Z","published":"2024-03-13T18:45:15Z","title":"Handoffs in User-Centric Cell-Free MIMO Networks: A POMDP Framework","summary":"  We study the problem of managing handoffs (HOs) in user-centric cell-free\nmassive MIMO (UC-mMIMO) networks. Motivated by the importance of controlling\nthe number of HOs and by the correlation between efficient HO decisions and the\ntemporal evolution of the channel conditions, we formulate a partially\nobservable Markov decision process (POMDP) with the state space representing\nthe discrete versions of the large-scale fading and the action space\nrepresenting the association decisions of the user with the access points\n(APs). We develop a novel algorithm that employs this model to derive a HO\npolicy for a mobile user based on current and future rewards. To alleviate the\nhigh complexity of our POMDP, we follow a divide-and-conquer approach by\nbreaking down the POMDP formulation into sub-problems, each solved separately.\nThen, the policy and the candidate pool of APs for the sub-problem that\nproduced the best total expected reward are used to perform HOs within a\nspecific time horizon. We then introduce modifications to our algorithm to\ndecrease the number of HOs. The results show that half of the number of HOs in\nthe UC-mMIMO networks can be eliminated. Namely, our novel solution can control\nthe number of HOs while maintaining a rate guarantee, where a 47%-70% reduction\nof the cumulative number of HOs is observed in networks with a density of 125\nAPs per km2. Most importantly, our results show that a POMDP-based HO scheme is\npromising to control HOs.\n","authors":["Hussein A. Ammar","Raviraj Adve","Shahram Shahbazpanahi","Gary Boudreau","Kothapalli Venkata Srinivas"],"pdf_url":"https://arxiv.org/pdf/2403.08900v1.pdf","comment":"Accepted in IEEE Transactions on Wireless Communications (TWC)"},{"id":"http://arxiv.org/abs/2403.08837v1","updated":"2024-03-13T08:39:21Z","published":"2024-03-13T08:39:21Z","title":"Cyclic Data Parallelism for Efficient Parallelism of Deep Neural\n  Networks","summary":"  Training large deep learning models requires parallelization techniques to\nscale. In existing methods such as Data Parallelism or ZeRO-DP, micro-batches\nof data are processed in parallel, which creates two drawbacks: the total\nmemory required to store the model's activations peaks at the end of the\nforward pass, and gradients must be simultaneously averaged at the end of the\nbackpropagation step. We propose Cyclic Data Parallelism, a novel paradigm\nshifting the execution of the micro-batches from simultaneous to sequential,\nwith a uniform delay. At the cost of a slight gradient delay, the total memory\ntaken by activations is constant, and the gradient communications are balanced\nduring the training step. With Model Parallelism, our technique reduces the\nnumber of GPUs needed, by sharing GPUs across micro-batches. Within the ZeRO-DP\nframework, our technique allows communication of the model states with\npoint-to-point operations rather than a collective broadcast operation. We\nillustrate the strength of our approach on the CIFAR-10 and ImageNet datasets.\n","authors":["Louis Fournier","Edouard Oyallon"],"pdf_url":"https://arxiv.org/pdf/2403.08837v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08896v1","updated":"2024-03-13T18:37:16Z","published":"2024-03-13T18:37:16Z","title":"One-Shot Averaging for Distributed TD($λ$) Under Markov Sampling","summary":"  We consider a distributed setup for reinforcement learning, where each agent\nhas a copy of the same Markov Decision Process but transitions are sampled from\nthe corresponding Markov chain independently by each agent. We show that in\nthis setting, we can achieve a linear speedup for TD($\\lambda$), a family of\npopular methods for policy evaluation, in the sense that $N$ agents can\nevaluate a policy $N$ times faster provided the target accuracy is small\nenough. Notably, this speedup is achieved by ``one shot averaging,'' a\nprocedure where the agents run TD($\\lambda$) with Markov sampling independently\nand only average their results after the final step. This significantly reduces\nthe amount of communication required to achieve a linear speedup relative to\nprevious work.\n","authors":["Haoxing Tian","Ioannis Ch. Paschalidis","Alex Olshevsky"],"pdf_url":"https://arxiv.org/pdf/2403.08896v1.pdf","comment":null}],"Artificial Intelligence":[{"id":"http://arxiv.org/abs/2403.08770v1","updated":"2024-03-13T17:59:56Z","published":"2024-03-13T17:59:56Z","title":"FastMAC: Stochastic Spectral Sampling of Correspondence Graph","summary":"  3D correspondence, i.e., a pair of 3D points, is a fundamental concept in\ncomputer vision. A set of 3D correspondences, when equipped with compatibility\nedges, forms a correspondence graph. This graph is a critical component in\nseveral state-of-the-art 3D point cloud registration approaches, e.g., the one\nbased on maximal cliques (MAC). However, its properties have not been well\nunderstood. So we present the first study that introduces graph signal\nprocessing into the domain of correspondence graph. We exploit the generalized\ndegree signal on correspondence graph and pursue sampling strategies that\npreserve high-frequency components of this signal. To address time-consuming\nsingular value decomposition in deterministic sampling, we resort to a\nstochastic approximate sampling strategy. As such, the core of our method is\nthe stochastic spectral sampling of correspondence graph. As an application, we\nbuild a complete 3D registration algorithm termed as FastMAC, that reaches\nreal-time speed while leading to little to none performance drop. Through\nextensive experiments, we validate that FastMAC works for both indoor and\noutdoor benchmarks. For example, FastMAC can accelerate MAC by 80 times while\nmaintaining high registration success rate on KITTI. Codes are publicly\navailable at https://github.com/Forrest-110/FastMAC.\n","authors":["Yifei Zhang","Hao Zhao","Hongyang Li","Siheng Chen"],"pdf_url":"https://arxiv.org/pdf/2403.08770v1.pdf","comment":"CVPR 2024, Code: https://github.com/Forrest-110/FastMAC"},{"id":"http://arxiv.org/abs/2403.08763v1","updated":"2024-03-13T17:58:57Z","published":"2024-03-13T17:58:57Z","title":"Simple and Scalable Strategies to Continually Pre-train Large Language\n  Models","summary":"  Large language models (LLMs) are routinely pre-trained on billions of tokens,\nonly to start the process over again once new data becomes available. A much\nmore efficient solution is to continually pre-train these models, saving\nsignificant compute compared to re-training. However, the distribution shift\ninduced by new data typically results in degraded performance on previous data\nor poor adaptation to the new data. In this work, we show that a simple and\nscalable combination of learning rate (LR) re-warming, LR re-decaying, and\nreplay of previous data is sufficient to match the performance of fully\nre-training from scratch on all available data, as measured by final loss and\nlanguage model (LM) evaluation benchmarks. Specifically, we show this for a\nweak but realistic distribution shift between two commonly used LLM\npre-training datasets (English$\\rightarrow$English) and a stronger distribution\nshift (English$\\rightarrow$German) at the $405$M parameter model scale with\nlarge dataset sizes (hundreds of billions of tokens). Selecting the weak but\nrealistic shift for larger-scale experiments, we also find that our continual\nlearning strategies match the re-training baseline for a 10B parameter LLM. Our\nresults demonstrate that LLMs can be successfully updated via simple and\nscalable continual learning strategies, matching the re-training baseline using\nonly a fraction of the compute. Finally, inspired by previous work, we propose\nalternatives to the cosine learning rate schedule that help circumvent\nforgetting induced by LR re-warming and that are not bound to a fixed token\nbudget.\n","authors":["Adam Ibrahim","Benjamin Thérien","Kshitij Gupta","Mats L. Richter","Quentin Anthony","Timothée Lesort","Eugene Belilovsky","Irina Rish"],"pdf_url":"https://arxiv.org/pdf/2403.08763v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08755v1","updated":"2024-03-13T17:53:47Z","published":"2024-03-13T17:53:47Z","title":"DAM: Dynamic Adapter Merging for Continual Video QA Learning","summary":"  We present a parameter-efficient method for continual video\nquestion-answering (VidQA) learning. Our method, named DAM, uses the proposed\nDynamic Adapter Merging to (i) mitigate catastrophic forgetting, (ii) enable\nefficient adaptation to continually arriving datasets, (iii) handle inputs from\nunknown datasets during inference, and (iv) enable knowledge sharing across\nsimilar dataset domains. Given a set of continually streaming VidQA datasets,\nwe sequentially train dataset-specific adapters for each dataset while freezing\nthe parameters of a large pretrained video-language backbone. During inference,\ngiven a video-question sample from an unknown domain, our method first uses the\nproposed non-parametric router function to compute a probability for each\nadapter, reflecting how relevant that adapter is to the current video-question\ninput instance. Subsequently, the proposed dynamic adapter merging scheme\naggregates all the adapter weights into a new adapter instance tailored for\nthat particular test sample to compute the final VidQA prediction, mitigating\nthe impact of inaccurate router predictions and facilitating knowledge sharing\nacross domains. Our DAM model outperforms prior state-of-the-art continual\nlearning approaches by 9.1% while exhibiting 1.9% less forgetting on 6 VidQA\ndatasets spanning various domains. We further extend DAM to continual image\nclassification and image QA and outperform prior methods by a large margin. The\ncode is publicly available at: https://github.com/klauscc/DAM\n","authors":["Feng Cheng","Ziyang Wang","Yi-Lin Sung","Yan-Bo Lin","Mohit Bansal","Gedas Bertasius"],"pdf_url":"https://arxiv.org/pdf/2403.08755v1.pdf","comment":"The first two authors contribute equally"},{"id":"http://arxiv.org/abs/2403.08743v1","updated":"2024-03-13T17:46:28Z","published":"2024-03-13T17:46:28Z","title":"Steering LLMs Towards Unbiased Responses: A Causality-Guided Debiasing\n  Framework","summary":"  Large language models (LLMs) can easily generate biased and discriminative\nresponses. As LLMs tap into consequential decision-making (e.g., hiring and\nhealthcare), it is of crucial importance to develop strategies to mitigate\nthese biases. This paper focuses on social bias, tackling the association\nbetween demographic information and LLM outputs. We propose a causality-guided\ndebiasing framework that utilizes causal understandings of (1) the\ndata-generating process of the training corpus fed to LLMs, and (2) the\ninternal reasoning process of LLM inference, to guide the design of prompts for\ndebiasing LLM outputs through selection mechanisms. Our framework unifies\nexisting de-biasing prompting approaches such as inhibitive instructions and\nin-context contrastive examples, and sheds light on new ways of debiasing by\nencouraging bias-free reasoning. Our strong empirical performance on real-world\ndatasets demonstrates that our framework provides principled guidelines on\ndebiasing LLM outputs even with only the black-box access.\n","authors":["Jingling Li","Zeyu Tang","Xiaoyu Liu","Peter Spirtes","Kun Zhang","Liu Leqi","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2403.08743v1.pdf","comment":"18 pages, 11 figures"},{"id":"http://arxiv.org/abs/2403.08739v1","updated":"2024-03-13T17:42:32Z","published":"2024-03-13T17:42:32Z","title":"The Garden of Forking Paths: Observing Dynamic Parameters Distribution\n  in Large Language Models","summary":"  A substantial gap persists in understanding the reasons behind the\nexceptional performance of the Transformer architecture in NLP. A particularly\nunexplored area involves the mechanistic description of how the distribution of\nparameters evolves over time during training. In this work we suggest that\nlooking at the time evolution of the statistic distribution of model\nparameters, and specifically at bifurcation effects, can help understanding the\nmodel quality, potentially reducing training costs and evaluation efforts and\nempirically showing the reasons behind the effectiveness of weights\nsparsification.\n","authors":["Carlo Nicolini","Jacopo Staiano","Bruno Lepri","Raffaele Marino"],"pdf_url":"https://arxiv.org/pdf/2403.08739v1.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2310.04475v2","updated":"2024-03-13T17:40:04Z","published":"2023-10-06T05:27:28Z","title":"Demystifying Embedding Spaces using Large Language Models","summary":"  Embeddings have become a pivotal means to represent complex, multi-faceted\ninformation about entities, concepts, and relationships in a condensed and\nuseful format. Nevertheless, they often preclude direct interpretation. While\ndownstream tasks make use of these compressed representations, meaningful\ninterpretation usually requires visualization using dimensionality reduction or\nspecialized machine learning interpretability methods. This paper addresses the\nchallenge of making such embeddings more interpretable and broadly useful, by\nemploying Large Language Models (LLMs) to directly interact with embeddings --\ntransforming abstract vectors into understandable narratives. By injecting\nembeddings into LLMs, we enable querying and exploration of complex embedding\ndata. We demonstrate our approach on a variety of diverse tasks, including:\nenhancing concept activation vectors (CAVs), communicating novel embedded\nentities, and decoding user preferences in recommender systems. Our work\ncouples the immense information potential of embeddings with the interpretative\npower of LLMs.\n","authors":["Guy Tennenholtz","Yinlam Chow","Chih-Wei Hsu","Jihwan Jeong","Lior Shani","Azamat Tulepbergenov","Deepak Ramachandran","Martin Mladenov","Craig Boutilier"],"pdf_url":"https://arxiv.org/pdf/2310.04475v2.pdf","comment":"Accepted to ICLR 2024"},{"id":"http://arxiv.org/abs/2312.07511v2","updated":"2024-03-13T17:38:27Z","published":"2023-12-12T18:44:19Z","title":"A Hitchhiker's Guide to Geometric GNNs for 3D Atomic Systems","summary":"  Recent advances in computational modelling of atomic systems, spanning\nmolecules, proteins, and materials, represent them as geometric graphs with\natoms embedded as nodes in 3D Euclidean space. In these graphs, the geometric\nattributes transform according to the inherent physical symmetries of 3D atomic\nsystems, including rotations and translations in Euclidean space, as well as\nnode permutations. In recent years, Geometric Graph Neural Networks have\nemerged as the preferred machine learning architecture powering applications\nranging from protein structure prediction to molecular simulations and material\ngeneration. Their specificity lies in the inductive biases they leverage - such\nas physical symmetries and chemical properties - to learn informative\nrepresentations of these geometric graphs.\n  In this opinionated paper, we provide a comprehensive and self-contained\noverview of the field of Geometric GNNs for 3D atomic systems. We cover\nfundamental background material and introduce a pedagogical taxonomy of\nGeometric GNN architectures: (1) invariant networks, (2) equivariant networks\nin Cartesian basis, (3) equivariant networks in spherical basis, and (4)\nunconstrained networks. Additionally, we outline key datasets and application\nareas and suggest future research directions. The objective of this work is to\npresent a structured perspective on the field, making it accessible to\nnewcomers and aiding practitioners in gaining an intuition for its mathematical\nabstractions.\n","authors":["Alexandre Duval","Simon V. Mathis","Chaitanya K. Joshi","Victor Schmidt","Santiago Miret","Fragkiskos D. Malliaros","Taco Cohen","Pietro Liò","Yoshua Bengio","Michael Bronstein"],"pdf_url":"https://arxiv.org/pdf/2312.07511v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08728v1","updated":"2024-03-13T17:28:20Z","published":"2024-03-13T17:28:20Z","title":"Ambient Diffusion Posterior Sampling: Solving Inverse Problems with\n  Diffusion Models trained on Corrupted Data","summary":"  We provide a framework for solving inverse problems with diffusion models\nlearned from linearly corrupted data. Our method, Ambient Diffusion Posterior\nSampling (A-DPS), leverages a generative model pre-trained on one type of\ncorruption (e.g. image inpainting) to perform posterior sampling conditioned on\nmeasurements from a potentially different forward process (e.g. image\nblurring). We test the efficacy of our approach on standard natural image\ndatasets (CelebA, FFHQ, and AFHQ) and we show that A-DPS can sometimes\noutperform models trained on clean data for several image restoration tasks in\nboth speed and performance. We further extend the Ambient Diffusion framework\nto train MRI models with access only to Fourier subsampled multi-coil MRI\nmeasurements at various acceleration factors (R=2, 4, 6, 8). We again observe\nthat models trained on highly subsampled data are better priors for solving\ninverse problems in the high acceleration regime than models trained on fully\nsampled data. We open-source our code and the trained Ambient Diffusion MRI\nmodels: https://github.com/utcsilab/ambient-diffusion-mri .\n","authors":["Asad Aali","Giannis Daras","Brett Levac","Sidharth Kumar","Alexandros G. Dimakis","Jonathan I. Tamir"],"pdf_url":"https://arxiv.org/pdf/2403.08728v1.pdf","comment":"Pre-print, work in progress"},{"id":"http://arxiv.org/abs/2310.07793v4","updated":"2024-03-13T17:10:48Z","published":"2023-10-11T18:27:12Z","title":"GenTKG: Generative Forecasting on Temporal Knowledge Graph","summary":"  The rapid advancements in large language models (LLMs) have ignited interest\nin the temporal knowledge graph (tKG) domain, where conventional\nembedding-based and rule-based methods dominate. The question remains open of\nwhether pre-trained LLMs can understand structured temporal relational data and\nreplace them as the foundation model for temporal relational forecasting.\nTherefore, we bring temporal knowledge forecasting into the generative setting.\nHowever, challenges occur in the huge chasms between complex temporal graph\ndata structure and sequential natural expressions LLMs can handle, and between\nthe enormous data sizes of tKGs and heavy computation costs of finetuning LLMs.\nTo address these challenges, we propose a novel retrieval-augmented generation\nframework named GenTKG combining a temporal logical rule-based retrieval\nstrategy and few-shot parameter-efficient instruction tuning to solve the above\nchallenges, respectively. Extensive experiments have shown that GenTKG\noutperforms conventional methods of temporal relational forecasting with low\ncomputation resources using extremely limited training data as few as 16\nsamples. GenTKG also highlights remarkable cross-domain generalizability with\noutperforming performance on unseen datasets without re-training, and in-domain\ngeneralizability regardless of time split in the same dataset. Our work reveals\nthe huge potential of LLMs in the tKG domain and opens a new frontier for\ngenerative forecasting on tKGs. Code and data are released here:\nhttps://github.com/mayhugotong/GenTKG.\n","authors":["Ruotong Liao","Xu Jia","Yunpu Ma","Yangzhe Li","Volker Tresp"],"pdf_url":"https://arxiv.org/pdf/2310.07793v4.pdf","comment":"14 pages, Findings of NAACL 2024"},{"id":"http://arxiv.org/abs/2403.08699v1","updated":"2024-03-13T17:02:27Z","published":"2024-03-13T17:02:27Z","title":"Implicit Regularization of Gradient Flow on One-Layer Softmax Attention","summary":"  We study gradient flow on the exponential loss for a classification problem\nwith a one-layer softmax attention model, where the key and query weight\nmatrices are trained separately. Under a separability assumption on the data,\nwe show that when gradient flow achieves the minimal loss value, it further\nimplicitly minimizes the nuclear norm of the product of the key and query\nweight matrices. Such implicit regularization can be described by a Support\nVector Machine (SVM) problem with respect to the attention weights. This\nfinding contrasts with prior results showing that the gradient descent induces\nan implicit regularization on the Frobenius norm on the product weight matrix\nwhen the key and query matrices are combined into a single weight matrix for\ntraining. For diagonal key and query matrices, our analysis builds upon the\nreparameterization technique and exploits approximate KKT conditions of the SVM\nassociated with the classification data. Moreover, the results are extended to\ngeneral weights configurations given proper alignment of the weight matrices'\nsingular spaces with the data features at initialization.\n","authors":["Heejune Sheen","Siyu Chen","Tianhao Wang","Harrison H. Zhou"],"pdf_url":"https://arxiv.org/pdf/2403.08699v1.pdf","comment":"34 pages"},{"id":"http://arxiv.org/abs/2011.12340v3","updated":"2024-03-13T17:01:54Z","published":"2020-11-24T19:47:53Z","title":"mForms : Multimodal Form-Filling with Question Answering","summary":"  This paper presents a new approach to form-filling by reformulating the task\nas multimodal natural language Question Answering (QA). The reformulation is\nachieved by first translating the elements on the GUI form (text fields,\nbuttons, icons, etc.) to natural language questions, where these questions\ncapture the element's multimodal semantics. After a match is determined between\nthe form element (Question) and the user utterance (Answer), the form element\nis filled through a pre-trained extractive QA system. By leveraging pre-trained\nQA models and not requiring form-specific training, this approach to\nform-filling is zero-shot. The paper also presents an approach to further\nrefine the form-filling by using multi-task training to incorporate a\npotentially large number of successive tasks. Finally, the paper introduces a\nmultimodal natural language form-filling dataset Multimodal Forms (mForms), as\nwell as a multimodal extension of the popular ATIS dataset to support future\nresearch and experimentation. Results show the new approach not only maintains\nrobust accuracy for sparse training conditions but achieves state-of-the-art F1\nof 0.97 on ATIS with approximately 1/10th of the training data.\n","authors":["Larry Heck","Simon Heck","Anirudh Sundar"],"pdf_url":"https://arxiv.org/pdf/2011.12340v3.pdf","comment":"5 pages, 6 figures, 4 tables"},{"id":"http://arxiv.org/abs/2310.01082v2","updated":"2024-03-13T16:48:27Z","published":"2023-10-02T10:48:42Z","title":"Linear attention is (maybe) all you need (to understand transformer\n  optimization)","summary":"  Transformer training is notoriously difficult, requiring a careful design of\noptimizers and use of various heuristics. We make progress towards\nunderstanding the subtleties of training Transformers by carefully studying a\nsimple yet canonical linearized shallow Transformer model. Specifically, we\ntrain linear Transformers to solve regression tasks, inspired by J.~von Oswald\net al.~(ICML 2023), and K.~Ahn et al.~(NeurIPS 2023). Most importantly, we\nobserve that our proposed linearized models can reproduce several prominent\naspects of Transformer training dynamics. Consequently, the results obtained in\nthis paper suggest that a simple linearized Transformer model could actually be\na valuable, realistic abstraction for understanding Transformer optimization.\n","authors":["Kwangjun Ahn","Xiang Cheng","Minhak Song","Chulhee Yun","Ali Jadbabaie","Suvrit Sra"],"pdf_url":"https://arxiv.org/pdf/2310.01082v2.pdf","comment":"Published at ICLR 2024"},{"id":"http://arxiv.org/abs/2304.12479v5","updated":"2024-03-13T16:47:04Z","published":"2023-04-24T22:31:59Z","title":"AGI: Artificial General Intelligence for Education","summary":"  Artificial general intelligence (AGI) has gained global recognition as a\nfuture technology due to the emergence of breakthrough large language models\nand chatbots such as GPT-4 and ChatGPT, respectively. Compared to conventional\nAI models, typically designed for a limited range of tasks, demand significant\namounts of domain-specific data for training and may not always consider\nintricate interpersonal dynamics in education. AGI, driven by the recent large\npre-trained models, represents a significant leap in the capability of machines\nto perform tasks that require human-level intelligence, such as reasoning,\nproblem-solving, decision-making, and even understanding human emotions and\nsocial interactions. This position paper reviews AGI's key concepts,\ncapabilities, scope, and potential within future education, including achieving\nfuture educational goals, designing pedagogy and curriculum, and performing\nassessments. It highlights that AGI can significantly improve intelligent\ntutoring systems, educational assessment, and evaluation procedures. AGI\nsystems can adapt to individual student needs, offering tailored learning\nexperiences. They can also provide comprehensive feedback on student\nperformance and dynamically adjust teaching methods based on student progress.\nThe paper emphasizes that AGI's capabilities extend to understanding human\nemotions and social interactions, which are critical in educational settings.\nThe paper discusses that ethical issues in education with AGI include data\nbias, fairness, and privacy and emphasizes the need for codes of conduct to\nensure responsible AGI use in academic settings like homework, teaching, and\nrecruitment. We also conclude that the development of AGI necessitates\ninterdisciplinary collaborations between educators and AI engineers to advance\nresearch and application efforts.\n","authors":["Ehsan Latif","Gengchen Mai","Matthew Nyaaba","Xuansheng Wu","Ninghao Liu","Guoyu Lu","Sheng Li","Tianming Liu","Xiaoming Zhai"],"pdf_url":"https://arxiv.org/pdf/2304.12479v5.pdf","comment":"Position Paper on AGI for Education, Submitted to Technology and\n  Society"},{"id":"http://arxiv.org/abs/2403.08688v1","updated":"2024-03-13T16:44:39Z","published":"2024-03-13T16:44:39Z","title":"Token Alignment via Character Matching for Subword Completion","summary":"  Generative models, widely utilized in various applications, can often\nstruggle with prompts corresponding to partial tokens. This struggle stems from\ntokenization, where partial tokens fall out of distribution during inference,\nleading to incorrect or nonsensical outputs. This paper examines a technique to\nalleviate the tokenization artifact on text completion in generative models,\nmaintaining performance even in regular non-subword cases. The method, termed\ntoken alignment, involves backtracking to the last complete tokens and ensuring\nthe model's generation aligns with the prompt. This approach showcases marked\nimprovement across many partial token scenarios, including nuanced cases like\nspace-prefix and partial indentation, with only a minor time increase. The\ntechnique and analysis detailed in this paper contribute to the continuous\nadvancement of generative models in handling partial inputs, bearing relevance\nfor applications like code completion and text autocompletion.\n","authors":["Ben Athiwaratkun","Shiqi Wang","Mingyue Shang","Yuchen Tian","Zijian Wang","Sujan Kumar Gonugondla","Sanjay Krishna Gouda","Rob Kwiatowski","Ramesh Nallapati","Bing Xiang"],"pdf_url":"https://arxiv.org/pdf/2403.08688v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.08913v4","updated":"2024-03-13T16:04:03Z","published":"2023-02-04T15:55:23Z","title":"Referential communication in heterogeneous communities of pre-trained\n  visual deep networks","summary":"  As large pre-trained image-processing neural networks are being embedded in\nautonomous agents such as self-driving cars or robots, the question arises of\nhow such systems can communicate with each other about the surrounding world,\ndespite their different architectures and training regimes. As a first step in\nthis direction, we systematically explore the task of \\textit{referential\ncommunication} in a community of heterogeneous state-of-the-art pre-trained\nvisual networks, showing that they can develop, in a self-supervised way, a\nshared protocol to refer to a target object among a set of candidates. This\nshared protocol can also be used, to some extent, to communicate about\npreviously unseen object categories of different granularity. Moreover, a\nvisual network that was not initially part of an existing community can learn\nthe community's protocol with remarkable ease. Finally, we study, both\nqualitatively and quantitatively, the properties of the emergent protocol,\nproviding some evidence that it is capturing high-level semantic features of\nobjects.\n","authors":["Matéo Mahaut","Francesca Franzon","Roberto Dessì","Marco Baroni"],"pdf_url":"https://arxiv.org/pdf/2302.08913v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.04780v2","updated":"2024-03-13T15:52:33Z","published":"2024-03-02T09:27:32Z","title":"MuseGraph: Graph-oriented Instruction Tuning of Large Language Models\n  for Generic Graph Mining","summary":"  Graphs with abundant attributes are essential in modeling interconnected\nentities and improving predictions in various real-world applications.\nTraditional Graph Neural Networks (GNNs), which are commonly used for modeling\nattributed graphs, need to be re-trained every time when applied to different\ngraph tasks and datasets. Although the emergence of Large Language Models\n(LLMs) has introduced a new paradigm in natural language processing, the\ngenerative potential of LLMs in graph mining remains largely under-explored. To\nthis end, we propose a novel framework MuseGraph, which seamlessly integrates\nthe strengths of GNNs and LLMs and facilitates a more effective and generic\napproach for graph mining across different tasks and datasets. Specifically, we\nfirst introduce a compact graph description via the proposed adaptive input\ngeneration to encapsulate key information from the graph under the constraints\nof language token limitations. Then, we propose a diverse instruction\ngeneration mechanism, which distills the reasoning capabilities from LLMs\n(e.g., GPT-4) to create task-specific Chain-of-Thought-based instruction\npackages for different graph tasks. Finally, we propose a graph-aware\ninstruction tuning with a dynamic instruction package allocation strategy\nacross tasks and datasets, ensuring the effectiveness and generalization of the\ntraining process. Our experimental results demonstrate significant improvements\nin different graph tasks, showcasing the potential of our MuseGraph in\nenhancing the accuracy of graph-oriented downstream tasks while keeping the\ngeneration powers of LLMs.\n","authors":["Yanchao Tan","Hang Lv","Xinyi Huang","Jiawei Zhang","Shiping Wang","Carl Yang"],"pdf_url":"https://arxiv.org/pdf/2403.04780v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08635v1","updated":"2024-03-13T15:47:26Z","published":"2024-03-13T15:47:26Z","title":"Human Alignment of Large Language Models through Online Preference\n  Optimisation","summary":"  Ensuring alignment of language models' outputs with human preferences is\ncritical to guarantee a useful, safe, and pleasant user experience. Thus, human\nalignment has been extensively studied recently and several methods such as\nReinforcement Learning from Human Feedback (RLHF), Direct Policy Optimisation\n(DPO) and Sequence Likelihood Calibration (SLiC) have emerged. In this paper,\nour contribution is two-fold. First, we show the equivalence between two recent\nalignment methods, namely Identity Policy Optimisation (IPO) and Nash Mirror\nDescent (Nash-MD). Second, we introduce a generalisation of IPO, named IPO-MD,\nthat leverages the regularised sampling approach proposed by Nash-MD.\n  This equivalence may seem surprising at first sight, since IPO is an offline\nmethod whereas Nash-MD is an online method using a preference model. However,\nthis equivalence can be proven when we consider the online version of IPO, that\nis when both generations are sampled by the online policy and annotated by a\ntrained preference model. Optimising the IPO loss with such a stream of data\nbecomes then equivalent to finding the Nash equilibrium of the preference model\nthrough self-play. Building on this equivalence, we introduce the IPO-MD\nalgorithm that generates data with a mixture policy (between the online and\nreference policy) similarly as the general Nash-MD algorithm. We compare\nonline-IPO and IPO-MD to different online versions of existing losses on\npreference data such as DPO and SLiC on a summarisation task.\n","authors":["Daniele Calandriello","Daniel Guo","Remi Munos","Mark Rowland","Yunhao Tang","Bernardo Avila Pires","Pierre Harvey Richemond","Charline Le Lan","Michal Valko","Tianqi Liu","Rishabh Joshi","Zeyu Zheng","Bilal Piot"],"pdf_url":"https://arxiv.org/pdf/2403.08635v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08618v1","updated":"2024-03-13T15:32:08Z","published":"2024-03-13T15:32:08Z","title":"Verifix: Post-Training Correction to Improve Label Noise Robustness with\n  Verified Samples","summary":"  Label corruption, where training samples have incorrect labels, can\nsignificantly degrade the performance of machine learning models. This\ncorruption often arises from non-expert labeling or adversarial attacks.\nAcquiring large, perfectly labeled datasets is costly, and retraining large\nmodels from scratch when a clean dataset becomes available is computationally\nexpensive. To address this challenge, we propose Post-Training Correction, a\nnew paradigm that adjusts model parameters after initial training to mitigate\nlabel noise, eliminating the need for retraining. We introduce Verifix, a novel\nSingular Value Decomposition (SVD) based algorithm that leverages a small,\nverified dataset to correct the model weights using a single update. Verifix\nuses SVD to estimate a Clean Activation Space and then projects the model's\nweights onto this space to suppress activations corresponding to corrupted\ndata. We demonstrate Verifix's effectiveness on both synthetic and real-world\nlabel noise. Experiments on the CIFAR dataset with 25% synthetic corruption\nshow 7.36% generalization improvements on average. Additionally, we observe\ngeneralization improvements of up to 2.63% on naturally corrupted datasets like\nWebVision1.0 and Clothing1M.\n","authors":["Sangamesh Kodge","Deepak Ravikumar","Gobinda Saha","Kaushik Roy"],"pdf_url":"https://arxiv.org/pdf/2403.08618v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08613v1","updated":"2024-03-13T15:23:55Z","published":"2024-03-13T15:23:55Z","title":"Link Prediction for Social Networks using Representation Learning and\n  Heuristic-based Features","summary":"  The exponential growth in scale and relevance of social networks enable them\nto provide expansive insights. Predicting missing links in social networks\nefficiently can help in various modern-day business applications ranging from\ngenerating recommendations to influence analysis. Several categories of\nsolutions exist for the same. Here, we explore various feature extraction\ntechniques to generate representations of nodes and edges in a social network\nthat allow us to predict missing links. We compare the results of using ten\nfeature extraction techniques categorized across Structural embeddings,\nNeighborhood-based embeddings, Graph Neural Networks, and Graph Heuristics,\nfollowed by modeling with ensemble classifiers and custom Neural Networks.\nFurther, we propose combining heuristic-based features and learned\nrepresentations that demonstrate improved performance for the link prediction\ntask on social network datasets. Using this method to generate accurate\nrecommendations for many applications is a matter of further study that appears\nvery promising. The code for all the experiments has been made public.\n","authors":["Samarth Khanna","Sree Bhattacharyya","Sudipto Ghosh","Kushagra Agarwal","Asit Kumar Das"],"pdf_url":"https://arxiv.org/pdf/2403.08613v1.pdf","comment":"Accepted to the MAISoN Workshop at IJCAI 2023"},{"id":"http://arxiv.org/abs/2403.08607v1","updated":"2024-03-13T15:20:30Z","published":"2024-03-13T15:20:30Z","title":"MedInsight: A Multi-Source Context Augmentation Framework for Generating\n  Patient-Centric Medical Responses using Large Language Models","summary":"  Large Language Models (LLMs) have shown impressive capabilities in generating\nhuman-like responses. However, their lack of domain-specific knowledge limits\ntheir applicability in healthcare settings, where contextual and comprehensive\nresponses are vital. To address this challenge and enable the generation of\npatient-centric responses that are contextually relevant and comprehensive, we\npropose MedInsight:a novel retrieval augmented framework that augments LLM\ninputs (prompts) with relevant background information from multiple sources.\nMedInsight extracts pertinent details from the patient's medical record or\nconsultation transcript. It then integrates information from authoritative\nmedical textbooks and curated web resources based on the patient's health\nhistory and condition. By constructing an augmented context combining the\npatient's record with relevant medical knowledge, MedInsight generates\nenriched, patient-specific responses tailored for healthcare applications such\nas diagnosis, treatment recommendations, or patient education. Experiments on\nthe MTSamples dataset validate MedInsight's effectiveness in generating\ncontextually appropriate medical responses. Quantitative evaluation using the\nRagas metric and TruLens for answer similarity and answer correctness\ndemonstrates the model's efficacy. Furthermore, human evaluation studies\ninvolving Subject Matter Expert (SMEs) confirm MedInsight's utility, with\nmoderate inter-rater agreement on the relevance and correctness of the\ngenerated responses.\n","authors":["Subash Neupane","Shaswata Mitra","Sudip Mittal","Noorbakhsh Amiri Golilarz","Shahram Rahimi","Amin Amirlatifi"],"pdf_url":"https://arxiv.org/pdf/2403.08607v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.12375v4","updated":"2024-03-13T15:00:20Z","published":"2023-07-23T16:54:41Z","title":"In-Context Learning Learns Label Relationships but Is Not Conventional\n  Learning","summary":"  The predictions of Large Language Models (LLMs) on downstream tasks often\nimprove significantly when including examples of the input--label relationship\nin the context. However, there is currently no consensus about how this\nin-context learning (ICL) ability of LLMs works. For example, while Xie et al.\n(2021) liken ICL to a general-purpose learning algorithm, Min et al. (2022)\nargue ICL does not even learn label relationships from in-context examples. In\nthis paper, we provide novel insights into how ICL leverages label information,\nrevealing both capabilities and limitations. To ensure we obtain a\ncomprehensive picture of ICL behavior, we study probabilistic aspects of ICL\npredictions and thoroughly examine the dynamics of ICL as more examples are\nprovided. Our experiments show that ICL predictions almost always depend on\nin-context labels and that ICL can learn truly novel tasks in-context. However,\nwe also find that ICL struggles to fully overcome prediction preferences\nacquired from pre-training data and, further, that ICL does not consider all\nin-context information equally.\n","authors":["Jannik Kossen","Yarin Gal","Tom Rainforth"],"pdf_url":"https://arxiv.org/pdf/2307.12375v4.pdf","comment":"Accepted for publication at ICLR 2024"},{"id":"http://arxiv.org/abs/2403.08593v1","updated":"2024-03-13T14:59:07Z","published":"2024-03-13T14:59:07Z","title":"Call Me When Necessary: LLMs can Efficiently and Faithfully Reason over\n  Structured Environments","summary":"  Large Language Models (LLMs) have shown potential in reasoning over\nstructured environments, e.g., knowledge graph and table. Such tasks typically\nrequire multi-hop reasoning, i.e., match natural language utterance with\ninstances in the environment. Previous methods leverage LLMs to incrementally\nbuild a reasoning path, where the LLMs either invoke tools or pick up schemas\nby step-by-step interacting with the environment. We propose\nReasoning-Path-Editing (Readi), a novel framework where LLMs can efficiently\nand faithfully reason over structured environments. In Readi, LLMs initially\ngenerate a reasoning path given a query, and edit the path only when necessary.\nWe instantiate the path on structured environments and provide feedback to edit\nthe path if anything goes wrong. Experimental results on three KGQA datasets\nand two TableQA datasets show the effectiveness of Readi, significantly\nsurpassing all LLM-based methods (by 9.1% on WebQSP, 12.4% on MQA-3H and 10.9%\non WTQ), comparable with state-of-the-art fine-tuned methods (67% on CWQ and\n74.7% on WebQSP) and substantially boosting the vanilla LLMs (by 14.9% on CWQ).\nOur code will be available upon publication.\n","authors":["Sitao Cheng","Ziyuan Zhuang","Yong Xu","Fangkai Yang","Chaoyun Zhang","Xiaoting Qin","Xiang Huang","Ling Chen","Qingwei Lin","Dongmei Zhang","Saravan Rajmohan","Qi Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.08593v1.pdf","comment":"17 pages, 8 figures, 9 tables"},{"id":"http://arxiv.org/abs/2312.10385v3","updated":"2024-03-13T14:48:36Z","published":"2023-12-16T08:48:46Z","title":"Imitate the Good and Avoid the Bad: An Incremental Approach to Safe\n  Reinforcement Learning","summary":"  A popular framework for enforcing safe actions in Reinforcement Learning (RL)\nis Constrained RL, where trajectory based constraints on expected cost (or\nother cost measures) are employed to enforce safety and more importantly these\nconstraints are enforced while maximizing expected reward. Most recent\napproaches for solving Constrained RL convert the trajectory based cost\nconstraint into a surrogate problem that can be solved using minor\nmodifications to RL methods. A key drawback with such approaches is an over or\nunderestimation of the cost constraint at each state. Therefore, we provide an\napproach that does not modify the trajectory based cost constraint and instead\nimitates ``good'' trajectories and avoids ``bad'' trajectories generated from\nincrementally improving policies. We employ an oracle that utilizes a reward\nthreshold (which is varied with learning) and the overall cost constraint to\nlabel trajectories as ``good'' or ``bad''. A key advantage of our approach is\nthat we are able to work from any starting policy or set of trajectories and\nimprove on it. In an exhaustive set of experiments, we demonstrate that our\napproach is able to outperform top benchmark approaches for solving Constrained\nRL problems, with respect to expected cost, CVaR cost, or even unknown cost\nconstraints.\n","authors":["Huy Hoang","Tien Mai","Pradeep Varakantham"],"pdf_url":"https://arxiv.org/pdf/2312.10385v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.15048v4","updated":"2024-03-13T14:24:28Z","published":"2023-09-26T16:25:57Z","title":"Class Incremental Learning via Likelihood Ratio Based Task Prediction","summary":"  Class incremental learning (CIL) is a challenging setting of continual\nlearning, which learns a series of tasks sequentially. Each task consists of a\nset of unique classes. The key feature of CIL is that no task identifier (or\ntask-id) is provided at test time. Predicting the task-id for each test sample\nis a challenging problem. An emerging theory-guided approach (called TIL+OOD)\nis to train a task-specific model for each task in a shared network for all\ntasks based on a task-incremental learning (TIL) method to deal with\ncatastrophic forgetting. The model for each task is an out-of-distribution\n(OOD) detector rather than a conventional classifier. The OOD detector can\nperform both within-task (in-distribution (IND)) class prediction and OOD\ndetection. The OOD detection capability is the key to task-id prediction during\ninference. However, this paper argues that using a traditional OOD detector for\ntask-id prediction is sub-optimal because additional information (e.g., the\nreplay data and the learned tasks) available in CIL can be exploited to design\na better and principled method for task-id prediction. We call the new method\nTPL (Task-id Prediction based on Likelihood Ratio). TPL markedly outperforms\nstrong CIL baselines and has negligible catastrophic forgetting. The code of\nTPL is publicly available at https://github.com/linhaowei1/TPL.\n","authors":["Haowei Lin","Yijia Shao","Weinan Qian","Ningxin Pan","Yiduo Guo","Bing Liu"],"pdf_url":"https://arxiv.org/pdf/2309.15048v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.02649v2","updated":"2024-03-13T14:20:40Z","published":"2023-04-03T20:19:56Z","title":"Medical Multimodal-Multitask Foundation Model for Superior Chest CT\n  Performance","summary":"  Patient management requires multitasking interaction with multimodal data.\nWhile today's AI, particularly large foundation models, promises unprecedented\nopportunities, progress remains relatively slow in developing medical\nmultimodal multitask foundation models. There are two main challenges along\nthis direction: the data challenge -- the high bar to curate medical multimodal\nmultitask datasets including 3D medical tomographic images in alignment with\nother clinical datasets, and the model challenge -- the unavailability of a\nscalable and adaptable foundation model architecture to synergize multimodal\ndatasets for diverse clinical tasks. Here we propose the first-of-its-kind\nmedical multimodal-multitask foundation model (M3FM) with an emphasis on lung\ncancer screening. To train our M3FM, we first curated a comprehensive\nmultimodal multitask dataset consisting of 163,725 3D chest CT exams, 48\nclinical data types, and 17 medical tasks on lung, heart, and other chest\ndiseases. Then, we created and applied a multimodal question-answering\nframework as a unified training strategy to effectively integrate multimodal\ninformation and naturally perform multiple tasks with free-text prompting.\nExtensive experimental results demonstrate that M3FM consistently outperforms\nthe previous state-of-the-art models. M3FM can identify informative multimodal\ndata elements that are relevant to specific clinical tasks, being instrumental\nin building AI models and gaining insights into correlations among multimodal\ndata and diseases. M3FM can be adapted to boost the performance of new tasks\nwith a small out-of-distribution dataset. M3FM has enabled superior volumetric\nCT imaging performance for lung cancer screening, cardiac disease prediction,\nand other CT-related tasks. M3FM can be extended to incorporate more data types\nand improve other medical tasks, towards AI-empowered precise and efficient\nmedicine.\n","authors":["Chuang Niu","Qing Lyu","Christopher D. Carothers","Parisa Kaviani","Josh Tan","Pingkun Yan","Mannudeep K. Kalra","Christopher T. Whitlow","Ge Wang"],"pdf_url":"https://arxiv.org/pdf/2304.02649v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08564v1","updated":"2024-03-13T14:19:08Z","published":"2024-03-13T14:19:08Z","title":"Non-discrimination Criteria for Generative Language Models","summary":"  Within recent years, generative AI, such as large language models, has\nundergone rapid development. As these models become increasingly available to\nthe public, concerns arise about perpetuating and amplifying harmful biases in\napplications. Gender stereotypes can be harmful and limiting for the\nindividuals they target, whether they consist of misrepresentation or\ndiscrimination. Recognizing gender bias as a pervasive societal construct, this\npaper studies how to uncover and quantify the presence of gender biases in\ngenerative language models. In particular, we derive generative AI analogues of\nthree well-known non-discrimination criteria from classification, namely\nindependence, separation and sufficiency. To demonstrate these criteria in\naction, we design prompts for each of the criteria with a focus on occupational\ngender stereotype, specifically utilizing the medical test to introduce the\nground truth in the generative AI context. Our results address the presence of\noccupational gender bias within such conversational language models.\n","authors":["Sara Sterlie","Nina Weng","Aasa Feragen"],"pdf_url":"https://arxiv.org/pdf/2403.08564v1.pdf","comment":"14 pages, 5 figures. Submitted to ACM Conference on Fairness,\n  Accountability, and Transparency (ACM FAccT 2024)"},{"id":"http://arxiv.org/abs/2403.08562v1","updated":"2024-03-13T14:14:47Z","published":"2024-03-13T14:14:47Z","title":"Structural perspective on constraint-based learning of Markov networks","summary":"  Markov networks are probabilistic graphical models that employ undirected\ngraphs to depict conditional independence relationships among variables. Our\nfocus lies in constraint-based structure learning, which entails learning the\nundirected graph from data through the execution of conditional independence\ntests. We establish theoretical limits concerning two critical aspects of\nconstraint-based learning of Markov networks: the number of tests and the sizes\nof the conditioning sets. These bounds uncover an exciting interplay between\nthe structural properties of the graph and the amount of tests required to\nlearn a Markov network. The starting point of our work is that the graph\nparameter maximum pairwise connectivity, $\\kappa$, that is, the maximum number\nof vertex-disjoint paths connecting a pair of vertices in the graph, is\nresponsible for the sizes of independence tests required to learn the graph. On\none hand, we show that at least one test with the size of the conditioning set\nat least $\\kappa$ is always necessary. On the other hand, we prove that any\ngraph can be learned by performing tests of size at most $\\kappa$. This\ncompletely resolves the question of the minimum size of conditioning sets\nrequired to learn the graph. When it comes to the number of tests, our upper\nbound on the sizes of conditioning sets implies that every $n$-vertex graph can\nbe learned by at most $n^{\\kappa}$ tests with conditioning sets of sizes at\nmost $\\kappa$. We show that for any upper bound $q$ on the sizes of the\nconditioning sets, there exist graphs with $O(n q)$ vertices that require at\nleast $n^{\\Omega(\\kappa)}$ tests to learn. This lower bound holds even when the\ntreewidth and the maximum degree of the graph are at most $\\kappa+2$. On the\npositive side, we prove that every graph of bounded treewidth can be learned by\na polynomial number of tests with conditioning sets of sizes at most $2\\kappa$.\n","authors":["Tuukka Korhonen","Fedor V. Fomin","Pekka Parviainen"],"pdf_url":"https://arxiv.org/pdf/2403.08562v1.pdf","comment":"AISTATS 2024"},{"id":"http://arxiv.org/abs/2402.18920v4","updated":"2024-03-13T14:13:04Z","published":"2024-02-29T07:26:23Z","title":"Spectral Meets Spatial: Harmonising 3D Shape Matching and Interpolation","summary":"  Although 3D shape matching and interpolation are highly interrelated, they\nare often studied separately and applied sequentially to relate different 3D\nshapes, thus resulting in sub-optimal performance. In this work we present a\nunified framework to predict both point-wise correspondences and shape\ninterpolation between 3D shapes. To this end, we combine the deep functional\nmap framework with classical surface deformation models to map shapes in both\nspectral and spatial domains. On the one hand, by incorporating spatial maps,\nour method obtains more accurate and smooth point-wise correspondences compared\nto previous functional map methods for shape matching. On the other hand, by\nintroducing spectral maps, our method gets rid of commonly used but\ncomputationally expensive geodesic distance constraints that are only valid for\nnear-isometric shape deformations. Furthermore, we propose a novel test-time\nadaptation scheme to capture both pose-dominant and shape-dominant\ndeformations. Using different challenging datasets, we demonstrate that our\nmethod outperforms previous state-of-the-art methods for both shape matching\nand interpolation, even compared to supervised approaches.\n","authors":["Dongliang Cao","Marvin Eisenberger","Nafie El Amrani","Daniel Cremers","Florian Bernard"],"pdf_url":"https://arxiv.org/pdf/2402.18920v4.pdf","comment":"accepted by CVPR2024"},{"id":"http://arxiv.org/abs/2403.08556v1","updated":"2024-03-13T14:08:25Z","published":"2024-03-13T14:08:25Z","title":"SM4Depth: Seamless Monocular Metric Depth Estimation across Multiple\n  Cameras and Scenes by One Model","summary":"  The generalization of monocular metric depth estimation (MMDE) has been a\nlongstanding challenge. Recent methods made progress by combining relative and\nmetric depth or aligning input image focal length. However, they are still\nbeset by challenges in camera, scene, and data levels: (1) Sensitivity to\ndifferent cameras; (2) Inconsistent accuracy across scenes; (3) Reliance on\nmassive training data. This paper proposes SM4Depth, a seamless MMDE method, to\naddress all the issues above within a single network. First, we reveal that a\nconsistent field of view (FOV) is the key to resolve ``metric ambiguity''\nacross cameras, which guides us to propose a more straightforward preprocessing\nunit. Second, to achieve consistently high accuracy across scenes, we\nexplicitly model the metric scale determination as discretizing the depth\ninterval into bins and propose variation-based unnormalized depth bins. This\nmethod bridges the depth gap of diverse scenes by reducing the ambiguity of the\nconventional metric bin. Third, to reduce the reliance on massive training\ndata, we propose a ``divide and conquer\" solution. Instead of estimating\ndirectly from the vast solution space, the correct metric bins are estimated\nfrom multiple solution sub-spaces for complexity reduction. Finally, with just\n150K RGB-D pairs and a consumer-grade GPU for training, SM4Depth achieves\nstate-of-the-art performance on most previously unseen datasets, especially\nsurpassing ZoeDepth and Metric3D on mRI$_\\theta$. The code can be found at\nhttps://github.com/1hao-Liu/SM4Depth.\n","authors":["Yihao Liu","Feng Xue","Anlong Ming"],"pdf_url":"https://arxiv.org/pdf/2403.08556v1.pdf","comment":"Project Page: xuefeng-cvr.github.io/SM4Depth"},{"id":"http://arxiv.org/abs/2403.08554v1","updated":"2024-03-13T14:06:51Z","published":"2024-03-13T14:06:51Z","title":"Federated Knowledge Graph Unlearning via Diffusion Model","summary":"  Federated learning (FL) promotes the development and application of\nartificial intelligence technologies by enabling model sharing and\ncollaboration while safeguarding data privacy. Knowledge graph (KG) embedding\nrepresentation provides a foundation for knowledge reasoning and applications\nby mapping entities and relations into vector space. Federated KG embedding\nenables the utilization of knowledge from diverse client sources while\nsafeguarding the privacy of local data. However, due to demands such as privacy\nprotection and the need to adapt to dynamic data changes, investigations into\nmachine unlearning (MU) have been sparked. However, it is challenging to\nmaintain the performance of KG embedding models while forgetting the influence\nof specific forgotten data on the model. In this paper, we propose FedDM, a\nnovel framework tailored for machine unlearning in federated knowledge graphs.\nLeveraging diffusion models, we generate noisy data to sensibly mitigate the\ninfluence of specific knowledge on FL models while preserving the overall\nperformance concerning the remaining data. We conduct experimental evaluations\non benchmark datasets to assess the efficacy of the proposed model. Extensive\nexperiments demonstrate that FedDM yields promising results in knowledge\nforgetting.\n","authors":["Bingchen Liu","Yuanyuan Fang"],"pdf_url":"https://arxiv.org/pdf/2403.08554v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.05984v2","updated":"2024-03-13T13:56:05Z","published":"2023-08-11T07:42:17Z","title":"Contrastive Explanations of Centralized Multi-agent Optimization\n  Solutions","summary":"  In many real-world scenarios, agents are involved in optimization problems.\nSince most of these scenarios are over-constrained, optimal solutions do not\nalways satisfy all agents. Some agents might be unhappy and ask questions of\nthe form ``Why does solution $S$ not satisfy property $P$?''. We propose CMAoE,\na domain-independent approach to obtain contrastive explanations by: (i)\ngenerating a new solution $S^\\prime$ where property $P$ is enforced, while also\nminimizing the differences between $S$ and $S^\\prime$; and (ii) highlighting\nthe differences between the two solutions, with respect to the features of the\nobjective function of the multi-agent system. Such explanations aim to help\nagents understanding why the initial solution is better in the context of the\nmulti-agent system than what they expected. We have carried out a computational\nevaluation that shows that CMAoE can generate contrastive explanations for\nlarge multi-agent optimization problems. We have also performed an extensive\nuser study in four different domains that shows that: (i) after being presented\nwith these explanations, humans' satisfaction with the original solution\nincreases; and (ii) the constrastive explanations generated by CMAoE are\npreferred or equally preferred by humans over the ones generated by state of\nthe art approaches.\n","authors":["Parisa Zehtabi","Alberto Pozanco","Ayala Bloch","Daniel Borrajo","Sarit Kraus"],"pdf_url":"https://arxiv.org/pdf/2308.05984v2.pdf","comment":"Paper accepted at ICAPS 2024. This is a extended version that\n  includes Supplementary Material"},{"id":"http://arxiv.org/abs/2403.08536v1","updated":"2024-03-13T13:51:02Z","published":"2024-03-13T13:51:02Z","title":"HOLMES: HOLonym-MEronym based Semantic inspection for Convolutional\n  Image Classifiers","summary":"  Convolutional Neural Networks (CNNs) are nowadays the model of choice in\nComputer Vision, thanks to their ability to automatize the feature extraction\nprocess in visual tasks. However, the knowledge acquired during training is\nfully subsymbolic, and hence difficult to understand and explain to end users.\nIn this paper, we propose a new technique called HOLMES (HOLonym-MEronym based\nSemantic inspection) that decomposes a label into a set of related concepts,\nand provides component-level explanations for an image classification model.\nSpecifically, HOLMES leverages ontologies, web scraping and transfer learning\nto automatically construct meronym (parts)-based detectors for a given holonym\n(class). Then, it produces heatmaps at the meronym level and finally, by\nprobing the holonym CNN with occluded images, it highlights the importance of\neach part on the classification output. Compared to state-of-the-art saliency\nmethods, HOLMES takes a step further and provides information about both where\nand what the holonym CNN is looking at, without relying on densely annotated\ndatasets and without forcing concepts to be associated to single computational\nunits. Extensive experimental evaluation on different categories of objects\n(animals, tools and vehicles) shows the feasibility of our approach. On\naverage, HOLMES explanations include at least two meronyms, and the ablation of\na single meronym roughly halves the holonym model confidence. The resulting\nheatmaps were quantitatively evaluated using the\ndeletion/insertion/preservation curves. All metrics were comparable to those\nachieved by GradCAM, while offering the advantage of further decomposing the\nheatmap in human-understandable concepts, thus highlighting both the relevance\nof meronyms to object classification, as well as HOLMES ability to capture it.\nThe code is available at https://github.com/FrancesC0de/HOLMES.\n","authors":["Francesco Dibitonto","Fabio Garcea","André Panisson","Alan Perotti","Lia Morra"],"pdf_url":"https://arxiv.org/pdf/2403.08536v1.pdf","comment":"This work has been accepted to be presented to The 1st World\n  Conference on eXplainable Artificial Intelligence (xAI 2023), July 26-28,\n  2023 - Lisboa, Portugal"},{"id":"http://arxiv.org/abs/2403.08528v1","updated":"2024-03-13T13:38:58Z","published":"2024-03-13T13:38:58Z","title":"Pig aggression classification using CNN, Transformers and Recurrent\n  Networks","summary":"  The development of techniques that can be used to analyze and detect animal\nbehavior is a crucial activity for the livestock sector, as it is possible to\nmonitor the stress and animal welfare and contributes to decision making in the\nfarm. Thus, the development of applications can assist breeders in making\ndecisions to improve production performance and reduce costs, once the animal\nbehavior is analyzed by humans and this can lead to susceptible errors and time\nconsumption. Aggressiveness in pigs is an example of behavior that is studied\nto reduce its impact through animal classification and identification. However,\nthis process is laborious and susceptible to errors, which can be reduced\nthrough automation by visually classifying videos captured in controlled\nenvironment. The captured videos can be used for training and, as a result, for\nclassification through computer vision and artificial intelligence, employing\nneural network techniques. The main techniques utilized in this study are\nvariants of transformers: STAM, TimeSformer, and ViViT, as well as techniques\nusing convolutions, such as ResNet3D2, Resnet(2+1)D, and CnnLstm. These\ntechniques were employed for pig video classification with the objective of\nidentifying aggressive and non-aggressive behaviors. In this work, various\ntechniques were compared to analyze the contribution of using transformers, in\naddition to the effectiveness of the convolution technique in video\nclassification. The performance was evaluated using accuracy, precision, and\nrecall. The TimerSformer technique showed the best results in video\nclassification, with median accuracy of 0.729.\n","authors":["Junior Silva Souza","Eduardo Bedin","Gabriel Toshio Hirokawa Higa","Newton Loebens","Hemerson Pistori"],"pdf_url":"https://arxiv.org/pdf/2403.08528v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08505v1","updated":"2024-03-13T13:12:57Z","published":"2024-03-13T13:12:57Z","title":"Content-aware Masked Image Modeling Transformer for Stereo Image\n  Compression","summary":"  Existing learning-based stereo image codec adopt sophisticated transformation\nwith simple entropy models derived from single image codecs to encode latent\nrepresentations. However, those entropy models struggle to effectively capture\nthe spatial-disparity characteristics inherent in stereo images, which leads to\nsuboptimal rate-distortion results. In this paper, we propose a stereo image\ncompression framework, named CAMSIC. CAMSIC independently transforms each image\nto latent representation and employs a powerful decoder-free Transformer\nentropy model to capture both spatial and disparity dependencies, by\nintroducing a novel content-aware masked image modeling (MIM) technique. Our\ncontent-aware MIM facilitates efficient bidirectional interaction between prior\ninformation and estimated tokens, which naturally obviates the need for an\nextra Transformer decoder. Experiments show that our stereo image codec\nachieves state-of-the-art rate-distortion performance on two stereo image\ndatasets Cityscapes and InStereo2K with fast encoding and decoding speed.\n","authors":["Xinjie Zhang","Shenyuan Gao","Zhening Liu","Xingtong Ge","Dailan He","Tongda Xu","Yan Wang","Jun Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.08505v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08502v1","updated":"2024-03-13T13:10:20Z","published":"2024-03-13T13:10:20Z","title":"Masked Generative Story Transformer with Character Guidance and Caption\n  Augmentation","summary":"  Story Visualization (SV) is a challenging generative vision task, that\nrequires both visual quality and consistency between different frames in\ngenerated image sequences. Previous approaches either employ some kind of\nmemory mechanism to maintain context throughout an auto-regressive generation\nof the image sequence, or model the generation of the characters and their\nbackground separately, to improve the rendering of characters. On the contrary,\nwe embrace a completely parallel transformer-based approach, exclusively\nrelying on Cross-Attention with past and future captions to achieve\nconsistency. Additionally, we propose a Character Guidance technique to focus\non the generation of characters in an implicit manner, by forming a combination\nof text-conditional and character-conditional logits in the logit space. We\nalso employ a caption-augmentation technique, carried out by a Large Language\nModel (LLM), to enhance the robustness of our approach. The combination of\nthese methods culminates into state-of-the-art (SOTA) results over various\nmetrics in the most prominent SV benchmark (Pororo-SV), attained with\nconstraint resources while achieving superior computational complexity compared\nto previous arts. The validity of our quantitative results is supported by a\nhuman survey.\n","authors":["Christos Papadimitriou","Giorgos Filandrianos","Maria Lymperaiou","Giorgos Stamou"],"pdf_url":"https://arxiv.org/pdf/2403.08502v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.01678v4","updated":"2024-03-13T13:02:57Z","published":"2023-12-04T07:01:54Z","title":"Jellyfish: A Large Language Model for Data Preprocessing","summary":"  This paper explores the utilization of LLMs for data preprocessing (DP), a\ncrucial step in the data mining pipeline that transforms raw data into a clean\nformat conducive to easy processing. Whereas the use of LLMs has sparked\ninterest in devising universal solutions to DP, recent initiatives in this\ndomain typically rely on GPT APIs, raising inevitable data breach concerns.\nUnlike these approaches, we consider instruction-tuning local LLMs (7 - 13B\nmodels) as universal DP ask solver. We select a collection of datasets across\nfour representative DP tasks and construct instruction-tuning data using\nserialization and knowledge injection techniques tailored to DP. As such, the\ninstruction-tuned LLMs empower users to manually craft instructions for DP.\nMeanwhile, they can operate on a local, single, and low-priced GPU, ensuring\ndata security and enabling further tuning. Our experiments show that our\ndataset constructed for DP instruction tuning, namely Jellyfish, effectively\nenhances LLMs' DP performances and barely compromises their abilities in NLP\ntasks. By tuning Mistral-7B and OpenOrca-Platypus2-13B with Jellyfish, the\nmodels deliver competitiveness compared to state-of-the-art DP methods and\nstrong generalizability to unseen tasks. The models' performance rivals that of\nGPT series models, and the interpretation offers enhanced reasoning\ncapabilities compared to GPT-3.5. The 7B and 13B Jellyfish models are available\nat Hugging Face: https://huggingface.co/NECOUDBFM/Jellyfish-7B\nhttps://huggingface.co/NECOUDBFM/Jellyfish-13B\n","authors":["Haochen Zhang","Yuyang Dong","Chuan Xiao","Masafumi Oyamada"],"pdf_url":"https://arxiv.org/pdf/2312.01678v4.pdf","comment":"a.k.a. \"Jellyfish: Instruction-Tuning Local Large Language Models for\n  Data Preprocessing''"},{"id":"http://arxiv.org/abs/2403.07805v2","updated":"2024-03-13T12:46:38Z","published":"2024-03-12T16:42:44Z","title":"Beyond Memorization: The Challenge of Random Memory Access in Language\n  Models","summary":"  Recent developments in Language Models (LMs) have shown their effectiveness\nin NLP tasks, particularly in knowledge-intensive tasks. However, the\nmechanisms underlying knowledge storage and memory access within their\nparameters remain elusive. In this paper, we investigate whether a generative\nLM (e.g., GPT-2) is able to access its memory sequentially or randomly. Through\ncarefully-designed synthetic tasks, covering the scenarios of full recitation,\nselective recitation and grounded question answering, we reveal that LMs manage\nto sequentially access their memory while encountering challenges in randomly\naccessing memorized content. We find that techniques including recitation and\npermutation improve the random memory access capability of LMs. Furthermore, by\napplying this intervention to realistic scenarios of open-domain question\nanswering, we validate that enhancing random access by recitation leads to\nnotable improvements in question answering. The code to reproduce our\nexperiments can be found at https://github.com/sail-sg/lm-random-memory-access.\n","authors":["Tongyao Zhu","Qian Liu","Liang Pang","Zhengbao Jiang","Min-Yen Kan","Min Lin"],"pdf_url":"https://arxiv.org/pdf/2403.07805v2.pdf","comment":"8 pages, 4 figures; fixed typos"},{"id":"http://arxiv.org/abs/2305.09144v2","updated":"2024-03-13T12:34:17Z","published":"2023-05-16T03:50:38Z","title":"Retentive or Forgetful? Diving into the Knowledge Memorizing Mechanism\n  of Language Models","summary":"  Memory is one of the most essential cognitive functions serving as a\nrepository of world knowledge and episodes of activities. In recent years,\nlarge-scale pre-trained language models have shown remarkable memorizing\nability. On the contrary, vanilla neural networks without pre-training have\nbeen long observed suffering from the catastrophic forgetting problem. To\ninvestigate such a retentive-forgetful contradiction and understand the memory\nmechanism of language models, we conduct thorough experiments by controlling\nthe target knowledge types, the learning strategies and the learning schedules.\nWe find that: 1) Vanilla language models are forgetful; 2) Pre-training leads\nto retentive language models; 3) Knowledge relevance and diversification\nsignificantly influence the memory formation. These conclusions are useful for\nunderstanding the abilities of pre-trained language models and shed light on\ndesigning and evaluating new learning and inference algorithms of language\nmodels.\n","authors":["Boxi Cao","Qiaoyu Tang","Hongyu Lin","Shanshan Jiang","Bin Dong","Xianpei Han","Jiawei Chen","Tianshu Wang","Le Sun"],"pdf_url":"https://arxiv.org/pdf/2305.09144v2.pdf","comment":"Accepted by LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2308.10156v2","updated":"2024-03-13T12:16:20Z","published":"2023-08-20T04:09:12Z","title":"SSMG: Spatial-Semantic Map Guided Diffusion Model for Free-form\n  Layout-to-Image Generation","summary":"  Despite significant progress in Text-to-Image (T2I) generative models, even\nlengthy and complex text descriptions still struggle to convey detailed\ncontrols. In contrast, Layout-to-Image (L2I) generation, aiming to generate\nrealistic and complex scene images from user-specified layouts, has risen to\nprominence. However, existing methods transform layout information into tokens\nor RGB images for conditional control in the generative process, leading to\ninsufficient spatial and semantic controllability of individual instances. To\naddress these limitations, we propose a novel Spatial-Semantic Map Guided\n(SSMG) diffusion model that adopts the feature map, derived from the layout, as\nguidance. Owing to rich spatial and semantic information encapsulated in\nwell-designed feature maps, SSMG achieves superior generation quality with\nsufficient spatial and semantic controllability compared to previous works.\nAdditionally, we propose the Relation-Sensitive Attention (RSA) and\nLocation-Sensitive Attention (LSA) mechanisms. The former aims to model the\nrelationships among multiple objects within scenes while the latter is designed\nto heighten the model's sensitivity to the spatial information embedded in the\nguidance. Extensive experiments demonstrate that SSMG achieves highly promising\nresults, setting a new state-of-the-art across a range of metrics encompassing\nfidelity, diversity, and controllability.\n","authors":["Chengyou Jia","Minnan Luo","Zhuohang Dang","Guang Dai","Xiaojun Chang","Mengmeng Wang","Jingdong Wang"],"pdf_url":"https://arxiv.org/pdf/2308.10156v2.pdf","comment":"Accepted to AAAI 2024"},{"id":"http://arxiv.org/abs/2403.08438v1","updated":"2024-03-13T11:44:30Z","published":"2024-03-13T11:44:30Z","title":"Reproducibility and Geometric Intrinsic Dimensionality: An Investigation\n  on Graph Neural Network Research","summary":"  Difficulties in replication and reproducibility of empirical evidences in\nmachine learning research have become a prominent topic in recent years.\nEnsuring that machine learning research results are sound and reliable requires\nreproducibility, which verifies the reliability of research findings using the\nsame code and data. This promotes open and accessible research, robust\nexperimental workflows, and the rapid integration of new findings. Evaluating\nthe degree to which research publications support these different aspects of\nreproducibility is one goal of the present work. For this we introduce an\nontology of reproducibility in machine learning and apply it to methods for\ngraph neural networks. Building on these efforts we turn towards another\ncritical challenge in machine learning, namely the curse of dimensionality,\nwhich poses challenges in data collection, representation, and analysis, making\nit harder to find representative data and impeding the training and inference\nprocesses. Using the closely linked concept of geometric intrinsic dimension we\ninvestigate to which extend the used machine learning models are influenced by\nthe intrinsic dimension of the data sets they are trained on.\n","authors":["Tobias Hille","Maximilian Stubbemann","Tom Hanika"],"pdf_url":"https://arxiv.org/pdf/2403.08438v1.pdf","comment":"39 pages, 9 figures"},{"id":"http://arxiv.org/abs/2301.10164v2","updated":"2024-03-13T11:43:22Z","published":"2023-01-17T15:06:33Z","title":"Lowering Detection in Sport Climbing Based on Orientation of the Sensor\n  Enhanced Quickdraw","summary":"  Tracking climbers' activity to improve services and make the best use of\ntheir infrastructure is a concern for climbing gyms. Each climbing session must\nbe analyzed from beginning till lowering of the climber. Therefore, spotting\nthe climbers descending is crucial since it indicates when the ascent has come\nto an end. This problem must be addressed while preserving privacy and\nconvenience of the climbers and the costs of the gyms. To this aim, a hardware\nprototype is developed to collect data using accelerometer sensors attached to\na piece of climbing equipment mounted on the wall, called quickdraw, that\nconnects the climbing rope to the bolt anchors. The corresponding sensors are\nconfigured to be energy-efficient, hence become practical in terms of expenses\nand time consumption for replacement when using in large quantity in a climbing\ngym. This paper describes hardware specifications, studies data measured by the\nsensors in ultra-low power mode, detect sensors' orientation patterns during\nlowering different routes, and develop an supervised approach to identify\nlowering.\n","authors":["Sadaf Moaveninejad","Andrea Janes","Camillo Porcaro"],"pdf_url":"https://arxiv.org/pdf/2301.10164v2.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2211.02680"},{"id":"http://arxiv.org/abs/2403.08430v1","updated":"2024-03-13T11:29:37Z","published":"2024-03-13T11:29:37Z","title":"Search-based Optimisation of LLM Learning Shots for Story Point\n  Estimation","summary":"  One of the ways Large Language Models (LLMs) are used to perform machine\nlearning tasks is to provide them with a few examples before asking them to\nproduce a prediction. This is a meta-learning process known as few-shot\nlearning. In this paper, we use available Search-Based methods to optimise the\nnumber and combination of examples that can improve an LLM's estimation\nperformance, when it is used to estimate story points for new agile tasks. Our\npreliminary results show that our SBSE technique improves the estimation\nperformance of the LLM by 59.34% on average (in terms of mean absolute error of\nthe estimation) over three datasets against a zero-shot setting.\n","authors":["Vali Tawosi","Salwa Alamir","Xiaomo Liu"],"pdf_url":"https://arxiv.org/pdf/2403.08430v1.pdf","comment":"6 pages, Accepted at SSBSE'23 NIER Track"},{"id":"http://arxiv.org/abs/2403.08429v1","updated":"2024-03-13T11:29:13Z","published":"2024-03-13T11:29:13Z","title":"Software Vulnerability and Functionality Assessment using LLMs","summary":"  While code review is central to the software development process, it can be\ntedious and expensive to carry out. In this paper, we investigate whether and\nhow Large Language Models (LLMs) can aid with code reviews. Our investigation\nfocuses on two tasks that we argue are fundamental to good reviews: (i)\nflagging code with security vulnerabilities and (ii) performing software\nfunctionality validation, i.e., ensuring that code meets its intended\nfunctionality. To test performance on both tasks, we use zero-shot and\nchain-of-thought prompting to obtain final ``approve or reject''\nrecommendations. As data, we employ seminal code generation datasets (HumanEval\nand MBPP) along with expert-written code snippets with security vulnerabilities\nfrom the Common Weakness Enumeration (CWE). Our experiments consider a mixture\nof three proprietary models from OpenAI and smaller open-source LLMs. We find\nthat the former outperforms the latter by a large margin. Motivated by\npromising results, we finally ask our models to provide detailed descriptions\nof security vulnerabilities. Results show that 36.7% of LLM-generated\ndescriptions can be associated with true CWE vulnerabilities.\n","authors":["Rasmus Ingemann Tuffveson Jensen","Vali Tawosi","Salwa Alamir"],"pdf_url":"https://arxiv.org/pdf/2403.08429v1.pdf","comment":"4 pages, accepted to NLBSE'24"},{"id":"http://arxiv.org/abs/2403.08426v1","updated":"2024-03-13T11:23:55Z","published":"2024-03-13T11:23:55Z","title":"Language-Driven Visual Consensus for Zero-Shot Semantic Segmentation","summary":"  The pre-trained vision-language model, exemplified by CLIP, advances\nzero-shot semantic segmentation by aligning visual features with class\nembeddings through a transformer decoder to generate semantic masks. Despite\nits effectiveness, prevailing methods within this paradigm encounter\nchallenges, including overfitting on seen classes and small fragmentation in\nmasks. To mitigate these issues, we propose a Language-Driven Visual Consensus\n(LDVC) approach, fostering improved alignment of semantic and visual\ninformation.Specifically, we leverage class embeddings as anchors due to their\ndiscrete and abstract nature, steering vision features toward class embeddings.\nMoreover, to circumvent noisy alignments from the vision part due to its\nredundant nature, we introduce route attention into self-attention for finding\nvisual consensus, thereby enhancing semantic consistency within the same\nobject. Equipped with a vision-language prompting strategy, our approach\nsignificantly boosts the generalization capacity of segmentation models for\nunseen classes. Experimental results underscore the effectiveness of our\napproach, showcasing mIoU gains of 4.5 on the PASCAL VOC 2012 and 3.6 on the\nCOCO-Stuff 164k for unseen classes compared with the state-of-the-art methods.\n","authors":["Zicheng Zhang","Tong Zhang","Yi Zhu","Jianzhuang Liu","Xiaodan Liang","QiXiang Ye","Wei Ke"],"pdf_url":"https://arxiv.org/pdf/2403.08426v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08425v1","updated":"2024-03-13T11:20:34Z","published":"2024-03-13T11:20:34Z","title":"Specification Overfitting in Artificial Intelligence","summary":"  Machine learning (ML) and artificial intelligence (AI) approaches are often\ncriticized for their inherent bias and for their lack of control,\naccountability, and transparency. Consequently, regulatory bodies struggle with\ncontaining this technology's potential negative side effects. High-level\nrequirements such as fairness and robustness need to be formalized into\nconcrete specification metrics, imperfect proxies that capture isolated aspects\nof the underlying requirements. Given possible trade-offs between different\nmetrics and their vulnerability to over-optimization, integrating specification\nmetrics in system development processes is not trivial. This paper defines\nspecification overfitting, a scenario where systems focus excessively on\nspecified metrics to the detriment of high-level requirements and task\nperformance. We present an extensive literature survey to categorize how\nresearchers propose, measure, and optimize specification metrics in several AI\nfields (e.g., natural language processing, computer vision, reinforcement\nlearning). Using a keyword-based search on papers from major AI conferences and\njournals between 2018 and mid-2023, we identify and analyze 74 papers that\npropose or optimize specification metrics. We find that although most papers\nimplicitly address specification overfitting (e.g., by reporting more than one\nspecification metric), they rarely discuss which role specification metrics\nshould play in system development or explicitly define the scope and\nassumptions behind metric formulations.\n","authors":["Benjamin Roth","Pedro Henrique Luz de Araujo","Yuxi Xia","Saskia Kaltenbrunner","Christoph Korab"],"pdf_url":"https://arxiv.org/pdf/2403.08425v1.pdf","comment":"40 pages, 2 figures"},{"id":"http://arxiv.org/abs/2403.07743v2","updated":"2024-03-13T11:20:19Z","published":"2024-03-12T15:22:05Z","title":"Equipping Computational Pathology Systems with Artifact Processing\n  Pipelines: A Showcase for Computation and Performance Trade-offs","summary":"  Histopathology is a gold standard for cancer diagnosis under a microscopic\nexamination. However, histological tissue processing procedures result in\nartifacts, which are ultimately transferred to the digitized version of glass\nslides, known as whole slide images (WSIs). Artifacts are diagnostically\nirrelevant areas and may result in wrong deep learning (DL) algorithms\npredictions. Therefore, detecting and excluding artifacts in the computational\npathology (CPATH) system is essential for reliable automated diagnosis. In this\npaper, we propose a mixture of experts (MoE) scheme for detecting five notable\nartifacts, including damaged tissue, blur, folded tissue, air bubbles, and\nhistologically irrelevant blood from WSIs. First, we train independent binary\nDL models as experts to capture particular artifact morphology. Then, we\nensemble their predictions using a fusion mechanism. We apply probabilistic\nthresholding over the final probability distribution to improve the sensitivity\nof the MoE. We developed DL pipelines using two MoEs and two multiclass models\nof state-of-the-art deep convolutional neural networks (DCNNs) and vision\ntransformers (ViTs). DCNNs-based MoE and ViTs-based MoE schemes outperformed\nsimpler multiclass models and were tested on datasets from different hospitals\nand cancer types, where MoE using DCNNs yielded the best results. The proposed\nMoE yields 86.15% F1 and 97.93% sensitivity scores on unseen data, retaining\nless computational cost for inference than MoE using ViTs. This best\nperformance of MoEs comes with relatively higher computational trade-offs than\nmulticlass models. The proposed artifact detection pipeline will not only\nensure reliable CPATH predictions but may also provide quality control.\n","authors":["Neel Kanwal","Farbod Khoraminia","Umay Kiraz","Andres Mosquera-Zamudio","Carlos Monteagudo","Emiel A. M. Janssen","Tahlita C. M. Zuiverloon","Chunmig Rong","Kjersti Engan"],"pdf_url":"https://arxiv.org/pdf/2403.07743v2.pdf","comment":"Submitted to BMC Medical Informatics and Decision Making Journal"},{"id":"http://arxiv.org/abs/2312.05720v3","updated":"2024-03-13T11:19:24Z","published":"2023-12-10T01:19:59Z","title":"Beyond Gradient and Priors in Privacy Attacks: Leveraging Pooler Layer\n  Inputs of Language Models in Federated Learning","summary":"  Language models trained via federated learning (FL) demonstrate impressive\ncapabilities in handling complex tasks while protecting user privacy. Recent\nstudies indicate that leveraging gradient information and prior knowledge can\npotentially reveal training samples within FL setting. However, these\ninvestigations have overlooked the potential privacy risks tied to the\nintrinsic architecture of the models. This paper presents a two-stage privacy\nattack strategy that targets the vulnerabilities in the architecture of\ncontemporary language models, significantly enhancing attack performance by\ninitially recovering certain feature directions as additional supervisory\nsignals. Our comparative experiments demonstrate superior attack performance\nacross various datasets and scenarios, highlighting the privacy leakage risk\nassociated with the increasingly complex architectures of language models. We\ncall for the community to recognize and address these potential privacy risks\nin designing large language models.\n","authors":["Jianwei Li","Sheng Liu","Qi Lei"],"pdf_url":"https://arxiv.org/pdf/2312.05720v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08424v1","updated":"2024-03-13T11:16:43Z","published":"2024-03-13T11:16:43Z","title":"Tastle: Distract Large Language Models for Automatic Jailbreak Attack","summary":"  Large language models (LLMs) have achieved significant advances in recent\ndays. Extensive efforts have been made before the public release of LLMs to\nalign their behaviors with human values. The primary goal of alignment is to\nensure their helpfulness, honesty and harmlessness. However, even meticulously\naligned LLMs remain vulnerable to malicious manipulations such as jailbreaking,\nleading to unintended behaviors. The jailbreak is to intentionally develop a\nmalicious prompt that escapes from the LLM security restrictions to produce\nuncensored detrimental contents. Previous works explore different jailbreak\nmethods for red teaming LLMs, yet they encounter challenges regarding to\neffectiveness and scalability. In this work, we propose Tastle, a novel\nblack-box jailbreak framework for automated red teaming of LLMs. We designed\nmalicious content concealing and memory reframing with an iterative\noptimization algorithm to jailbreak LLMs, motivated by the research about the\ndistractibility and over-confidence phenomenon of LLMs. Extensive experiments\nof jailbreaking both open-source and proprietary LLMs demonstrate the\nsuperiority of our framework in terms of effectiveness, scalability and\ntransferability. We also evaluate the effectiveness of existing jailbreak\ndefense methods against our attack and highlight the crucial need to develop\nmore effective and practical defense strategies.\n","authors":["Zeguan Xiao","Yan Yang","Guanhua Chen","Yun Chen"],"pdf_url":"https://arxiv.org/pdf/2403.08424v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08414v1","updated":"2024-03-13T10:58:55Z","published":"2024-03-13T10:58:55Z","title":"Causal Graph Neural Networks for Wildfire Danger Prediction","summary":"  Wildfire forecasting is notoriously hard due to the complex interplay of\ndifferent factors such as weather conditions, vegetation types and human\nactivities. Deep learning models show promise in dealing with this complexity\nby learning directly from data. However, to inform critical decision making, we\nargue that we need models that are right for the right reasons; that is, the\nimplicit rules learned should be grounded by the underlying processes driving\nwildfires. In that direction, we propose integrating causality with Graph\nNeural Networks (GNNs) that explicitly model the causal mechanism among complex\nvariables via graph learning. The causal adjacency matrix considers the\nsynergistic effect among variables and removes the spurious links from highly\ncorrelated impacts. Our methodology's effectiveness is demonstrated through\nsuperior performance forecasting wildfire patterns in the European boreal and\nmediterranean biome. The gain is especially prominent in a highly imbalanced\ndataset, showcasing an enhanced robustness of the model to adapt to regime\nshifts in functional relationships. Furthermore, SHAP values from our trained\nmodel further enhance our understanding of the model's inner workings.\n","authors":["Shan Zhao","Ioannis Prapas","Ilektra Karasante","Zhitong Xiong","Ioannis Papoutsis","Gustau Camps-Valls","Xiao Xiang Zhu"],"pdf_url":"https://arxiv.org/pdf/2403.08414v1.pdf","comment":"Accepted by ICLR 2024 Machine Learning for Remote Sensing (ML4RS)\n  Workshop"}],"Operating Systems":[{"id":"http://arxiv.org/abs/2403.08656v1","updated":"2024-03-13T16:10:04Z","published":"2024-03-13T16:10:04Z","title":"Physical Memory Attacks and a Memory Safe Management System for Memory\n  Defense","summary":"  Programming errors, defective hardware components (such as hard disk spindle\ndefects), and environmental hazards can lead to invalid memory operations. In\naddition, less predictable forms of environmental stress, such as radiation,\nthermal influence, and energy fluctuations, can induce hardware faults.\nSometimes, a soft error can occur instead of a complete failure, such as a\nbit-flip. The 'natural' factors that can cause bit-flips are replicable through\ntargeted attacks that result in significant compromises, including full\nprivileged system access. Existing physical defense solutions have consistently\nbeen circumvented shortly after deployment. We will explore the concept of a\nnovel software-based low-level layer that can protect vulnerable memory\ntargeted by physical attack vectors related to bit-flip vulnerabilities.\n","authors":["Alon Hillel-Tuch","Aspen Olmstead"],"pdf_url":"https://arxiv.org/pdf/2403.08656v1.pdf","comment":"Computer Science, Computer Engineering, and Applied Computing (CSCE)\n  Conference 2022"},{"id":"http://arxiv.org/abs/2309.03628v3","updated":"2024-03-13T08:57:29Z","published":"2023-09-07T10:50:32Z","title":"OSMOSIS: Enabling Multi-Tenancy in Datacenter SmartNICs","summary":"  Multi-tenancy is essential for unleashing SmartNIC's potential in\ndatacenters. Our systematic analysis in this work shows that existing on-path\nSmartNICs have resource multiplexing limitations. For example, existing\nsolutions lack multi-tenancy capabilities such as performance isolation and QoS\nprovisioning for compute and IO resources. Compared to standard NIC data paths\nwith a well-defined set of offloaded functions, unpredictable execution times\nof SmartNIC kernels make conventional approaches for multi-tenancy and QoS\ninsufficient. We fill this gap with OSMOSIS, a SmartNICs resource manager\nco-design. OSMOSIS extends existing OS mechanisms to enable dynamic hardware\nresource multiplexing of the on-path packet processing data plane. We integrate\nOSMOSIS within an open-source RISC-V-based 400Gbit/s SmartNIC. Our performance\nresults demonstrate that OSMOSIS fully supports multi-tenancy and enables\nbroader adoption of SmartNICs in datacenters with low overhead.\n","authors":["Mikhail Khalilov","Marcin Chrapek","Siyuan Shen","Alessandro Vezzu","Thomas Benz","Salvatore Di Girolamo","Timo Schneider","Daniele De Sensi","Luca Benini","Torsten Hoefler"],"pdf_url":"https://arxiv.org/pdf/2309.03628v3.pdf","comment":"12 pages, 14 figures, 103 references"}],"Databases":[{"id":"http://arxiv.org/abs/2306.02194v2","updated":"2024-03-13T13:19:55Z","published":"2023-06-03T20:53:22Z","title":"PathFinder: A unified approach for handling paths in graph query\n  languages","summary":"  Path queries are a core feature of modern graph query languages such as\nCypher, SQL/PGQ, and GQL. These languages provide a rich set of features for\nmatching paths, such as restricting to certain path modes (shortest, simple,\ntrail) and constraining the edge labels along the path by a regular expression.\nIn this paper we present PathFinder, a unifying approach for dealing with path\nqueries in all these query languages. PathFinder leverages a compact\nrepresentation of the (potentially exponential number of) paths that can match\na given query, extends it with pipelined execution, and supports all commonly\nused path modes. In the paper we describe the algorithmic backbone of\nPathFinder, provide a reference implementation, and test it over a large set of\nreal-world queries and datasets. Our results show that PathFinder exhibits very\nstable behavior, even on large data and complex queries, and its performance is\nan order of magnitude better than that of many modern graph engines.\n","authors":["Benjamín Farías","Wim Martens","Carlos Rojas","Domagoj Vrgoč"],"pdf_url":"https://arxiv.org/pdf/2306.02194v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.01678v4","updated":"2024-03-13T13:02:57Z","published":"2023-12-04T07:01:54Z","title":"Jellyfish: A Large Language Model for Data Preprocessing","summary":"  This paper explores the utilization of LLMs for data preprocessing (DP), a\ncrucial step in the data mining pipeline that transforms raw data into a clean\nformat conducive to easy processing. Whereas the use of LLMs has sparked\ninterest in devising universal solutions to DP, recent initiatives in this\ndomain typically rely on GPT APIs, raising inevitable data breach concerns.\nUnlike these approaches, we consider instruction-tuning local LLMs (7 - 13B\nmodels) as universal DP ask solver. We select a collection of datasets across\nfour representative DP tasks and construct instruction-tuning data using\nserialization and knowledge injection techniques tailored to DP. As such, the\ninstruction-tuned LLMs empower users to manually craft instructions for DP.\nMeanwhile, they can operate on a local, single, and low-priced GPU, ensuring\ndata security and enabling further tuning. Our experiments show that our\ndataset constructed for DP instruction tuning, namely Jellyfish, effectively\nenhances LLMs' DP performances and barely compromises their abilities in NLP\ntasks. By tuning Mistral-7B and OpenOrca-Platypus2-13B with Jellyfish, the\nmodels deliver competitiveness compared to state-of-the-art DP methods and\nstrong generalizability to unseen tasks. The models' performance rivals that of\nGPT series models, and the interpretation offers enhanced reasoning\ncapabilities compared to GPT-3.5. The 7B and 13B Jellyfish models are available\nat Hugging Face: https://huggingface.co/NECOUDBFM/Jellyfish-7B\nhttps://huggingface.co/NECOUDBFM/Jellyfish-13B\n","authors":["Haochen Zhang","Yuyang Dong","Chuan Xiao","Masafumi Oyamada"],"pdf_url":"https://arxiv.org/pdf/2312.01678v4.pdf","comment":"a.k.a. \"Jellyfish: Instruction-Tuning Local Large Language Models for\n  Data Preprocessing''"},{"id":"http://arxiv.org/abs/2403.08444v1","updated":"2024-03-13T11:56:10Z","published":"2024-03-13T11:56:10Z","title":"COSTREAM: Learned Cost Models for Operator Placement in Edge-Cloud\n  Environments","summary":"  In this work, we present COSTREAM, a novel learned cost model for Distributed\nStream Processing Systems that provides accurate predictions of the execution\ncosts of a streaming query in an edge-cloud environment. The cost model can be\nused to find an initial placement of operators across heterogeneous hardware,\nwhich is particularly important in these environments. In our evaluation, we\ndemonstrate that COSTREAM can produce highly accurate cost estimates for the\ninitial operator placement and even generalize to unseen placements, queries,\nand hardware. When using COSTREAM to optimize the placements of streaming\noperators, a median speed-up of around 21x can be achieved compared to\nbaselines.\n","authors":["Roman Heinrich","Carsten Binnig","Harald Kornmayer","Manisha Luthra"],"pdf_url":"https://arxiv.org/pdf/2403.08444v1.pdf","comment":"This paper has been accepted by IEEE ICDE 2024"},{"id":"http://arxiv.org/abs/2403.08375v1","updated":"2024-03-13T09:38:39Z","published":"2024-03-13T09:38:39Z","title":"Translating between SQL Dialects for Cloud Migration","summary":"  Migrations of systems from on-site premises to the cloud has been a\nfundamental endeavor by many industrial institutions. A crucial component of\nsuch cloud migrations is the transition of databases to be hosted online. In\nthis work, we consider the difficulties of this migration for SQL databases.\nWhile SQL is one of the prominent methods for storing database procedures,\nthere are a plethora of different SQL dialects (e.g., MySQL, Postgres, etc.)\nwhich can complicate migrations when the on-premise SQL dialect differs to the\ndialect hosted on the cloud. Tools exist by common cloud provides such as AWS\nand Azure to aid in translating between dialects in order to mitigate the\nmajority of the difficulties. However, these tools do not successfully\ntranslate $100\\%$ of the code. Consequently, software engineers must manually\nconvert the remainder of the untranslated database. For large organizations,\nthis task quickly becomes intractable and so more innovative solutions are\nrequired. We consider this challenge a novel yet vital industrial research\nproblem for any large corporation that is considering cloud migrations.\nFurthermore, we introduce potential avenues of research to tackle this\nchallenge that have yielded promising preliminary results.\n","authors":["Ran Zmigrod","Salwa Alamir","Xiaomo Liu"],"pdf_url":"https://arxiv.org/pdf/2403.08375v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08257v1","updated":"2024-03-13T05:20:35Z","published":"2024-03-13T05:20:35Z","title":"Reconciling Conflicting Data Curation Actions: Transparency Through\n  Argumentation","summary":"  We propose a new approach for modeling and reconciling conflicting data\ncleaning actions. Such conflicts arise naturally in collaborative data curation\nsettings where multiple experts work independently and then aim to put their\nefforts together to improve and accelerate data cleaning. The key idea of our\napproach is to model conflicting updates as a formal \\emph{argumentation\nframework}(AF). Such argumentation frameworks can be automatically analyzed and\nsolved by translating them to a logic program $P_{AF}$ whose declarative\nsemantics yield a transparent solution with many desirable properties, e.g.,\nuncontroversial updates are accepted, unjustified ones are rejected, and the\nremaining ambiguities are exposed and presented to users for further analysis.\nAfter motivating the problem, we introduce our approach and illustrate it with\na detailed running example introducing both well-founded and stable semantics\nto help understand the AF solutions. We have begun to develop open source tools\nand Jupyter notebooks that demonstrate the practicality of our approach. In\nfuture work we plan to develop a toolkit for conflict resolution that can be\nused in conjunction with OpenRefine, a popular interactive data cleaning tool.\n","authors":["Yilin Xia","Shawn Bowers","Lan Li","Bertram Ludäscher"],"pdf_url":"https://arxiv.org/pdf/2403.08257v1.pdf","comment":"Accepted to IDCC 2024. Source code is available at\n  https://github.com/idaks/Games-and-Argumentation/tree/idcc"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2403.08763v1","updated":"2024-03-13T17:58:57Z","published":"2024-03-13T17:58:57Z","title":"Simple and Scalable Strategies to Continually Pre-train Large Language\n  Models","summary":"  Large language models (LLMs) are routinely pre-trained on billions of tokens,\nonly to start the process over again once new data becomes available. A much\nmore efficient solution is to continually pre-train these models, saving\nsignificant compute compared to re-training. However, the distribution shift\ninduced by new data typically results in degraded performance on previous data\nor poor adaptation to the new data. In this work, we show that a simple and\nscalable combination of learning rate (LR) re-warming, LR re-decaying, and\nreplay of previous data is sufficient to match the performance of fully\nre-training from scratch on all available data, as measured by final loss and\nlanguage model (LM) evaluation benchmarks. Specifically, we show this for a\nweak but realistic distribution shift between two commonly used LLM\npre-training datasets (English$\\rightarrow$English) and a stronger distribution\nshift (English$\\rightarrow$German) at the $405$M parameter model scale with\nlarge dataset sizes (hundreds of billions of tokens). Selecting the weak but\nrealistic shift for larger-scale experiments, we also find that our continual\nlearning strategies match the re-training baseline for a 10B parameter LLM. Our\nresults demonstrate that LLMs can be successfully updated via simple and\nscalable continual learning strategies, matching the re-training baseline using\nonly a fraction of the compute. Finally, inspired by previous work, we propose\nalternatives to the cosine learning rate schedule that help circumvent\nforgetting induced by LR re-warming and that are not bound to a fixed token\nbudget.\n","authors":["Adam Ibrahim","Benjamin Thérien","Kshitij Gupta","Mats L. Richter","Quentin Anthony","Timothée Lesort","Eugene Belilovsky","Irina Rish"],"pdf_url":"https://arxiv.org/pdf/2403.08763v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08757v1","updated":"2024-03-13T17:55:34Z","published":"2024-03-13T17:55:34Z","title":"Efficient Combinatorial Optimization via Heat Diffusion","summary":"  Combinatorial optimization problems are widespread but inherently challenging\ndue to their discrete nature.The primary limitation of existing methods is that\nthey can only access a small fraction of the solution space at each iteration,\nresulting in limited efficiency for searching the global optimal. To overcome\nthis challenge, diverging from conventional efforts of expanding the solver's\nsearch scope, we focus on enabling information to actively propagate to the\nsolver through heat diffusion. By transforming the target function while\npreserving its optima, heat diffusion facilitates information flow from distant\nregions to the solver, providing more efficient navigation. Utilizing heat\ndiffusion, we propose a framework for solving general combinatorial\noptimization problems. The proposed methodology demonstrates superior\nperformance across a range of the most challenging and widely encountered\ncombinatorial optimizations. Echoing recent advancements in harnessing\nthermodynamics for generative artificial intelligence, our study further\nreveals its significant potential in advancing combinatorial optimization.\n","authors":["Hengyuan Ma","Wenlian Lu","Jianfeng Feng"],"pdf_url":"https://arxiv.org/pdf/2403.08757v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08755v1","updated":"2024-03-13T17:53:47Z","published":"2024-03-13T17:53:47Z","title":"DAM: Dynamic Adapter Merging for Continual Video QA Learning","summary":"  We present a parameter-efficient method for continual video\nquestion-answering (VidQA) learning. Our method, named DAM, uses the proposed\nDynamic Adapter Merging to (i) mitigate catastrophic forgetting, (ii) enable\nefficient adaptation to continually arriving datasets, (iii) handle inputs from\nunknown datasets during inference, and (iv) enable knowledge sharing across\nsimilar dataset domains. Given a set of continually streaming VidQA datasets,\nwe sequentially train dataset-specific adapters for each dataset while freezing\nthe parameters of a large pretrained video-language backbone. During inference,\ngiven a video-question sample from an unknown domain, our method first uses the\nproposed non-parametric router function to compute a probability for each\nadapter, reflecting how relevant that adapter is to the current video-question\ninput instance. Subsequently, the proposed dynamic adapter merging scheme\naggregates all the adapter weights into a new adapter instance tailored for\nthat particular test sample to compute the final VidQA prediction, mitigating\nthe impact of inaccurate router predictions and facilitating knowledge sharing\nacross domains. Our DAM model outperforms prior state-of-the-art continual\nlearning approaches by 9.1% while exhibiting 1.9% less forgetting on 6 VidQA\ndatasets spanning various domains. We further extend DAM to continual image\nclassification and image QA and outperform prior methods by a large margin. The\ncode is publicly available at: https://github.com/klauscc/DAM\n","authors":["Feng Cheng","Ziyang Wang","Yi-Lin Sung","Yan-Bo Lin","Mohit Bansal","Gedas Bertasius"],"pdf_url":"https://arxiv.org/pdf/2403.08755v1.pdf","comment":"The first two authors contribute equally"},{"id":"http://arxiv.org/abs/2403.08750v1","updated":"2024-03-13T17:51:02Z","published":"2024-03-13T17:51:02Z","title":"Neural reproducing kernel Banach spaces and representer theorems for\n  deep networks","summary":"  Studying the function spaces defined by neural networks helps to understand\nthe corresponding learning models and their inductive bias. While in some\nlimits neural networks correspond to function spaces that are reproducing\nkernel Hilbert spaces, these regimes do not capture the properties of the\nnetworks used in practice. In contrast, in this paper we show that deep neural\nnetworks define suitable reproducing kernel Banach spaces.\n  These spaces are equipped with norms that enforce a form of sparsity,\nenabling them to adapt to potential latent structures within the input data and\ntheir representations. In particular, leveraging the theory of reproducing\nkernel Banach spaces, combined with variational results, we derive representer\ntheorems that justify the finite architectures commonly employed in\napplications. Our study extends analogous results for shallow networks and can\nbe seen as a step towards considering more practically plausible neural\narchitectures.\n","authors":["Francesca Bartolucci","Ernesto De Vito","Lorenzo Rosasco","Stefano Vigogna"],"pdf_url":"https://arxiv.org/pdf/2403.08750v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08743v1","updated":"2024-03-13T17:46:28Z","published":"2024-03-13T17:46:28Z","title":"Steering LLMs Towards Unbiased Responses: A Causality-Guided Debiasing\n  Framework","summary":"  Large language models (LLMs) can easily generate biased and discriminative\nresponses. As LLMs tap into consequential decision-making (e.g., hiring and\nhealthcare), it is of crucial importance to develop strategies to mitigate\nthese biases. This paper focuses on social bias, tackling the association\nbetween demographic information and LLM outputs. We propose a causality-guided\ndebiasing framework that utilizes causal understandings of (1) the\ndata-generating process of the training corpus fed to LLMs, and (2) the\ninternal reasoning process of LLM inference, to guide the design of prompts for\ndebiasing LLM outputs through selection mechanisms. Our framework unifies\nexisting de-biasing prompting approaches such as inhibitive instructions and\nin-context contrastive examples, and sheds light on new ways of debiasing by\nencouraging bias-free reasoning. Our strong empirical performance on real-world\ndatasets demonstrates that our framework provides principled guidelines on\ndebiasing LLM outputs even with only the black-box access.\n","authors":["Jingling Li","Zeyu Tang","Xiaoyu Liu","Peter Spirtes","Kun Zhang","Liu Leqi","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2403.08743v1.pdf","comment":"18 pages, 11 figures"},{"id":"http://arxiv.org/abs/2212.14511v2","updated":"2024-03-13T17:44:52Z","published":"2022-12-30T01:42:04Z","title":"Can Direct Latent Model Learning Solve Linear Quadratic Gaussian\n  Control?","summary":"  We study the task of learning state representations from potentially\nhigh-dimensional observations, with the goal of controlling an unknown\npartially observable system. We pursue a direct latent model learning approach,\nwhere a dynamic model in some latent state space is learned by predicting\nquantities directly related to planning (e.g., costs) without reconstructing\nthe observations. In particular, we focus on an intuitive cost-driven state\nrepresentation learning method for solving Linear Quadratic Gaussian (LQG)\ncontrol, one of the most fundamental partially observable control problems. As\nour main results, we establish finite-sample guarantees of finding a\nnear-optimal state representation function and a near-optimal controller using\nthe directly learned latent model. To the best of our knowledge, despite\nvarious empirical successes, prior to this work it was unclear if such a\ncost-driven latent model learner enjoys finite-sample guarantees. Our work\nunderscores the value of predicting multi-step costs, an idea that is key to\nour theory, and notably also an idea that is known to be empirically valuable\nfor learning state representations.\n","authors":["Yi Tian","Kaiqing Zhang","Russ Tedrake","Suvrit Sra"],"pdf_url":"https://arxiv.org/pdf/2212.14511v2.pdf","comment":"37 pages; Updated structure and proofs"},{"id":"http://arxiv.org/abs/2403.08741v1","updated":"2024-03-13T17:44:16Z","published":"2024-03-13T17:44:16Z","title":"Learning How to Strategically Disclose Information","summary":"  Strategic information disclosure, in its simplest form, considers a game\nbetween an information provider (sender) who has access to some private\ninformation that an information receiver is interested in. While the receiver\ntakes an action that affects the utilities of both players, the sender can\ndesign information (or modify beliefs) of the receiver through signal\ncommitment, hence posing a Stackelberg game. However, obtaining a Stackelberg\nequilibrium for this game traditionally requires the sender to have access to\nthe receiver's objective. In this work, we consider an online version of\ninformation design where a sender interacts with a receiver of an unknown type\nwho is adversarially chosen at each round. Restricting attention to Gaussian\nprior and quadratic costs for the sender and the receiver, we show that\n$\\mathcal{O}(\\sqrt{T})$ regret is achievable with full information feedback,\nwhere $T$ is the total number of interactions between the sender and the\nreceiver. Further, we propose a novel parametrization that allows the sender to\nachieve $\\mathcal{O}(\\sqrt{T})$ regret for a general convex utility function.\nWe then consider the Bayesian Persuasion problem with an additional cost term\nin the objective function, which penalizes signaling policies that are more\ninformative and obtain $\\mathcal{O}(\\log(T))$ regret. Finally, we establish a\nsublinear regret bound for the partial information feedback setting and provide\nsimulations to support our theoretical results.\n","authors":["Raj Kiriti Velicheti","Melih Bastopcu","S. Rasoul Etesami","Tamer Başar"],"pdf_url":"https://arxiv.org/pdf/2403.08741v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.04475v2","updated":"2024-03-13T17:40:04Z","published":"2023-10-06T05:27:28Z","title":"Demystifying Embedding Spaces using Large Language Models","summary":"  Embeddings have become a pivotal means to represent complex, multi-faceted\ninformation about entities, concepts, and relationships in a condensed and\nuseful format. Nevertheless, they often preclude direct interpretation. While\ndownstream tasks make use of these compressed representations, meaningful\ninterpretation usually requires visualization using dimensionality reduction or\nspecialized machine learning interpretability methods. This paper addresses the\nchallenge of making such embeddings more interpretable and broadly useful, by\nemploying Large Language Models (LLMs) to directly interact with embeddings --\ntransforming abstract vectors into understandable narratives. By injecting\nembeddings into LLMs, we enable querying and exploration of complex embedding\ndata. We demonstrate our approach on a variety of diverse tasks, including:\nenhancing concept activation vectors (CAVs), communicating novel embedded\nentities, and decoding user preferences in recommender systems. Our work\ncouples the immense information potential of embeddings with the interpretative\npower of LLMs.\n","authors":["Guy Tennenholtz","Yinlam Chow","Chih-Wei Hsu","Jihwan Jeong","Lior Shani","Azamat Tulepbergenov","Deepak Ramachandran","Martin Mladenov","Craig Boutilier"],"pdf_url":"https://arxiv.org/pdf/2310.04475v2.pdf","comment":"Accepted to ICLR 2024"},{"id":"http://arxiv.org/abs/2312.07511v2","updated":"2024-03-13T17:38:27Z","published":"2023-12-12T18:44:19Z","title":"A Hitchhiker's Guide to Geometric GNNs for 3D Atomic Systems","summary":"  Recent advances in computational modelling of atomic systems, spanning\nmolecules, proteins, and materials, represent them as geometric graphs with\natoms embedded as nodes in 3D Euclidean space. In these graphs, the geometric\nattributes transform according to the inherent physical symmetries of 3D atomic\nsystems, including rotations and translations in Euclidean space, as well as\nnode permutations. In recent years, Geometric Graph Neural Networks have\nemerged as the preferred machine learning architecture powering applications\nranging from protein structure prediction to molecular simulations and material\ngeneration. Their specificity lies in the inductive biases they leverage - such\nas physical symmetries and chemical properties - to learn informative\nrepresentations of these geometric graphs.\n  In this opinionated paper, we provide a comprehensive and self-contained\noverview of the field of Geometric GNNs for 3D atomic systems. We cover\nfundamental background material and introduce a pedagogical taxonomy of\nGeometric GNN architectures: (1) invariant networks, (2) equivariant networks\nin Cartesian basis, (3) equivariant networks in spherical basis, and (4)\nunconstrained networks. Additionally, we outline key datasets and application\nareas and suggest future research directions. The objective of this work is to\npresent a structured perspective on the field, making it accessible to\nnewcomers and aiding practitioners in gaining an intuition for its mathematical\nabstractions.\n","authors":["Alexandre Duval","Simon V. Mathis","Chaitanya K. Joshi","Victor Schmidt","Santiago Miret","Fragkiskos D. Malliaros","Taco Cohen","Pietro Liò","Yoshua Bengio","Michael Bronstein"],"pdf_url":"https://arxiv.org/pdf/2312.07511v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08728v1","updated":"2024-03-13T17:28:20Z","published":"2024-03-13T17:28:20Z","title":"Ambient Diffusion Posterior Sampling: Solving Inverse Problems with\n  Diffusion Models trained on Corrupted Data","summary":"  We provide a framework for solving inverse problems with diffusion models\nlearned from linearly corrupted data. Our method, Ambient Diffusion Posterior\nSampling (A-DPS), leverages a generative model pre-trained on one type of\ncorruption (e.g. image inpainting) to perform posterior sampling conditioned on\nmeasurements from a potentially different forward process (e.g. image\nblurring). We test the efficacy of our approach on standard natural image\ndatasets (CelebA, FFHQ, and AFHQ) and we show that A-DPS can sometimes\noutperform models trained on clean data for several image restoration tasks in\nboth speed and performance. We further extend the Ambient Diffusion framework\nto train MRI models with access only to Fourier subsampled multi-coil MRI\nmeasurements at various acceleration factors (R=2, 4, 6, 8). We again observe\nthat models trained on highly subsampled data are better priors for solving\ninverse problems in the high acceleration regime than models trained on fully\nsampled data. We open-source our code and the trained Ambient Diffusion MRI\nmodels: https://github.com/utcsilab/ambient-diffusion-mri .\n","authors":["Asad Aali","Giannis Daras","Brett Levac","Sidharth Kumar","Alexandros G. Dimakis","Jonathan I. Tamir"],"pdf_url":"https://arxiv.org/pdf/2403.08728v1.pdf","comment":"Pre-print, work in progress"},{"id":"http://arxiv.org/abs/2306.14306v2","updated":"2024-03-13T17:20:27Z","published":"2023-06-25T18:29:29Z","title":"Adaptive Sharpness-Aware Pruning for Robust Sparse Networks","summary":"  Robustness and compactness are two essential attributes of deep learning\nmodels that are deployed in the real world. The goals of robustness and\ncompactness may seem to be at odds, since robustness requires generalization\nacross domains, while the process of compression exploits specificity in one\ndomain. We introduce Adaptive Sharpness-Aware Pruning (AdaSAP), which unifies\nthese goals through the lens of network sharpness. The AdaSAP method produces\nsparse networks that are robust to input variations which are unseen at\ntraining time. We achieve this by strategically incorporating weight\nperturbations in order to optimize the loss landscape. This allows the model to\nbe both primed for pruning and regularized for improved robustness. AdaSAP\nimproves the robust accuracy of pruned models on image classification by up to\n+6% on ImageNet C and +4% on ImageNet V2, and on object detection by +4% on a\ncorrupted Pascal VOC dataset, over a wide range of compression ratios, pruning\ncriteria, and network architectures, outperforming recent pruning art by large\nmargins.\n","authors":["Anna Bair","Hongxu Yin","Maying Shen","Pavlo Molchanov","Jose Alvarez"],"pdf_url":"https://arxiv.org/pdf/2306.14306v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.14615v2","updated":"2024-03-13T17:11:20Z","published":"2023-02-24T01:26:56Z","title":"Randomized Kaczmarz in Adversarial Distributed Setting","summary":"  Developing large-scale distributed methods that are robust to the presence of\nadversarial or corrupted workers is an important part of making such methods\npractical for real-world problems. In this paper, we propose an iterative\napproach that is adversary-tolerant for convex optimization problems. By\nleveraging simple statistics, our method ensures convergence and is capable of\nadapting to adversarial distributions. Additionally, the efficiency of the\nproposed methods for solving convex problems is shown in simulations with the\npresence of adversaries. Through simulations, we demonstrate the efficiency of\nour approach in the presence of adversaries and its ability to identify\nadversarial workers with high accuracy and tolerate varying levels of adversary\nrates.\n","authors":["Longxiu Huang","Xia Li","Deanna Needell"],"pdf_url":"https://arxiv.org/pdf/2302.14615v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07793v4","updated":"2024-03-13T17:10:48Z","published":"2023-10-11T18:27:12Z","title":"GenTKG: Generative Forecasting on Temporal Knowledge Graph","summary":"  The rapid advancements in large language models (LLMs) have ignited interest\nin the temporal knowledge graph (tKG) domain, where conventional\nembedding-based and rule-based methods dominate. The question remains open of\nwhether pre-trained LLMs can understand structured temporal relational data and\nreplace them as the foundation model for temporal relational forecasting.\nTherefore, we bring temporal knowledge forecasting into the generative setting.\nHowever, challenges occur in the huge chasms between complex temporal graph\ndata structure and sequential natural expressions LLMs can handle, and between\nthe enormous data sizes of tKGs and heavy computation costs of finetuning LLMs.\nTo address these challenges, we propose a novel retrieval-augmented generation\nframework named GenTKG combining a temporal logical rule-based retrieval\nstrategy and few-shot parameter-efficient instruction tuning to solve the above\nchallenges, respectively. Extensive experiments have shown that GenTKG\noutperforms conventional methods of temporal relational forecasting with low\ncomputation resources using extremely limited training data as few as 16\nsamples. GenTKG also highlights remarkable cross-domain generalizability with\noutperforming performance on unseen datasets without re-training, and in-domain\ngeneralizability regardless of time split in the same dataset. Our work reveals\nthe huge potential of LLMs in the tKG domain and opens a new frontier for\ngenerative forecasting on tKGs. Code and data are released here:\nhttps://github.com/mayhugotong/GenTKG.\n","authors":["Ruotong Liao","Xu Jia","Yunpu Ma","Yangzhe Li","Volker Tresp"],"pdf_url":"https://arxiv.org/pdf/2310.07793v4.pdf","comment":"14 pages, Findings of NAACL 2024"},{"id":"http://arxiv.org/abs/2403.08700v1","updated":"2024-03-13T17:04:56Z","published":"2024-03-13T17:04:56Z","title":"Diffusion-based Iterative Counterfactual Explanations for Fetal\n  Ultrasound Image Quality Assessment","summary":"  Obstetric ultrasound image quality is crucial for accurate diagnosis and\nmonitoring of fetal health. However, producing high-quality standard planes is\ndifficult, influenced by the sonographer's expertise and factors like the\nmaternal BMI or the fetus dynamics. In this work, we propose using\ndiffusion-based counterfactual explainable AI to generate realistic\nhigh-quality standard planes from low-quality non-standard ones. Through\nquantitative and qualitative evaluation, we demonstrate the effectiveness of\nour method in producing plausible counterfactuals of increased quality. This\nshows future promise both for enhancing training of clinicians by providing\nvisual feedback, as well as for improving image quality and, consequently,\ndownstream diagnosis and monitoring.\n","authors":["Paraskevas Pegios","Manxi Lin","Nina Weng","Morten Bo Søndergaard Svendsen","Zahra Bashir","Siavash Bigdeli","Anders Nymark Christensen","Martin Tolsgaard","Aasa Feragen"],"pdf_url":"https://arxiv.org/pdf/2403.08700v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.16664v2","updated":"2024-03-13T17:03:40Z","published":"2024-01-30T01:28:48Z","title":"Fast Dual-Regularized Autoencoder for Sparse Biological Data","summary":"  Relationship inference from sparse data is an important task with\napplications ranging from product recommendation to drug discovery. A recently\nproposed linear model for sparse matrix completion has demonstrated surprising\nadvantage in speed and accuracy over more sophisticated recommender systems\nalgorithms. Here we extend the linear model to develop a shallow autoencoder\nfor the dual neighborhood-regularized matrix completion problem. We demonstrate\nthe speed and accuracy advantage of our approach over the existing\nstate-of-the-art in predicting drug-target interactions and drug-disease\nassociations.\n","authors":["Aleksandar Poleksic"],"pdf_url":"https://arxiv.org/pdf/2401.16664v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08699v1","updated":"2024-03-13T17:02:27Z","published":"2024-03-13T17:02:27Z","title":"Implicit Regularization of Gradient Flow on One-Layer Softmax Attention","summary":"  We study gradient flow on the exponential loss for a classification problem\nwith a one-layer softmax attention model, where the key and query weight\nmatrices are trained separately. Under a separability assumption on the data,\nwe show that when gradient flow achieves the minimal loss value, it further\nimplicitly minimizes the nuclear norm of the product of the key and query\nweight matrices. Such implicit regularization can be described by a Support\nVector Machine (SVM) problem with respect to the attention weights. This\nfinding contrasts with prior results showing that the gradient descent induces\nan implicit regularization on the Frobenius norm on the product weight matrix\nwhen the key and query matrices are combined into a single weight matrix for\ntraining. For diagonal key and query matrices, our analysis builds upon the\nreparameterization technique and exploits approximate KKT conditions of the SVM\nassociated with the classification data. Moreover, the results are extended to\ngeneral weights configurations given proper alignment of the weight matrices'\nsingular spaces with the data features at initialization.\n","authors":["Heejune Sheen","Siyu Chen","Tianhao Wang","Harrison H. Zhou"],"pdf_url":"https://arxiv.org/pdf/2403.08699v1.pdf","comment":"34 pages"},{"id":"http://arxiv.org/abs/2403.06829v2","updated":"2024-03-13T17:01:57Z","published":"2024-03-11T15:44:40Z","title":"Constructing Variables Using Classifiers as an Aid to Regression: An\n  Empirical Assessment","summary":"  This paper proposes a method for the automatic creation of variables (in the\ncase of regression) that complement the information contained in the initial\ninput vector. The method works as a pre-processing step in which the continuous\nvalues of the variable to be regressed are discretized into a set of intervals\nwhich are then used to define value thresholds. Then classifiers are trained to\npredict whether the value to be regressed is less than or equal to each of\nthese thresholds. The different outputs of the classifiers are then\nconcatenated in the form of an additional vector of variables that enriches the\ninitial vector of the regression problem. The implemented system can thus be\nconsidered as a generic pre-processing tool. We tested the proposed enrichment\nmethod with 5 types of regressors and evaluated it in 33 regression datasets.\nOur experimental results confirm the interest of the approach.\n","authors":["Colin Troisemaine","Vincent Lemaire"],"pdf_url":"https://arxiv.org/pdf/2403.06829v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.01082v2","updated":"2024-03-13T16:48:27Z","published":"2023-10-02T10:48:42Z","title":"Linear attention is (maybe) all you need (to understand transformer\n  optimization)","summary":"  Transformer training is notoriously difficult, requiring a careful design of\noptimizers and use of various heuristics. We make progress towards\nunderstanding the subtleties of training Transformers by carefully studying a\nsimple yet canonical linearized shallow Transformer model. Specifically, we\ntrain linear Transformers to solve regression tasks, inspired by J.~von Oswald\net al.~(ICML 2023), and K.~Ahn et al.~(NeurIPS 2023). Most importantly, we\nobserve that our proposed linearized models can reproduce several prominent\naspects of Transformer training dynamics. Consequently, the results obtained in\nthis paper suggest that a simple linearized Transformer model could actually be\na valuable, realistic abstraction for understanding Transformer optimization.\n","authors":["Kwangjun Ahn","Xiang Cheng","Minhak Song","Chulhee Yun","Ali Jadbabaie","Suvrit Sra"],"pdf_url":"https://arxiv.org/pdf/2310.01082v2.pdf","comment":"Published at ICLR 2024"},{"id":"http://arxiv.org/abs/2403.08687v1","updated":"2024-03-13T16:44:36Z","published":"2024-03-13T16:44:36Z","title":"Digital Twin-assisted Reinforcement Learning for Resource-aware\n  Microservice Offloading in Edge Computing","summary":"  Collaborative edge computing (CEC) has emerged as a promising paradigm,\nenabling edge nodes to collaborate and execute microservices from end devices.\nMicroservice offloading, a fundamentally important problem, decides when and\nwhere microservices are executed upon the arrival of services. However, the\ndynamic nature of the real-world CEC environment often leads to inefficient\nmicroservice offloading strategies, resulting in underutilized resources and\nnetwork congestion. To address this challenge, we formulate an online joint\nmicroservice offloading and bandwidth allocation problem, JMOBA, to minimize\nthe average completion time of services. In this paper, we introduce a novel\nmicroservice offloading algorithm, DTDRLMO, which leverages deep reinforcement\nlearning (DRL) and digital twin technology. Specifically, we employ digital\ntwin techniques to predict and adapt to changing edge node loads and network\nconditions of CEC in real-time. Furthermore, this approach enables the\ngeneration of an efficient offloading plan, selecting the most suitable edge\nnode for each microservice. Simulation results on real-world and synthetic\ndatasets demonstrate that DTDRLMO outperforms heuristic and learning-based\nmethods in average service completion time.\n","authors":["Xiangchun Chen","Jiannong Cao","Zhixuan Liang","Yuvraj Sahni","Mingjin Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.08687v1.pdf","comment":"9 pages, 5 figures"},{"id":"http://arxiv.org/abs/2312.11598v2","updated":"2024-03-13T16:29:50Z","published":"2023-12-18T18:16:52Z","title":"SkillDiffuser: Interpretable Hierarchical Planning via Skill\n  Abstractions in Diffusion-Based Task Execution","summary":"  Diffusion models have demonstrated strong potential for robotic trajectory\nplanning. However, generating coherent trajectories from high-level\ninstructions remains challenging, especially for long-range composition tasks\nrequiring multiple sequential skills. We propose SkillDiffuser, an end-to-end\nhierarchical planning framework integrating interpretable skill learning with\nconditional diffusion planning to address this problem. At the higher level,\nthe skill abstraction module learns discrete, human-understandable skill\nrepresentations from visual observations and language instructions. These\nlearned skill embeddings are then used to condition the diffusion model to\ngenerate customized latent trajectories aligned with the skills. This allows\ngenerating diverse state trajectories that adhere to the learnable skills. By\nintegrating skill learning with conditional trajectory generation,\nSkillDiffuser produces coherent behavior following abstract instructions across\ndiverse tasks. Experiments on multi-task robotic manipulation benchmarks like\nMeta-World and LOReL demonstrate state-of-the-art performance and\nhuman-interpretable skill representations from SkillDiffuser. More\nvisualization results and information could be found on our website.\n","authors":["Zhixuan Liang","Yao Mu","Hengbo Ma","Masayoshi Tomizuka","Mingyu Ding","Ping Luo"],"pdf_url":"https://arxiv.org/pdf/2312.11598v2.pdf","comment":"Accepted by CVPR 2024. Camera ready version. Project page:\n  https://skilldiffuser.github.io/"},{"id":"http://arxiv.org/abs/2403.08673v1","updated":"2024-03-13T16:25:55Z","published":"2024-03-13T16:25:55Z","title":"When can we Approximate Wide Contrastive Models with Neural Tangent\n  Kernels and Principal Component Analysis?","summary":"  Contrastive learning is a paradigm for learning representations from\nunlabelled data that has been highly successful for image and text data.\nSeveral recent works have examined contrastive losses to claim that contrastive\nmodels effectively learn spectral embeddings, while few works show relations\nbetween (wide) contrastive models and kernel principal component analysis\n(PCA). However, it is not known if trained contrastive models indeed correspond\nto kernel methods or PCA. In this work, we analyze the training dynamics of\ntwo-layer contrastive models, with non-linear activation, and answer when these\nmodels are close to PCA or kernel methods. It is well known in the supervised\nsetting that neural networks are equivalent to neural tangent kernel (NTK)\nmachines, and that the NTK of infinitely wide networks remains constant during\ntraining. We provide the first convergence results of NTK for contrastive\nlosses, and present a nuanced picture: NTK of wide networks remains almost\nconstant for cosine similarity based contrastive losses, but not for losses\nbased on dot product similarity. We further study the training dynamics of\ncontrastive models with orthogonality constraints on output layer, which is\nimplicitly assumed in works relating contrastive learning to spectral\nembedding. Our deviation bounds suggest that representations learned by\ncontrastive models are close to the principal components of a certain matrix\ncomputed from random features. We empirically show that our theoretical results\npossibly hold beyond two-layer networks.\n","authors":["Gautham Govind Anil","Pascal Esser","Debarghya Ghoshdastidar"],"pdf_url":"https://arxiv.org/pdf/2403.08673v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08664v1","updated":"2024-03-13T16:17:09Z","published":"2024-03-13T16:17:09Z","title":"Zero-shot and Few-shot Generation Strategies for Artificial Clinical\n  Records","summary":"  The challenge of accessing historical patient data for clinical research,\nwhile adhering to privacy regulations, is a significant obstacle in medical\nscience. An innovative approach to circumvent this issue involves utilising\nsynthetic medical records that mirror real patient data without compromising\nindividual privacy. The creation of these synthetic datasets, particularly\nwithout using actual patient data to train Large Language Models (LLMs),\npresents a novel solution as gaining access to sensitive patient information to\ntrain models is also a challenge. This study assesses the capability of the\nLlama 2 LLM to create synthetic medical records that accurately reflect real\npatient information, employing zero-shot and few-shot prompting strategies for\ncomparison against fine-tuned methodologies that do require sensitive patient\ndata during training. We focus on generating synthetic narratives for the\nHistory of Present Illness section, utilising data from the MIMIC-IV dataset\nfor comparison. In this work introduce a novel prompting technique that\nleverages a chain-of-thought approach, enhancing the model's ability to\ngenerate more accurate and contextually relevant medical narratives without\nprior fine-tuning. Our findings suggest that this chain-of-thought prompted\napproach allows the zero-shot model to achieve results on par with those of\nfine-tuned models, based on Rouge metrics evaluation.\n","authors":["Erlend Frayling","Jake Lever","Graham McDonald"],"pdf_url":"https://arxiv.org/pdf/2403.08664v1.pdf","comment":"4 pages"},{"id":"http://arxiv.org/abs/2403.08662v1","updated":"2024-03-13T16:16:20Z","published":"2024-03-13T16:16:20Z","title":"Self-Supervised Learning for Covariance Estimation","summary":"  We consider the use of deep learning for covariance estimation. We propose to\nglobally learn a neural network that will then be applied locally at inference\ntime. Leveraging recent advancements in self-supervised foundational models, we\ntrain the network without any labeling by simply masking different samples and\nlearning to predict their covariance given their surrounding neighbors. The\narchitecture is based on the popular attention mechanism. Its main advantage\nover classical methods is the automatic exploitation of global characteristics\nwithout any distributional assumptions or regularization. It can be pre-trained\nas a foundation model and then be repurposed for various downstream tasks,\ne.g., adaptive target detection in radar or hyperspectral imagery.\n","authors":["Tzvi Diskin","Ami Wiesel"],"pdf_url":"https://arxiv.org/pdf/2403.08662v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.10294v2","updated":"2024-03-13T16:08:01Z","published":"2024-01-17T23:07:59Z","title":"Tight Group-Level DP Guarantees for DP-SGD with Sampling via Mixture of\n  Gaussians Mechanisms","summary":"  We give a procedure for computing group-level $(\\epsilon, \\delta)$-DP\nguarantees for DP-SGD, when using Poisson sampling or fixed batch size\nsampling. Up to discretization errors in the implementation, the DP guarantees\ncomputed by this procedure are tight (assuming we release every intermediate\niterate).\n","authors":["Arun Ganesh"],"pdf_url":"https://arxiv.org/pdf/2401.10294v2.pdf","comment":"v2: Added links to open-source implementation of PLD accounting for\n  MoG mechanisms"},{"id":"http://arxiv.org/abs/2403.08652v1","updated":"2024-03-13T16:06:26Z","published":"2024-03-13T16:06:26Z","title":"Extracting Explanations, Justification, and Uncertainty from Black-Box\n  Deep Neural Networks","summary":"  Deep Neural Networks (DNNs) do not inherently compute or exhibit\nempirically-justified task confidence. In mission critical applications, it is\nimportant to both understand associated DNN reasoning and its supporting\nevidence. In this paper, we propose a novel Bayesian approach to extract\nexplanations, justifications, and uncertainty estimates from DNNs. Our approach\nis efficient both in terms of memory and computation, and can be applied to any\nblack box DNN without any retraining, including applications to anomaly\ndetection and out-of-distribution detection tasks. We validate our approach on\nthe CIFAR-10 dataset, and show that it can significantly improve the\ninterpretability and reliability of DNNs.\n","authors":["Paul Ardis","Arjuna Flenner"],"pdf_url":"https://arxiv.org/pdf/2403.08652v1.pdf","comment":"8 pages, 5 figures, SPIE DCS 2024"},{"id":"http://arxiv.org/abs/2302.08913v4","updated":"2024-03-13T16:04:03Z","published":"2023-02-04T15:55:23Z","title":"Referential communication in heterogeneous communities of pre-trained\n  visual deep networks","summary":"  As large pre-trained image-processing neural networks are being embedded in\nautonomous agents such as self-driving cars or robots, the question arises of\nhow such systems can communicate with each other about the surrounding world,\ndespite their different architectures and training regimes. As a first step in\nthis direction, we systematically explore the task of \\textit{referential\ncommunication} in a community of heterogeneous state-of-the-art pre-trained\nvisual networks, showing that they can develop, in a self-supervised way, a\nshared protocol to refer to a target object among a set of candidates. This\nshared protocol can also be used, to some extent, to communicate about\npreviously unseen object categories of different granularity. Moreover, a\nvisual network that was not initially part of an existing community can learn\nthe community's protocol with remarkable ease. Finally, we study, both\nqualitatively and quantitatively, the properties of the emergent protocol,\nproviding some evidence that it is capturing high-level semantic features of\nobjects.\n","authors":["Matéo Mahaut","Francesca Franzon","Roberto Dessì","Marco Baroni"],"pdf_url":"https://arxiv.org/pdf/2302.08913v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08638v1","updated":"2024-03-13T15:51:03Z","published":"2024-03-13T15:51:03Z","title":"Disparate Effect Of Missing Mediators On Transportability of Causal\n  Effects","summary":"  Transported mediation effects provide an avenue to understand how upstream\ninterventions (such as improved neighborhood conditions like green spaces)\nwould work differently when applied to different populations as a result of\nfactors that mediate the effects. However, when mediators are missing in the\npopulation where the effect is to be transported, these estimates could be\nbiased. We study this issue of missing mediators, motivated by challenges in\npublic health, wherein mediators can be missing, not at random. We propose a\nsensitivity analysis framework that quantifies the impact of missing mediator\ndata on transported mediation effects. This framework enables us to identify\nthe settings under which the conditional transported mediation effect is\nrendered insignificant for the subgroup with missing mediator data.\nSpecifically, we provide the bounds on the transported mediation effect as a\nfunction of missingness. We then apply the framework to longitudinal data from\nthe Moving to Opportunity Study, a large-scale housing voucher experiment, to\nquantify the effect of missing mediators on transport effect estimates of\nvoucher receipt, an upstream intervention on living location, in childhood on\nsubsequent risk of mental health or substance use disorder mediated through\nparental health across sites. Our findings provide a tangible understanding of\nhow much missing data can be withstood for unbiased effect estimates.\n","authors":["Vishwali Mhasawade","Rumi Chunara"],"pdf_url":"https://arxiv.org/pdf/2403.08638v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08635v1","updated":"2024-03-13T15:47:26Z","published":"2024-03-13T15:47:26Z","title":"Human Alignment of Large Language Models through Online Preference\n  Optimisation","summary":"  Ensuring alignment of language models' outputs with human preferences is\ncritical to guarantee a useful, safe, and pleasant user experience. Thus, human\nalignment has been extensively studied recently and several methods such as\nReinforcement Learning from Human Feedback (RLHF), Direct Policy Optimisation\n(DPO) and Sequence Likelihood Calibration (SLiC) have emerged. In this paper,\nour contribution is two-fold. First, we show the equivalence between two recent\nalignment methods, namely Identity Policy Optimisation (IPO) and Nash Mirror\nDescent (Nash-MD). Second, we introduce a generalisation of IPO, named IPO-MD,\nthat leverages the regularised sampling approach proposed by Nash-MD.\n  This equivalence may seem surprising at first sight, since IPO is an offline\nmethod whereas Nash-MD is an online method using a preference model. However,\nthis equivalence can be proven when we consider the online version of IPO, that\nis when both generations are sampled by the online policy and annotated by a\ntrained preference model. Optimising the IPO loss with such a stream of data\nbecomes then equivalent to finding the Nash equilibrium of the preference model\nthrough self-play. Building on this equivalence, we introduce the IPO-MD\nalgorithm that generates data with a mixture policy (between the online and\nreference policy) similarly as the general Nash-MD algorithm. We compare\nonline-IPO and IPO-MD to different online versions of existing losses on\npreference data such as DPO and SLiC on a summarisation task.\n","authors":["Daniele Calandriello","Daniel Guo","Remi Munos","Mark Rowland","Yunhao Tang","Bernardo Avila Pires","Pierre Harvey Richemond","Charline Le Lan","Michal Valko","Tianqi Liu","Rishabh Joshi","Zeyu Zheng","Bilal Piot"],"pdf_url":"https://arxiv.org/pdf/2403.08635v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08632v1","updated":"2024-03-13T15:46:37Z","published":"2024-03-13T15:46:37Z","title":"A Decade's Battle on Dataset Bias: Are We There Yet?","summary":"  We revisit the \"dataset classification\" experiment suggested by Torralba and\nEfros a decade ago, in the new era with large-scale, diverse, and hopefully\nless biased datasets as well as more capable neural network architectures.\nSurprisingly, we observe that modern neural networks can achieve excellent\naccuracy in classifying which dataset an image is from: e.g., we report 84.7%\naccuracy on held-out validation data for the three-way classification problem\nconsisting of the YFCC, CC, and DataComp datasets. Our further experiments show\nthat such a dataset classifier could learn semantic features that are\ngeneralizable and transferable, which cannot be simply explained by\nmemorization. We hope our discovery will inspire the community to rethink the\nissue involving dataset bias and model capabilities.\n","authors":["Zhuang Liu","Kaiming He"],"pdf_url":"https://arxiv.org/pdf/2403.08632v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08630v1","updated":"2024-03-13T15:45:29Z","published":"2024-03-13T15:45:29Z","title":"Leveraging Non-Decimated Wavelet Packet Features and Transformer Models\n  for Time Series Forecasting","summary":"  This article combines wavelet analysis techniques with machine learning\nmethods for univariate time series forecasting, focusing on three main\ncontributions. Firstly, we consider the use of Daubechies wavelets with\ndifferent numbers of vanishing moments as input features to both non-temporal\nand temporal forecasting methods, by selecting these numbers during the\ncross-validation phase. Secondly, we compare the use of both the non-decimated\nwavelet transform and the non-decimated wavelet packet transform for computing\nthese features, the latter providing a much larger set of potentially useful\ncoefficient vectors. The wavelet coefficients are computed using a shifted\nversion of the typical pyramidal algorithm to ensure no leakage of future\ninformation into these inputs. Thirdly, we evaluate the use of these wavelet\nfeatures on a significantly wider set of forecasting methods than previous\nstudies, including both temporal and non-temporal models, and both statistical\nand deep learning-based methods. The latter include state-of-the-art\ntransformer-based neural network architectures. Our experiments suggest\nsignificant benefit in replacing higher-order lagged features with wavelet\nfeatures across all examined non-temporal methods for one-step-forward\nforecasting, and modest benefit when used as inputs for temporal deep\nlearning-based models for long-horizon forecasting.\n","authors":["Guy P Nason","James L. Wei"],"pdf_url":"https://arxiv.org/pdf/2403.08630v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08627v1","updated":"2024-03-13T15:40:17Z","published":"2024-03-13T15:40:17Z","title":"Multifidelity linear regression for scientific machine learning from\n  scarce data","summary":"  Machine learning (ML) methods, which fit to data the parameters of a given\nparameterized model class, have garnered significant interest as potential\nmethods for learning surrogate models for complex engineering systems for which\ntraditional simulation is expensive. However, in many scientific and\nengineering settings, generating high-fidelity data on which to train ML models\nis expensive, and the available budget for generating training data is limited.\nML models trained on the resulting scarce high-fidelity data have high variance\nand are sensitive to vagaries of the training data set. We propose a new\nmultifidelity training approach for scientific machine learning that exploits\nthe scientific context where data of varying fidelities and costs are\navailable; for example high-fidelity data may be generated by an expensive\nfully resolved physics simulation whereas lower-fidelity data may arise from a\ncheaper model based on simplifying assumptions. We use the multifidelity data\nto define new multifidelity Monte Carlo estimators for the unknown parameters\nof linear regression models, and provide theoretical analyses that guarantee\nthe approach's accuracy and improved robustness to small training budgets.\nNumerical results verify the theoretical analysis and demonstrate that\nmultifidelity learned models trained on scarce high-fidelity data and\nadditional low-fidelity data achieve order-of-magnitude lower model variance\nthan standard models trained on only high-fidelity data of comparable cost.\nThis illustrates that in the scarce data regime, our multifidelity training\nstrategy yields models with lower expected error than standard training\napproaches.\n","authors":["Elizabeth Qian","Anirban Chaudhuri","Dayoung Kang","Vignesh Sella"],"pdf_url":"https://arxiv.org/pdf/2403.08627v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08618v1","updated":"2024-03-13T15:32:08Z","published":"2024-03-13T15:32:08Z","title":"Verifix: Post-Training Correction to Improve Label Noise Robustness with\n  Verified Samples","summary":"  Label corruption, where training samples have incorrect labels, can\nsignificantly degrade the performance of machine learning models. This\ncorruption often arises from non-expert labeling or adversarial attacks.\nAcquiring large, perfectly labeled datasets is costly, and retraining large\nmodels from scratch when a clean dataset becomes available is computationally\nexpensive. To address this challenge, we propose Post-Training Correction, a\nnew paradigm that adjusts model parameters after initial training to mitigate\nlabel noise, eliminating the need for retraining. We introduce Verifix, a novel\nSingular Value Decomposition (SVD) based algorithm that leverages a small,\nverified dataset to correct the model weights using a single update. Verifix\nuses SVD to estimate a Clean Activation Space and then projects the model's\nweights onto this space to suppress activations corresponding to corrupted\ndata. We demonstrate Verifix's effectiveness on both synthetic and real-world\nlabel noise. Experiments on the CIFAR dataset with 25% synthetic corruption\nshow 7.36% generalization improvements on average. Additionally, we observe\ngeneralization improvements of up to 2.63% on naturally corrupted datasets like\nWebVision1.0 and Clothing1M.\n","authors":["Sangamesh Kodge","Deepak Ravikumar","Gobinda Saha","Kaushik Roy"],"pdf_url":"https://arxiv.org/pdf/2403.08618v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09481v2","updated":"2024-03-13T15:24:19Z","published":"2023-12-15T01:38:26Z","title":"Continual Adversarial Defense","summary":"  In response to the rapidly evolving nature of adversarial attacks against\nvisual classifiers on a monthly basis, numerous defenses have been proposed to\ngeneralize against as many known attacks as possible. However, designing a\ndefense method that generalizes to all types of attacks is not realistic\nbecause the environment in which defense systems operate is dynamic and\ncomprises various unique attacks that emerge as time goes on. The defense\nsystem must gather online few-shot defense feedback to promptly enhance itself,\nleveraging efficient memory utilization. Therefore, we propose the first\ncontinual adversarial defense (CAD) framework that adapts to any attacks in a\ndynamic scenario, where various attacks emerge stage by stage. In practice, CAD\nis modeled under four principles: (1) continual adaptation to new attacks\nwithout catastrophic forgetting, (2) few-shot adaptation, (3) memory-efficient\nadaptation, and (4) high accuracy on both clean and adversarial images. We\nexplore and integrate cutting-edge continual learning, few-shot learning, and\nensemble learning techniques to qualify the principles. Experiments conducted\non CIFAR-10 and ImageNet-100 validate the effectiveness of our approach against\nmultiple stages of modern adversarial attacks and demonstrate significant\nimprovements over numerous baseline methods. In particular, CAD is capable of\nquickly adapting with minimal feedback and a low cost of defense failure, while\nmaintaining good performance against previous attacks. Our research sheds light\non a brand-new paradigm for continual defense adaptation against dynamic and\nevolving attacks.\n","authors":["Qian Wang","Yaoyao Liu","Hefei Ling","Yingwei Li","Qihao Liu","Ping Li","Jiazhong Chen","Alan Yuille","Ning Yu"],"pdf_url":"https://arxiv.org/pdf/2312.09481v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08613v1","updated":"2024-03-13T15:23:55Z","published":"2024-03-13T15:23:55Z","title":"Link Prediction for Social Networks using Representation Learning and\n  Heuristic-based Features","summary":"  The exponential growth in scale and relevance of social networks enable them\nto provide expansive insights. Predicting missing links in social networks\nefficiently can help in various modern-day business applications ranging from\ngenerating recommendations to influence analysis. Several categories of\nsolutions exist for the same. Here, we explore various feature extraction\ntechniques to generate representations of nodes and edges in a social network\nthat allow us to predict missing links. We compare the results of using ten\nfeature extraction techniques categorized across Structural embeddings,\nNeighborhood-based embeddings, Graph Neural Networks, and Graph Heuristics,\nfollowed by modeling with ensemble classifiers and custom Neural Networks.\nFurther, we propose combining heuristic-based features and learned\nrepresentations that demonstrate improved performance for the link prediction\ntask on social network datasets. Using this method to generate accurate\nrecommendations for many applications is a matter of further study that appears\nvery promising. The code for all the experiments has been made public.\n","authors":["Samarth Khanna","Sree Bhattacharyya","Sudipto Ghosh","Kushagra Agarwal","Asit Kumar Das"],"pdf_url":"https://arxiv.org/pdf/2403.08613v1.pdf","comment":"Accepted to the MAISoN Workshop at IJCAI 2023"},{"id":"http://arxiv.org/abs/2307.12375v4","updated":"2024-03-13T15:00:20Z","published":"2023-07-23T16:54:41Z","title":"In-Context Learning Learns Label Relationships but Is Not Conventional\n  Learning","summary":"  The predictions of Large Language Models (LLMs) on downstream tasks often\nimprove significantly when including examples of the input--label relationship\nin the context. However, there is currently no consensus about how this\nin-context learning (ICL) ability of LLMs works. For example, while Xie et al.\n(2021) liken ICL to a general-purpose learning algorithm, Min et al. (2022)\nargue ICL does not even learn label relationships from in-context examples. In\nthis paper, we provide novel insights into how ICL leverages label information,\nrevealing both capabilities and limitations. To ensure we obtain a\ncomprehensive picture of ICL behavior, we study probabilistic aspects of ICL\npredictions and thoroughly examine the dynamics of ICL as more examples are\nprovided. Our experiments show that ICL predictions almost always depend on\nin-context labels and that ICL can learn truly novel tasks in-context. However,\nwe also find that ICL struggles to fully overcome prediction preferences\nacquired from pre-training data and, further, that ICL does not consider all\nin-context information equally.\n","authors":["Jannik Kossen","Yarin Gal","Tom Rainforth"],"pdf_url":"https://arxiv.org/pdf/2307.12375v4.pdf","comment":"Accepted for publication at ICLR 2024"},{"id":"http://arxiv.org/abs/2403.08592v1","updated":"2024-03-13T14:57:10Z","published":"2024-03-13T14:57:10Z","title":"Data-Efficient Sleep Staging with Synthetic Time Series Pretraining","summary":"  Analyzing electroencephalographic (EEG) time series can be challenging,\nespecially with deep neural networks, due to the large variability among human\nsubjects and often small datasets. To address these challenges, various\nstrategies, such as self-supervised learning, have been suggested, but they\ntypically rely on extensive empirical datasets. Inspired by recent advances in\ncomputer vision, we propose a pretraining task termed \"frequency pretraining\"\nto pretrain a neural network for sleep staging by predicting the frequency\ncontent of randomly generated synthetic time series. Our experiments\ndemonstrate that our method surpasses fully supervised learning in scenarios\nwith limited data and few subjects, and matches its performance in regimes with\nmany subjects. Furthermore, our results underline the relevance of frequency\ninformation for sleep stage scoring, while also demonstrating that deep neural\nnetworks utilize information beyond frequencies to enhance sleep staging\nperformance, which is consistent with previous research. We anticipate that our\napproach will be advantageous across a broad spectrum of applications where EEG\ndata is limited or derived from a small number of subjects, including the\ndomain of brain-computer interfaces.\n","authors":["Niklas Grieger","Siamak Mehrkanoon","Stephan Bialonski"],"pdf_url":"https://arxiv.org/pdf/2403.08592v1.pdf","comment":"10 pages, 4 figures"},{"id":"http://arxiv.org/abs/2310.02895v2","updated":"2024-03-13T14:56:11Z","published":"2023-10-04T15:32:27Z","title":"CoLiDE: Concomitant Linear DAG Estimation","summary":"  We deal with the combinatorial problem of learning directed acyclic graph\n(DAG) structure from observational data adhering to a linear structural\nequation model (SEM). Leveraging advances in differentiable, nonconvex\ncharacterizations of acyclicity, recent efforts have advocated a continuous\nconstrained optimization paradigm to efficiently explore the space of DAGs.\nMost existing methods employ lasso-type score functions to guide this search,\nwhich (i) require expensive penalty parameter retuning when the\n$\\textit{unknown}$ SEM noise variances change across problem instances; and\n(ii) implicitly rely on limiting homoscedasticity assumptions. In this work, we\npropose a new convex score function for sparsity-aware learning of linear DAGs,\nwhich incorporates concomitant estimation of scale and thus effectively\ndecouples the sparsity parameter from the exogenous noise levels.\nRegularization via a smooth, nonconvex acyclicity penalty term yields CoLiDE\n($\\textbf{Co}$ncomitant $\\textbf{Li}$near $\\textbf{D}$AG\n$\\textbf{E}$stimation), a regression-based criterion amenable to efficient\ngradient computation and closed-form estimation of noise variances in\nheteroscedastic scenarios. Our algorithm outperforms state-of-the-art methods\nwithout incurring added complexity, especially when the DAGs are larger and the\nnoise level profile is heterogeneous. We also find CoLiDE exhibits enhanced\nstability manifested via reduced standard deviations in several domain-specific\nmetrics, underscoring the robustness of our novel linear DAG estimator.\n","authors":["Seyed Saman Saboksayr","Gonzalo Mateos","Mariano Tepper"],"pdf_url":"https://arxiv.org/pdf/2310.02895v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.03853v2","updated":"2024-03-13T14:52:47Z","published":"2023-12-06T19:07:38Z","title":"Dr. Jekyll and Mr. Hyde: Two Faces of LLMs","summary":"  Only a year ago, we witnessed a rise in the use of Large Language Models\n(LLMs), especially when combined with applications like chatbot assistants.\nSafety mechanisms and specialized training procedures are implemented to\nprevent improper responses from these assistants. In this work, we bypass these\nmeasures for ChatGPT and Bard (and, to some extent, Bing chat) by making them\nimpersonate complex personas with opposite characteristics as those of the\ntruthful assistants they are supposed to be. We start by creating elaborate\nbiographies of these personas, which we then use in a new session with the same\nchatbots. Our conversation followed a role-play style to get the response the\nassistant was not allowed to provide. By making use of personas, we show that\nthe response that is prohibited is actually provided, making it possible to\nobtain unauthorized, illegal, or harmful information. This work shows that by\nusing adversarial personas, one can overcome safety mechanisms set out by\nChatGPT and Bard. We also introduce several ways of activating such adversarial\npersonas, altogether showing that both chatbots are vulnerable to this kind of\nattack. With the same principle, we introduce two defenses that push the model\nto interpret trustworthy personalities and make it more robust against such\nattacks.\n","authors":["Matteo Gioele Collu","Tom Janssen-Groesbeek","Stefanos Koffas","Mauro Conti","Stjepan Picek"],"pdf_url":"https://arxiv.org/pdf/2312.03853v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08589v1","updated":"2024-03-13T14:51:16Z","published":"2024-03-13T14:51:16Z","title":"Can physical information aid the generalization ability of Neural\n  Networks for hydraulic modeling?","summary":"  Application of Neural Networks to river hydraulics is fledgling, despite the\nfield suffering from data scarcity, a challenge for machine learning\ntechniques. Consequently, many purely data-driven Neural Networks proved to\nlack predictive capabilities. In this work, we propose to mitigate such problem\nby introducing physical information into the training phase. The idea is\nborrowed from Physics-Informed Neural Networks which have been recently\nproposed in other contexts. Physics-Informed Neural Networks embed physical\ninformation in the form of the residual of the Partial Differential Equations\n(PDEs) governing the phenomenon and, as such, are conceived as neural solvers,\ni.e. an alternative to traditional numerical solvers. Such approach is seldom\nsuitable for environmental hydraulics, where epistemic uncertainties are large,\nand computing residuals of PDEs exhibits difficulties similar to those faced by\nclassical numerical methods. Instead, we envisaged the employment of Neural\nNetworks as neural operators, featuring physical constraints formulated without\nresorting to PDEs. The proposed novel methodology shares similarities with data\naugmentation and regularization. We show that incorporating such soft physical\ninformation can improve predictive capabilities.\n","authors":["Gianmarco Guglielmo","Andrea Montessori","Jean-Michel Tucny","Michele La Rocca","Pietro Prestininzi"],"pdf_url":"https://arxiv.org/pdf/2403.08589v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10385v3","updated":"2024-03-13T14:48:36Z","published":"2023-12-16T08:48:46Z","title":"Imitate the Good and Avoid the Bad: An Incremental Approach to Safe\n  Reinforcement Learning","summary":"  A popular framework for enforcing safe actions in Reinforcement Learning (RL)\nis Constrained RL, where trajectory based constraints on expected cost (or\nother cost measures) are employed to enforce safety and more importantly these\nconstraints are enforced while maximizing expected reward. Most recent\napproaches for solving Constrained RL convert the trajectory based cost\nconstraint into a surrogate problem that can be solved using minor\nmodifications to RL methods. A key drawback with such approaches is an over or\nunderestimation of the cost constraint at each state. Therefore, we provide an\napproach that does not modify the trajectory based cost constraint and instead\nimitates ``good'' trajectories and avoids ``bad'' trajectories generated from\nincrementally improving policies. We employ an oracle that utilizes a reward\nthreshold (which is varied with learning) and the overall cost constraint to\nlabel trajectories as ``good'' or ``bad''. A key advantage of our approach is\nthat we are able to work from any starting policy or set of trajectories and\nimprove on it. In an exhaustive set of experiments, we demonstrate that our\napproach is able to outperform top benchmark approaches for solving Constrained\nRL problems, with respect to expected cost, CVaR cost, or even unknown cost\nconstraints.\n","authors":["Huy Hoang","Tien Mai","Pradeep Varakantham"],"pdf_url":"https://arxiv.org/pdf/2312.10385v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.11155v5","updated":"2024-03-13T14:46:05Z","published":"2022-03-16T12:23:25Z","title":"A New Quantum CNN Model for Image Classification","summary":"  Quantum density matrix represents all the information of the entire quantum\nsystem, and novel models of meaning employing density matrices naturally model\nlinguistic phenomena such as hyponymy and linguistic ambiguity, among others in\nquantum question answering tasks. Naturally, we argue that the quantum density\nmatrix can enhance the image feature information and the relationship between\nthe features for the classical image classification. Specifically, we (i)\ncombine density matrices and CNN to design a new mechanism; (ii) apply the new\nmechanism to some representative classical image classification tasks. A series\nof experiments show that the application of quantum density matrix in image\nclassification has the generalization and high efficiency on different\ndatasets. The application of quantum density matrix both in classical question\nanswering tasks and classical image classification tasks show more effective\nperformance.\n","authors":["X. Q. Zhao","T. L. Chen"],"pdf_url":"https://arxiv.org/pdf/2203.11155v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08585v1","updated":"2024-03-13T14:42:06Z","published":"2024-03-13T14:42:06Z","title":"Improving Implicit Regularization of SGD with Preconditioning for Least\n  Square Problems","summary":"  Stochastic gradient descent (SGD) exhibits strong algorithmic regularization\neffects in practice and plays an important role in the generalization of modern\nmachine learning. However, prior research has revealed instances where the\ngeneralization performance of SGD is worse than ridge regression due to uneven\noptimization along different dimensions. Preconditioning offers a natural\nsolution to this issue by rebalancing optimization across different directions.\nYet, the extent to which preconditioning can enhance the generalization\nperformance of SGD and whether it can bridge the existing gap with ridge\nregression remains uncertain. In this paper, we study the generalization\nperformance of SGD with preconditioning for the least squared problem. We make\na comprehensive comparison between preconditioned SGD and (standard \\&\npreconditioned) ridge regression. Our study makes several key contributions\ntoward understanding and improving SGD with preconditioning. First, we\nestablish excess risk bounds (generalization performance) for preconditioned\nSGD and ridge regression under an arbitrary preconditions matrix. Second,\nleveraging the excessive risk characterization of preconditioned SGD and ridge\nregression, we show that (through construction) there exists a simple\npreconditioned matrix that can outperform (standard \\& preconditioned) ridge\nregression. Finally, we show that our proposed preconditioning matrix is\nstraightforward enough to allow robust estimation from finite samples while\nmaintaining a theoretical advantage over ridge regression. Our empirical\nresults align with our theoretical findings, collectively showcasing the\nenhanced regularization effect of preconditioned SGD.\n","authors":["Junwei Su","Difan Zou","Chuan Wu"],"pdf_url":"https://arxiv.org/pdf/2403.08585v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08584v1","updated":"2024-03-13T14:37:00Z","published":"2024-03-13T14:37:00Z","title":"Local Binary and Multiclass SVMs Trained on a Quantum Annealer","summary":"  Support vector machines (SVMs) are widely used machine learning models (e.g.,\nin remote sensing), with formulations for both classification and regression\ntasks. In the last years, with the advent of working quantum annealers, hybrid\nSVM models characterised by quantum training and classical execution have been\nintroduced. These models have demonstrated comparable performance to their\nclassical counterparts. However, they are limited in the training set size due\nto the restricted connectivity of the current quantum annealers. Hence, to take\nadvantage of large datasets (like those related to Earth observation), a\nstrategy is required. In the classical domain, local SVMs, namely, SVMs trained\non the data samples selected by a k-nearest neighbors model, have already\nproven successful. Here, the local application of quantum-trained SVM models is\nproposed and empirically assessed. In particular, this approach allows\novercoming the constraints on the training set size of the quantum-trained\nmodels while enhancing their performance. In practice, the FaLK-SVM method,\ndesigned for efficient local SVMs, has been combined with quantum-trained SVM\nmodels for binary and multiclass classification. In addition, for comparison,\nFaLK-SVM has been interfaced for the first time with a classical single-step\nmulticlass SVM model (CS SVM). Concerning the empirical evaluation, D-Wave's\nquantum annealers and real-world datasets taken from the remote sensing domain\nhave been employed. The results have shown the effectiveness and scalability of\nthe proposed approach, but also its practical applicability in a real-world\nlarge-scale scenario.\n","authors":["Enrico Zardini","Amer Delilbasic","Enrico Blanzieri","Gabriele Cavallaro","Davide Pastorello"],"pdf_url":"https://arxiv.org/pdf/2403.08584v1.pdf","comment":"12 pages, 1 figure, 11 tables"},{"id":"http://arxiv.org/abs/2403.08579v1","updated":"2024-03-13T14:34:34Z","published":"2024-03-13T14:34:34Z","title":"Machine Learning Optimized Orthogonal Basis Piecewise Polynomial\n  Approximation","summary":"  Piecewise Polynomials (PPs) are utilized in several engineering disciplines,\nlike trajectory planning, to approximate position profiles given in the form of\na set of points. While the approximation target along with domain-specific\nrequirements, like Ck -continuity, can be formulated as a system of equations\nand a result can be computed directly, such closed-form solutions posses\nlimited flexibility with respect to polynomial degrees, polynomial bases or\nadding further domain-specific requirements. Sufficiently complex optimization\ngoals soon call for the use of numerical methods, like gradient descent. Since\ngradient descent lies at the heart of training Artificial Neural Networks\n(ANNs), modern Machine Learning (ML) frameworks like TensorFlow come with a set\nof gradient-based optimizers potentially suitable for a wide range of\noptimization problems beyond the training task for ANNs. Our approach is to\nutilize the versatility of PP models and combine it with the potential of\nmodern ML optimizers for the use in function approximation in 1D trajectory\nplanning in the context of electronic cam design. We utilize available\noptimizers of the ML framework TensorFlow directly, outside of the scope of\nANNs, to optimize model parameters of our PP model. In this paper, we show how\nan orthogonal polynomial basis contributes to improving approximation and\ncontinuity optimization performance. Utilizing Chebyshev polynomials of the\nfirst kind, we develop a novel regularization approach enabling clearly\nimproved convergence behavior. We show that, using this regularization\napproach, Chebyshev basis performs better than power basis for all relevant\noptimizers in the combined approximation and continuity optimization setting\nand demonstrate usability of the presented approach within the electronic cam\ndomain.\n","authors":["Hannes Waclawek","Stefan Huber"],"pdf_url":"https://arxiv.org/pdf/2403.08579v1.pdf","comment":"Submitted to LION18"},{"id":"http://arxiv.org/abs/2403.08572v1","updated":"2024-03-13T14:28:02Z","published":"2024-03-13T14:28:02Z","title":"Caformer: Rethinking Time Series Analysis from Causal Perspective","summary":"  Time series analysis is a vital task with broad applications in various\ndomains. However, effectively capturing cross-dimension and cross-time\ndependencies in non-stationary time series poses significant challenges,\nparticularly in the context of environmental factors. The spurious correlation\ninduced by the environment confounds the causal relationships between\ncross-dimension and cross-time dependencies. In this paper, we introduce a\nnovel framework called Caformer (\\underline{\\textbf{Ca}}usal\nTrans\\underline{\\textbf{former}}) for time series analysis from a causal\nperspective. Specifically, our framework comprises three components: Dynamic\nLearner, Environment Learner, and Dependency Learner. The Dynamic Learner\nunveils dynamic interactions among dimensions, the Environment Learner\nmitigates spurious correlations caused by environment with a back-door\nadjustment, and the Dependency Learner aims to infer robust interactions across\nboth time and dimensions. Our Caformer demonstrates consistent state-of-the-art\nperformance across five mainstream time series analysis tasks, including long-\nand short-term forecasting, imputation, classification, and anomaly detection,\nwith proper interpretability.\n","authors":["Kexuan Zhang","Xiaobei Zou","Yang Tang"],"pdf_url":"https://arxiv.org/pdf/2403.08572v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.00902v3","updated":"2024-03-13T14:27:46Z","published":"2023-10-02T04:59:19Z","title":"DataInf: Efficiently Estimating Data Influence in LoRA-tuned LLMs and\n  Diffusion Models","summary":"  Quantifying the impact of training data points is crucial for understanding\nthe outputs of machine learning models and for improving the transparency of\nthe AI pipeline. The influence function is a principled and popular data\nattribution method, but its computational cost often makes it challenging to\nuse. This issue becomes more pronounced in the setting of large language models\nand text-to-image models. In this work, we propose DataInf, an efficient\ninfluence approximation method that is practical for large-scale generative AI\nmodels. Leveraging an easy-to-compute closed-form expression, DataInf\noutperforms existing influence computation algorithms in terms of computational\nand memory efficiency. Our theoretical analysis shows that DataInf is\nparticularly well-suited for parameter-efficient fine-tuning techniques such as\nLoRA. Through systematic empirical evaluations, we show that DataInf accurately\napproximates influence scores and is orders of magnitude faster than existing\nmethods. In applications to RoBERTa-large, Llama-2-13B-chat, and\nstable-diffusion-v1.5 models, DataInf effectively identifies the most\ninfluential fine-tuning examples better than other approximate influence\nscores. Moreover, it can help to identify which data points are mislabeled.\n","authors":["Yongchan Kwon","Eric Wu","Kevin Wu","James Zou"],"pdf_url":"https://arxiv.org/pdf/2310.00902v3.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2403.08569v1","updated":"2024-03-13T14:25:15Z","published":"2024-03-13T14:25:15Z","title":"A Physics-driven GraphSAGE Method for Physical Process Simulations\n  Described by Partial Differential Equations","summary":"  Physics-informed neural networks (PINNs) have successfully addressed various\ncomputational physics problems based on partial differential equations (PDEs).\nHowever, while tackling issues related to irregularities like singularities and\noscillations, trained solutions usually suffer low accuracy. In addition, most\ncurrent works only offer the trained solution for predetermined input\nparameters. If any change occurs in input parameters, transfer learning or\nretraining is required, and traditional numerical techniques also need an\nindependent simulation. In this work, a physics-driven GraphSAGE approach\n(PD-GraphSAGE) based on the Galerkin method and piecewise polynomial nodal\nbasis functions is presented to solve computational problems governed by\nirregular PDEs and to develop parametric PDE surrogate models. This approach\nemploys graph representations of physical domains, thereby reducing the demands\nfor evaluated points due to local refinement. A distance-related edge feature\nand a feature mapping strategy are devised to help training and convergence for\nsingularity and oscillation situations, respectively. The merits of the\nproposed method are demonstrated through a couple of cases. Moreover, the\nrobust PDE surrogate model for heat conduction problems parameterized by the\nGaussian random field source is successfully established, which not only\nprovides the solution accurately but is several times faster than the finite\nelement method in our experiments.\n","authors":["Hang Hu","Sidi Wu","Guoxiong Cai","Na Liu"],"pdf_url":"https://arxiv.org/pdf/2403.08569v1.pdf","comment":"18 pages,11 figures, 3 tables"},{"id":"http://arxiv.org/abs/2309.15048v4","updated":"2024-03-13T14:24:28Z","published":"2023-09-26T16:25:57Z","title":"Class Incremental Learning via Likelihood Ratio Based Task Prediction","summary":"  Class incremental learning (CIL) is a challenging setting of continual\nlearning, which learns a series of tasks sequentially. Each task consists of a\nset of unique classes. The key feature of CIL is that no task identifier (or\ntask-id) is provided at test time. Predicting the task-id for each test sample\nis a challenging problem. An emerging theory-guided approach (called TIL+OOD)\nis to train a task-specific model for each task in a shared network for all\ntasks based on a task-incremental learning (TIL) method to deal with\ncatastrophic forgetting. The model for each task is an out-of-distribution\n(OOD) detector rather than a conventional classifier. The OOD detector can\nperform both within-task (in-distribution (IND)) class prediction and OOD\ndetection. The OOD detection capability is the key to task-id prediction during\ninference. However, this paper argues that using a traditional OOD detector for\ntask-id prediction is sub-optimal because additional information (e.g., the\nreplay data and the learned tasks) available in CIL can be exploited to design\na better and principled method for task-id prediction. We call the new method\nTPL (Task-id Prediction based on Likelihood Ratio). TPL markedly outperforms\nstrong CIL baselines and has negligible catastrophic forgetting. The code of\nTPL is publicly available at https://github.com/linhaowei1/TPL.\n","authors":["Haowei Lin","Yijia Shao","Weinan Qian","Ningxin Pan","Yiduo Guo","Bing Liu"],"pdf_url":"https://arxiv.org/pdf/2309.15048v4.pdf","comment":null}],"Computer Science and Game Theory":[{"id":"http://arxiv.org/abs/2403.08741v1","updated":"2024-03-13T17:44:16Z","published":"2024-03-13T17:44:16Z","title":"Learning How to Strategically Disclose Information","summary":"  Strategic information disclosure, in its simplest form, considers a game\nbetween an information provider (sender) who has access to some private\ninformation that an information receiver is interested in. While the receiver\ntakes an action that affects the utilities of both players, the sender can\ndesign information (or modify beliefs) of the receiver through signal\ncommitment, hence posing a Stackelberg game. However, obtaining a Stackelberg\nequilibrium for this game traditionally requires the sender to have access to\nthe receiver's objective. In this work, we consider an online version of\ninformation design where a sender interacts with a receiver of an unknown type\nwho is adversarially chosen at each round. Restricting attention to Gaussian\nprior and quadratic costs for the sender and the receiver, we show that\n$\\mathcal{O}(\\sqrt{T})$ regret is achievable with full information feedback,\nwhere $T$ is the total number of interactions between the sender and the\nreceiver. Further, we propose a novel parametrization that allows the sender to\nachieve $\\mathcal{O}(\\sqrt{T})$ regret for a general convex utility function.\nWe then consider the Bayesian Persuasion problem with an additional cost term\nin the objective function, which penalizes signaling policies that are more\ninformative and obtain $\\mathcal{O}(\\log(T))$ regret. Finally, we establish a\nsublinear regret bound for the partial information feedback setting and provide\nsimulations to support our theoretical results.\n","authors":["Raj Kiriti Velicheti","Melih Bastopcu","S. Rasoul Etesami","Tamer Başar"],"pdf_url":"https://arxiv.org/pdf/2403.08741v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08610v1","updated":"2024-03-13T15:22:18Z","published":"2024-03-13T15:22:18Z","title":"An Algorithmic Theory of Simplicity in Mechanism Design","summary":"  A growing body of work in economics and computation focuses on the trade-off\nbetween implementability and simplicity in mechanism design. The goal is to\ndevelop a theory that not only allows to design an incentive structure easy to\ngrasp for imperfectly rational agents, but also understand the ensuing\nlimitations on the class of mechanisms that enforce it. In this context, the\nconcept of OSP mechanisms has assumed a prominent role since they provably\naccount for the absence of contingent reasoning skills, a specific cognitive\nlimitation. For single-dimensional agents, it is known that OSP mechanisms need\nto use certain greedy algorithms.\n  In this work, we introduce a notion that interpolates between OSP and SOSP, a\nmore stringent notion where agents only plan a subset of their own future\nmoves. We provide an algorithmic characterization of this novel class of\nmechanisms for single-dimensional domains and binary allocation problems, that\nprecisely measures the interplay between simplicity and implementability. We\nbuild on this to show how mechanisms based on reverse greedy algorithms\n(a.k.a., deferred acceptance auctions) are algorithmically more robust to\nimperfectly rationality than those adopting greedy algorithms.\n","authors":["Diodato Ferraioli","Carmine Ventre"],"pdf_url":"https://arxiv.org/pdf/2403.08610v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08421v1","updated":"2024-03-13T11:13:56Z","published":"2024-03-13T11:13:56Z","title":"Measures of relevance to the success of streaming platforms","summary":"  Digital streaming platforms, including Twitch, Spotify, Netflix, Disney, and\nKindle, have emerged as one of the main sources of entertainment with\nsignificant growth potential. Many of these platforms distribute royalties\namong streamers, artists, producers, or writers based on their impact. In this\npaper, we measure the relevance of each of these contributors to the overall\nsuccess of the platform, which is information that can play a key role in\nrevenue allocation. We perform an axiomatic analysis to provide normative\nfoundations for three relevance metrics: the uniform, the proportional, and the\nsubscriber-proportional indicators. The last two indicators implement the\nso-called pro-rata and user-centric models, which are extensively applied to\ndistribute revenues in the music streaming market. The axioms we propose\nformalize different principles of fairness, stability, and non-manipulability,\nand are tailor-made for the streaming context. We complete our analysis with a\ncase study that measures the influence of the 19 most-followed streamers\nworldwide on the Twitch platform.\n","authors":["Juan Carlos Gonçalves-Dosantos","Ricardo Martínez","Joaquín Sánchez-Soriano"],"pdf_url":"https://arxiv.org/pdf/2403.08421v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2201.02323v4","updated":"2024-03-13T09:45:31Z","published":"2022-01-07T04:28:52Z","title":"Distributed Nash Equilibrium Seeking over Time-Varying Directed\n  Communication Networks","summary":"  We study distributed algorithms for finding a Nash equilibrium (NE) in a\nclass of non-cooperative convex games under partial information. Specifically,\neach agent has access only to its own smooth local cost function and can\nreceive information from its neighbors in a time-varying directed communication\nnetwork. To this end, we propose a distributed gradient play algorithm to\ncompute a NE by utilizing local information exchange among the players. In this\nalgorithm, every agent performs a gradient step to minimize its own cost\nfunction while sharing and retrieving information locally among its neighbors.\nThe existing methods impose strong assumptions such as balancedness of the\nmixing matrices and global knowledge of the network communication structure,\nincluding Perron-Frobenius eigenvector of the adjacency matrix and other graph\nconnectivity constants. In contrast, our approach relies only on a reasonable\nand widely-used assumption of row-stochasticity of the mixing matrices. We\nanalyze the algorithm for time-varying directed graphs and prove its\nconvergence to the NE, when the agents' cost functions are strongly convex and\nhave Lipschitz continuous gradients. Numerical simulations are performed for a\nNash-Cournot game to illustrate the efficacy of the proposed algorithm.\n","authors":["Duong Thuy Anh Nguyen","Duong Tung Nguyen","Angelia Nedić"],"pdf_url":"https://arxiv.org/pdf/2201.02323v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.06622v2","updated":"2024-03-13T08:15:50Z","published":"2023-12-08T12:13:26Z","title":"Search and Rescue on a Poset","summary":"  A Search and Rescue game (SR game) is a new type of game on a graph that has\nquickly found applications in scheduling, object detection, and adaptive\nsearch. In this paper, we broaden the definition of SR games by putting them\ninto the context of ordered sets and Bayesian networks, extending known\nsolutions of these games and opening up the way to further applications.\n","authors":["Jan-Tino Brethouwer","Robbert Fokkink"],"pdf_url":"https://arxiv.org/pdf/2312.06622v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08171v1","updated":"2024-03-13T01:51:30Z","published":"2024-03-13T01:51:30Z","title":"Tractable Local Equilibria in Non-Concave Games","summary":"  While Online Gradient Descent and other no-regret learning procedures are\nknown to efficiently converge to coarse correlated equilibrium in games where\neach agent's utility is concave in their own strategy, this is not the case\nwhen the utilities are non-concave, a situation that is common in machine\nlearning applications where the agents' strategies are parameterized by deep\nneural networks, or the agents' utilities are computed by a neural network, or\nboth. Indeed, non-concave games present a host of game-theoretic and\noptimization challenges: (i) Nash equilibria may fail to exist; (ii) local Nash\nequilibria exist but are intractable; and (iii) mixed Nash, correlated, and\ncoarse correlated equilibria have infinite support in general, and are\nintractable. To sidestep these challenges we propose a new solution concept,\ntermed $(\\varepsilon, \\Phi(\\delta))$-local equilibrium, which generalizes local\nNash equilibrium in non-concave games, as well as (coarse) correlated\nequilibrium in concave games. Importantly, we show that two instantiations of\nthis solution concept capture the convergence guarantees of Online Gradient\nDescent and no-regret learning, which we show efficiently converge to this type\nof equilibrium in non-concave games with smooth utilities.\n","authors":["Yang Cai","Constantinos Daskalakis","Haipeng Luo","Chen-Yu Wei","Weiqiang Zheng"],"pdf_url":"https://arxiv.org/pdf/2403.08171v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08145v1","updated":"2024-03-13T00:11:27Z","published":"2024-03-13T00:11:27Z","title":"Algorithmic Information Disclosure in Optimal Auctions","summary":"  This paper studies a joint design problem where a seller can design both the\nsignal structures for the agents to learn their values, and the allocation and\npayment rules for selling the item. In his seminal work, Myerson (1981) shows\nhow to design the optimal auction with exogenous signals. We show that the\nproblem becomes NP-hard when the seller also has the ability to design the\nsignal structures. Our main result is a polynomial-time approximation scheme\n(PTAS) for computing the optimal joint design with at most an $\\epsilon$\nmultiplicative loss in expected revenue. Moreover, we show that in our joint\ndesign problem, the seller can significantly reduce the information rent of the\nagents by providing partial information, which ensures a revenue that is at\nleast $1 - \\frac{1}{e}$ of the optimal welfare for all valuation distributions.\n","authors":["Yang Cai","Yingkai Li","Jinzhao Wu"],"pdf_url":"https://arxiv.org/pdf/2403.08145v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.02779v2","updated":"2024-03-13T22:57:44Z","published":"2023-10-04T12:50:29Z","title":"Expected flow networks in stochastic environments and two-player\n  zero-sum games","summary":"  Generative flow networks (GFlowNets) are sequential sampling models trained\nto match a given distribution. GFlowNets have been successfully applied to\nvarious structured object generation tasks, sampling a diverse set of\nhigh-reward objects quickly. We propose expected flow networks (EFlowNets),\nwhich extend GFlowNets to stochastic environments. We show that EFlowNets\noutperform other GFlowNet formulations in stochastic tasks such as protein\ndesign. We then extend the concept of EFlowNets to adversarial environments,\nproposing adversarial flow networks (AFlowNets) for two-player zero-sum games.\nWe show that AFlowNets learn to find above 80% of optimal moves in Connect-4\nvia self-play and outperform AlphaZero in tournaments.\n","authors":["Marco Jiralerspong","Bilun Sun","Danilo Vucetic","Tianyu Zhang","Yoshua Bengio","Gauthier Gidel","Nikolay Malkin"],"pdf_url":"https://arxiv.org/pdf/2310.02779v2.pdf","comment":"ICLR 2024; code: https://github.com/GFNOrg/AdversarialFlowNetworks"},{"id":"http://arxiv.org/abs/2403.08948v1","updated":"2024-03-13T20:27:35Z","published":"2024-03-13T20:27:35Z","title":"Model-free Resilient Controller Design based on Incentive Feedback\n  Stackelberg Game and Q-learning","summary":"  In the swift evolution of Cyber-Physical Systems (CPSs) within intelligent\nenvironments, especially in the industrial domain shaped by Industry 4.0, the\nsurge in development brings forth unprecedented security challenges. This paper\nexplores the intricate security issues of Industrial CPSs (ICPSs), with a\nspecific focus on the unique threats presented by intelligent attackers capable\nof directly compromising the controller, thereby posing a direct risk to\nphysical security. Within the framework of hierarchical control and incentive\nfeedback Stackelberg game, we design a resilient leading controller (leader)\nthat is adaptive to a compromised following controller (follower) such that the\ncompromised follower acts cooperatively with the leader, aligning its\nstrategies with the leader's objective to achieve a team-optimal solution.\nFirst, we provide sufficient conditions for the existence of an incentive\nStackelberg solution when system dynamics are known. Then, we propose a\nQ-learning-based Approximate Dynamic Programming (ADP) approach, and\ncorresponding algorithms for the online resolution of the incentive Stackelberg\nsolution without requiring prior knowledge of system dynamics. Last but not\nleast, we prove the convergence of our approach to the optimum.\n","authors":["Jiajun Shen","Fengjun Li","Morteza Hashemi","Huazhen Fang"],"pdf_url":"https://arxiv.org/pdf/2403.08948v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2403.08944v1","updated":"2024-03-13T20:21:20Z","published":"2024-03-13T20:21:20Z","title":"Language-based game theory in the age of artificial intelligence","summary":"  Understanding human behaviour in decision problems and strategic interactions\nhas wide-ranging applications in economics, psychology, and artificial\nintelligence. Game theory offers a robust foundation for this understanding,\nbased on the idea that individuals aim to maximize a utility function. However,\nthe exact factors influencing strategy choices remain elusive. While\ntraditional models try to explain human behaviour as a function of the outcomes\nof available actions, recent experimental research reveals that linguistic\ncontent significantly impacts decision-making, thus prompting a paradigm shift\nfrom outcome-based to language-based utility functions. This shift is more\nurgent than ever, given the advancement of generative AI, which has the\npotential to support humans in making critical decisions through language-based\ninteractions. We propose sentiment analysis as a fundamental tool for this\nshift and take an initial step by analyzing 61 experimental instructions from\nthe dictator game, an economic game capturing the balance between self-interest\nand the interest of others, which is at the core of many social interactions.\nOur meta-analysis shows that sentiment analysis can explain human behaviour\nbeyond economic outcomes. We discuss future research directions. We hope this\nwork sets the stage for a novel game theoretical approach that emphasizes the\nimportance of language in human decisions.\n","authors":["Valerio Capraro","Roberto Di Paolo","Matjaz Perc","Veronica Pizziol"],"pdf_url":"https://arxiv.org/pdf/2403.08944v1.pdf","comment":null}],"Information Theory":[{"id":"http://arxiv.org/abs/2403.08741v1","updated":"2024-03-13T17:44:16Z","published":"2024-03-13T17:44:16Z","title":"Learning How to Strategically Disclose Information","summary":"  Strategic information disclosure, in its simplest form, considers a game\nbetween an information provider (sender) who has access to some private\ninformation that an information receiver is interested in. While the receiver\ntakes an action that affects the utilities of both players, the sender can\ndesign information (or modify beliefs) of the receiver through signal\ncommitment, hence posing a Stackelberg game. However, obtaining a Stackelberg\nequilibrium for this game traditionally requires the sender to have access to\nthe receiver's objective. In this work, we consider an online version of\ninformation design where a sender interacts with a receiver of an unknown type\nwho is adversarially chosen at each round. Restricting attention to Gaussian\nprior and quadratic costs for the sender and the receiver, we show that\n$\\mathcal{O}(\\sqrt{T})$ regret is achievable with full information feedback,\nwhere $T$ is the total number of interactions between the sender and the\nreceiver. Further, we propose a novel parametrization that allows the sender to\nachieve $\\mathcal{O}(\\sqrt{T})$ regret for a general convex utility function.\nWe then consider the Bayesian Persuasion problem with an additional cost term\nin the objective function, which penalizes signaling policies that are more\ninformative and obtain $\\mathcal{O}(\\log(T))$ regret. Finally, we establish a\nsublinear regret bound for the partial information feedback setting and provide\nsimulations to support our theoretical results.\n","authors":["Raj Kiriti Velicheti","Melih Bastopcu","S. Rasoul Etesami","Tamer Başar"],"pdf_url":"https://arxiv.org/pdf/2403.08741v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08727v1","updated":"2024-03-13T17:28:06Z","published":"2024-03-13T17:28:06Z","title":"The q-ary Gilbert-Varshamov bound can be improved for all but finitely\n  many positive integers q","summary":"  For any positive integer $q\\geq 2$ and any real number $\\delta\\in(0,1)$, let\n$\\alpha_q(n,\\delta n)$ denote the maximum size of a subset of $\\mathbb{Z}_q^n$\nwith minimum Hamming distance at least $\\delta n$, where\n$\\mathbb{Z}_q=\\{0,1,\\dotsc,q-1\\}$ and $n\\in\\mathbb{N}$. The asymptotic rate\nfunction is defined by $ R_q(\\delta) =\n\\limsup_{n\\rightarrow\\infty}\\frac{1}{n}\\log_q\\alpha_q(n,\\delta n). $ The famous\n$q$-ary asymptotic Gilbert-Varshamov bound, obtained in the 1950s, states that\n\\[ R_q(\\delta) \\geq 1 -\n\\delta\\log_q(q-1)-\\delta\\log_q\\frac{1}{\\delta}-(1-\\delta)\\log_q\\frac{1}{1-\\delta}\n\\stackrel{\\mathrm{def}}{=}R_\\mathrm{GV}(\\delta,q) \\] for all positive integers\n$q\\geq 2$ and $0<\\delta<1-q^{-1}$. In the case that $q$ is an even power of a\nprime with $q\\geq 49$, the $q$-ary Gilbert-Varshamov bound was firstly improved\nby using algebraic geometry codes in the works of Tsfasman, Vladut, and Zink\nand of Ihara in the 1980s. The further investigation in algebraic geometry\ncodes has shown that the $q$-ary Gilbert-Varshamov bound can also be improved\nin the case that $q$ is an odd power of a prime but not a prime with $q > 125$.\nHowever, it remains a long-standing open problem whether the $q$-ary\nGilbert-Varshamov bound would be tight for those infinitely many integers $q$\nwhich is a prime, except for Fermat primes not less than 257, and which is a\ngeneric positive integer not being a prime power.\n  In this paper, we prove that the $q$-ary Gilbert-Varshamov bound can be\nimproved for all but finitely many positive integers $q\\geq 2$. It is shown\nthat $ R_q(1/2) > R_\\mathrm{GV}(1/2,q) $ for all integers $q > \\exp(29)$.\nFurthermore, we show that the growth of the rate function $R_q(\\delta)$ for\n$\\delta\\in(0,1)$ fixed and $q$ growing large has a nontrivial lower bound.\nThese new lower bounds are achieved by using codes from geometry of numbers\nintroduced by Lenstra in the 1980s.\n","authors":["Xue-Bin Liang"],"pdf_url":"https://arxiv.org/pdf/2403.08727v1.pdf","comment":"33 pages"},{"id":"http://arxiv.org/abs/2403.08719v1","updated":"2024-03-13T17:19:05Z","published":"2024-03-13T17:19:05Z","title":"Improved Trade-offs Between Amortization and Download Bandwidth for\n  Linear HSS","summary":"  A Homomorphic Secret Sharing (HSS) scheme is a secret-sharing scheme that\nshares a secret $x$ among $s$ servers, and additionally allows an output client\nto reconstruct some function $f(x)$ using information that can be locally\ncomputed by each server. A key parameter in HSS schemes is download rate, which\nquantifies how much information the output client needs to download from the\nservers. Often, download rate is improved by amortizing over $\\ell$ instances\nof the problem, making $\\ell$ also a key parameter of interest.\n  Recent work (Fosli, Ishai, Kolobov, and Wootters 2022) established a limit on\nthe download rate of linear HSS schemes for computing low-degree polynomials\nand constructed schemes that achieve this optimal download rate; their schemes\nrequired amortization over $\\ell = \\Omega(s \\log(s))$ instances of the problem.\nSubsequent work (Blackwell and Wootters, 2023) completely characterized linear\nHSS schemes that achieve optimal download rate in terms of a coding-theoretic\nnotion termed optimal labelweight codes. A consequence of this characterization\nwas that $\\ell = \\Omega(s \\log(s))$ is in fact necessary to achieve optimal\ndownload rate.\n  In this paper, we characterize all linear HSS schemes, showing that schemes\nof any download rate are equivalent to a generalization of optimal labelweight\ncodes. This equivalence is constructive and provides a way to obtain an\nexplicit linear HSS scheme from any linear code. Using this characterization,\nwe present explicit linear HSS schemes with slightly sub-optimal rate but with\nmuch improved amortization $\\ell = O(s)$. Our constructions are based on\nalgebraic geometry codes (specifically Hermitian codes and Goppa codes).\n","authors":["Keller Blackwell","Mary Wootters"],"pdf_url":"https://arxiv.org/pdf/2403.08719v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2311.14842"},{"id":"http://arxiv.org/abs/2403.08648v1","updated":"2024-03-13T16:02:18Z","published":"2024-03-13T16:02:18Z","title":"Meta Reinforcement Learning for Resource Allocation in Aerial\n  Active-RIS-assisted Networks with Rate-Splitting Multiple Access","summary":"  Mounting a reconfigurable intelligent surface (RIS) on an unmanned aerial\nvehicle (UAV) holds promise for improving traditional terrestrial network\nperformance. Unlike conventional methods deploying passive RIS on UAVs, this\nstudy delves into the efficacy of an aerial active RIS (AARIS). Specifically,\nthe downlink transmission of an AARIS network is investigated, where the base\nstation (BS) leverages rate-splitting multiple access (RSMA) for effective\ninterference management and benefits from the support of an AARIS for jointly\namplifying and reflecting the BS's transmit signals. Considering both the\nnon-trivial energy consumption of the active RIS and the limited energy storage\nof the UAV, we propose an innovative element selection strategy for optimizing\nthe on/off status of RIS elements, which adaptively and remarkably manages the\nsystem's power consumption. To this end, a resource management problem is\nformulated, aiming to maximize the system energy efficiency (EE) by jointly\noptimizing the transmit beamforming at the BS, the element activation, the\nphase shift and the amplification factor at the RIS, the RSMA common data rate\nat users, as well as the UAV's trajectory. Due to the dynamicity nature of UAV\nand user mobility, a deep reinforcement learning (DRL) algorithm is designed\nfor resource allocation, utilizing meta-learning to adaptively handle fast\ntime-varying system dynamics. Simulations indicate that incorporating an active\nRIS at the UAV leads to substantial EE gain, compared to passive RIS-aided UAV.\nWe observe the superiority of the RSMA-based AARIS system in terms of EE,\ncompared to existing approaches adopting non-orthogonal multiple access (NOMA).\n","authors":["Sajad Faramarzi","Sepideh Javadi","Farshad Zeinali","Hosein Zarini","Mohammad Robat Mili","Mehdi Bennis","Yonghui Li","Kai-Kit Wong"],"pdf_url":"https://arxiv.org/pdf/2403.08648v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.15030v3","updated":"2024-03-13T15:24:42Z","published":"2023-09-26T16:00:05Z","title":"Quadratic Detection in Noncoherent Massive SIMO Systems over Correlated\n  Channels","summary":"  With the goal of enabling ultrareliable and low-latency wireless\ncommunications for industrial internet of things (IIoT), this paper studies the\nuse of energy-based modulations in noncoherent massive single-input\nmultiple-output (SIMO) systems. We consider a one-shot communication over a\nchannel with correlated Rayleigh fading and colored Gaussian noise, in which\nthe receiver has statistical channel state information (CSI). We first provide\na theoretical analysis on the limitations of unipolar pulse-amplitude\nmodulation (PAM) in systems of this kind, based on maximum likelihood\ndetection. The existence of a fundamental error floor at high signal-to-noise\nratio (SNR) regimes is proved for constellations with more than two energy\nlevels, when no (statistical) CSI is available at the transmitter. In the main\nbody of the paper, we present a design framework for quadratic detectors that\ngeneralizes the widely-used energy detector, to better exploit the statistical\nknowledge of the channel. This allows us to design receivers optimized\naccording to information-theoretic criteria that exhibit lower error rates at\nmoderate and high SNR. We subsequently derive an analytic approximation for the\nerror probability of a general class of quadratic detectors in the large array\nregime. Finally, we numerically validate it and discuss the outage probability\nof the system.\n","authors":["Marc Vilà-Insa","Aniol Martí","Jaume Riba","Meritxell Lamarca"],"pdf_url":"https://arxiv.org/pdf/2309.15030v3.pdf","comment":"This work has been submitted to the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2403.08411v1","updated":"2024-03-13T10:57:22Z","published":"2024-03-13T10:57:22Z","title":"Robust Distributed Compression with Learned Heegard-Berger Scheme","summary":"  We consider lossy compression of an information source when decoder-only side\ninformation may be absent. This setup, also referred to as the Heegard-Berger\nor Kaspi problem, is a special case of robust distributed source coding.\nBuilding upon previous works on neural network-based distributed compressors\ndeveloped for the decoder-only side information (Wyner-Ziv) case, we propose\nlearning-based schemes that are amenable to the availability of side\ninformation. We find that our learned compressors mimic the achievability part\nof the Heegard-Berger theorem and yield interpretable results operating close\nto information-theoretic bounds. Depending on the availability of the side\ninformation, our neural compressors recover characteristics of the\npoint-to-point (i.e., with no side information) and the Wyner-Ziv coding\nstrategies that include binning in the source space, although no structure\nexploiting knowledge of the source and side information was imposed into the\ndesign.\n","authors":["Eyyup Tasci","Ezgi Ozyilkan","Oguzhan Kubilay Ulger","Elza Erkip"],"pdf_url":"https://arxiv.org/pdf/2403.08411v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09444v2","updated":"2024-03-13T09:22:29Z","published":"2023-11-10T12:09:44Z","title":"BICM-compatible Rate Adaptive Geometric Constellation Shaping Using\n  Optimized Many-to-one Labeling","summary":"  In this paper, a rate adaptive geometric constellation shaping (GCS) scheme\nwhich is fully backward-compatible with existing state of the art\nbit-interleaved coded modulation (BICM) systems is proposed and experimentally\ndemonstrated. The system relies on optimization of the positions of the\nquadrature amplitude modulation (QAM) points on the I/Q plane for maximized\nachievable information rate, while maintaining quantization and fiber nonlinear\nnoise robustness. Furthermore, `dummy' bits are multiplexed with coded bits\nbefore mapping to symbols. Rate adaptivity is achieved by tuning the ratio of\ncoded and `dummy' bits, while maintaining a fixed forward error-correction\nblock and a fixed modulation format size. The points' positions and their\nlabeling are optimized using automatic differentiation. The proposed GCS scheme\nis compared to a time-sharing hybrid (TH) QAM modulation and the now mainstream\nprobabilistic amplitude shaping (PAS) scheme. The TH without shaping is\noutperformed for all studied data rates in a simulated linear channel by up to\n0.7 dB. In a linear channel, PAS is shown to outperform the proposed GCS\nscheme, while similar performances are reported for PAS and the proposed GCS in\na simulated nonlinear fiber channel. The GCS scheme is experimentally\ndemonstrated in a multi-span recirculating loop coherent optical fiber\ntransmission system with a total distance of up to 3000 km. Near-continuous\nzero-error flexible throughput is reported as a function of the transmission\ndistance. Up to 1-2 spans of increased reach gains are achieved at the same net\ndata rate w.r.t. conventional QAM. At a given distance, up to 0.79 bits/2D\nsymbol of gain w.r.t. conventional QAM is achieved. In the experiment, similar\nperformance to PAS is demonstrated.\n","authors":["Metodi Plamenov Yankov","Smaranika Swain","Ognjen Jovanovic","Darko Zibar","Francesco Da Ros"],"pdf_url":"https://arxiv.org/pdf/2312.09444v2.pdf","comment":"Submitted to Journal of Lightwave Technology as a special extended\n  version of a 'top-scored' paper at the European Conference on Optical\n  Communications (ECOC) 2023"},{"id":"http://arxiv.org/abs/2403.08343v1","updated":"2024-03-13T08:48:15Z","published":"2024-03-13T08:48:15Z","title":"Coverage and Rate Analysis for Integrated Sensing and Communication\n  Networks","summary":"  Integrated sensing and communication (ISAC) is increasingly recognized as a\npivotal technology for next-generation cellular networks, offering mutual\nbenefits in both sensing and communication capabilities. This advancement\nnecessitates a re-examination of the fundamental limits within networks where\nthese two functions coexist via shared spectrum and infrastructures. However,\ntraditional stochastic geometry-based performance analyses are confined to\neither communication or sensing networks separately. This paper bridges this\ngap by introducing a generalized stochastic geometry framework in ISAC\nnetworks. Based on this framework, we define and calculate the coverage and\nergodic rate of sensing and communication performance under resource\nconstraints. Then, we shed light on the fundamental limits of ISAC networks by\npresenting theoretical results for the coverage rate of the unified\nperformance, taking into account the coupling effects of dual functions in\ncoexistence networks. Further, we obtain the analytical formulations for\nevaluating the ergodic sensing rate constrained by the maximum communication\nrate, and the ergodic communication rate constrained by the maximum sensing\nrate. Extensive numerical results validate the accuracy of all theoretical\nderivations, and also indicate that denser networks significantly enhance ISAC\ncoverage. Specifically, increasing the base station density from $1$\n$\\text{km}^{-2}$ to $10$ $\\text{km}^{-2}$ can boost the ISAC coverage rate from\n$1.4\\%$ to $39.8\\%$. Further, results also reveal that with the increase of the\nconstrained sensing rate, the ergodic communication rate improves\nsignificantly, but the reverse is not obvious.\n","authors":["Xu Gan","Chongwen Huang","Zhaohui Yang","Xiaoming Chen","Jiguang He","Zhaoyang Zhang","Chau Yuen","Yong Liang Guan","Mérouane Debbah"],"pdf_url":"https://arxiv.org/pdf/2403.08343v1.pdf","comment":null}]},"2024-03-14T00:00:00Z":{"Distributed, Parallel, and Cluster Computing":[{"id":"http://arxiv.org/abs/2403.05861v2","updated":"2024-03-14T14:10:21Z","published":"2024-03-09T10:17:14Z","title":"DeepVM: Integrating Spot and On-Demand VMs for Cost-Efficient Deep\n  Learning Clusters in the Cloud","summary":"  Distributed Deep Learning (DDL), as a paradigm, dictates the use of GPU-based\nclusters as the optimal infrastructure for training large-scale Deep Neural\nNetworks (DNNs). However, the high cost of such resources makes them\ninaccessible to many users. Public cloud services, particularly Spot Virtual\nMachines (VMs), offer a cost-effective alternative, but their unpredictable\navailability poses a significant challenge to the crucial checkpointing process\nin DDL. To address this, we introduce DeepVM, a novel solution that recommends\ncost-effective cluster configurations by intelligently balancing the use of\nSpot and On-Demand VMs. DeepVM leverages a four-stage process that analyzes\ninstance performance using the FLOPP (FLoating-point Operations Per Price)\nmetric, performs architecture-level analysis with linear programming, and\nidentifies the optimal configuration for the user-specific needs. Extensive\nsimulations and real-world deployments in the AWS environment demonstrate that\nDeepVM consistently outperforms other policies, reducing training costs and\noverall makespan. By enabling cost-effective checkpointing with Spot VMs,\nDeepVM opens up DDL to a wider range of users and facilitates a more efficient\ntraining of complex DNNs.\n","authors":["Yoochan Kim","Kihyun Kim","Yonghyeon Cho","Jinwoo Kim","Awais Khan","Ki-Dong Kang","Baik-Song An","Myung-Hoon Cha","Hong-Yeon Kim","Youngjae Kim"],"pdf_url":"https://arxiv.org/pdf/2403.05861v2.pdf","comment":"14 pages, 8 figures"},{"id":"http://arxiv.org/abs/2402.07011v2","updated":"2024-03-14T15:45:56Z","published":"2024-02-10T18:14:57Z","title":"FedImpro: Measuring and Improving Client Update in Federated Learning","summary":"  Federated Learning (FL) models often experience client drift caused by\nheterogeneous data, where the distribution of data differs across clients. To\naddress this issue, advanced research primarily focuses on manipulating the\nexisting gradients to achieve more consistent client models. In this paper, we\npresent an alternative perspective on client drift and aim to mitigate it by\ngenerating improved local models. First, we analyze the generalization\ncontribution of local training and conclude that this generalization\ncontribution is bounded by the conditional Wasserstein distance between the\ndata distribution of different clients. Then, we propose FedImpro, to construct\nsimilar conditional distributions for local training. Specifically, FedImpro\ndecouples the model into high-level and low-level components, and trains the\nhigh-level portion on reconstructed feature distributions. This approach\nenhances the generalization contribution and reduces the dissimilarity of\ngradients in FL. Experimental results show that FedImpro can help FL defend\nagainst data heterogeneity and enhance the generalization performance of the\nmodel.\n","authors":["Zhenheng Tang","Yonggang Zhang","Shaohuai Shi","Xinmei Tian","Tongliang Liu","Bo Han","Xiaowen Chu"],"pdf_url":"https://arxiv.org/pdf/2402.07011v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.16648v2","updated":"2024-03-14T15:06:02Z","published":"2022-11-30T00:32:37Z","title":"COMET: A Comprehensive Cluster Design Methodology for Distributed Deep\n  Learning Training","summary":"  Modern Deep Learning (DL) models have grown to sizes requiring massive\nclusters of specialized, high-end nodes to train. Designing such clusters to\nmaximize both performance and utilization--to amortize their steep cost--is a\nchallenging task requiring careful balance of compute, memory, and network\nresources. Moreover, a plethora of each model's tuning knobs drastically affect\nthe performance, with optimal values often depending on the underlying\ncluster's characteristics, which necessitates a complex cluster-workload\nco-design process. To facilitate the design space exploration of such massive\nDL training clusters, we introduce COMET, a holistic cluster design methodology\nand workflow to jointly study the impact of parallelization strategies and key\ncluster resource provisioning on the performance of distributed DL training. We\ndevelop a step-by-step process to establish a reusable and flexible\nmethodology, and demonstrate its application with case studies of training\nlarge models on cluster configurations of variable compute, memory, and network\nresources. Our case studies demonstrate COMET's utility in identifying\npromising architectural optimization directions and guiding system designers in\nconfiguring key model and cluster parameters. To illustrate, cluster\nconfiguration comparisons identify performance differences of up to 7.7x and\nhighlight performance optimization opportunities of up to 1.4x when employing\nmemory expansion as an optimization technique.\n","authors":["Divya Kiran Kadiyala","Saeed Rashidi","Taekyung Heo","Abhimanyu Rajeshkumar Bambhaniya","Tushar Krishna","Alexandros Daglis"],"pdf_url":"https://arxiv.org/pdf/2211.16648v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09445v1","updated":"2024-03-14T14:42:30Z","published":"2024-03-14T14:42:30Z","title":"Benchmarking Distributed Coordination Systems: A Survey and Analysis","summary":"  Coordination services and protocols are critical components of distributed\nsystems and are essential for providing consistency, fault tolerance, and\nscalability. However, due to lack of a standard benchmarking tool for\ndistributed coordination services, coordination service developers/researchers\neither use a NoSQL standard benchmark and omit evaluating consistency,\ndistribution, and fault-tolerance; or create their own ad-hoc microbenchmarks\nand skip comparability with other services. In this paper, we analyze and\ncompare known and widely used distributed coordination services, their\nevaluations, and the tools used to benchmark those systems. We identify\nimportant requirements of distributed coordination service benchmarking, like\nthe metrics and parameters that need to be evaluated and their evaluation\nsetups and tools.\n","authors":["Bekir Turkkan","Tevfik Kosar","Aleksey Charapko","Ailidani Ailijiang","Murat Demirbas"],"pdf_url":"https://arxiv.org/pdf/2403.09445v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09347v1","updated":"2024-03-14T12:51:58Z","published":"2024-03-14T12:51:58Z","title":"BurstAttention: An Efficient Distributed Attention Framework for\n  Extremely Long Sequences","summary":"  Effective attention modules have played a crucial role in the success of\nTransformer-based large language models (LLMs), but the quadratic time and\nmemory complexities of these attention modules also pose a challenge when\nprocessing long sequences. One potential solution for the long sequence problem\nis to utilize distributed clusters to parallelize the computation of attention\nmodules across multiple devices (e.g., GPUs). However, adopting a distributed\napproach inevitably introduces extra memory overheads to store local attention\nresults and incurs additional communication costs to aggregate local results\ninto global ones. In this paper, we propose a distributed attention framework\nnamed ``BurstAttention'' to optimize memory access and communication operations\nat both the global cluster and local device levels. In our experiments, we\ncompare BurstAttention with other competitive distributed attention solutions\nfor long sequence processing. The experimental results under different length\nsettings demonstrate that BurstAttention offers significant advantages for\nprocessing long sequences compared with these competitive baselines, reducing\n40% communication overheads and achieving 2 X speedup during training 32K\nsequence length on 8 X A100.\n","authors":["Sun Ao","Weilin Zhao","Xu Han","Cheng Yang","Zhiyuan Liu","Chuan Shi","Maosong Sun","Shengnan Wang","Teng Su"],"pdf_url":"https://arxiv.org/pdf/2403.09347v1.pdf","comment":"13 pages, 7 figures"},{"id":"http://arxiv.org/abs/2403.09284v1","updated":"2024-03-14T11:12:10Z","published":"2024-03-14T11:12:10Z","title":"DA-PFL: Dynamic Affinity Aggregation for Personalized Federated Learning","summary":"  Personalized federated learning becomes a hot research topic that can learn a\npersonalized learning model for each client. Existing personalized federated\nlearning models prefer to aggregate similar clients with similar data\ndistribution to improve the performance of learning models. However,\nsimilaritybased personalized federated learning methods may exacerbate the\nclass imbalanced problem. In this paper, we propose a novel Dynamic\nAffinity-based Personalized Federated Learning model (DA-PFL) to alleviate the\nclass imbalanced problem during federated learning. Specifically, we build an\naffinity metric from a complementary perspective to guide which clients should\nbe aggregated. Then we design a dynamic aggregation strategy to dynamically\naggregate clients based on the affinity metric in each round to reduce the\nclass imbalanced risk. Extensive experiments show that the proposed DA-PFL\nmodel can significantly improve the accuracy of each client in three real-world\ndatasets with state-of-the-art comparison methods.\n","authors":["Xu Yang","Jiyuan Feng","Songyue Guo","Ye Wang","Ye Ding","Binxing Fang","Qing Liao"],"pdf_url":"https://arxiv.org/pdf/2403.09284v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09141v1","updated":"2024-03-14T07:40:32Z","published":"2024-03-14T07:40:32Z","title":"Uncertainty Estimation in Multi-Agent Distributed Learning for\n  AI-Enabled Edge Devices","summary":"  Initially considered as low-power units with limited autonomous processing,\nEdge IoT devices have seen a paradigm shift with the introduction of FPGAs and\nAI accelerators. This advancement has vastly amplified their computational\ncapabilities, emphasizing the practicality of edge AI. Such progress introduces\nnew challenges of optimizing AI tasks for the limitations of energy and network\nresources typical in Edge computing environments. Our study explores methods\nthat enable distributed data processing through AI-enabled edge devices,\nenhancing collaborative learning capabilities. A key focus of our research is\nthe challenge of determining confidence levels in learning outcomes,\nconsidering the spatial and temporal variability of data sets encountered by\nindependent agents. To address this issue, we investigate the application of\nBayesian neural networks, proposing a novel approach to manage uncertainty in\ndistributed learning environments.\n","authors":["Gleb Radchenko","Victoria Andrea Fill"],"pdf_url":"https://arxiv.org/pdf/2403.09141v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.04354v3","updated":"2024-03-14T01:34:45Z","published":"2022-06-09T08:55:57Z","title":"Long-Term or Temporary? Hybrid Worker Recruitment for Mobile Crowd\n  Sensing and Computing","summary":"  This paper investigates a novel hybrid worker recruitment problem where the\nmobile crowd sensing and computing (MCSC) platform employs workers to serve\nMCSC tasks with diverse quality requirements and budget constraints, under\nuncertainties in workers' participation and their local workloads.We propose a\nhybrid worker recruitment framework consisting of offline and online trading\nmodes. The former enables the platform to overbook long-term workers (services)\nto cope with dynamic service supply via signing contracts in advance, which is\nformulated as 0-1 integer linear programming (ILP) with probabilistic\nconstraints of service quality and budget.Besides, motivated by the existing\nuncertainties which may render long-term workers fail to meet the service\nquality requirement of each task, we augment our methodology with an online\ntemporary worker recruitment scheme as a backup Plan B to support seamless\nservice provisioning for MCSC tasks, which also represents a 0-1 ILP problem.\nTo tackle these problems which are proved to be NP-hard, we develop three\nalgorithms, namely, i) exhaustive searching, ii) unique index-based stochastic\nsearching with risk-aware filter constraint, iii) geometric programming-based\nsuccessive convex algorithm, which achieve the optimal or sub-optimal\nsolutions. Experimental results demonstrate our effectiveness in terms of\nservice quality, time efficiency, etc.\n","authors":["Minghui Liwang","Zhibin Gao","Seyyedali Hosseinalipour","Zhipeng Cheng","Xianbin Wang","Zhenzhen Jiao"],"pdf_url":"https://arxiv.org/pdf/2206.04354v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09904v1","updated":"2024-03-14T22:29:59Z","published":"2024-03-14T22:29:59Z","title":"FedComLoc: Communication-Efficient Distributed Training of Sparse and\n  Quantized Models","summary":"  Federated Learning (FL) has garnered increasing attention due to its unique\ncharacteristic of allowing heterogeneous clients to process their private data\nlocally and interact with a central server, while being respectful of privacy.\nA critical bottleneck in FL is the communication cost. A pivotal strategy to\nmitigate this burden is \\emph{Local Training}, which involves running multiple\nlocal stochastic gradient descent iterations between communication phases. Our\nwork is inspired by the innovative \\emph{Scaffnew} algorithm, which has\nconsiderably advanced the reduction of communication complexity in FL. We\nintroduce FedComLoc (Federated Compressed and Local Training), integrating\npractical and effective compression into \\emph{Scaffnew} to further enhance\ncommunication efficiency. Extensive experiments, using the popular TopK\ncompressor and quantization, demonstrate its prowess in substantially reducing\ncommunication overheads in heterogeneous settings.\n","authors":["Kai Yi","Georg Meinhardt","Laurent Condat","Peter Richtárik"],"pdf_url":"https://arxiv.org/pdf/2403.09904v1.pdf","comment":null}],"Artificial Intelligence":[{"id":"http://arxiv.org/abs/2403.08551v2","updated":"2024-03-14T06:32:00Z","published":"2024-03-13T14:02:54Z","title":"GaussianImage: 1000 FPS Image Representation and Compression by 2D\n  Gaussian Splatting","summary":"  Implicit neural representations (INRs) recently achieved great success in\nimage representation and compression, offering high visual quality and fast\nrendering speeds with 10-1000 FPS, assuming sufficient GPU resources are\navailable. However, this requirement often hinders their use on low-end devices\nwith limited memory. In response, we propose a groundbreaking paradigm of image\nrepresentation and compression by 2D Gaussian Splatting, named GaussianImage.\nWe first introduce 2D Gaussian to represent the image, where each Gaussian has\n8 parameters including position, covariance and color. Subsequently, we unveil\na novel rendering algorithm based on accumulated summation. Remarkably, our\nmethod with a minimum of 3$\\times$ lower GPU memory usage and 5$\\times$ faster\nfitting time not only rivals INRs (e.g., WIRE, I-NGP) in representation\nperformance, but also delivers a faster rendering speed of 1500-2000 FPS\nregardless of parameter size. Furthermore, we integrate existing vector\nquantization technique to build an image codec. Experimental results\ndemonstrate that our codec attains rate-distortion performance comparable to\ncompression-based INRs such as COIN and COIN++, while facilitating decoding\nspeeds of approximately 1000 FPS. Additionally, preliminary proof of concept\nshows that our codec surpasses COIN and COIN++ in performance when using\npartial bits-back coding.\n","authors":["Xinjie Zhang","Xingtong Ge","Tongda Xu","Dailan He","Yan Wang","Hongwei Qin","Guo Lu","Jing Geng","Jun Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.08551v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09635v1","updated":"2024-03-14T17:59:14Z","published":"2024-03-14T17:59:14Z","title":"Transformers Get Stable: An End-to-End Signal Propagation Theory for\n  Language Models","summary":"  In spite of their huge success, transformer models remain difficult to scale\nin depth. In this work, we develop a unified signal propagation theory and\nprovide formulae that govern the moments of the forward and backward signal\nthrough the transformer model. Our framework can be used to understand and\nmitigate vanishing/exploding gradients, rank collapse, and instability\nassociated with high attention scores. We also propose DeepScaleLM, an\ninitialization and scaling scheme that conserves unit output/gradient moments\nthroughout the model, enabling the training of very deep models with 100s of\nlayers. We find that transformer models could be much deeper - our deep models\nwith fewer parameters outperform shallow models in Language Modeling, Speech\nTranslation, and Image Classification, across Encoder-only, Decoder-only and\nEncoder-Decoder variants, for both Pre-LN and Post-LN transformers, for\nmultiple datasets and model sizes. These improvements also translate into\nimproved performance on downstream Question Answering tasks and improved\nrobustness for image classification.\n","authors":["Akhil Kedia","Mohd Abbas Zaidi","Sushil Khyalia","Jungho Jung","Harshith Goka","Haejun Lee"],"pdf_url":"https://arxiv.org/pdf/2403.09635v1.pdf","comment":"Akhil Kedia, Mohd Abbas Zaidi, Sushil Khyalia equal contribution.\n  Source code is available at\n  https://github.com/akhilkedia/TranformersGetStable"},{"id":"http://arxiv.org/abs/2403.09631v1","updated":"2024-03-14T17:58:41Z","published":"2024-03-14T17:58:41Z","title":"3D-VLA: A 3D Vision-Language-Action Generative World Model","summary":"  Recent vision-language-action (VLA) models rely on 2D inputs, lacking\nintegration with the broader realm of the 3D physical world. Furthermore, they\nperform action prediction by learning a direct mapping from perception to\naction, neglecting the vast dynamics of the world and the relations between\nactions and dynamics. In contrast, human beings are endowed with world models\nthat depict imagination about future scenarios to plan actions accordingly. To\nthis end, we propose 3D-VLA by introducing a new family of embodied foundation\nmodels that seamlessly link 3D perception, reasoning, and action through a\ngenerative world model. Specifically, 3D-VLA is built on top of a 3D-based\nlarge language model (LLM), and a set of interaction tokens is introduced to\nengage with the embodied environment. Furthermore, to inject generation\nabilities into the model, we train a series of embodied diffusion models and\nalign them into the LLM for predicting the goal images and point clouds. To\ntrain our 3D-VLA, we curate a large-scale 3D embodied instruction dataset by\nextracting vast 3D-related information from existing robotics datasets. Our\nexperiments on held-in datasets demonstrate that 3D-VLA significantly improves\nthe reasoning, multimodal generation, and planning capabilities in embodied\nenvironments, showcasing its potential in real-world applications.\n","authors":["Haoyu Zhen","Xiaowen Qiu","Peihao Chen","Jincheng Yang","Xin Yan","Yilun Du","Yining Hong","Chuang Gan"],"pdf_url":"https://arxiv.org/pdf/2403.09631v1.pdf","comment":"Project page: https://vis-www.cs.umass.edu/3dvla/"},{"id":"http://arxiv.org/abs/2403.09629v1","updated":"2024-03-14T17:58:16Z","published":"2024-03-14T17:58:16Z","title":"Quiet-STaR: Language Models Can Teach Themselves to Think Before\n  Speaking","summary":"  When writing and talking, people sometimes pause to think. Although\nreasoning-focused works have often framed reasoning as a method of answering\nquestions or completing agentic tasks, reasoning is implicit in almost all\nwritten text. For example, this applies to the steps not stated between the\nlines of a proof or to the theory of mind underlying a conversation. In the\nSelf-Taught Reasoner (STaR, Zelikman et al. 2022), useful thinking is learned\nby inferring rationales from few-shot examples in question-answering and\nlearning from those that lead to a correct answer. This is a highly constrained\nsetting -- ideally, a language model could instead learn to infer unstated\nrationales in arbitrary text. We present Quiet-STaR, a generalization of STaR\nin which LMs learn to generate rationales at each token to explain future text,\nimproving their predictions. We address key challenges, including 1) the\ncomputational cost of generating continuations, 2) the fact that the LM does\nnot initially know how to generate or use internal thoughts, and 3) the need to\npredict beyond individual next tokens. To resolve these, we propose a tokenwise\nparallel sampling algorithm, using learnable tokens indicating a thought's\nstart and end, and an extended teacher-forcing technique. Encouragingly,\ngenerated rationales disproportionately help model difficult-to-predict tokens\nand improve the LM's ability to directly answer difficult questions. In\nparticular, after continued pretraining of an LM on a corpus of internet text\nwith Quiet-STaR, we find zero-shot improvements on GSM8K\n(5.9%$\\rightarrow$10.9%) and CommonsenseQA (36.3%$\\rightarrow$47.2%) and\nobserve a perplexity improvement of difficult tokens in natural text.\nCrucially, these improvements require no fine-tuning on these tasks. Quiet-STaR\nmarks a step towards LMs that can learn to reason in a more general and\nscalable way.\n","authors":["Eric Zelikman","Georges Harik","Yijia Shao","Varuna Jayasiri","Nick Haber","Noah D. Goodman"],"pdf_url":"https://arxiv.org/pdf/2403.09629v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09621v1","updated":"2024-03-14T17:55:10Z","published":"2024-03-14T17:55:10Z","title":"Minimax Optimal and Computationally Efficient Algorithms for\n  Distributionally Robust Offline Reinforcement Learning","summary":"  Distributionally robust offline reinforcement learning (RL), which seeks\nrobust policy training against environment perturbation by modeling dynamics\nuncertainty, calls for function approximations when facing large state-action\nspaces. However, the consideration of dynamics uncertainty introduces essential\nnonlinearity and computational burden, posing unique challenges for analyzing\nand practically employing function approximation. Focusing on a basic setting\nwhere the nominal model and perturbed models are linearly parameterized, we\npropose minimax optimal and computationally efficient algorithms realizing\nfunction approximation and initiate the study on instance-dependent\nsuboptimality analysis in the context of robust offline RL. Our results uncover\nthat function approximation in robust offline RL is essentially distinct from\nand probably harder than that in standard offline RL. Our algorithms and\ntheoretical results crucially depend on a variety of new techniques, involving\na novel function approximation mechanism incorporating variance information, a\nnew procedure of suboptimality and estimation uncertainty decomposition, a\nquantification of the robust value function shrinkage, and a meticulously\ndesigned family of hard instances, which might be of independent interest.\n","authors":["Zhishuai Liu","Pan Xu"],"pdf_url":"https://arxiv.org/pdf/2403.09621v1.pdf","comment":"53 pages, 1 figure, 1 table"},{"id":"http://arxiv.org/abs/2403.09606v1","updated":"2024-03-14T17:47:20Z","published":"2024-03-14T17:47:20Z","title":"Large Language Models and Causal Inference in Collaboration: A\n  Comprehensive Survey","summary":"  Causal inference has shown potential in enhancing the predictive accuracy,\nfairness, robustness, and explainability of Natural Language Processing (NLP)\nmodels by capturing causal relationships among variables. The emergence of\ngenerative Large Language Models (LLMs) has significantly impacted various NLP\ndomains, particularly through their advanced reasoning capabilities. This\nsurvey focuses on evaluating and improving LLMs from a causal view in the\nfollowing areas: understanding and improving the LLMs' reasoning capacity,\naddressing fairness and safety issues in LLMs, complementing LLMs with\nexplanations, and handling multimodality. Meanwhile, LLMs' strong reasoning\ncapacities can in turn contribute to the field of causal inference by aiding\ncausal relationship discovery and causal effect estimations. This review\nexplores the interplay between causal inference frameworks and LLMs from both\nperspectives, emphasizing their collective potential to further the development\nof more advanced and equitable artificial intelligence systems.\n","authors":["Xiaoyu Liu","Paiheng Xu","Junda Wu","Jiaxin Yuan","Yifan Yang","Yuhang Zhou","Fuxiao Liu","Tianrui Guan","Haoliang Wang","Tong Yu","Julian McAuley","Wei Ai","Furong Huang"],"pdf_url":"https://arxiv.org/pdf/2403.09606v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09605v1","updated":"2024-03-14T17:47:01Z","published":"2024-03-14T17:47:01Z","title":"Counterfactual contrastive learning: robust representations via causal\n  image synthesis","summary":"  Contrastive pretraining is well-known to improve downstream task performance\nand model generalisation, especially in limited label settings. However, it is\nsensitive to the choice of augmentation pipeline. Positive pairs should\npreserve semantic information while destroying domain-specific information.\nStandard augmentation pipelines emulate domain-specific changes with\npre-defined photometric transformations, but what if we could simulate\nrealistic domain changes instead? In this work, we show how to utilise recent\nprogress in counterfactual image generation to this effect. We propose\nCF-SimCLR, a counterfactual contrastive learning approach which leverages\napproximate counterfactual inference for positive pair creation. Comprehensive\nevaluation across five datasets, on chest radiography and mammography,\ndemonstrates that CF-SimCLR substantially improves robustness to acquisition\nshift with higher downstream performance on both in- and out-of-distribution\ndata, particularly for domains which are under-represented during training.\n","authors":["Melanie Roschewitz","Fabio De Sousa Ribeiro","Tian Xia","Galvin Khara","Ben Glocker"],"pdf_url":"https://arxiv.org/pdf/2403.09605v1.pdf","comment":"Code available at\n  https://github.com/biomedia-mira/counterfactual-contrastive"},{"id":"http://arxiv.org/abs/2403.09603v1","updated":"2024-03-14T17:44:35Z","published":"2024-03-14T17:44:35Z","title":"Optimistic Verifiable Training by Controlling Hardware Nondeterminism","summary":"  The increasing compute demands of AI systems has led to the emergence of\nservices that train models on behalf of clients lacking necessary resources.\nHowever, ensuring correctness of training and guarding against potential\ntraining-time attacks, such as data poisoning, poses challenges. Existing works\non verifiable training largely fall into two classes: proof-based systems,\nwhich struggle to scale due to requiring cryptographic techniques, and\n\"optimistic\" methods that consider a trusted third-party auditor who replicates\nthe training process. A key challenge with the latter is that hardware\nnondeterminism between GPU types during training prevents an auditor from\nreplicating the training process exactly, and such schemes are therefore\nnon-robust. We propose a method that combines training in a higher precision\nthan the target model, rounding after intermediate computation steps, and\nstoring rounding decisions based on an adaptive thresholding procedure, to\nsuccessfully control for nondeterminism. Across three different NVIDIA GPUs\n(A40, Titan XP, RTX 2080 Ti), we achieve exact training replication at FP32\nprecision for both full-training and fine-tuning of ResNet-50 (23M) and GPT-2\n(117M) models. Our verifiable training scheme significantly decreases the\nstorage and time costs compared to proof-based systems.\n","authors":["Megha Srivastava","Simran Arora","Dan Boneh"],"pdf_url":"https://arxiv.org/pdf/2403.09603v1.pdf","comment":"11 pages, 5 figures, preprint"},{"id":"http://arxiv.org/abs/2309.14496v4","updated":"2024-03-14T17:31:06Z","published":"2023-09-25T19:45:45Z","title":"Era Splitting -- Invariant Learning for Decision Trees","summary":"  Real-life machine learning problems exhibit distributional shifts in the data\nfrom one time to another or from one place to another. This behavior is beyond\nthe scope of the traditional empirical risk minimization paradigm, which\nassumes i.i.d. distribution of data over time and across locations. The\nemerging field of out-of-distribution (OOD) generalization addresses this\nreality with new theory and algorithms which incorporate environmental, or\nera-wise information into the algorithms. So far, most research has been\nfocused on linear models and/or neural networks. In this research we develop\ntwo new splitting criteria for decision trees, which allow us to apply ideas\nfrom OOD generalization research to decision tree models, namely, gradient\nboosting decision trees (GBDT). The new splitting criteria use era-wise\ninformation associated with the data to grow tree-based models that are optimal\nacross all disjoint eras in the data, instead of optimal over the entire data\nset pooled together, which is the default setting. In this paper, two new\nsplitting criteria are defined and analyzed theoretically. Effectiveness is\ntested on four experiments, ranging from simple, synthetic to complex,\nreal-world applications. In particular we cast the OOD domain-adaptation\nproblem in the context of financial markets, where the new models out-perform\nstate-of-the-art GBDT models on the Numerai data set. The new criteria are\nincorporated into the Scikit-Learn code base and made freely available online.\n","authors":["Timothy DeLise"],"pdf_url":"https://arxiv.org/pdf/2309.14496v4.pdf","comment":"20 pages, 8 figures"},{"id":"http://arxiv.org/abs/2401.15621v2","updated":"2024-03-14T17:22:37Z","published":"2024-01-28T10:20:15Z","title":"SNAP: Semantic Stories for Next Activity Prediction","summary":"  Predicting the next activity in an ongoing process is one of the most common\nclassification tasks in the business process management (BPM) domain. It allows\nbusinesses to optimize resource allocation, enhance operational efficiency, and\naids in risk mitigation and strategic decision-making. This provides a\ncompetitive edge in the rapidly evolving confluence of BPM and AI. Existing\nstate-of-the-art AI models for business process prediction do not fully\ncapitalize on available semantic information within process event logs. As\ncurrent advanced AI-BPM systems provide semantically-richer textual data, the\nneed for novel adequate models grows. To address this gap, we propose the novel\nSNAP method that leverages language foundation models by constructing semantic\ncontextual stories from the process historical event logs and using them for\nthe next activity prediction. We compared the SNAP algorithm with nine\nstate-of-the-art models on six benchmark datasets and show that SNAP\nsignificantly outperforms them, especially for datasets with high levels of\nsemantic content.\n","authors":["Alon Oved","Segev Shlomov","Sergey Zeltyn","Nir Mashkif","Avi Yaeli"],"pdf_url":"https://arxiv.org/pdf/2401.15621v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.00860v3","updated":"2024-03-14T17:21:37Z","published":"2023-11-01T21:28:24Z","title":"Zero Coordinate Shift: Whetted Automatic Differentiation for\n  Physics-informed Operator Learning","summary":"  Automatic differentiation (AD) is a critical step in physics-informed machine\nlearning, required for computing the high-order derivatives of network output\nw.r.t. coordinates of collocation points. In this paper, we present a novel and\nlightweight algorithm to conduct AD for physics-informed operator learning,\nwhich we call the trick of Zero Coordinate Shift (ZCS). Instead of making all\nsampled coordinates as leaf variables, ZCS introduces only one scalar-valued\nleaf variable for each spatial or temporal dimension, simplifying the wanted\nderivatives from \"many-roots-many-leaves\" to \"one-root-many-leaves\" whereby\nreverse-mode AD becomes directly utilisable. It has led to an outstanding\nperformance leap by avoiding the duplication of the computational graph along\nthe dimension of functions (physical parameters). ZCS is easy to implement with\ncurrent deep learning libraries; our own implementation is achieved by\nextending the DeepXDE package. We carry out a comprehensive benchmark analysis\nand several case studies, training physics-informed DeepONets to solve partial\ndifferential equations (PDEs) without data. The results show that ZCS has\npersistently reduced GPU memory consumption and wall time for training by an\norder of magnitude, and such reduction factor scales with the number of\nfunctions. As a low-level optimisation technique, ZCS imposes no restrictions\non data, physics (PDE) or network architecture and does not compromise training\nresults from any aspect.\n","authors":["Kuangdai Leng","Mallikarjun Shankar","Jeyan Thiyagalingam"],"pdf_url":"https://arxiv.org/pdf/2311.00860v3.pdf","comment":"Published in Journal of Computational Physics.\n  https://doi.org/10.1016/j.jcp.2024.112904"},{"id":"http://arxiv.org/abs/2403.07769v2","updated":"2024-03-14T17:16:18Z","published":"2024-03-12T15:56:10Z","title":"Transforming Competition into Collaboration: The Revolutionary Role of\n  Multi-Agent Systems and Language Models in Modern Organizations","summary":"  This article explores the dynamic influence of computational entities based\non multi-agent systems theory (SMA) combined with large language models (LLM),\nwhich are characterized by their ability to simulate complex human\ninteractions, as a possibility to revolutionize human user interaction from the\nuse of specialized artificial agents to support everything from operational\norganizational processes to strategic decision making based on applied\nknowledge and human orchestration. Previous investigations reveal that there\nare limitations, particularly in the autonomous approach of artificial agents,\nespecially when dealing with new challenges and pragmatic tasks such as\ninducing logical reasoning and problem solving. It is also considered that\ntraditional techniques, such as the stimulation of chains of thoughts, require\nexplicit human guidance. In our approach we employ agents developed from large\nlanguage models (LLM), each with distinct prototyping that considers behavioral\nelements, driven by strategies that stimulate the generation of knowledge based\non the use case proposed in the scenario (role-play) business, using a\ndiscussion approach between agents (guided conversation). We demonstrate the\npotential of developing agents useful for organizational strategies, based on\nmulti-agent system theories (SMA) and innovative uses based on large language\nmodels (LLM based), offering a differentiated and adaptable experiment to\ndifferent applications, complexities, domains, and capabilities from LLM.\n","authors":["Carlos Jose Xavier Cruz"],"pdf_url":"https://arxiv.org/pdf/2403.07769v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09580v1","updated":"2024-03-14T17:14:53Z","published":"2024-03-14T17:14:53Z","title":"Algorithmic syntactic causal identification","summary":"  Causal identification in causal Bayes nets (CBNs) is an important tool in\ncausal inference allowing the derivation of interventional distributions from\nobservational distributions where this is possible in principle. However, most\nexisting formulations of causal identification using techniques such as\nd-separation and do-calculus are expressed within the mathematical language of\nclassical probability theory on CBNs. However, there are many causal settings\nwhere probability theory and hence current causal identification techniques are\ninapplicable such as relational databases, dataflow programs such as hardware\ndescription languages, distributed systems and most modern machine learning\nalgorithms. We show that this restriction can be lifted by replacing the use of\nclassical probability theory with the alternative axiomatic foundation of\nsymmetric monoidal categories. In this alternative axiomatization, we show how\nan unambiguous and clean distinction can be drawn between the general syntax of\ncausal models and any specific semantic implementation of that causal model.\nThis allows a purely syntactic algorithmic description of general causal\nidentification by a translation of recent formulations of the general ID\nalgorithm through fixing. Our description is given entirely in terms of the\nnon-parametric ADMG structure specifying a causal model and the algebraic\nsignature of the corresponding monoidal category, to which a sequence of\nmanipulations is then applied so as to arrive at a modified monoidal category\nin which the desired, purely syntactic interventional causal model, is\nobtained. We use this idea to derive purely syntactic analogues of classical\nback-door and front-door causal adjustment, and illustrate an application to a\nmore complex causal model.\n","authors":["Dhurim Cakiqi","Max A. Little"],"pdf_url":"https://arxiv.org/pdf/2403.09580v1.pdf","comment":"11 pages, 2 TikZ figures"},{"id":"http://arxiv.org/abs/2310.08617v2","updated":"2024-03-14T17:12:53Z","published":"2023-10-12T16:00:16Z","title":"The Impact of Explanations on Fairness in Human-AI Decision-Making:\n  Protected vs Proxy Features","summary":"  AI systems have been known to amplify biases in real-world data. Explanations\nmay help human-AI teams address these biases for fairer decision-making.\nTypically, explanations focus on salient input features. If a model is biased\nagainst some protected group, explanations may include features that\ndemonstrate this bias, but when biases are realized through proxy features, the\nrelationship between this proxy feature and the protected one may be less clear\nto a human. In this work, we study the effect of the presence of protected and\nproxy features on participants' perception of model fairness and their ability\nto improve demographic parity over an AI alone. Further, we examine how\ndifferent treatments -- explanations, model bias disclosure and proxy\ncorrelation disclosure -- affect fairness perception and parity. We find that\nexplanations help people detect direct but not indirect biases. Additionally,\nregardless of bias type, explanations tend to increase agreement with model\nbiases. Disclosures can help mitigate this effect for indirect biases,\nimproving both unfairness recognition and decision-making fairness. We hope\nthat our findings can help guide further research into advancing explanations\nin support of fair human-AI decision-making.\n","authors":["Navita Goyal","Connor Baumler","Tin Nguyen","Hal Daumé III"],"pdf_url":"https://arxiv.org/pdf/2310.08617v2.pdf","comment":"IUI 2024"},{"id":"http://arxiv.org/abs/2403.07865v2","updated":"2024-03-14T16:57:37Z","published":"2024-03-12T17:55:38Z","title":"Exploring Safety Generalization Challenges of Large Language Models via\n  Code","summary":"  The rapid advancement of Large Language Models (LLMs) has brought about\nremarkable capabilities in natural language processing but also raised concerns\nabout their potential misuse. While strategies like supervised fine-tuning and\nreinforcement learning from human feedback have enhanced their safety, these\nmethods primarily focus on natural languages, which may not generalize to other\ndomains. This paper introduces CodeAttack, a framework that transforms natural\nlanguage inputs into code inputs, presenting a novel environment for testing\nthe safety generalization of LLMs. Our comprehensive studies on\nstate-of-the-art LLMs including GPT-4, Claude-2, and Llama-2 series reveal a\ncommon safety vulnerability of these models against code input: CodeAttack\nconsistently bypasses the safety guardrails of all models more than 80% of the\ntime. Furthermore, we find that a larger distribution gap between CodeAttack\nand natural language leads to weaker safety generalization, such as encoding\nnatural language input with data structures or using less popular programming\nlanguages. These findings highlight new safety risks in the code domain and the\nneed for more robust safety alignment algorithms to match the code capabilities\nof LLMs.\n","authors":["Qibing Ren","Chang Gao","Jing Shao","Junchi Yan","Xin Tan","Yu Qiao","Wai Lam","Lizhuang Ma"],"pdf_url":"https://arxiv.org/pdf/2403.07865v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09567v1","updated":"2024-03-14T16:57:18Z","published":"2024-03-14T16:57:18Z","title":"Enhancing Trust in Autonomous Agents: An Architecture for Accountability\n  and Explainability through Blockchain and Large Language Models","summary":"  The deployment of autonomous agents in environments involving human\ninteraction has increasingly raised security concerns. Consequently,\nunderstanding the circumstances behind an event becomes critical, requiring the\ndevelopment of capabilities to justify their behaviors to non-expert users.\nSuch explanations are essential in enhancing trustworthiness and safety, acting\nas a preventive measure against failures, errors, and misunderstandings.\nAdditionally, they contribute to improving communication, bridging the gap\nbetween the agent and the user, thereby improving the effectiveness of their\ninteractions. This work presents an accountability and explainability\narchitecture implemented for ROS-based mobile robots. The proposed solution\nconsists of two main components. Firstly, a black box-like element to provide\naccountability, featuring anti-tampering properties achieved through blockchain\ntechnology. Secondly, a component in charge of generating natural language\nexplanations by harnessing the capabilities of Large Language Models (LLMs)\nover the data contained within the previously mentioned black box. The study\nevaluates the performance of our solution in three different scenarios, each\ninvolving autonomous agent navigation functionalities. This evaluation includes\na thorough examination of accountability and explainability metrics,\ndemonstrating the effectiveness of our approach in using accountable data from\nrobot actions to obtain coherent, accurate and understandable explanations,\neven when facing challenges inherent in the use of autonomous agents in\nreal-world scenarios.\n","authors":["Laura Fernández-Becerra","Miguel Ángel González-Santamarta","Ángel Manuel Guerrero-Higueras","Francisco Javier Rodríguez-Lera","Vicente Matellán Olivera"],"pdf_url":"https://arxiv.org/pdf/2403.09567v1.pdf","comment":"21 pages, 12 figures"},{"id":"http://arxiv.org/abs/2403.09565v1","updated":"2024-03-14T16:56:52Z","published":"2024-03-14T16:56:52Z","title":"Welcome Your New AI Teammate: On Safety Analysis by Leashing Large\n  Language Models","summary":"  DevOps is a necessity in many industries, including the development of\nAutonomous Vehicles. In those settings, there are iterative activities that\nreduce the speed of SafetyOps cycles. One of these activities is \"Hazard\nAnalysis & Risk Assessment\" (HARA), which is an essential step to start the\nsafety requirements specification. As a potential approach to increase the\nspeed of this step in SafetyOps, we have delved into the capabilities of Large\nLanguage Models (LLMs).\n  Our objective is to systematically assess their potential for application in\nthe field of safety engineering. To that end, we propose a framework to support\na higher degree of automation of HARA with LLMs. Despite our endeavors to\nautomate as much of the process as possible, expert review remains crucial to\nensure the validity and correctness of the analysis results, with necessary\nmodifications made accordingly.\n","authors":["Ali Nouri","Beatriz Cabrero-Daniel","Fredrik Törner","Hȧkan Sivencrona","Christian Berger"],"pdf_url":"https://arxiv.org/pdf/2403.09565v1.pdf","comment":"Accepted in CAIN 2024, 6 pages, 1 figure"},{"id":"http://arxiv.org/abs/2403.09549v1","updated":"2024-03-14T16:38:02Z","published":"2024-03-14T16:38:02Z","title":"Generalizing Denoising to Non-Equilibrium Structures Improves\n  Equivariant Force Fields","summary":"  Understanding the interactions of atoms such as forces in 3D atomistic\nsystems is fundamental to many applications like molecular dynamics and\ncatalyst design. However, simulating these interactions requires\ncompute-intensive ab initio calculations and thus results in limited data for\ntraining neural networks. In this paper, we propose to use denoising\nnon-equilibrium structures (DeNS) as an auxiliary task to better leverage\ntraining data and improve performance. For training with DeNS, we first corrupt\na 3D structure by adding noise to its 3D coordinates and then predict the\nnoise. Different from previous works on denoising, which are limited to\nequilibrium structures, the proposed method generalizes denoising to a much\nlarger set of non-equilibrium structures. The main difference is that a\nnon-equilibrium structure does not correspond to local energy minima and has\nnon-zero forces, and therefore it can have many possible atomic positions\ncompared to an equilibrium structure. This makes denoising non-equilibrium\nstructures an ill-posed problem since the target of denoising is not uniquely\ndefined. Our key insight is to additionally encode the forces of the original\nnon-equilibrium structure to specify which non-equilibrium structure we are\ndenoising. Concretely, given a corrupted non-equilibrium structure and the\nforces of the original one, we predict the non-equilibrium structure satisfying\nthe input forces instead of any arbitrary structures. Since DeNS requires\nencoding forces, DeNS favors equivariant networks, which can easily incorporate\nforces and other higher-order tensors in node embeddings. We study the\neffectiveness of training equivariant networks with DeNS on OC20, OC22 and MD17\ndatasets and demonstrate that DeNS can achieve new state-of-the-art results on\nOC20 and OC22 and significantly improve training efficiency on MD17.\n","authors":["Yi-Lun Liao","Tess Smidt","Abhishek Das"],"pdf_url":"https://arxiv.org/pdf/2403.09549v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.01308v2","updated":"2024-03-14T16:37:37Z","published":"2024-03-02T20:40:11Z","title":"VBART: The Turkish LLM","summary":"  We present VBART, the first Turkish sequence-to-sequence Large Language\nModels (LLMs) pre-trained on a large corpus from scratch. VBART are compact\nLLMs based on good ideas leveraged from BART and mBART models and come in two\nsizes, Large and XLarge. Fine-tuned VBART models surpass the prior\nstate-of-the-art results in abstractive text summarization, title generation,\ntext paraphrasing, question answering and question generation tasks. They allow\nfine-tuning for future text generation tasks and datasets, carving a new path\nfor Turkish Natural Language Processing (NLP) research. Our work shows that\nhaving a pre-trained LLM for Turkish outperforms up to 3x multilingual models,\nimproving existing results and providing efficient models for training and\ninference. Moreover, we show that our monolingual tokenizer is up to 11x more\nefficient than multilingual tokenizers. Last but not least, we introduce a\nmethod to enlarge an existing pre-trained LLM and question the relevancy of\nChinchilla Scaling Law to sequence-to-sequence masked language models. Our\nfine-tuned models, tokenizer and cleaned vngrs-web-corpus of 135 GB are\npublicly available at huggingface.co/vngrs-ai.\n","authors":["Meliksah Turker","Mehmet Erdi Ari","Aydin Han"],"pdf_url":"https://arxiv.org/pdf/2403.01308v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.01977v2","updated":"2024-03-14T16:30:48Z","published":"2024-03-04T12:20:29Z","title":"TTA-Nav: Test-time Adaptive Reconstruction for Point-Goal Navigation\n  under Visual Corruptions","summary":"  Robot navigation under visual corruption presents a formidable challenge. To\naddress this, we propose a Test-time Adaptation (TTA) method, named as TTA-Nav,\nfor point-goal navigation under visual corruptions. Our \"plug-and-play\" method\nincorporates a top-down decoder to a pre-trained navigation model. Firstly, the\npre-trained navigation model gets a corrupted image and extracts features.\nSecondly, the top-down decoder produces the reconstruction given the high-level\nfeatures extracted by the pre-trained model. Then, it feeds the reconstruction\nof a corrupted image back to the pre-trained model. Finally, the pre-trained\nmodel does forward pass again to output action. Despite being trained solely on\nclean images, the top-down decoder can reconstruct cleaner images from\ncorrupted ones without the need for gradient-based adaptation. The pre-trained\nnavigation model with our top-down decoder significantly enhances navigation\nperformance across almost all visual corruptions in our benchmarks. Our method\nimproves the success rate of point-goal navigation from the state-of-the-art\nresult of 46% to 94% on the most severe corruption. This suggests its potential\nfor broader application in robotic visual navigation. Project page:\nhttps://sites.google.com/view/tta-nav\n","authors":["Maytus Piriyajitakonkij","Mingfei Sun","Mengmi Zhang","Wei Pan"],"pdf_url":"https://arxiv.org/pdf/2403.01977v2.pdf","comment":"Submitted to IROS2024"},{"id":"http://arxiv.org/abs/2309.10639v4","updated":"2024-03-14T16:29:56Z","published":"2023-09-19T14:20:55Z","title":"Geometric structure of Deep Learning networks and construction of global\n  ${\\mathcal L}^2$ minimizers","summary":"  In this paper, we explicitly determine local and global minimizers of the\n$\\mathcal{L}^2$ cost function in underparametrized Deep Learning (DL) networks;\nour main goal is to shed light on their geometric structure and properties. We\naccomplish this by a direct construction, without invoking the gradient descent\nflow at any point of this work. We specifically consider $L$ hidden layers, a\nReLU ramp activation function, an $\\mathcal{L}^2$ Schatten class (or\nHilbert-Schmidt) cost function, input and output spaces $\\mathbb{R}^Q$ with\nequal dimension $Q\\geq1$, and hidden layers also defined on $\\mathbb{R}^{Q}$;\nthe training inputs are assumed to be sufficiently clustered. The training\ninput size $N$ can be arbitrarily large - thus, we are considering the\nunderparametrized regime. More general settings are left to future work. We\nconstruct an explicit family of minimizers for the global minimum of the cost\nfunction in the case $L\\geq Q$, which we show to be degenerate. Moreover, we\ndetermine a set of $2^Q-1$ distinct degenerate local minima of the cost\nfunction. In the context presented here, the concatenation of hidden layers of\nthe DL network is reinterpreted as a recursive application of a {\\em truncation\nmap} which \"curates\" the training inputs by minimizing their noise to signal\nratio.\n","authors":["Thomas Chen","Patricia Muñoz Ewald"],"pdf_url":"https://arxiv.org/pdf/2309.10639v4.pdf","comment":"AMS Latex, 22 pages. Typos corrected, slightly extended"},{"id":"http://arxiv.org/abs/2403.09539v1","updated":"2024-03-14T16:27:49Z","published":"2024-03-14T16:27:49Z","title":"Logits of API-Protected LLMs Leak Proprietary Information","summary":"  The commercialization of large language models (LLMs) has led to the common\npractice of high-level API-only access to proprietary models. In this work, we\nshow that even with a conservative assumption about the model architecture, it\nis possible to learn a surprisingly large amount of non-public information\nabout an API-protected LLM from a relatively small number of API queries (e.g.,\ncosting under $1,000 for OpenAI's gpt-3.5-turbo). Our findings are centered on\none key observation: most modern LLMs suffer from a softmax bottleneck, which\nrestricts the model outputs to a linear subspace of the full output space. We\nshow that this lends itself to a model image or a model signature which unlocks\nseveral capabilities with affordable cost: efficiently discovering the LLM's\nhidden size, obtaining full-vocabulary outputs, detecting and disambiguating\ndifferent model updates, identifying the source LLM given a single full LLM\noutput, and even estimating the output layer parameters. Our empirical\ninvestigations show the effectiveness of our methods, which allow us to\nestimate the embedding size of OpenAI's gpt-3.5-turbo to be about 4,096.\nLastly, we discuss ways that LLM providers can guard against these attacks, as\nwell as how these capabilities can be viewed as a feature (rather than a bug)\nby allowing for greater transparency and accountability.\n","authors":["Matthew Finlayson","Swabha Swayamdipta","Xiang Ren"],"pdf_url":"https://arxiv.org/pdf/2403.09539v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.12749v3","updated":"2024-03-14T16:13:36Z","published":"2024-02-20T06:37:31Z","title":"Me LLaMA: Foundation Large Language Models for Medical Applications","summary":"  Recent large language models (LLMs) such as ChatGPT and LLaMA have shown\ngreat promise in many AI applications. However, their performance on medical\ntasks is suboptimal and can be improved by training on extensive\ndomain-specific datasets. This study introduces Me LLaMA, a medical LLM family\nthat includes foundation models - Me LLaMA 13/70B, along with their\nchat-enhanced versions - Me LLaMA 13/70B-chat, developed through continual\npre-training and instruction tuning of LLaMA2 using large medical datasets. Our\ndomain-specific data suite for training and evaluation includes a large-scale,\ncontinual pre-training dataset with 129B tokens, an instruction tuning dataset\nwith 214k samples, and a new medical evaluation benchmark (MIBE) across six\ntasks with 12 datasets. Our extensive evaluation using the MIBE shows that Me\nLLaMA models achieve overall better performance than existing open-source\nmedical LLMs in zero-shot, few-shot and supervised learning abilities. Their\nzero-shot performance is comparable with ChatGPT across 7 out of 8 datasets,\nwith a slight variance of within 3%, and yet falls short when compared to\nGPT-4. In addition, we investigated the catastrophic forgetting problem, and\nour results show that Me LLaMA models outperform other open-source medical LLMs\nin mitigating this issue. Me LLaMA is one of the largest open-source medical\nfoundation LLMs that use both biomedical and clinical data. It exhibits\nsuperior performance across both general and medical tasks compared to other\nopen-source medical LLMs, rendering it an attractive choice for medical AI\napplications. We release our models, datasets, and evaluation scripts at:\nhttps://github.com/BIDS-Xu-Lab/Me-LLaMA.\n","authors":["Qianqian Xie","Qingyu Chen","Aokun Chen","Cheng Peng","Yan Hu","Fongci Lin","Xueqing Peng","Jimin Huang","Jeffrey Zhang","Vipina Keloth","Xingyu Zhou","Huan He","Lucila Ohno-Machado","Yonghui Wu","Hua Xu","Jiang Bian"],"pdf_url":"https://arxiv.org/pdf/2402.12749v3.pdf","comment":"21 pages, 3 figures, 8 tables"},{"id":"http://arxiv.org/abs/2403.09530v1","updated":"2024-03-14T16:13:00Z","published":"2024-03-14T16:13:00Z","title":"VisionGPT-3D: A Generalized Multimodal Agent for Enhanced 3D Vision\n  Understanding","summary":"  The evolution of text to visual components facilitates people's daily lives,\nsuch as generating image, videos from text and identifying the desired elements\nwithin the images. Computer vision models involving the multimodal abilities in\nthe previous days are focused on image detection, classification based on\nwell-defined objects. Large language models (LLMs) introduces the\ntransformation from nature language to visual objects, which present the visual\nlayout for text contexts. OpenAI GPT-4 has emerged as the pinnacle in LLMs,\nwhile the computer vision (CV) domain boasts a plethora of state-of-the-art\n(SOTA) models and algorithms to convert 2D images to their 3D representations.\nHowever, the mismatching between the algorithms with the problem could lead to\nundesired results. In response to this challenge, we propose an unified\nVisionGPT-3D framework to consolidate the state-of-the-art vision models,\nthereby facilitating the development of vision-oriented AI. VisionGPT-3D\nprovides a versatile multimodal framework building upon the strengths of\nmultimodal foundation models. It seamlessly integrates various SOTA vision\nmodels and brings the automation in the selection of SOTA vision models,\nidentifies the suitable 3D mesh creation algorithms corresponding to 2D depth\nmaps analysis, generates optimal results based on diverse multimodal inputs\nsuch as text prompts.\n  Keywords: VisionGPT-3D, 3D vision understanding, Multimodal agent\n","authors":["Chris Kelly","Luhui Hu","Jiayin Hu","Yu Tian","Deshun Yang","Bang Yang","Cindy Yang","Zihao Li","Zaoshan Huang","Yuexian Zou"],"pdf_url":"https://arxiv.org/pdf/2403.09530v1.pdf","comment":"12 pages, 7 figures, pending conference"},{"id":"http://arxiv.org/abs/2403.09513v1","updated":"2024-03-14T15:57:13Z","published":"2024-03-14T15:57:13Z","title":"AdaShield: Safeguarding Multimodal Large Language Models from\n  Structure-based Attack via Adaptive Shield Prompting","summary":"  With the advent and widespread deployment of Multimodal Large Language Models\n(MLLMs), the imperative to ensure their safety has become increasingly\npronounced. However, with the integration of additional modalities, MLLMs are\nexposed to new vulnerabilities, rendering them prone to structured-based\njailbreak attacks, where semantic content (e.g., \"harmful text\") has been\ninjected into the images to mislead MLLMs. In this work, we aim to defend\nagainst such threats. Specifically, we propose \\textbf{Ada}ptive\n\\textbf{Shield} Prompting (\\textbf{AdaShield}), which prepends inputs with\ndefense prompts to defend MLLMs against structure-based jailbreak attacks\nwithout fine-tuning MLLMs or training additional modules (e.g., post-stage\ncontent detector). Initially, we present a manually designed static defense\nprompt, which thoroughly examines the image and instruction content step by\nstep and specifies response methods to malicious queries. Furthermore, we\nintroduce an adaptive auto-refinement framework, consisting of a target MLLM\nand a LLM-based defense prompt generator (Defender). These components\ncollaboratively and iteratively communicate to generate a defense prompt.\nExtensive experiments on the popular structure-based jailbreak attacks and\nbenign datasets show that our methods can consistently improve MLLMs'\nrobustness against structure-based jailbreak attacks without compromising the\nmodel's general capabilities evaluated on standard benign tasks. Our code is\navailable at https://github.com/rain305f/AdaShield.\n","authors":["Yu Wang","Xiaogeng Liu","Yu Li","Muhao Chen","Chaowei Xiao"],"pdf_url":"https://arxiv.org/pdf/2403.09513v1.pdf","comment":"Multimodal Large Language Models Defense, 25 Pages"},{"id":"http://arxiv.org/abs/2403.09510v1","updated":"2024-03-14T15:56:39Z","published":"2024-03-14T15:56:39Z","title":"Trust AI Regulation? Discerning users are vital to build trust and\n  effective AI regulation","summary":"  There is general agreement that some form of regulation is necessary both for\nAI creators to be incentivised to develop trustworthy systems, and for users to\nactually trust those systems. But there is much debate about what form these\nregulations should take and how they should be implemented. Most work in this\narea has been qualitative, and has not been able to make formal predictions.\nHere, we propose that evolutionary game theory can be used to quantitatively\nmodel the dilemmas faced by users, AI creators, and regulators, and provide\ninsights into the possible effects of different regulatory regimes. We show\nthat creating trustworthy AI and user trust requires regulators to be\nincentivised to regulate effectively. We demonstrate the effectiveness of two\nmechanisms that can achieve this. The first is where governments can recognise\nand reward regulators that do a good job. In that case, if the AI system is not\ntoo risky for users then some level of trustworthy development and user trust\nevolves. We then consider an alternative solution, where users can condition\ntheir trust decision on the effectiveness of the regulators. This leads to\neffective regulation, and consequently the development of trustworthy AI and\nuser trust, provided that the cost of implementing regulations is not too high.\nOur findings highlight the importance of considering the effect of different\nregulatory regimes from an evolutionary game theoretic perspective.\n","authors":["Zainab Alalawi","Paolo Bova","Theodor Cimpeanu","Alessandro Di Stefano","Manh Hong Duong","Elias Fernandez Domingos","The Anh Han","Marcus Krellner","Bianca Ogbo","Simon T. Powers","Filippo Zimmaro"],"pdf_url":"https://arxiv.org/pdf/2403.09510v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09506v1","updated":"2024-03-14T15:53:04Z","published":"2024-03-14T15:53:04Z","title":"Don't Judge by the Look: A Motion Coherent Augmentation for Video\n  Recognition","summary":"  Current training pipelines in object recognition neglect Hue Jittering when\ndoing data augmentation as it not only brings appearance changes that are\ndetrimental to classification, but also the implementation is inefficient in\npractice. In this study, we investigate the effect of hue variance in the\ncontext of video recognition and find this variance to be beneficial since\nstatic appearances are less important in videos that contain motion\ninformation. Based on this observation, we propose a data augmentation method\nfor video recognition, named Motion Coherent Augmentation (MCA), that\nintroduces appearance variation in videos and implicitly encourages the model\nto prioritize motion patterns, rather than static appearances. Concretely, we\npropose an operation SwapMix to efficiently modify the appearance of video\nsamples, and introduce Variation Alignment (VA) to resolve the distribution\nshift caused by SwapMix, enforcing the model to learn appearance invariant\nrepresentations. Comprehensive empirical evaluation across various\narchitectures and different datasets solidly validates the effectiveness and\ngeneralization ability of MCA, and the application of VA in other augmentation\nmethods. Code is available at https://github.com/BeSpontaneous/MCA-pytorch.\n","authors":["Yitian Zhang","Yue Bai","Huan Wang","Yizhou Wang","Yun Fu"],"pdf_url":"https://arxiv.org/pdf/2403.09506v1.pdf","comment":"Accepted by ICLR2024"},{"id":"http://arxiv.org/abs/2402.07011v2","updated":"2024-03-14T15:45:56Z","published":"2024-02-10T18:14:57Z","title":"FedImpro: Measuring and Improving Client Update in Federated Learning","summary":"  Federated Learning (FL) models often experience client drift caused by\nheterogeneous data, where the distribution of data differs across clients. To\naddress this issue, advanced research primarily focuses on manipulating the\nexisting gradients to achieve more consistent client models. In this paper, we\npresent an alternative perspective on client drift and aim to mitigate it by\ngenerating improved local models. First, we analyze the generalization\ncontribution of local training and conclude that this generalization\ncontribution is bounded by the conditional Wasserstein distance between the\ndata distribution of different clients. Then, we propose FedImpro, to construct\nsimilar conditional distributions for local training. Specifically, FedImpro\ndecouples the model into high-level and low-level components, and trains the\nhigh-level portion on reconstructed feature distributions. This approach\nenhances the generalization contribution and reduces the dissimilarity of\ngradients in FL. Experimental results show that FedImpro can help FL defend\nagainst data heterogeneity and enhance the generalization performance of the\nmodel.\n","authors":["Zhenheng Tang","Yonggang Zhang","Shaohuai Shi","Xinmei Tian","Tongliang Liu","Bo Han","Xiaowen Chu"],"pdf_url":"https://arxiv.org/pdf/2402.07011v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09502v1","updated":"2024-03-14T15:44:19Z","published":"2024-03-14T15:44:19Z","title":"EquiAV: Leveraging Equivariance for Audio-Visual Contrastive Learning","summary":"  Recent advancements in self-supervised audio-visual representation learning\nhave demonstrated its potential to capture rich and comprehensive\nrepresentations. However, despite the advantages of data augmentation verified\nin many learning methods, audio-visual learning has struggled to fully harness\nthese benefits, as augmentations can easily disrupt the correspondence between\ninput pairs. To address this limitation, we introduce EquiAV, a novel framework\nthat leverages equivariance for audio-visual contrastive learning. Our approach\nbegins with extending equivariance to audio-visual learning, facilitated by a\nshared attention-based transformation predictor. It enables the aggregation of\nfeatures from diverse augmentations into a representative embedding, providing\nrobust supervision. Notably, this is achieved with minimal computational\noverhead. Extensive ablation studies and qualitative results verify the\neffectiveness of our method. EquiAV outperforms previous works across various\naudio-visual benchmarks.\n","authors":["Jongsuk Kim","Hyeongkeun Lee","Kyeongha Rho","Junmo Kim","Joon Son Chung"],"pdf_url":"https://arxiv.org/pdf/2403.09502v1.pdf","comment":"14 pages, 3 figures"},{"id":"http://arxiv.org/abs/2403.09499v1","updated":"2024-03-14T15:42:26Z","published":"2024-03-14T15:42:26Z","title":"A Reinforcement Learning Approach to Dairy Farm Battery Management using\n  Q Learning","summary":"  Dairy farming consumes a significant amount of energy, making it an\nenergy-intensive sector within agriculture. Integrating renewable energy\ngeneration into dairy farming could help address this challenge. Effective\nbattery management is important for integrating renewable energy generation.\nManaging battery charging and discharging poses significant challenges because\nof fluctuations in electrical consumption, the intermittent nature of renewable\nenergy generation, and fluctuations in energy prices. Artificial Intelligence\n(AI) has the potential to significantly improve the use of renewable energy in\ndairy farming, however, there is limited research conducted in this particular\ndomain. This research considers Ireland as a case study as it works towards\nattaining its 2030 energy strategy centered on the utilization of renewable\nsources. This study proposes a Q-learning-based algorithm for scheduling\nbattery charging and discharging in a dairy farm setting. This research also\nexplores the effect of the proposed algorithm by adding wind generation data\nand considering additional case studies. The proposed algorithm reduces the\ncost of imported electricity from the grid by 13.41\\%, peak demand by 2\\%, and\n24.49\\% when utilizing wind generation. These results underline how\nreinforcement learning is highly effective in managing batteries in the dairy\nfarming sector.\n","authors":["Nawazish Ali","Abdul Wahid","Rachael Shaw","Karl Mason"],"pdf_url":"https://arxiv.org/pdf/2403.09499v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09498v1","updated":"2024-03-14T15:40:13Z","published":"2024-03-14T15:40:13Z","title":"From Skepticism to Acceptance: Simulating the Attitude Dynamics Toward\n  Fake News","summary":"  In the digital era, the rapid propagation of fake news and rumors via social\nnetworks brings notable societal challenges and impacts public opinion\nregulation. Traditional fake news modeling typically forecasts the general\npopularity trends of different groups or numerically represents opinions shift.\nHowever, these methods often oversimplify real-world complexities and overlook\nthe rich semantic information of news text. The advent of large language models\n(LLMs) provides the possibility of modeling subtle dynamics of opinion.\nConsequently, in this work, we introduce a Fake news Propagation Simulation\nframework (FPS) based on LLM, which studies the trends and control of fake news\npropagation in detail. Specifically, each agent in the simulation represents an\nindividual with a distinct personality. They are equipped with both short-term\nand long-term memory, as well as a reflective mechanism to mimic human-like\nthinking. Every day, they engage in random opinion exchanges, reflect on their\nthinking, and update their opinions. Our simulation results uncover patterns in\nfake news propagation related to topic relevance, and individual traits,\naligning with real-world observations. Additionally, we evaluate various\nintervention strategies and demonstrate that early and appropriately frequent\ninterventions strike a balance between governance cost and effectiveness,\noffering valuable insights for practical applications. Our study underscores\nthe significant utility and potential of LLMs in combating fake news.\n","authors":["Yuhan Liu","Xiuying Chen","Xiaoqing Zhang","Xing Gao","Ji Zhang","Rui Yan"],"pdf_url":"https://arxiv.org/pdf/2403.09498v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.05342v2","updated":"2024-03-14T15:40:01Z","published":"2023-11-29T12:58:42Z","title":"Most discriminative stimuli for functional cell type clustering","summary":"  Identifying cell types and understanding their functional properties is\ncrucial for unraveling the mechanisms underlying perception and cognition. In\nthe retina, functional types can be identified by carefully selected stimuli,\nbut this requires expert domain knowledge and biases the procedure towards\npreviously known cell types. In the visual cortex, it is still unknown what\nfunctional types exist and how to identify them. Thus, for unbiased\nidentification of the functional cell types in retina and visual cortex, new\napproaches are needed. Here we propose an optimization-based clustering\napproach using deep predictive models to obtain functional clusters of neurons\nusing Most Discriminative Stimuli (MDS). Our approach alternates between\nstimulus optimization with cluster reassignment akin to an\nexpectation-maximization algorithm. The algorithm recovers functional clusters\nin mouse retina, marmoset retina and macaque visual area V4. This demonstrates\nthat our approach can successfully find discriminative stimuli across species,\nstages of the visual system and recording techniques. The resulting most\ndiscriminative stimuli can be used to assign functional cell types fast and on\nthe fly, without the need to train complex predictive models or show a large\nnatural scene dataset, paving the way for experiments that were previously\nlimited by experimental time. Crucially, MDS are interpretable: they visualize\nthe distinctive stimulus patterns that most unambiguously identify a specific\ntype of neuron.\n","authors":["Max F. Burg","Thomas Zenkel","Michaela Vystrčilová","Jonathan Oesterle","Larissa Höfling","Konstantin F. Willeke","Jan Lause","Sarah Müller","Paul G. Fahey","Zhiwei Ding","Kelli Restivo","Shashwat Sridhar","Tim Gollisch","Philipp Berens","Andreas S. Tolias","Thomas Euler","Matthias Bethge","Alexander S. Ecker"],"pdf_url":"https://arxiv.org/pdf/2401.05342v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.01818v3","updated":"2024-03-14T15:39:51Z","published":"2024-03-04T08:06:41Z","title":"AllSpark: Reborn Labeled Features from Unlabeled in Transformer for\n  Semi-Supervised Semantic Segmentation","summary":"  Semi-supervised semantic segmentation (SSSS) has been proposed to alleviate\nthe burden of time-consuming pixel-level manual labeling, which leverages\nlimited labeled data along with larger amounts of unlabeled data. Current\nstate-of-the-art methods train the labeled data with ground truths and\nunlabeled data with pseudo labels. However, the two training flows are\nseparate, which allows labeled data to dominate the training process, resulting\nin low-quality pseudo labels and, consequently, sub-optimal results. To\nalleviate this issue, we present AllSpark, which reborns the labeled features\nfrom unlabeled ones with the channel-wise cross-attention mechanism. We further\nintroduce a Semantic Memory along with a Channel Semantic Grouping strategy to\nensure that unlabeled features adequately represent labeled features. The\nAllSpark shed new light on the architecture level designs of SSSS rather than\nframework level, which avoids increasingly complicated training pipeline\ndesigns. It can also be regarded as a flexible bottleneck module that can be\nseamlessly integrated into a general transformer-based segmentation model. The\nproposed AllSpark outperforms existing methods across all evaluation protocols\non Pascal, Cityscapes and COCO benchmarks without bells-and-whistles. Code and\nmodel weights are available at: https://github.com/xmed-lab/AllSpark.\n","authors":["Haonan Wang","Qixiang Zhang","Yi Li","Xiaomeng Li"],"pdf_url":"https://arxiv.org/pdf/2403.01818v3.pdf","comment":"Accepted by CVPR 2024; correct typos; this is not the camera-ready\n  version"},{"id":"http://arxiv.org/abs/2403.09488v1","updated":"2024-03-14T15:30:14Z","published":"2024-03-14T15:30:14Z","title":"Rectifying Demonstration Shortcut in In-Context Learning","summary":"  Large language models (LLMs) are able to solve various tasks with only a few\ndemonstrations utilizing their in-context learning (ICL) abilities. However,\nLLMs often rely on their pre-trained semantic priors of demonstrations rather\nthan on the input-label relationships to proceed with ICL prediction. In this\nwork, we term this phenomenon as the `Demonstration Shortcut'. While previous\nworks have primarily focused on improving ICL prediction results for predefined\ntasks, we aim to rectify the Demonstration Shortcut, thereby enabling the LLM\nto effectively learn new input-label relationships from demonstrations. To\nachieve this, we introduce In-Context Calibration, a demonstration-aware\ncalibration method. We evaluate the effectiveness of the proposed method in two\nsettings: (1) the Original ICL Task using the standard label space and (2) the\nTask Learning setting, where the label space is replaced with semantically\nunrelated tokens. In both settings, In-Context Calibration demonstrates\nsubstantial improvements, with results generalized across three LLM families\n(OPT, GPT, and Llama2) under various configurations.\n","authors":["Joonwon Jang","Sanghwan Jang","Wonbin Kweon","Minjin Jeon","Hwanjo Yu"],"pdf_url":"https://arxiv.org/pdf/2403.09488v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09481v1","updated":"2024-03-14T15:25:23Z","published":"2024-03-14T15:25:23Z","title":"Clinical Reasoning over Tabular Data and Text with Bayesian Networks","summary":"  Bayesian networks are well-suited for clinical reasoning on tabular data, but\nare less compatible with natural language data, for which neural networks\nprovide a successful framework. This paper compares and discusses strategies to\naugment Bayesian networks with neural text representations, both in a\ngenerative and discriminative manner. This is illustrated with simulation\nresults for a primary care use case (diagnosis of pneumonia) and discussed in a\nbroader clinical context.\n","authors":["Paloma Rabaey","Johannes Deleu","Stefan Heytens","Thomas Demeester"],"pdf_url":"https://arxiv.org/pdf/2403.09481v1.pdf","comment":"10 pages, 2 figures"},{"id":"http://arxiv.org/abs/2203.03668v6","updated":"2024-03-14T15:25:16Z","published":"2022-03-04T14:16:50Z","title":"A Typology for Exploring the Mitigation of Shortcut Behavior","summary":"  As machine learning models become increasingly larger, trained weakly\nsupervised on large, possibly uncurated data sets, it becomes increasingly\nimportant to establish mechanisms for inspecting, interacting, and revising\nmodels to mitigate learning shortcuts and guarantee their learned knowledge is\naligned with human knowledge. The recently proposed XIL framework was developed\nfor this purpose, and several such methods have been introduced, each with\nindividual motivations and methodological details. In this work, we provide a\nunification of various XIL methods into a single typology by establishing a\ncommon set of basic modules. In doing so, we pave the way for a principled\ncomparison of existing, but, importantly, also future XIL approaches. In\naddition, we discuss existing and introduce novel measures and benchmarks for\nevaluating the overall abilities of a XIL method. Given this extensive toolbox,\nincluding our typology, measures, and benchmarks, we finally compare several\nrecent XIL methods methodologically and quantitatively. In our evaluations, all\nmethods prove to revise a model successfully. However, we found remarkable\ndifferences in individual benchmark tasks, revealing valuable\napplication-relevant aspects for integrating these benchmarks in developing\nfuture methods.\n","authors":["Felix Friedrich","Wolfgang Stammer","Patrick Schramowski","Kristian Kersting"],"pdf_url":"https://arxiv.org/pdf/2203.03668v6.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09480v1","updated":"2024-03-14T15:22:33Z","published":"2024-03-14T15:22:33Z","title":"What Sketch Explainability Really Means for Downstream Tasks","summary":"  In this paper, we explore the unique modality of sketch for explainability,\nemphasising the profound impact of human strokes compared to conventional\npixel-oriented studies. Beyond explanations of network behavior, we discern the\ngenuine implications of explainability across diverse downstream sketch-related\ntasks. We propose a lightweight and portable explainability solution -- a\nseamless plugin that integrates effortlessly with any pre-trained model,\neliminating the need for re-training. Demonstrating its adaptability, we\npresent four applications: highly studied retrieval and generation, and\ncompletely novel assisted drawing and sketch adversarial attacks. The\ncentrepiece to our solution is a stroke-level attribution map that takes\ndifferent forms when linked with downstream tasks. By addressing the inherent\nnon-differentiability of rasterisation, we enable explanations at both coarse\nstroke level (SLA) and partial stroke level (P-SLA), each with its advantages\nfor specific downstream tasks.\n","authors":["Hmrishav Bandyopadhyay","Pinaki Nath Chowdhury","Ayan Kumar Bhunia","Aneeshan Sain","Tao Xiang","Yi-Zhe Song"],"pdf_url":"https://arxiv.org/pdf/2403.09480v1.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2310.11482v2","updated":"2024-03-14T15:10:05Z","published":"2023-10-17T13:06:39Z","title":"Rethinking Class-incremental Learning in the Era of Large Pre-trained\n  Models via Test-Time Adaptation","summary":"  Class-incremental learning (CIL) is a challenging task that involves\nsequentially learning to categorize classes from new tasks without forgetting\npreviously learned information. The advent of large pre-trained models (PTMs)\nhas fast-tracked the progress in CIL due to the highly transferable PTM\nrepresentations, where tuning a small set of parameters leads to\nstate-of-the-art performance when compared with the traditional CIL methods\nthat are trained from scratch. However, repeated fine-tuning on each task\ndestroys the rich representations of the PTMs and further leads to forgetting\nprevious tasks. To strike a balance between the stability and plasticity of\nPTMs for CIL, we propose a novel perspective of eliminating training on every\nnew task and instead train PTM only on the first task, and then refine its\nrepresentation at inference time using test-time adaptation (TTA). Concretely,\nwe propose Test-Time Adaptation for Class-Incremental Learning (TTACIL) that\nfirst fine-tunes PTMs using Adapters on the first task, then adjusts Layer Norm\nparameters of the PTM on each test instance for learning task-specific\nfeatures, and finally resets them back to the adapted model to preserve\nstability. As a consequence, our TTACIL does not undergo any forgetting, while\nbenefiting each task with the rich PTM features. Additionally, by design, our\nTTACIL is robust to common data corruptions. Our method outperforms several\nstate-of-the-art CIL methods when evaluated on multiple CIL benchmarks under\nboth clean and corrupted data. Code is available at:\nhttps://github.com/IemProg/TTACIL.\n","authors":["Imad Eddine Marouf","Subhankar Roy","Enzo Tartaglione","Stéphane Lathuilière"],"pdf_url":"https://arxiv.org/pdf/2310.11482v2.pdf","comment":"8 pages,5 figures"},{"id":"http://arxiv.org/abs/2211.16648v2","updated":"2024-03-14T15:06:02Z","published":"2022-11-30T00:32:37Z","title":"COMET: A Comprehensive Cluster Design Methodology for Distributed Deep\n  Learning Training","summary":"  Modern Deep Learning (DL) models have grown to sizes requiring massive\nclusters of specialized, high-end nodes to train. Designing such clusters to\nmaximize both performance and utilization--to amortize their steep cost--is a\nchallenging task requiring careful balance of compute, memory, and network\nresources. Moreover, a plethora of each model's tuning knobs drastically affect\nthe performance, with optimal values often depending on the underlying\ncluster's characteristics, which necessitates a complex cluster-workload\nco-design process. To facilitate the design space exploration of such massive\nDL training clusters, we introduce COMET, a holistic cluster design methodology\nand workflow to jointly study the impact of parallelization strategies and key\ncluster resource provisioning on the performance of distributed DL training. We\ndevelop a step-by-step process to establish a reusable and flexible\nmethodology, and demonstrate its application with case studies of training\nlarge models on cluster configurations of variable compute, memory, and network\nresources. Our case studies demonstrate COMET's utility in identifying\npromising architectural optimization directions and guiding system designers in\nconfiguring key model and cluster parameters. To illustrate, cluster\nconfiguration comparisons identify performance differences of up to 7.7x and\nhighlight performance optimization opportunities of up to 1.4x when employing\nmemory expansion as an optimization technique.\n","authors":["Divya Kiran Kadiyala","Saeed Rashidi","Taekyung Heo","Abhimanyu Rajeshkumar Bambhaniya","Tushar Krishna","Alexandros Daglis"],"pdf_url":"https://arxiv.org/pdf/2211.16648v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.19791v3","updated":"2024-03-14T14:54:54Z","published":"2023-10-30T17:55:02Z","title":"LILO: Learning Interpretable Libraries by Compressing and Documenting\n  Code","summary":"  While large language models (LLMs) now excel at code generation, a key aspect\nof software development is the art of refactoring: consolidating code into\nlibraries of reusable and readable programs. In this paper, we introduce LILO,\na neurosymbolic framework that iteratively synthesizes, compresses, and\ndocuments code to build libraries tailored to particular problem domains. LILO\ncombines LLM-guided program synthesis with recent algorithmic advances in\nautomated refactoring from Stitch: a symbolic compression system that\nefficiently identifies optimal lambda abstractions across large code corpora.\nTo make these abstractions interpretable, we introduce an auto-documentation\n(AutoDoc) procedure that infers natural language names and docstrings based on\ncontextual examples of usage. In addition to improving human readability, we\nfind that AutoDoc boosts performance by helping LILO's synthesizer to interpret\nand deploy learned abstractions. We evaluate LILO on three inductive program\nsynthesis benchmarks for string editing, scene reasoning, and graphics\ncomposition. Compared to existing neural and symbolic methods - including the\nstate-of-the-art library learning algorithm DreamCoder - LILO solves more\ncomplex tasks and learns richer libraries that are grounded in linguistic\nknowledge.\n","authors":["Gabriel Grand","Lionel Wong","Matthew Bowers","Theo X. Olausson","Muxin Liu","Joshua B. Tenenbaum","Jacob Andreas"],"pdf_url":"https://arxiv.org/pdf/2310.19791v3.pdf","comment":"ICLR 2024 camera-ready"},{"id":"http://arxiv.org/abs/2402.19078v2","updated":"2024-03-14T14:47:54Z","published":"2024-02-29T12:03:05Z","title":"Smooth Tchebycheff Scalarization for Multi-Objective Optimization","summary":"  Multi-objective optimization problems can be found in many real-world\napplications, where the objectives often conflict each other and cannot be\noptimized by a single solution. In the past few decades, numerous methods have\nbeen proposed to find Pareto solutions that represent different optimal\ntrade-offs among the objectives for a given problem. However, these existing\nmethods could have high computational complexity or may not have good\ntheoretical properties for solving a general differentiable multi-objective\noptimization problem. In this work, by leveraging the smooth optimization\ntechnique, we propose a novel and lightweight smooth Tchebycheff scalarization\napproach for gradient-based multi-objective optimization. It has good\ntheoretical properties for finding all Pareto solutions with valid trade-off\npreferences, while enjoying significantly lower computational complexity\ncompared to other methods. Experimental results on various real-world\napplication problems fully demonstrate the effectiveness of our proposed\nmethod.\n","authors":["Xi Lin","Xiaoyuan Zhang","Zhiyuan Yang","Fei Liu","Zhenkun Wang","Qingfu Zhang"],"pdf_url":"https://arxiv.org/pdf/2402.19078v2.pdf","comment":"fix some typos"},{"id":"http://arxiv.org/abs/2403.09442v1","updated":"2024-03-14T14:35:53Z","published":"2024-03-14T14:35:53Z","title":"LLM-based agents for automating the enhancement of user story quality:\n  An early report","summary":"  In agile software development, maintaining high-quality user stories is\ncrucial, but also challenging. This study explores the use of large language\nmodels to automatically improve the user story quality in Austrian Post Group\nIT agile teams. We developed a reference model for an Autonomous LLM-based\nAgent System and implemented it at the company. The quality of user stories in\nthe study and the effectiveness of these agents for user story quality\nimprovement was assessed by 11 participants across six agile teams. Our\nfindings demonstrate the potential of LLMs in improving user story quality,\ncontributing to the research on AI role in agile development, and providing a\npractical example of the transformative impact of AI in an industry setting.\n","authors":["Zheying Zhang","Maruf Rayhan","Tomas Herda","Manuel Goisauf","Pekka Abrahamsson"],"pdf_url":"https://arxiv.org/pdf/2403.09442v1.pdf","comment":"16 pages, 5 figures, 2 tables"},{"id":"http://arxiv.org/abs/2306.10322v3","updated":"2024-03-14T14:33:51Z","published":"2023-06-17T11:44:04Z","title":"CorNav: Autonomous Agent with Self-Corrected Planning for Zero-Shot\n  Vision-and-Language Navigation","summary":"  Understanding and following natural language instructions while navigating\nthrough complex, real-world environments poses a significant challenge for\ngeneral-purpose robots. These environments often include obstacles and\npedestrians, making it essential for autonomous agents to possess the\ncapability of self-corrected planning to adjust their actions based on feedback\nfrom the surroundings. However, the majority of existing vision-and-language\nnavigation (VLN) methods primarily operate in less realistic simulator settings\nand do not incorporate environmental feedback into their decision-making\nprocesses. To address this gap, we introduce a novel zero-shot framework called\nCorNav, utilizing a large language model for decision-making and comprising two\nkey components: 1) incorporating environmental feedback for refining future\nplans and adjusting its actions, and 2) multiple domain experts for parsing\ninstructions, scene understanding, and refining predicted actions. In addition\nto the framework, we develop a 3D simulator that renders realistic scenarios\nusing Unreal Engine 5. To evaluate the effectiveness and generalization of\nnavigation agents in a zero-shot multi-task setting, we create a benchmark\ncalled NavBench. Extensive experiments demonstrate that CorNav consistently\noutperforms all baselines by a significant margin across all tasks. On average,\nCorNav achieves a success rate of 28.1\\%, surpassing the best baseline's\nperformance of 20.5\\%.\n","authors":["Xiwen Liang","Liang Ma","Shanshan Guo","Jianhua Han","Hang Xu","Shikui Ma","Xiaodan Liang"],"pdf_url":"https://arxiv.org/pdf/2306.10322v3.pdf","comment":"22 pages"},{"id":"http://arxiv.org/abs/2403.09439v1","updated":"2024-03-14T14:31:22Z","published":"2024-03-14T14:31:22Z","title":"3D-SceneDreamer: Text-Driven 3D-Consistent Scene Generation","summary":"  Text-driven 3D scene generation techniques have made rapid progress in recent\nyears. Their success is mainly attributed to using existing generative models\nto iteratively perform image warping and inpainting to generate 3D scenes.\nHowever, these methods heavily rely on the outputs of existing models, leading\nto error accumulation in geometry and appearance that prevent the models from\nbeing used in various scenarios (e.g., outdoor and unreal scenarios). To\naddress this limitation, we generatively refine the newly generated local views\nby querying and aggregating global 3D information, and then progressively\ngenerate the 3D scene. Specifically, we employ a tri-plane features-based NeRF\nas a unified representation of the 3D scene to constrain global 3D consistency,\nand propose a generative refinement network to synthesize new contents with\nhigher quality by exploiting the natural image prior from 2D diffusion model as\nwell as the global 3D information of the current scene. Our extensive\nexperiments demonstrate that, in comparison to previous methods, our approach\nsupports wide variety of scene generation and arbitrary camera trajectories\nwith improved visual quality and 3D consistency.\n","authors":["Frank Zhang","Yibo Zhang","Quan Zheng","Rui Ma","Wei Hua","Hujun Bao","Weiwei Xu","Changqing Zou"],"pdf_url":"https://arxiv.org/pdf/2403.09439v1.pdf","comment":"11 pages, 7 figures"},{"id":"http://arxiv.org/abs/2402.07398v2","updated":"2024-03-14T14:30:14Z","published":"2024-02-12T04:13:16Z","title":"VisLingInstruct: Elevating Zero-Shot Learning in Multi-Modal Language\n  Models with Autonomous Instruction Optimization","summary":"  This paper presents VisLingInstruct, a novel approach to advancing\nMulti-Modal Language Models (MMLMs) in zero-shot learning. Current MMLMs show\nimpressive zero-shot abilities in multi-modal tasks, but their performance\ndepends heavily on the quality of instructions. VisLingInstruct tackles this by\nautonomously evaluating and optimizing instructional texts through In-Context\nLearning, improving the synergy between visual perception and linguistic\nexpression in MMLMs. Alongside this instructional advancement, we have also\noptimized the visual feature extraction modules in MMLMs, further augmenting\ntheir responsiveness to textual cues. Our comprehensive experiments on MMLMs,\nbased on FlanT5 and Vicuna, show that VisLingInstruct significantly improves\nzero-shot performance in visual multi-modal tasks. Notably, it achieves a 13.1%\nand 9% increase in accuracy over the prior state-of-the-art on the TextVQA and\nHatefulMemes datasets.\n","authors":["Dongsheng Zhu","Xunzhu Tang","Weidong Han","Jinghui Lu","Yukun Zhao","Guoliang Xing","Junfeng Wang","Dawei Yin"],"pdf_url":"https://arxiv.org/pdf/2402.07398v2.pdf","comment":"Accepted to NAACL2024 main conference"},{"id":"http://arxiv.org/abs/2403.00795v2","updated":"2024-03-14T14:25:13Z","published":"2024-02-23T05:31:36Z","title":"Executing Natural Language-Described Algorithms with Large Language\n  Models: An Investigation","summary":"  Executing computer programs described in natural language has long been a\npursuit of computer science. With the advent of enhanced natural language\nunderstanding capabilities exhibited by large language models (LLMs), the path\ntoward this goal has been illuminated. In this paper, we seek to examine the\ncapacity of present-day LLMs to comprehend and execute algorithms outlined in\nnatural language. We established an algorithm test set sourced from\nIntroduction to Algorithm, a well-known textbook that contains many\nrepresentative widely-used algorithms. To systematically assess LLMs' code\nexecution abilities, we selected 30 algorithms, generated 300 random-sampled\ninstances in total, and evaluated whether popular LLMs can understand and\nexecute these algorithms. Our findings reveal that LLMs, notably GPT-4, can\neffectively execute programs described in natural language, as long as no heavy\nnumeric computation is involved. We believe our findings contribute to\nevaluating LLMs' code execution abilities and would encourage further\ninvestigation and application for the computation power of LLMs.\n","authors":["Xin Zheng","Qiming Zhu","Hongyu Lin","Yaojie Lu","Xianpei Han","Le Sun"],"pdf_url":"https://arxiv.org/pdf/2403.00795v2.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.09422v1","updated":"2024-03-14T14:14:47Z","published":"2024-03-14T14:14:47Z","title":"Mitigating attribute amplification in counterfactual image generation","summary":"  Causal generative modelling is gaining interest in medical imaging due to its\nability to answer interventional and counterfactual queries. Most work focuses\non generating counterfactual images that look plausible, using auxiliary\nclassifiers to enforce effectiveness of simulated interventions. We investigate\npitfalls in this approach, discovering the issue of attribute amplification,\nwhere unrelated attributes are spuriously affected during interventions,\nleading to biases across protected characteristics and disease status. We show\nthat attribute amplification is caused by the use of hard labels in the\ncounterfactual training process and propose soft counterfactual fine-tuning to\nmitigate this issue. Our method substantially reduces the amplification effect\nwhile maintaining effectiveness of generated images, demonstrated on a large\nchest X-ray dataset. Our work makes an important advancement towards more\nfaithful and unbiased causal modelling in medical imaging.\n","authors":["Tian Xia","Mélanie Roschewitz","Fabio De Sousa Ribeiro","Charles Jones","Ben Glocker"],"pdf_url":"https://arxiv.org/pdf/2403.09422v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09412v1","updated":"2024-03-14T14:03:29Z","published":"2024-03-14T14:03:29Z","title":"OpenGraph: Open-Vocabulary Hierarchical 3D Graph Representation in\n  Large-Scale Outdoor Environments","summary":"  Environment maps endowed with sophisticated semantics are pivotal for\nfacilitating seamless interaction between robots and humans, enabling them to\neffectively carry out various tasks. Open-vocabulary maps, powered by\nVisual-Language models (VLMs), possess inherent advantages, including\nmultimodal retrieval and open-set classes. However, existing open-vocabulary\nmaps are constrained to closed indoor scenarios and VLM features, thereby\ndiminishing their usability and inference capabilities. Moreover, the absence\nof topological relationships further complicates the accurate querying of\nspecific instances. In this work, we propose OpenGraph, a representation of\nopen-vocabulary hierarchical graph structure designed for large-scale outdoor\nenvironments. OpenGraph initially extracts instances and their captions from\nvisual images using 2D foundation models, encoding the captions with features\nto enhance textual reasoning. Subsequently, 3D incremental panoramic mapping\nwith feature embedding is achieved by projecting images onto LiDAR point\nclouds. Finally, the environment is segmented based on lane graph connectivity\nto construct a hierarchical graph. Validation results from real public dataset\nSemanticKITTI demonstrate that, even without fine-tuning the models, OpenGraph\nexhibits the ability to generalize to novel semantic classes and achieve the\nhighest segmentation and query accuracy. The source code of OpenGraph is\npublicly available at https://github.com/BIT-DYN/OpenGraph.\n","authors":["Yinan Deng","Jiahui Wang","Jingyu Zhao","Xinyu Tian","Guangyan Chen","Yi Yang","Yufeng Yue"],"pdf_url":"https://arxiv.org/pdf/2403.09412v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09410v1","updated":"2024-03-14T14:02:01Z","published":"2024-03-14T14:02:01Z","title":"XCoOp: Explainable Prompt Learning for Computer-Aided Diagnosis via\n  Concept-guided Context Optimization","summary":"  Utilizing potent representations of the large vision-language models (VLMs)\nto accomplish various downstream tasks has attracted increasing attention.\nWithin this research field, soft prompt learning has become a representative\napproach for efficiently adapting VLMs such as CLIP, to tasks like image\nclassification. However, most existing prompt learning methods learn text\ntokens that are unexplainable, which cannot satisfy the stringent\ninterpretability requirements of Explainable Artificial Intelligence (XAI) in\nhigh-stakes scenarios like healthcare. To address this issue, we propose a\nnovel explainable prompt learning framework that leverages medical knowledge by\naligning the semantics of images, learnable prompts, and clinical\nconcept-driven prompts at multiple granularities. Moreover, our framework\naddresses the lack of valuable concept annotations by eliciting knowledge from\nlarge language models and offers both visual and textual explanations for the\nprompts. Extensive experiments and explainability analyses conducted on various\ndatasets, with and without concept labels, demonstrate that our method\nsimultaneously achieves superior diagnostic performance, flexibility, and\ninterpretability, shedding light on the effectiveness of foundation models in\nfacilitating XAI. The code will be made publically available.\n","authors":["Yequan Bie","Luyang Luo","Zhixuan Chen","Hao Chen"],"pdf_url":"https://arxiv.org/pdf/2403.09410v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07130v3","updated":"2024-03-14T14:01:56Z","published":"2023-12-12T10:04:43Z","title":"Divide-and-Conquer Attack: Harnessing the Power of LLM to Bypass Safety\n  Filters of Text-to-Image Models","summary":"  Text-to-image (TTI) models offer many innovative services but also raise\nethical concerns due to their potential to generate unethical images. Most\npublic TTI services employ safety filters to prevent unintended images. In this\nwork, we introduce the Divide-and-Conquer Attack to circumvent the safety\nfilters of state-of the-art TTI models, including DALL-E 3 and Midjourney. Our\nattack leverages LLMs as text transformation agents to create adversarial\nprompts. We design attack helper prompts that effectively guide LLMs to break\ndown an unethical drawing intent into multiple benign descriptions of\nindividual image elements, allowing them to bypass safety filters while still\ngenerating unethical images. Because the latent harmful meaning only becomes\napparent when all individual elements are drawn together. Our evaluation\ndemonstrates that our attack successfully circumvents multiple strong\nclosed-box safety filters. The comprehensive success rate of DACA bypassing the\nsafety filters of the state-of-the-art TTI engine DALL-E 3 is above 85%, while\nthe success rate for bypassing Midjourney V6 exceeds 75%. Our findings have\nmore severe security implications than methods of manual crafting or iterative\nTTI model querying due to lower attack barrier, enhanced interpretability , and\nbetter adaptation to defense. Our prototype is available at:\nhttps://github.com/researchcode001/Divide-and-Conquer-Attack\n","authors":["Yimo Deng","Huangxun Chen"],"pdf_url":"https://arxiv.org/pdf/2312.07130v3.pdf","comment":"23 pages, 11 figures, under review"},{"id":"http://arxiv.org/abs/2403.09409v1","updated":"2024-03-14T14:01:26Z","published":"2024-03-14T14:01:26Z","title":"\"Like a Nesting Doll\": Analyzing Recursion Analogies Generated by CS\n  Students using Large Language Models","summary":"  Grasping complex computing concepts often poses a challenge for students who\nstruggle to anchor these new ideas to familiar experiences and understandings.\nTo help with this, a good analogy can bridge the gap between unfamiliar\nconcepts and familiar ones, providing an engaging way to aid understanding.\nHowever, creating effective educational analogies is difficult even for\nexperienced instructors. We investigate to what extent large language models\n(LLMs), specifically ChatGPT, can provide access to personally relevant\nanalogies on demand. Focusing on recursion, a challenging threshold concept, we\nconducted an investigation analyzing the analogies generated by more than 350\nfirst-year computing students. They were provided with a code snippet and\ntasked to generate their own recursion-based analogies using ChatGPT,\noptionally including personally relevant topics in their prompts. We observed a\ngreat deal of diversity in the analogies produced with student-prescribed\ntopics, in contrast to the otherwise generic analogies, highlighting the value\nof student creativity when working with LLMs. Not only did students enjoy the\nactivity and report an improved understanding of recursion, but they described\nmore easily remembering analogies that were personally and culturally relevant.\n","authors":["Seth Bernstein","Paul Denny","Juho Leinonen","Lauren Kan","Arto Hellas","Matt Littlefield Sami Sarsa","Stephen MacNeil"],"pdf_url":"https://arxiv.org/pdf/2403.09409v1.pdf","comment":"7 pages, 2 figures, ITiCSE 2024 preprint"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2403.08609v2","updated":"2024-03-14T10:01:45Z","published":"2024-03-13T15:21:14Z","title":"On the Convergence of Locally Adaptive and Scalable Diffusion-Based\n  Sampling Methods for Deep Bayesian Neural Network Posteriors","summary":"  Achieving robust uncertainty quantification for deep neural networks\nrepresents an important requirement in many real-world applications of deep\nlearning such as medical imaging where it is necessary to assess the\nreliability of a neural network's prediction. Bayesian neural networks are a\npromising approach for modeling uncertainties in deep neural networks.\nUnfortunately, generating samples from the posterior distribution of neural\nnetworks is a major challenge. One significant advance in that direction would\nbe the incorporation of adaptive step sizes, similar to modern neural network\noptimizers, into Monte Carlo Markov chain sampling algorithms without\nsignificantly increasing computational demand. Over the past years, several\npapers have introduced sampling algorithms with claims that they achieve this\nproperty. However, do they indeed converge to the correct distribution? In this\npaper, we demonstrate that these methods can have a substantial bias in the\ndistribution they sample, even in the limit of vanishing step sizes and at full\nbatch size.\n","authors":["Tim Rensmeyer","Oliver Niggemann"],"pdf_url":"https://arxiv.org/pdf/2403.08609v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08568v2","updated":"2024-03-14T12:26:17Z","published":"2024-03-13T14:24:09Z","title":"Consistent Prompting for Rehearsal-Free Continual Learning","summary":"  Continual learning empowers models to adapt autonomously to the ever-changing\nenvironment or data streams without forgetting old knowledge. Prompt-based\napproaches are built on frozen pre-trained models to learn the task-specific\nprompts and classifiers efficiently. Existing prompt-based methods are\ninconsistent between training and testing, limiting their effectiveness. Two\ntypes of inconsistency are revealed. Test predictions are made from all\nclassifiers while training only focuses on the current task classifier without\nholistic alignment, leading to Classifier inconsistency. Prompt inconsistency\nindicates that the prompt selected during testing may not correspond to the one\nassociated with this task during training. In this paper, we propose a novel\nprompt-based method, Consistent Prompting (CPrompt), for more aligned training\nand testing. Specifically, all existing classifiers are exposed to prompt\ntraining, resulting in classifier consistency learning. In addition, prompt\nconsistency learning is proposed to enhance prediction robustness and boost\nprompt selection accuracy. Our Consistent Prompting surpasses its prompt-based\ncounterparts and achieves state-of-the-art performance on multiple continual\nlearning benchmarks. Detailed analysis shows that improvements come from more\nconsistent training and testing.\n","authors":["Zhanxin Gao","Jun Cen","Xiaobin Chang"],"pdf_url":"https://arxiv.org/pdf/2403.08568v2.pdf","comment":"Accepted by CVPR2024"},{"id":"http://arxiv.org/abs/2403.09635v1","updated":"2024-03-14T17:59:14Z","published":"2024-03-14T17:59:14Z","title":"Transformers Get Stable: An End-to-End Signal Propagation Theory for\n  Language Models","summary":"  In spite of their huge success, transformer models remain difficult to scale\nin depth. In this work, we develop a unified signal propagation theory and\nprovide formulae that govern the moments of the forward and backward signal\nthrough the transformer model. Our framework can be used to understand and\nmitigate vanishing/exploding gradients, rank collapse, and instability\nassociated with high attention scores. We also propose DeepScaleLM, an\ninitialization and scaling scheme that conserves unit output/gradient moments\nthroughout the model, enabling the training of very deep models with 100s of\nlayers. We find that transformer models could be much deeper - our deep models\nwith fewer parameters outperform shallow models in Language Modeling, Speech\nTranslation, and Image Classification, across Encoder-only, Decoder-only and\nEncoder-Decoder variants, for both Pre-LN and Post-LN transformers, for\nmultiple datasets and model sizes. These improvements also translate into\nimproved performance on downstream Question Answering tasks and improved\nrobustness for image classification.\n","authors":["Akhil Kedia","Mohd Abbas Zaidi","Sushil Khyalia","Jungho Jung","Harshith Goka","Haejun Lee"],"pdf_url":"https://arxiv.org/pdf/2403.09635v1.pdf","comment":"Akhil Kedia, Mohd Abbas Zaidi, Sushil Khyalia equal contribution.\n  Source code is available at\n  https://github.com/akhilkedia/TranformersGetStable"},{"id":"http://arxiv.org/abs/2310.07240v3","updated":"2024-03-14T17:58:52Z","published":"2023-10-11T07:08:20Z","title":"CacheGen: Fast Context Loading for Language Model Applications via KV\n  Cache Streaming","summary":"  As large language models (LLMs) take on complex tasks, their inputs are\nsupplemented with longer contexts that incorporate domain knowledge or\nuser-specific information. Yet using long contexts poses a challenge for\nresponsive LLM systems, as nothing can be generated until the whole context is\nprocessed by the LLM. While the context-processing delay can be reduced by\nreusing the KV cache of a context across different inputs, fetching the KV\ncache, which contains large tensors, over the network can cause extra network\ndelays.\n  CacheGen is a fast context-loading module for LLM systems. First, CacheGen\nuses a custom tensor encoder, which embraces KV cache's distributional\nproperties, to encode a KV cache into more compact bitstream representations\nwith negligible encoding/decoding overhead. This reduces the bandwidth demand\nto fetch the KV cache. Second, to maintain low context-loading delay and high\ngeneration quality, CacheGen adapts the streaming strategies to cope with\nchanges in available bandwidth. When available bandwidth drops, CacheGen may\nraise the compression level for a part of the context or choose to recompute\nits KV cache on the fly. We test CacheGen on four popular LLMs of various sizes\nand four datasets (662 contexts in total). Compared to the recent systems that\nreuse the KV cache, CacheGen reduces the KV cache size by 3.7-4.3x and the\ntotal delay in fetching and processing contexts by 2.7-3.2x while having\nnegligible impact on the LLM response quality in accuracy or perplexity.\n","authors":["Yuhan Liu","Hanchen Li","Yihua Cheng","Siddhant Ray","Yuyang Huang","Qizheng Zhang","Kuntai Du","Jiayi Yao","Shan Lu","Ganesh Ananthanarayanan","Michael Maire","Henry Hoffmann","Ari Holtzman","Junchen Jiang"],"pdf_url":"https://arxiv.org/pdf/2310.07240v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09629v1","updated":"2024-03-14T17:58:16Z","published":"2024-03-14T17:58:16Z","title":"Quiet-STaR: Language Models Can Teach Themselves to Think Before\n  Speaking","summary":"  When writing and talking, people sometimes pause to think. Although\nreasoning-focused works have often framed reasoning as a method of answering\nquestions or completing agentic tasks, reasoning is implicit in almost all\nwritten text. For example, this applies to the steps not stated between the\nlines of a proof or to the theory of mind underlying a conversation. In the\nSelf-Taught Reasoner (STaR, Zelikman et al. 2022), useful thinking is learned\nby inferring rationales from few-shot examples in question-answering and\nlearning from those that lead to a correct answer. This is a highly constrained\nsetting -- ideally, a language model could instead learn to infer unstated\nrationales in arbitrary text. We present Quiet-STaR, a generalization of STaR\nin which LMs learn to generate rationales at each token to explain future text,\nimproving their predictions. We address key challenges, including 1) the\ncomputational cost of generating continuations, 2) the fact that the LM does\nnot initially know how to generate or use internal thoughts, and 3) the need to\npredict beyond individual next tokens. To resolve these, we propose a tokenwise\nparallel sampling algorithm, using learnable tokens indicating a thought's\nstart and end, and an extended teacher-forcing technique. Encouragingly,\ngenerated rationales disproportionately help model difficult-to-predict tokens\nand improve the LM's ability to directly answer difficult questions. In\nparticular, after continued pretraining of an LM on a corpus of internet text\nwith Quiet-STaR, we find zero-shot improvements on GSM8K\n(5.9%$\\rightarrow$10.9%) and CommonsenseQA (36.3%$\\rightarrow$47.2%) and\nobserve a perplexity improvement of difficult tokens in natural text.\nCrucially, these improvements require no fine-tuning on these tasks. Quiet-STaR\nmarks a step towards LMs that can learn to reason in a more general and\nscalable way.\n","authors":["Eric Zelikman","Georges Harik","Yijia Shao","Varuna Jayasiri","Nick Haber","Noah D. Goodman"],"pdf_url":"https://arxiv.org/pdf/2403.09629v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09625v1","updated":"2024-03-14T17:57:04Z","published":"2024-03-14T17:57:04Z","title":"Make-Your-3D: Fast and Consistent Subject-Driven 3D Content Generation","summary":"  Recent years have witnessed the strong power of 3D generation models, which\noffer a new level of creative flexibility by allowing users to guide the 3D\ncontent generation process through a single image or natural language. However,\nit remains challenging for existing 3D generation methods to create\nsubject-driven 3D content across diverse prompts. In this paper, we introduce a\nnovel 3D customization method, dubbed Make-Your-3D that can personalize\nhigh-fidelity and consistent 3D content from only a single image of a subject\nwith text description within 5 minutes. Our key insight is to harmonize the\ndistributions of a multi-view diffusion model and an identity-specific 2D\ngenerative model, aligning them with the distribution of the desired 3D\nsubject. Specifically, we design a co-evolution framework to reduce the\nvariance of distributions, where each model undergoes a process of learning\nfrom the other through identity-aware optimization and subject-prior\noptimization, respectively. Extensive experiments demonstrate that our method\ncan produce high-quality, consistent, and subject-specific 3D content with\ntext-driven modifications that are unseen in subject image.\n","authors":["Fangfu Liu","Hanyang Wang","Weiliang Chen","Haowen Sun","Yueqi Duan"],"pdf_url":"https://arxiv.org/pdf/2403.09625v1.pdf","comment":"Project page: https://liuff19.github.io/Make-Your-3D"},{"id":"http://arxiv.org/abs/2403.09621v1","updated":"2024-03-14T17:55:10Z","published":"2024-03-14T17:55:10Z","title":"Minimax Optimal and Computationally Efficient Algorithms for\n  Distributionally Robust Offline Reinforcement Learning","summary":"  Distributionally robust offline reinforcement learning (RL), which seeks\nrobust policy training against environment perturbation by modeling dynamics\nuncertainty, calls for function approximations when facing large state-action\nspaces. However, the consideration of dynamics uncertainty introduces essential\nnonlinearity and computational burden, posing unique challenges for analyzing\nand practically employing function approximation. Focusing on a basic setting\nwhere the nominal model and perturbed models are linearly parameterized, we\npropose minimax optimal and computationally efficient algorithms realizing\nfunction approximation and initiate the study on instance-dependent\nsuboptimality analysis in the context of robust offline RL. Our results uncover\nthat function approximation in robust offline RL is essentially distinct from\nand probably harder than that in standard offline RL. Our algorithms and\ntheoretical results crucially depend on a variety of new techniques, involving\na novel function approximation mechanism incorporating variance information, a\nnew procedure of suboptimality and estimation uncertainty decomposition, a\nquantification of the robust value function shrinkage, and a meticulously\ndesigned family of hard instances, which might be of independent interest.\n","authors":["Zhishuai Liu","Pan Xu"],"pdf_url":"https://arxiv.org/pdf/2403.09621v1.pdf","comment":"53 pages, 1 figure, 1 table"},{"id":"http://arxiv.org/abs/2310.17467v2","updated":"2024-03-14T17:51:56Z","published":"2023-10-26T15:15:01Z","title":"The statistical thermodynamics of generative diffusion models: Phase\n  transitions, symmetry breaking and critical instability","summary":"  Generative diffusion models have achieved spectacular performance in many\nareas of generative modeling. While the fundamental ideas behind these models\ncome from non-equilibrium physics, variational inference and stochastic\ncalculus, in this paper we show that many aspects of these models can be\nunderstood using the tools of equilibrium statistical mechanics. Using this\nreformulation, we show that generative diffusion models undergo second-order\nphase transitions corresponding to symmetry breaking phenomena. We show that\nthese phase-transitions are always in a mean-field universality class, as they\nare the result of a self-consistency condition in the generative dynamics. We\nargue that the critical instability that arises from the phase transitions lies\nat the heart of their generative capabilities, which are characterized by a set\nof mean field critical exponents. Furthermore, using the statistical physics of\ndisordered systems, we show that memorization can be understood as a form of\ncritical condensation corresponding to a disordered phase transition. Finally,\nwe show that the dynamic equation of the generative process can be interpreted\nas a stochastic adiabatic transformation that minimizes the free energy while\nkeeping the system in thermal equilibrium.\n","authors":["Luca Ambrogioni"],"pdf_url":"https://arxiv.org/pdf/2310.17467v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09613v1","updated":"2024-03-14T17:51:54Z","published":"2024-03-14T17:51:54Z","title":"Reawakening knowledge: Anticipatory recovery from catastrophic\n  interference via structured training","summary":"  We explore the training dynamics of neural networks in a structured non-IID\nsetting where documents are presented cyclically in a fixed, repeated sequence.\nTypically, networks suffer from catastrophic interference when training on a\nsequence of documents; however, we discover a curious and remarkable property\nof LLMs fine-tuned sequentially in this setting: they exhibit anticipatory\nbehavior, recovering from the forgetting on documents before encountering them\nagain. The behavior emerges and becomes more robust as the architecture scales\nup its number of parameters. Through comprehensive experiments and\nvisualizations, we uncover new insights into training over-parameterized\nnetworks in structured environments.\n","authors":["Yanlai Yang","Matt Jones","Michael C. Mozer","Mengye Ren"],"pdf_url":"https://arxiv.org/pdf/2403.09613v1.pdf","comment":"19 pages, 18 figures"},{"id":"http://arxiv.org/abs/2403.09612v1","updated":"2024-03-14T17:51:38Z","published":"2024-03-14T17:51:38Z","title":"Compute-first optical detection for noise-resilient visual perception","summary":"  In the context of visual perception, the optical signal from a scene is\ntransferred into the electronic domain by detectors in the form of image data,\nwhich are then processed for the extraction of visual information. In noisy and\nweak-signal environments such as thermal imaging for night vision applications,\nhowever, the performance of neural computing tasks faces a significant\nbottleneck due to the inherent degradation of data quality upon noisy\ndetection. Here, we propose a concept of optical signal processing before\ndetection to address this issue. We demonstrate that spatially redistributing\noptical signals through a properly designed linear transformer can enhance the\ndetection noise resilience of visual perception tasks, as benchmarked with the\nMNIST classification. Our idea is supported by a quantitative analysis\ndetailing the relationship between signal concentration and noise robustness,\nas well as its practical implementation in an incoherent imaging system. This\ncompute-first detection scheme can pave the way for advancing infrared machine\nvision technologies widely used for industrial and defense applications.\n","authors":["Jungmin Kim","Nanfang Yu","Zongfu Yu"],"pdf_url":"https://arxiv.org/pdf/2403.09612v1.pdf","comment":"Main 9 pages, 5 figures, Supplementary information 5 pages"},{"id":"http://arxiv.org/abs/2403.09611v1","updated":"2024-03-14T17:51:32Z","published":"2024-03-14T17:51:32Z","title":"MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training","summary":"  In this work, we discuss building performant Multimodal Large Language Models\n(MLLMs). In particular, we study the importance of various architecture\ncomponents and data choices. Through careful and comprehensive ablations of the\nimage encoder, the vision language connector, and various pre-training data\nchoices, we identified several crucial design lessons. For example, we\ndemonstrate that for large-scale multimodal pre-training using a careful mix of\nimage-caption, interleaved image-text, and text-only data is crucial for\nachieving state-of-the-art (SOTA) few-shot results across multiple benchmarks,\ncompared to other published pre-training results. Further, we show that the\nimage encoder together with image resolution and the image token count has\nsubstantial impact, while the vision-language connector design is of\ncomparatively negligible importance. By scaling up the presented recipe, we\nbuild MM1, a family of multimodal models up to 30B parameters, consisting of\nboth dense models and mixture-of-experts (MoE) variants, that are SOTA in\npre-training metrics and achieve competitive performance after supervised\nfine-tuning on a range of established multimodal benchmarks. Thanks to\nlarge-scale pre-training, MM1 enjoys appealing properties such as enhanced\nin-context learning, and multi-image reasoning, enabling few-shot\nchain-of-thought prompting.\n","authors":["Brandon McKinzie","Zhe Gan","Jean-Philippe Fauconnier","Sam Dodge","Bowen Zhang","Philipp Dufter","Dhruti Shah","Xianzhi Du","Futang Peng","Floris Weers","Anton Belyi","Haotian Zhang","Karanjeet Singh","Doug Kang","Hongyu Hè","Max Schwarzer","Tom Gunter","Xiang Kong","Aonan Zhang","Jianyu Wang","Chong Wang","Nan Du","Tao Lei","Sam Wiseman","Mark Lee","Zirui Wang","Ruoming Pang","Peter Grasch","Alexander Toshev","Yinfei Yang"],"pdf_url":"https://arxiv.org/pdf/2403.09611v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09603v1","updated":"2024-03-14T17:44:35Z","published":"2024-03-14T17:44:35Z","title":"Optimistic Verifiable Training by Controlling Hardware Nondeterminism","summary":"  The increasing compute demands of AI systems has led to the emergence of\nservices that train models on behalf of clients lacking necessary resources.\nHowever, ensuring correctness of training and guarding against potential\ntraining-time attacks, such as data poisoning, poses challenges. Existing works\non verifiable training largely fall into two classes: proof-based systems,\nwhich struggle to scale due to requiring cryptographic techniques, and\n\"optimistic\" methods that consider a trusted third-party auditor who replicates\nthe training process. A key challenge with the latter is that hardware\nnondeterminism between GPU types during training prevents an auditor from\nreplicating the training process exactly, and such schemes are therefore\nnon-robust. We propose a method that combines training in a higher precision\nthan the target model, rounding after intermediate computation steps, and\nstoring rounding decisions based on an adaptive thresholding procedure, to\nsuccessfully control for nondeterminism. Across three different NVIDIA GPUs\n(A40, Titan XP, RTX 2080 Ti), we achieve exact training replication at FP32\nprecision for both full-training and fine-tuning of ResNet-50 (23M) and GPT-2\n(117M) models. Our verifiable training scheme significantly decreases the\nstorage and time costs compared to proof-based systems.\n","authors":["Megha Srivastava","Simran Arora","Dan Boneh"],"pdf_url":"https://arxiv.org/pdf/2403.09603v1.pdf","comment":"11 pages, 5 figures, preprint"},{"id":"http://arxiv.org/abs/2403.09598v1","updated":"2024-03-14T17:39:14Z","published":"2024-03-14T17:39:14Z","title":"Mixture of Mixups for Multi-label Classification of Rare Anuran Sounds","summary":"  Multi-label imbalanced classification poses a significant challenge in\nmachine learning, particularly evident in bioacoustics where animal sounds\noften co-occur, and certain sounds are much less frequent than others. This\npaper focuses on the specific case of classifying anuran species sounds using\nthe dataset AnuraSet, that contains both class imbalance and multi-label\nexamples. To address these challenges, we introduce Mixture of Mixups (Mix2), a\nframework that leverages mixing regularization methods Mixup, Manifold Mixup,\nand MultiMix. Experimental results show that these methods, individually, may\nlead to suboptimal results; however, when applied randomly, with one selected\nat each training iteration, they prove effective in addressing the mentioned\nchallenges, particularly for rare classes with few occurrences. Further\nanalysis reveals that Mix2 is also proficient in classifying sounds across\nvarious levels of class co-occurrences.\n","authors":["Ilyass Moummad","Nicolas Farrugia","Romain Serizel","Jeremy Froidevaux","Vincent Lostanlen"],"pdf_url":"https://arxiv.org/pdf/2403.09598v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.14496v4","updated":"2024-03-14T17:31:06Z","published":"2023-09-25T19:45:45Z","title":"Era Splitting -- Invariant Learning for Decision Trees","summary":"  Real-life machine learning problems exhibit distributional shifts in the data\nfrom one time to another or from one place to another. This behavior is beyond\nthe scope of the traditional empirical risk minimization paradigm, which\nassumes i.i.d. distribution of data over time and across locations. The\nemerging field of out-of-distribution (OOD) generalization addresses this\nreality with new theory and algorithms which incorporate environmental, or\nera-wise information into the algorithms. So far, most research has been\nfocused on linear models and/or neural networks. In this research we develop\ntwo new splitting criteria for decision trees, which allow us to apply ideas\nfrom OOD generalization research to decision tree models, namely, gradient\nboosting decision trees (GBDT). The new splitting criteria use era-wise\ninformation associated with the data to grow tree-based models that are optimal\nacross all disjoint eras in the data, instead of optimal over the entire data\nset pooled together, which is the default setting. In this paper, two new\nsplitting criteria are defined and analyzed theoretically. Effectiveness is\ntested on four experiments, ranging from simple, synthetic to complex,\nreal-world applications. In particular we cast the OOD domain-adaptation\nproblem in the context of financial markets, where the new models out-perform\nstate-of-the-art GBDT models on the Numerai data set. The new criteria are\nincorporated into the Scikit-Learn code base and made freely available online.\n","authors":["Timothy DeLise"],"pdf_url":"https://arxiv.org/pdf/2309.14496v4.pdf","comment":"20 pages, 8 figures"},{"id":"http://arxiv.org/abs/2403.09588v1","updated":"2024-03-14T17:26:00Z","published":"2024-03-14T17:26:00Z","title":"Iterative Forgetting: Online Data Stream Regression Using\n  Database-Inspired Adaptive Granulation","summary":"  Many modern systems, such as financial, transportation, and\ntelecommunications systems, are time-sensitive in the sense that they demand\nlow-latency predictions for real-time decision-making. Such systems often have\nto contend with continuous unbounded data streams as well as concept drift,\nwhich are challenging requirements that traditional regression techniques are\nunable to cater to. There exists a need to create novel data stream regression\nmethods that can handle these scenarios. We present a database-inspired\ndatastream regression model that (a) uses inspiration from R*-trees to create\ngranules from incoming datastreams such that relevant information is retained,\n(b) iteratively forgets granules whose information is deemed to be outdated,\nthus maintaining a list of only recent, relevant granules, and (c) uses the\nrecent data and granules to provide low-latency predictions. The\nR*-tree-inspired approach also makes the algorithm amenable to integration with\ndatabase systems. Our experiments demonstrate that the ability of this method\nto discard data produces a significant order-of-magnitude improvement in\nlatency and training time when evaluated against the most accurate\nstate-of-the-art algorithms, while the R*-tree-inspired granulation technique\nprovides competitively accurate predictions\n","authors":["Niket Kathiriya","Hossein Haeri","Cindy Chen","Kshitij Jerath"],"pdf_url":"https://arxiv.org/pdf/2403.09588v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.00860v3","updated":"2024-03-14T17:21:37Z","published":"2023-11-01T21:28:24Z","title":"Zero Coordinate Shift: Whetted Automatic Differentiation for\n  Physics-informed Operator Learning","summary":"  Automatic differentiation (AD) is a critical step in physics-informed machine\nlearning, required for computing the high-order derivatives of network output\nw.r.t. coordinates of collocation points. In this paper, we present a novel and\nlightweight algorithm to conduct AD for physics-informed operator learning,\nwhich we call the trick of Zero Coordinate Shift (ZCS). Instead of making all\nsampled coordinates as leaf variables, ZCS introduces only one scalar-valued\nleaf variable for each spatial or temporal dimension, simplifying the wanted\nderivatives from \"many-roots-many-leaves\" to \"one-root-many-leaves\" whereby\nreverse-mode AD becomes directly utilisable. It has led to an outstanding\nperformance leap by avoiding the duplication of the computational graph along\nthe dimension of functions (physical parameters). ZCS is easy to implement with\ncurrent deep learning libraries; our own implementation is achieved by\nextending the DeepXDE package. We carry out a comprehensive benchmark analysis\nand several case studies, training physics-informed DeepONets to solve partial\ndifferential equations (PDEs) without data. The results show that ZCS has\npersistently reduced GPU memory consumption and wall time for training by an\norder of magnitude, and such reduction factor scales with the number of\nfunctions. As a low-level optimisation technique, ZCS imposes no restrictions\non data, physics (PDE) or network architecture and does not compromise training\nresults from any aspect.\n","authors":["Kuangdai Leng","Mallikarjun Shankar","Jeyan Thiyagalingam"],"pdf_url":"https://arxiv.org/pdf/2311.00860v3.pdf","comment":"Published in Journal of Computational Physics.\n  https://doi.org/10.1016/j.jcp.2024.112904"},{"id":"http://arxiv.org/abs/2403.09580v1","updated":"2024-03-14T17:14:53Z","published":"2024-03-14T17:14:53Z","title":"Algorithmic syntactic causal identification","summary":"  Causal identification in causal Bayes nets (CBNs) is an important tool in\ncausal inference allowing the derivation of interventional distributions from\nobservational distributions where this is possible in principle. However, most\nexisting formulations of causal identification using techniques such as\nd-separation and do-calculus are expressed within the mathematical language of\nclassical probability theory on CBNs. However, there are many causal settings\nwhere probability theory and hence current causal identification techniques are\ninapplicable such as relational databases, dataflow programs such as hardware\ndescription languages, distributed systems and most modern machine learning\nalgorithms. We show that this restriction can be lifted by replacing the use of\nclassical probability theory with the alternative axiomatic foundation of\nsymmetric monoidal categories. In this alternative axiomatization, we show how\nan unambiguous and clean distinction can be drawn between the general syntax of\ncausal models and any specific semantic implementation of that causal model.\nThis allows a purely syntactic algorithmic description of general causal\nidentification by a translation of recent formulations of the general ID\nalgorithm through fixing. Our description is given entirely in terms of the\nnon-parametric ADMG structure specifying a causal model and the algebraic\nsignature of the corresponding monoidal category, to which a sequence of\nmanipulations is then applied so as to arrive at a modified monoidal category\nin which the desired, purely syntactic interventional causal model, is\nobtained. We use this idea to derive purely syntactic analogues of classical\nback-door and front-door causal adjustment, and illustrate an application to a\nmore complex causal model.\n","authors":["Dhurim Cakiqi","Max A. Little"],"pdf_url":"https://arxiv.org/pdf/2403.09580v1.pdf","comment":"11 pages, 2 TikZ figures"},{"id":"http://arxiv.org/abs/2403.09579v1","updated":"2024-03-14T17:13:37Z","published":"2024-03-14T17:13:37Z","title":"uaMix-MAE: Efficient Tuning of Pretrained Audio Transformers with\n  Unsupervised Audio Mixtures","summary":"  Masked Autoencoders (MAEs) learn rich low-level representations from\nunlabeled data but require substantial labeled data to effectively adapt to\ndownstream tasks. Conversely, Instance Discrimination (ID) emphasizes\nhigh-level semantics, offering a potential solution to alleviate annotation\nrequirements in MAEs. Although combining these two approaches can address\ndownstream tasks with limited labeled data, naively integrating ID into MAEs\nleads to extended training times and high computational costs. To address this\nchallenge, we introduce uaMix-MAE, an efficient ID tuning strategy that\nleverages unsupervised audio mixtures. Utilizing contrastive tuning, uaMix-MAE\naligns the representations of pretrained MAEs, thereby facilitating effective\nadaptation to task-specific semantics. To optimize the model with small amounts\nof unlabeled data, we propose an audio mixing technique that manipulates audio\nsamples in both input and virtual label spaces. Experiments in low/few-shot\nsettings demonstrate that \\modelname achieves 4-6% accuracy improvements over\nvarious benchmarks when tuned with limited unlabeled data, such as\nAudioSet-20K. Code is available at https://github.com/PLAN-Lab/uamix-MAE\n","authors":["Afrina Tabassum","Dung Tran","Trung Dang","Ismini Lourentzou","Kazuhito Koishida"],"pdf_url":"https://arxiv.org/pdf/2403.09579v1.pdf","comment":"5 pages, 6 figures, 4 tables. To appear in ICASSP'2024"},{"id":"http://arxiv.org/abs/2403.02524v2","updated":"2024-03-14T17:04:37Z","published":"2024-03-04T22:28:20Z","title":"Koopman operators with intrinsic observables in rigged reproducing\n  kernel Hilbert spaces","summary":"  This paper presents a novel approach for estimating the Koopman operator\ndefined on a reproducing kernel Hilbert space (RKHS) and its spectra. We\npropose an estimation method, what we call Jet Dynamic Mode Decomposition\n(JetDMD), leveraging the intrinsic structure of RKHS and the geometric notion\nknown as jets to enhance the estimation of the Koopman operator. This method\nrefines the traditional Extended Dynamic Mode Decomposition (EDMD) in accuracy,\nespecially in the numerical estimation of eigenvalues. This paper proves\nJetDMD's superiority through explicit error bounds and convergence rate for\nspecial positive definite kernels, offering a solid theoretical foundation for\nits performance. We also delve into the spectral analysis of the Koopman\noperator, proposing the notion of extended Koopman operator within a framework\nof rigged Hilbert space. This notion leads to a deeper understanding of\nestimated Koopman eigenfunctions and capturing them outside the original\nfunction space. Through the theory of rigged Hilbert space, our study provides\na principled methodology to analyze the estimated spectrum and eigenfunctions\nof Koopman operators, and enables eigendecomposition within a rigged RKHS. We\nalso propose a new effective method for reconstructing the dynamical system\nfrom temporally-sampled trajectory data of the dynamical system with solid\ntheoretical guarantee. We conduct several numerical simulations using the van\nder Pol oscillator, the Duffing oscillator, the H\\'enon map, and the Lorenz\nattractor, and illustrate the performance of JetDMD with clear numerical\ncomputations of eigenvalues and accurate predictions of the dynamical systems.\n","authors":["Isao Ishikawa","Yuka Hashimoto","Masahiro Ikeda","Yoshinobu Kawahara"],"pdf_url":"https://arxiv.org/pdf/2403.02524v2.pdf","comment":"We correct several typos. We have released the code for the numerical\n  simulation at https://github.com/1sa014kawa/JetDMD"},{"id":"http://arxiv.org/abs/2403.09571v1","updated":"2024-03-14T17:00:29Z","published":"2024-03-14T17:00:29Z","title":"Are you a robot? Detecting Autonomous Vehicles from Behavior Analysis","summary":"  The tremendous hype around autonomous driving is eagerly calling for emerging\nand novel technologies to support advanced mobility use cases. As car\nmanufactures keep developing SAE level 3+ systems to improve the safety and\ncomfort of passengers, traffic authorities need to establish new procedures to\nmanage the transition from human-driven to fully-autonomous vehicles while\nproviding a feedback-loop mechanism to fine-tune envisioned autonomous systems.\nThus, a way to automatically profile autonomous vehicles and differentiate\nthose from human-driven ones is a must. In this paper, we present a\nfully-fledged framework that monitors active vehicles using camera images and\nstate information in order to determine whether vehicles are autonomous,\nwithout requiring any active notification from the vehicles themselves.\nEssentially, it builds on the cooperation among vehicles, which share their\ndata acquired on the road feeding a machine learning model to identify\nautonomous cars. We extensively tested our solution and created the NexusStreet\ndataset, by means of the CARLA simulator, employing an autonomous driving\ncontrol agent and a steering wheel maneuvered by licensed drivers. Experiments\nshow it is possible to discriminate the two behaviors by analyzing video clips\nwith an accuracy of 80%, which improves up to 93% when the target state\ninformation is available. Lastly, we deliberately degraded the state to observe\nhow the framework performs under non-ideal data collection conditions.\n","authors":["Fabio Maresca","Filippo Grazioli","Antonio Albanese","Vincenzo Sciancalepore","Gianpiero Negri","Xavier Costa-Perez"],"pdf_url":"https://arxiv.org/pdf/2403.09571v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09570v1","updated":"2024-03-14T17:00:01Z","published":"2024-03-14T17:00:01Z","title":"Multi-Fidelity Bayesian Optimization With Across-Task Transferable\n  Max-Value Entropy Search","summary":"  In many applications, ranging from logistics to engineering, a designer is\nfaced with a sequence of optimization tasks for which the objectives are in the\nform of black-box functions that are costly to evaluate. For example, the\ndesigner may need to tune the hyperparameters of neural network models for\ndifferent learning tasks over time. Rather than evaluating the objective\nfunction for each candidate solution, the designer may have access to\napproximations of the objective functions, for which higher-fidelity\nevaluations entail a larger cost. Existing multi-fidelity black-box\noptimization strategies select candidate solutions and fidelity levels with the\ngoal of maximizing the information accrued about the optimal value or solution\nfor the current task. Assuming that successive optimization tasks are related,\nthis paper introduces a novel information-theoretic acquisition function that\nbalances the need to acquire information about the current task with the goal\nof collecting information transferable to future tasks. The proposed method\nincludes shared inter-task latent variables, which are transferred across tasks\nby implementing particle-based variational Bayesian updates. Experimental\nresults across synthetic and real-world examples reveal that the proposed\nprovident acquisition strategy that caters to future tasks can significantly\nimprove the optimization efficiency as soon as a sufficient number of tasks is\nprocessed.\n","authors":["Yunchuan Zhang","Sangwoo Park","Osvaldo Simeone"],"pdf_url":"https://arxiv.org/pdf/2403.09570v1.pdf","comment":"submitted to IEEE for review"},{"id":"http://arxiv.org/abs/2403.07865v2","updated":"2024-03-14T16:57:37Z","published":"2024-03-12T17:55:38Z","title":"Exploring Safety Generalization Challenges of Large Language Models via\n  Code","summary":"  The rapid advancement of Large Language Models (LLMs) has brought about\nremarkable capabilities in natural language processing but also raised concerns\nabout their potential misuse. While strategies like supervised fine-tuning and\nreinforcement learning from human feedback have enhanced their safety, these\nmethods primarily focus on natural languages, which may not generalize to other\ndomains. This paper introduces CodeAttack, a framework that transforms natural\nlanguage inputs into code inputs, presenting a novel environment for testing\nthe safety generalization of LLMs. Our comprehensive studies on\nstate-of-the-art LLMs including GPT-4, Claude-2, and Llama-2 series reveal a\ncommon safety vulnerability of these models against code input: CodeAttack\nconsistently bypasses the safety guardrails of all models more than 80% of the\ntime. Furthermore, we find that a larger distribution gap between CodeAttack\nand natural language leads to weaker safety generalization, such as encoding\nnatural language input with data structures or using less popular programming\nlanguages. These findings highlight new safety risks in the code domain and the\nneed for more robust safety alignment algorithms to match the code capabilities\nof LLMs.\n","authors":["Qibing Ren","Chang Gao","Jing Shao","Junchi Yan","Xin Tan","Yu Qiao","Wai Lam","Lizhuang Ma"],"pdf_url":"https://arxiv.org/pdf/2403.07865v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09560v1","updated":"2024-03-14T16:52:57Z","published":"2024-03-14T16:52:57Z","title":"Self-Consistency Training for Hamiltonian Prediction","summary":"  Hamiltonian prediction is a versatile formulation to leverage machine\nlearning for solving molecular science problems. Yet, its applicability is\nlimited by insufficient labeled data for training. In this work, we highlight\nthat Hamiltonian prediction possesses a self-consistency principle, based on\nwhich we propose an exact training method that does not require labeled data.\nThis merit addresses the data scarcity difficulty, and distinguishes the task\nfrom other property prediction formulations with unique benefits: (1)\nself-consistency training enables the model to be trained on a large amount of\nunlabeled data, hence substantially enhances generalization; (2)\nself-consistency training is more efficient than labeling data with DFT for\nsupervised training, since it is an amortization of DFT calculation over a set\nof molecular structures. We empirically demonstrate the better generalization\nin data-scarce and out-of-distribution scenarios, and the better efficiency\nfrom the amortization. These benefits push forward the applicability of\nHamiltonian prediction to an ever larger scale.\n","authors":["He Zhang","Chang Liu","Zun Wang","Xinran Wei","Siyuan Liu","Nanning Zheng","Bin Shao","Tie-Yan Liu"],"pdf_url":"https://arxiv.org/pdf/2403.09560v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.08399v2","updated":"2024-03-14T16:49:24Z","published":"2022-12-16T10:46:20Z","title":"Assessing the Impact of Sequence Length Learning on Classification Tasks\n  for Transformer Encoder Models","summary":"  Classification algorithms using Transformer architectures can be affected by\nthe sequence length learning problem whenever observations from different\nclasses have a different length distribution. This problem causes models to use\nsequence length as a predictive feature instead of relying on important textual\ninformation. Although most public datasets are not affected by this problem,\nprivately owned corpora for fields such as medicine and insurance may carry\nthis data bias. The exploitation of this sequence length feature poses\nchallenges throughout the value chain as these machine learning models can be\nused in critical applications. In this paper, we empirically expose this\nproblem and present approaches to minimize its impacts.\n","authors":["Jean-Thomas Baillargeon","Luc Lamontagne"],"pdf_url":"https://arxiv.org/pdf/2212.08399v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08757v2","updated":"2024-03-14T16:40:51Z","published":"2024-03-13T17:55:34Z","title":"Efficient Combinatorial Optimization via Heat Diffusion","summary":"  Combinatorial optimization problems are widespread but inherently challenging\ndue to their discrete nature.The primary limitation of existing methods is that\nthey can only access a small fraction of the solution space at each iteration,\nresulting in limited efficiency for searching the global optimal. To overcome\nthis challenge, diverging from conventional efforts of expanding the solver's\nsearch scope, we focus on enabling information to actively propagate to the\nsolver through heat diffusion. By transforming the target function while\npreserving its optima, heat diffusion facilitates information flow from distant\nregions to the solver, providing more efficient navigation. Utilizing heat\ndiffusion, we propose a framework for solving general combinatorial\noptimization problems. The proposed methodology demonstrates superior\nperformance across a range of the most challenging and widely encountered\ncombinatorial optimizations. Echoing recent advancements in harnessing\nthermodynamics for generative artificial intelligence, our study further\nreveals its significant potential in advancing combinatorial optimization.\n","authors":["Hengyuan Ma","Wenlian Lu","Jianfeng Feng"],"pdf_url":"https://arxiv.org/pdf/2403.08757v2.pdf","comment":"Code is available in https://github.com/AwakerMhy/HeO"},{"id":"http://arxiv.org/abs/2305.10424v8","updated":"2024-03-14T16:38:36Z","published":"2023-05-17T17:56:59Z","title":"ZeroFlow: Scalable Scene Flow via Distillation","summary":"  Scene flow estimation is the task of describing the 3D motion field between\ntemporally successive point clouds. State-of-the-art methods use strong priors\nand test-time optimization techniques, but require on the order of tens of\nseconds to process full-size point clouds, making them unusable as computer\nvision primitives for real-time applications such as open world object\ndetection. Feedforward methods are considerably faster, running on the order of\ntens to hundreds of milliseconds for full-size point clouds, but require\nexpensive human supervision. To address both limitations, we propose Scene Flow\nvia Distillation, a simple, scalable distillation framework that uses a\nlabel-free optimization method to produce pseudo-labels to supervise a\nfeedforward model. Our instantiation of this framework, ZeroFlow, achieves\nstate-of-the-art performance on the Argoverse 2 Self-Supervised Scene Flow\nChallenge while using zero human labels by simply training on large-scale,\ndiverse unlabeled data. At test-time, ZeroFlow is over 1000x faster than\nlabel-free state-of-the-art optimization-based methods on full-size point\nclouds (34 FPS vs 0.028 FPS) and over 1000x cheaper to train on unlabeled data\ncompared to the cost of human annotation (\\$394 vs ~\\$750,000). To facilitate\nfurther research, we release our code, trained model weights, and high quality\npseudo-labels for the Argoverse 2 and Waymo Open datasets at\nhttps://vedder.io/zeroflow.html\n","authors":["Kyle Vedder","Neehar Peri","Nathaniel Chodosh","Ishan Khatri","Eric Eaton","Dinesh Jayaraman","Yang Liu","Deva Ramanan","James Hays"],"pdf_url":"https://arxiv.org/pdf/2305.10424v8.pdf","comment":"Accepted to ICLR 2024. 9 pages, 4 pages of citations, 6 pages of\n  Supplemental. Project page with data releases is at\n  http://vedder.io/zeroflow.html"},{"id":"http://arxiv.org/abs/2403.09549v1","updated":"2024-03-14T16:38:02Z","published":"2024-03-14T16:38:02Z","title":"Generalizing Denoising to Non-Equilibrium Structures Improves\n  Equivariant Force Fields","summary":"  Understanding the interactions of atoms such as forces in 3D atomistic\nsystems is fundamental to many applications like molecular dynamics and\ncatalyst design. However, simulating these interactions requires\ncompute-intensive ab initio calculations and thus results in limited data for\ntraining neural networks. In this paper, we propose to use denoising\nnon-equilibrium structures (DeNS) as an auxiliary task to better leverage\ntraining data and improve performance. For training with DeNS, we first corrupt\na 3D structure by adding noise to its 3D coordinates and then predict the\nnoise. Different from previous works on denoising, which are limited to\nequilibrium structures, the proposed method generalizes denoising to a much\nlarger set of non-equilibrium structures. The main difference is that a\nnon-equilibrium structure does not correspond to local energy minima and has\nnon-zero forces, and therefore it can have many possible atomic positions\ncompared to an equilibrium structure. This makes denoising non-equilibrium\nstructures an ill-posed problem since the target of denoising is not uniquely\ndefined. Our key insight is to additionally encode the forces of the original\nnon-equilibrium structure to specify which non-equilibrium structure we are\ndenoising. Concretely, given a corrupted non-equilibrium structure and the\nforces of the original one, we predict the non-equilibrium structure satisfying\nthe input forces instead of any arbitrary structures. Since DeNS requires\nencoding forces, DeNS favors equivariant networks, which can easily incorporate\nforces and other higher-order tensors in node embeddings. We study the\neffectiveness of training equivariant networks with DeNS on OC20, OC22 and MD17\ndatasets and demonstrate that DeNS can achieve new state-of-the-art results on\nOC20 and OC22 and significantly improve training efficiency on MD17.\n","authors":["Yi-Lun Liao","Tess Smidt","Abhishek Das"],"pdf_url":"https://arxiv.org/pdf/2403.09549v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.01308v2","updated":"2024-03-14T16:37:37Z","published":"2024-03-02T20:40:11Z","title":"VBART: The Turkish LLM","summary":"  We present VBART, the first Turkish sequence-to-sequence Large Language\nModels (LLMs) pre-trained on a large corpus from scratch. VBART are compact\nLLMs based on good ideas leveraged from BART and mBART models and come in two\nsizes, Large and XLarge. Fine-tuned VBART models surpass the prior\nstate-of-the-art results in abstractive text summarization, title generation,\ntext paraphrasing, question answering and question generation tasks. They allow\nfine-tuning for future text generation tasks and datasets, carving a new path\nfor Turkish Natural Language Processing (NLP) research. Our work shows that\nhaving a pre-trained LLM for Turkish outperforms up to 3x multilingual models,\nimproving existing results and providing efficient models for training and\ninference. Moreover, we show that our monolingual tokenizer is up to 11x more\nefficient than multilingual tokenizers. Last but not least, we introduce a\nmethod to enlarge an existing pre-trained LLM and question the relevancy of\nChinchilla Scaling Law to sequence-to-sequence masked language models. Our\nfine-tuned models, tokenizer and cleaned vngrs-web-corpus of 135 GB are\npublicly available at huggingface.co/vngrs-ai.\n","authors":["Meliksah Turker","Mehmet Erdi Ari","Aydin Han"],"pdf_url":"https://arxiv.org/pdf/2403.01308v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09548v1","updated":"2024-03-14T16:35:43Z","published":"2024-03-14T16:35:43Z","title":"Breast Cancer Classification Using Gradient Boosting Algorithms Focusing\n  on Reducing the False Negative and SHAP for Explainability","summary":"  Cancer is one of the diseases that kill the most women in the world, with\nbreast cancer being responsible for the highest number of cancer cases and\nconsequently deaths. However, it can be prevented by early detection and,\nconsequently, early treatment. Any development for detection or perdition this\nkind of cancer is important for a better healthy life. Many studies focus on a\nmodel with high accuracy in cancer prediction, but sometimes accuracy alone may\nnot always be a reliable metric. This study implies an investigative approach\nto studying the performance of different machine learning algorithms based on\nboosting to predict breast cancer focusing on the recall metric. Boosting\nmachine learning algorithms has been proven to be an effective tool for\ndetecting medical diseases. The dataset of the University of California, Irvine\n(UCI) repository has been utilized to train and test the model classifier that\ncontains their attributes. The main objective of this study is to use\nstate-of-the-art boosting algorithms such as AdaBoost, XGBoost, CatBoost and\nLightGBM to predict and diagnose breast cancer and to find the most effective\nmetric regarding recall, ROC-AUC, and confusion matrix. Furthermore, our study\nis the first to use these four boosting algorithms with Optuna, a library for\nhyperparameter optimization, and the SHAP method to improve the\ninterpretability of our model, which can be used as a support to identify and\npredict breast cancer. We were able to improve AUC or recall for all the models\nand reduce the False Negative for AdaBoost and LigthGBM the final AUC were more\nthan 99.41\\% for all models.\n","authors":["João Manoel Herrera Pinheiro","Marcelo Becker"],"pdf_url":"https://arxiv.org/pdf/2403.09548v1.pdf","comment":"9 pages, 16 figures"},{"id":"http://arxiv.org/abs/2403.06726v2","updated":"2024-03-14T16:35:41Z","published":"2024-03-11T13:44:49Z","title":"Probabilistic Contrastive Learning for Long-Tailed Visual Recognition","summary":"  Long-tailed distributions frequently emerge in real-world data, where a large\nnumber of minority categories contain a limited number of samples. Such\nimbalance issue considerably impairs the performance of standard supervised\nlearning algorithms, which are mainly designed for balanced training sets.\nRecent investigations have revealed that supervised contrastive learning\nexhibits promising potential in alleviating the data imbalance. However, the\nperformance of supervised contrastive learning is plagued by an inherent\nchallenge: it necessitates sufficiently large batches of training data to\nconstruct contrastive pairs that cover all categories, yet this requirement is\ndifficult to meet in the context of class-imbalanced data. To overcome this\nobstacle, we propose a novel probabilistic contrastive (ProCo) learning\nalgorithm that estimates the data distribution of the samples from each class\nin the feature space, and samples contrastive pairs accordingly. In fact,\nestimating the distributions of all classes using features in a small batch,\nparticularly for imbalanced data, is not feasible. Our key idea is to introduce\na reasonable and simple assumption that the normalized features in contrastive\nlearning follow a mixture of von Mises-Fisher (vMF) distributions on unit\nspace, which brings two-fold benefits. First, the distribution parameters can\nbe estimated using only the first sample moment, which can be efficiently\ncomputed in an online manner across different batches. Second, based on the\nestimated distribution, the vMF distribution allows us to sample an infinite\nnumber of contrastive pairs and derive a closed form of the expected\ncontrastive loss for efficient optimization. Our code is available at\nhttps://github.com/LeapLabTHU/ProCo.\n","authors":["Chaoqun Du","Yulin Wang","Shiji Song","Gao Huang"],"pdf_url":"https://arxiv.org/pdf/2403.06726v2.pdf","comment":"Accepted by IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (T-PAMI)"},{"id":"http://arxiv.org/abs/2403.09547v1","updated":"2024-03-14T16:35:39Z","published":"2024-03-14T16:35:39Z","title":"How do Machine Learning Projects use Continuous Integration Practices?\n  An Empirical Study on GitHub Actions","summary":"  Continuous Integration (CI) is a well-established practice in traditional\nsoftware development, but its nuances in the domain of Machine Learning (ML)\nprojects remain relatively unexplored. Given the distinctive nature of ML\ndevelopment, understanding how CI practices are adopted in this context is\ncrucial for tailoring effective approaches. In this study, we conduct a\ncomprehensive analysis of 185 open-source projects on GitHub (93 ML and 92\nnon-ML projects). Our investigation comprises both quantitative and qualitative\ndimensions, aiming to uncover differences in CI adoption between ML and non-ML\nprojects. Our findings indicate that ML projects often require longer build\ndurations, and medium-sized ML projects exhibit lower test coverage compared to\nnon-ML projects. Moreover, small and medium-sized ML projects show a higher\nprevalence of increasing build duration trends compared to their non-ML\ncounterparts. Additionally, our qualitative analysis illuminates the\ndiscussions around CI in both ML and non-ML projects, encompassing themes like\nCI Build Execution and Status, CI Testing, and CI Infrastructure. These\ninsights shed light on the unique challenges faced by ML projects in adopting\nCI practices effectively.\n","authors":["João Helis Bernardo","Daniel Alencar da Costa","Sérgio Queiroz de Medeiros","Uirá Kulesza"],"pdf_url":"https://arxiv.org/pdf/2403.09547v1.pdf","comment":"10 pages, Mining Software Repositories, MSR 2024"},{"id":"http://arxiv.org/abs/2403.09543v1","updated":"2024-03-14T16:30:52Z","published":"2024-03-14T16:30:52Z","title":"Explorations in Texture Learning","summary":"  In this work, we investigate \\textit{texture learning}: the identification of\ntextures learned by object classification models, and the extent to which they\nrely on these textures. We build texture-object associations that uncover new\ninsights about the relationships between texture and object classes in CNNs and\nfind three classes of results: associations that are strong and expected,\nstrong and not expected, and expected but not present. Our analysis\ndemonstrates that investigations in texture learning enable new methods for\ninterpretability and have the potential to uncover unexpected biases.\n","authors":["Blaine Hoak","Patrick McDaniel"],"pdf_url":"https://arxiv.org/pdf/2403.09543v1.pdf","comment":"Accepted to ICLR 2024, Tiny Papers Track"},{"id":"http://arxiv.org/abs/2309.10639v4","updated":"2024-03-14T16:29:56Z","published":"2023-09-19T14:20:55Z","title":"Geometric structure of Deep Learning networks and construction of global\n  ${\\mathcal L}^2$ minimizers","summary":"  In this paper, we explicitly determine local and global minimizers of the\n$\\mathcal{L}^2$ cost function in underparametrized Deep Learning (DL) networks;\nour main goal is to shed light on their geometric structure and properties. We\naccomplish this by a direct construction, without invoking the gradient descent\nflow at any point of this work. We specifically consider $L$ hidden layers, a\nReLU ramp activation function, an $\\mathcal{L}^2$ Schatten class (or\nHilbert-Schmidt) cost function, input and output spaces $\\mathbb{R}^Q$ with\nequal dimension $Q\\geq1$, and hidden layers also defined on $\\mathbb{R}^{Q}$;\nthe training inputs are assumed to be sufficiently clustered. The training\ninput size $N$ can be arbitrarily large - thus, we are considering the\nunderparametrized regime. More general settings are left to future work. We\nconstruct an explicit family of minimizers for the global minimum of the cost\nfunction in the case $L\\geq Q$, which we show to be degenerate. Moreover, we\ndetermine a set of $2^Q-1$ distinct degenerate local minima of the cost\nfunction. In the context presented here, the concatenation of hidden layers of\nthe DL network is reinterpreted as a recursive application of a {\\em truncation\nmap} which \"curates\" the training inputs by minimizing their noise to signal\nratio.\n","authors":["Thomas Chen","Patricia Muñoz Ewald"],"pdf_url":"https://arxiv.org/pdf/2309.10639v4.pdf","comment":"AMS Latex, 22 pages. Typos corrected, slightly extended"},{"id":"http://arxiv.org/abs/2403.09539v1","updated":"2024-03-14T16:27:49Z","published":"2024-03-14T16:27:49Z","title":"Logits of API-Protected LLMs Leak Proprietary Information","summary":"  The commercialization of large language models (LLMs) has led to the common\npractice of high-level API-only access to proprietary models. In this work, we\nshow that even with a conservative assumption about the model architecture, it\nis possible to learn a surprisingly large amount of non-public information\nabout an API-protected LLM from a relatively small number of API queries (e.g.,\ncosting under $1,000 for OpenAI's gpt-3.5-turbo). Our findings are centered on\none key observation: most modern LLMs suffer from a softmax bottleneck, which\nrestricts the model outputs to a linear subspace of the full output space. We\nshow that this lends itself to a model image or a model signature which unlocks\nseveral capabilities with affordable cost: efficiently discovering the LLM's\nhidden size, obtaining full-vocabulary outputs, detecting and disambiguating\ndifferent model updates, identifying the source LLM given a single full LLM\noutput, and even estimating the output layer parameters. Our empirical\ninvestigations show the effectiveness of our methods, which allow us to\nestimate the embedding size of OpenAI's gpt-3.5-turbo to be about 4,096.\nLastly, we discuss ways that LLM providers can guard against these attacks, as\nwell as how these capabilities can be viewed as a feature (rather than a bug)\nby allowing for greater transparency and accountability.\n","authors":["Matthew Finlayson","Swabha Swayamdipta","Xiang Ren"],"pdf_url":"https://arxiv.org/pdf/2403.09539v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.13991v2","updated":"2024-03-14T16:20:50Z","published":"2023-05-23T12:20:29Z","title":"Expressive Losses for Verified Robustness via Convex Combinations","summary":"  In order to train networks for verified adversarial robustness, it is common\nto over-approximate the worst-case loss over perturbation regions, resulting in\nnetworks that attain verifiability at the expense of standard performance. As\nshown in recent work, better trade-offs between accuracy and robustness can be\nobtained by carefully coupling adversarial training with over-approximations.\nWe hypothesize that the expressivity of a loss function, which we formalize as\nthe ability to span a range of trade-offs between lower and upper bounds to the\nworst-case loss through a single parameter (the over-approximation\ncoefficient), is key to attaining state-of-the-art performance. To support our\nhypothesis, we show that trivial expressive losses, obtained via convex\ncombinations between adversarial attacks and IBP bounds, yield state-of-the-art\nresults across a variety of settings in spite of their conceptual simplicity.\nWe provide a detailed analysis of the relationship between the\nover-approximation coefficient and performance profiles across different\nexpressive losses, showing that, while expressivity is essential, better\napproximations of the worst-case loss are not necessarily linked to superior\nrobustness-accuracy trade-offs.\n","authors":["Alessandro De Palma","Rudy Bunel","Krishnamurthy Dvijotham","M. Pawan Kumar","Robert Stanforth","Alessio Lomuscio"],"pdf_url":"https://arxiv.org/pdf/2305.13991v2.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2403.09516v1","updated":"2024-03-14T15:58:36Z","published":"2024-03-14T15:58:36Z","title":"Leveraging Prototypical Representations for Mitigating Social Bias\n  without Demographic Information","summary":"  Mitigating social biases typically requires identifying the social groups\nassociated with each data sample. In this paper, we present DAFair, a novel\napproach to address social bias in language models. Unlike traditional methods\nthat rely on explicit demographic labels, our approach does not require any\nsuch information. Instead, we leverage predefined prototypical demographic\ntexts and incorporate a regularization term during the fine-tuning process to\nmitigate bias in the model's representations. Our empirical results across two\ntasks and two models demonstrate the effectiveness of our method compared to\nprevious approaches that do not rely on labeled data. Moreover, with limited\ndemographic-annotated data, our approach outperforms common debiasing\napproaches.\n","authors":["Shadi Iskander","Kira Radinsky","Yonatan Belinkov"],"pdf_url":"https://arxiv.org/pdf/2403.09516v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08664v2","updated":"2024-03-14T15:57:59Z","published":"2024-03-13T16:17:09Z","title":"Zero-shot and Few-shot Generation Strategies for Artificial Clinical\n  Records","summary":"  The challenge of accessing historical patient data for clinical research,\nwhile adhering to privacy regulations, is a significant obstacle in medical\nscience. An innovative approach to circumvent this issue involves utilising\nsynthetic medical records that mirror real patient data without compromising\nindividual privacy. The creation of these synthetic datasets, particularly\nwithout using actual patient data to train Large Language Models (LLMs),\npresents a novel solution as gaining access to sensitive patient information to\ntrain models is also a challenge. This study assesses the capability of the\nLlama 2 LLM to create synthetic medical records that accurately reflect real\npatient information, employing zero-shot and few-shot prompting strategies for\ncomparison against fine-tuned methodologies that do require sensitive patient\ndata during training. We focus on generating synthetic narratives for the\nHistory of Present Illness section, utilising data from the MIMIC-IV dataset\nfor comparison. In this work introduce a novel prompting technique that\nleverages a chain-of-thought approach, enhancing the model's ability to\ngenerate more accurate and contextually relevant medical narratives without\nprior fine-tuning. Our findings suggest that this chain-of-thought prompted\napproach allows the zero-shot model to achieve results on par with those of\nfine-tuned models, based on Rouge metrics evaluation.\n","authors":["Erlend Frayling","Jake Lever","Graham McDonald"],"pdf_url":"https://arxiv.org/pdf/2403.08664v2.pdf","comment":"4 pages"},{"id":"http://arxiv.org/abs/2403.09509v1","updated":"2024-03-14T15:56:02Z","published":"2024-03-14T15:56:02Z","title":"On STPA for Distributed Development of Safe Autonomous Driving: An\n  Interview Study","summary":"  Safety analysis is used to identify hazards and build knowledge during the\ndesign phase of safety-relevant functions. This is especially true for complex\nAI-enabled and software intensive systems such as Autonomous Drive (AD).\nSystem-Theoretic Process Analysis (STPA) is a novel method applied in\nsafety-related fields like defense and aerospace, which is also becoming\npopular in the automotive industry. However, STPA assumes prerequisites that\nare not fully valid in the automotive system engineering with distributed\nsystem development and multi-abstraction design levels. This would inhibit\nsoftware developers from using STPA to analyze their software as part of a\nbigger system, resulting in a lack of traceability. This can be seen as a\nmaintainability challenge in continuous development and deployment (DevOps). In\nthis paper, STPA's different guidelines for the automotive industry, e.g.\nJ31887/ISO21448/STPA handbook, are firstly compared to assess their\napplicability to the distributed development of complex AI-enabled systems like\nAD. Further, an approach to overcome the challenges of using STPA in a\nmulti-level design context is proposed. By conducting an interview study with\nautomotive industry experts for the development of AD, the challenges are\nvalidated and the effectiveness of the proposed approach is evaluated.\n","authors":["Ali Nouri","Christian Berger","Fredrik Törner"],"pdf_url":"https://arxiv.org/pdf/2403.09509v1.pdf","comment":"Accepted at SEAA. 8 pages, 2 figures"},{"id":"http://arxiv.org/abs/2305.03571v2","updated":"2024-03-14T15:54:10Z","published":"2023-05-05T14:27:58Z","title":"Model-free Reinforcement Learning of Semantic Communication by\n  Stochastic Policy Gradient","summary":"  Following the recent success of Machine Learning tools in wireless\ncommunications, the idea of semantic communication by Weaver from 1949 has\ngained attention. It breaks with Shannon's classic design paradigm by aiming to\ntransmit the meaning, i.e., semantics, of a message instead of its exact\nversion, allowing for information rate savings. In this work, we apply the\nStochastic Policy Gradient (SPG) to design a semantic communication system by\nreinforcement learning, separating transmitter and receiver, and not requiring\na known or differentiable channel model -- a crucial step towards deployment in\npractice. Further, we derive the use of SPG for both classic and semantic\ncommunication from the maximization of the mutual information between received\nand target variables. Numerical results show that our approach achieves\ncomparable performance to a model-aware approach based on the reparametrization\ntrick, albeit with a decreased convergence rate.\n","authors":["Edgar Beck","Carsten Bockelmann","Armin Dekorsy"],"pdf_url":"https://arxiv.org/pdf/2305.03571v2.pdf","comment":"Accepted for publication in IEEE International Conference on Machine\n  Learning for Communication and Networking (ICMLCN 2024), Source Code:\n  https://github.com/ant-uni-bremen/SINFONY"},{"id":"http://arxiv.org/abs/2403.09506v1","updated":"2024-03-14T15:53:04Z","published":"2024-03-14T15:53:04Z","title":"Don't Judge by the Look: A Motion Coherent Augmentation for Video\n  Recognition","summary":"  Current training pipelines in object recognition neglect Hue Jittering when\ndoing data augmentation as it not only brings appearance changes that are\ndetrimental to classification, but also the implementation is inefficient in\npractice. In this study, we investigate the effect of hue variance in the\ncontext of video recognition and find this variance to be beneficial since\nstatic appearances are less important in videos that contain motion\ninformation. Based on this observation, we propose a data augmentation method\nfor video recognition, named Motion Coherent Augmentation (MCA), that\nintroduces appearance variation in videos and implicitly encourages the model\nto prioritize motion patterns, rather than static appearances. Concretely, we\npropose an operation SwapMix to efficiently modify the appearance of video\nsamples, and introduce Variation Alignment (VA) to resolve the distribution\nshift caused by SwapMix, enforcing the model to learn appearance invariant\nrepresentations. Comprehensive empirical evaluation across various\narchitectures and different datasets solidly validates the effectiveness and\ngeneralization ability of MCA, and the application of VA in other augmentation\nmethods. Code is available at https://github.com/BeSpontaneous/MCA-pytorch.\n","authors":["Yitian Zhang","Yue Bai","Huan Wang","Yizhou Wang","Yun Fu"],"pdf_url":"https://arxiv.org/pdf/2403.09506v1.pdf","comment":"Accepted by ICLR2024"},{"id":"http://arxiv.org/abs/2310.13459v4","updated":"2024-03-14T15:52:16Z","published":"2023-10-20T12:45:12Z","title":"Stable Nonconvex-Nonconcave Training via Linear Interpolation","summary":"  This paper presents a theoretical analysis of linear interpolation as a\nprincipled method for stabilizing (large-scale) neural network training. We\nargue that instabilities in the optimization process are often caused by the\nnonmonotonicity of the loss landscape and show how linear interpolation can\nhelp by leveraging the theory of nonexpansive operators. We construct a new\noptimization scheme called relaxed approximate proximal point (RAPP), which is\nthe first explicit method without anchoring to achieve last iterate convergence\nrates for $\\rho$-comonotone problems while only requiring $\\rho >\n-\\tfrac{1}{2L}$. The construction extends to constrained and regularized\nsettings. By replacing the inner optimizer in RAPP we rediscover the family of\nLookahead algorithms for which we establish convergence in cohypomonotone\nproblems even when the base optimizer is taken to be gradient descent ascent.\nThe range of cohypomonotone problems in which Lookahead converges is further\nexpanded by exploiting that Lookahead inherits the properties of the base\noptimizer. We corroborate the results with experiments on generative\nadversarial networks which demonstrates the benefits of the linear\ninterpolation present in both RAPP and Lookahead.\n","authors":["Thomas Pethick","Wanyun Xie","Volkan Cevher"],"pdf_url":"https://arxiv.org/pdf/2310.13459v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.07011v2","updated":"2024-03-14T15:45:56Z","published":"2024-02-10T18:14:57Z","title":"FedImpro: Measuring and Improving Client Update in Federated Learning","summary":"  Federated Learning (FL) models often experience client drift caused by\nheterogeneous data, where the distribution of data differs across clients. To\naddress this issue, advanced research primarily focuses on manipulating the\nexisting gradients to achieve more consistent client models. In this paper, we\npresent an alternative perspective on client drift and aim to mitigate it by\ngenerating improved local models. First, we analyze the generalization\ncontribution of local training and conclude that this generalization\ncontribution is bounded by the conditional Wasserstein distance between the\ndata distribution of different clients. Then, we propose FedImpro, to construct\nsimilar conditional distributions for local training. Specifically, FedImpro\ndecouples the model into high-level and low-level components, and trains the\nhigh-level portion on reconstructed feature distributions. This approach\nenhances the generalization contribution and reduces the dissimilarity of\ngradients in FL. Experimental results show that FedImpro can help FL defend\nagainst data heterogeneity and enhance the generalization performance of the\nmodel.\n","authors":["Zhenheng Tang","Yonggang Zhang","Shaohuai Shi","Xinmei Tian","Tongliang Liu","Bo Han","Xiaowen Chu"],"pdf_url":"https://arxiv.org/pdf/2402.07011v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09502v1","updated":"2024-03-14T15:44:19Z","published":"2024-03-14T15:44:19Z","title":"EquiAV: Leveraging Equivariance for Audio-Visual Contrastive Learning","summary":"  Recent advancements in self-supervised audio-visual representation learning\nhave demonstrated its potential to capture rich and comprehensive\nrepresentations. However, despite the advantages of data augmentation verified\nin many learning methods, audio-visual learning has struggled to fully harness\nthese benefits, as augmentations can easily disrupt the correspondence between\ninput pairs. To address this limitation, we introduce EquiAV, a novel framework\nthat leverages equivariance for audio-visual contrastive learning. Our approach\nbegins with extending equivariance to audio-visual learning, facilitated by a\nshared attention-based transformation predictor. It enables the aggregation of\nfeatures from diverse augmentations into a representative embedding, providing\nrobust supervision. Notably, this is achieved with minimal computational\noverhead. Extensive ablation studies and qualitative results verify the\neffectiveness of our method. EquiAV outperforms previous works across various\naudio-visual benchmarks.\n","authors":["Jongsuk Kim","Hyeongkeun Lee","Kyeongha Rho","Junmo Kim","Joon Son Chung"],"pdf_url":"https://arxiv.org/pdf/2403.09502v1.pdf","comment":"14 pages, 3 figures"},{"id":"http://arxiv.org/abs/2403.09499v1","updated":"2024-03-14T15:42:26Z","published":"2024-03-14T15:42:26Z","title":"A Reinforcement Learning Approach to Dairy Farm Battery Management using\n  Q Learning","summary":"  Dairy farming consumes a significant amount of energy, making it an\nenergy-intensive sector within agriculture. Integrating renewable energy\ngeneration into dairy farming could help address this challenge. Effective\nbattery management is important for integrating renewable energy generation.\nManaging battery charging and discharging poses significant challenges because\nof fluctuations in electrical consumption, the intermittent nature of renewable\nenergy generation, and fluctuations in energy prices. Artificial Intelligence\n(AI) has the potential to significantly improve the use of renewable energy in\ndairy farming, however, there is limited research conducted in this particular\ndomain. This research considers Ireland as a case study as it works towards\nattaining its 2030 energy strategy centered on the utilization of renewable\nsources. This study proposes a Q-learning-based algorithm for scheduling\nbattery charging and discharging in a dairy farm setting. This research also\nexplores the effect of the proposed algorithm by adding wind generation data\nand considering additional case studies. The proposed algorithm reduces the\ncost of imported electricity from the grid by 13.41\\%, peak demand by 2\\%, and\n24.49\\% when utilizing wind generation. These results underline how\nreinforcement learning is highly effective in managing batteries in the dairy\nfarming sector.\n","authors":["Nawazish Ali","Abdul Wahid","Rachael Shaw","Karl Mason"],"pdf_url":"https://arxiv.org/pdf/2403.09499v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.05342v2","updated":"2024-03-14T15:40:01Z","published":"2023-11-29T12:58:42Z","title":"Most discriminative stimuli for functional cell type clustering","summary":"  Identifying cell types and understanding their functional properties is\ncrucial for unraveling the mechanisms underlying perception and cognition. In\nthe retina, functional types can be identified by carefully selected stimuli,\nbut this requires expert domain knowledge and biases the procedure towards\npreviously known cell types. In the visual cortex, it is still unknown what\nfunctional types exist and how to identify them. Thus, for unbiased\nidentification of the functional cell types in retina and visual cortex, new\napproaches are needed. Here we propose an optimization-based clustering\napproach using deep predictive models to obtain functional clusters of neurons\nusing Most Discriminative Stimuli (MDS). Our approach alternates between\nstimulus optimization with cluster reassignment akin to an\nexpectation-maximization algorithm. The algorithm recovers functional clusters\nin mouse retina, marmoset retina and macaque visual area V4. This demonstrates\nthat our approach can successfully find discriminative stimuli across species,\nstages of the visual system and recording techniques. The resulting most\ndiscriminative stimuli can be used to assign functional cell types fast and on\nthe fly, without the need to train complex predictive models or show a large\nnatural scene dataset, paving the way for experiments that were previously\nlimited by experimental time. Crucially, MDS are interpretable: they visualize\nthe distinctive stimulus patterns that most unambiguously identify a specific\ntype of neuron.\n","authors":["Max F. Burg","Thomas Zenkel","Michaela Vystrčilová","Jonathan Oesterle","Larissa Höfling","Konstantin F. Willeke","Jan Lause","Sarah Müller","Paul G. Fahey","Zhiwei Ding","Kelli Restivo","Shashwat Sridhar","Tim Gollisch","Philipp Berens","Andreas S. Tolias","Thomas Euler","Matthias Bethge","Alexander S. Ecker"],"pdf_url":"https://arxiv.org/pdf/2401.05342v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.05930v2","updated":"2024-03-14T15:39:55Z","published":"2023-12-10T16:33:41Z","title":"A Comprehensive Dataset and Automated Pipeline for Nailfold Capillary\n  Analysis","summary":"  Nailfold capillaroscopy is widely used in assessing health conditions,\nhighlighting the pressing need for an automated nailfold capillary analysis\nsystem. In this study, we present a pioneering effort in constructing a\ncomprehensive nailfold capillary dataset-321 images, 219 videos from 68\nsubjects, with clinic reports and expert annotations-that serves as a crucial\nresource for training deep-learning models. Leveraging this dataset, we\nfinetuned three deep learning models with expert annotations as supervised\nlabels and integrated them into a novel end-to-end nailfold capillary analysis\npipeline. This pipeline excels in automatically detecting and measuring a wide\nrange of size factors, morphological features, and dynamic aspects of nailfold\ncapillaries. We compared our outcomes with clinical reports. Experiment results\nshowed that our automated pipeline achieves an average of sub-pixel level\nprecision in measurements and 89.9% accuracy in identifying morphological\nabnormalities. These results underscore its potential for advancing\nquantitative medical research and enabling pervasive computing in healthcare.\nOur data and code are available at\nhttps://github.com/THU-CS-PI-LAB/ANFC-Automated-Nailfold-Capillary.\n","authors":["Linxi Zhao","Jiankai Tang","Dongyu Chen","Xiaohong Liu","Yong Zhou","Yuanchun Shi","Guangyu Wang","Yuntao Wang"],"pdf_url":"https://arxiv.org/pdf/2312.05930v2.pdf","comment":"Dataset, code, pretrained models:\n  https://github.com/THU-CS-PI-LAB/ANFC-Automated-Nailfold-Capillary"},{"id":"http://arxiv.org/abs/2309.11093v2","updated":"2024-03-14T15:36:17Z","published":"2023-09-20T06:54:55Z","title":"K-pop Lyric Translation: Dataset, Analysis, and Neural-Modelling","summary":"  Lyric translation, a field studied for over a century, is now attracting\ncomputational linguistics researchers. We identified two limitations in\nprevious studies. Firstly, lyric translation studies have predominantly focused\non Western genres and languages, with no previous study centering on K-pop\ndespite its popularity. Second, the field of lyric translation suffers from a\nlack of publicly available datasets; to the best of our knowledge, no such\ndataset exists. To broaden the scope of genres and languages in lyric\ntranslation studies, we introduce a novel singable lyric translation dataset,\napproximately 89\\% of which consists of K-pop song lyrics. This dataset aligns\nKorean and English lyrics line-by-line and section-by-section. We leveraged\nthis dataset to unveil unique characteristics of K-pop lyric translation,\ndistinguishing it from other extensively studied genres, and to construct a\nneural lyric translation model, thereby underscoring the importance of a\ndedicated dataset for singable lyric translations.\n","authors":["Haven Kim","Jongmin Jung","Dasaem Jeong","Juhan Nam"],"pdf_url":"https://arxiv.org/pdf/2309.11093v2.pdf","comment":"LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2211.11612v2","updated":"2024-03-14T15:35:59Z","published":"2022-11-21T16:13:23Z","title":"Plug and Play Active Learning for Object Detection","summary":"  Annotating datasets for object detection is an expensive and time-consuming\nendeavor. To minimize this burden, active learning (AL) techniques are employed\nto select the most informative samples for annotation within a constrained\n\"annotation budget\". Traditional AL strategies typically rely on model\nuncertainty or sample diversity for query sampling, while more advanced methods\nhave focused on developing AL-specific object detector architectures to enhance\nperformance. However, these specialized approaches are not readily adaptable to\ndifferent object detectors due to the significant engineering effort required\nfor integration. To overcome this challenge, we introduce Plug and Play Active\nLearning (PPAL), a simple and effective AL strategy for object detection. PPAL\nis a two-stage method comprising uncertainty-based and diversity-based sampling\nphases. In the first stage, our Difficulty Calibrated Uncertainty Sampling\nleverage a category-wise difficulty coefficient that combines both\nclassification and localisation difficulties to re-weight instance\nuncertainties, from which we sample a candidate pool for the subsequent\ndiversity-based sampling. In the second stage, we propose Category Conditioned\nMatching Similarity to better compute the similarities of multi-instance images\nas ensembles of their instance similarities, which is used by the k-Means++\nalgorithm to sample the final AL queries. PPAL makes no change to model\narchitectures or detector training pipelines; hence it can be easily\ngeneralized to different object detectors. We benchmark PPAL on the MS-COCO and\nPascal VOC datasets using different detector architectures and show that our\nmethod outperforms prior work by a large margin. Code is available at\nhttps://github.com/ChenhongyiYang/PPAL\n","authors":["Chenhongyi Yang","Lichao Huang","Elliot J. Crowley"],"pdf_url":"https://arxiv.org/pdf/2211.11612v2.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2403.09491v1","updated":"2024-03-14T15:32:25Z","published":"2024-03-14T15:32:25Z","title":"On using Machine Learning Algorithms for Motorcycle Collision Detection","summary":"  Globally, motorcycles attract vast and varied users. However, since the rate\nof severe injury and fatality in motorcycle accidents far exceeds passenger car\naccidents, efforts have been directed toward increasing passive safety systems.\nImpact simulations show that the risk of severe injury or death in the event of\na motorcycle-to-car impact can be greatly reduced if the motorcycle is equipped\nwith passive safety measures such as airbags and seat belts. For the passive\nsafety systems to be activated, a collision must be detected within\nmilliseconds for a wide variety of impact configurations, but under no\ncircumstances may it be falsely triggered. For the challenge of reliably\ndetecting impending collisions, this paper presents an investigation towards\nthe applicability of machine learning algorithms. First, a series of\nsimulations of accidents and driving operation is introduced to collect data to\ntrain machine learning classification models. Their performance is henceforth\nassessed and compared via multiple representative and application-oriented\ncriteria.\n","authors":["Philipp Rodegast","Steffen Maier","Jonas Kneifl","Jörg Fehr"],"pdf_url":"https://arxiv.org/pdf/2403.09491v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.03668v6","updated":"2024-03-14T15:25:16Z","published":"2022-03-04T14:16:50Z","title":"A Typology for Exploring the Mitigation of Shortcut Behavior","summary":"  As machine learning models become increasingly larger, trained weakly\nsupervised on large, possibly uncurated data sets, it becomes increasingly\nimportant to establish mechanisms for inspecting, interacting, and revising\nmodels to mitigate learning shortcuts and guarantee their learned knowledge is\naligned with human knowledge. The recently proposed XIL framework was developed\nfor this purpose, and several such methods have been introduced, each with\nindividual motivations and methodological details. In this work, we provide a\nunification of various XIL methods into a single typology by establishing a\ncommon set of basic modules. In doing so, we pave the way for a principled\ncomparison of existing, but, importantly, also future XIL approaches. In\naddition, we discuss existing and introduce novel measures and benchmarks for\nevaluating the overall abilities of a XIL method. Given this extensive toolbox,\nincluding our typology, measures, and benchmarks, we finally compare several\nrecent XIL methods methodologically and quantitatively. In our evaluations, all\nmethods prove to revise a model successfully. However, we found remarkable\ndifferences in individual benchmark tasks, revealing valuable\napplication-relevant aspects for integrating these benchmarks in developing\nfuture methods.\n","authors":["Felix Friedrich","Wolfgang Stammer","Patrick Schramowski","Kristian Kersting"],"pdf_url":"https://arxiv.org/pdf/2203.03668v6.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09479v1","updated":"2024-03-14T15:20:54Z","published":"2024-03-14T15:20:54Z","title":"Laying the Foundation First? Investigating the Generalization from\n  Atomic Skills to Complex Reasoning Tasks","summary":"  Current language models have demonstrated their capability to develop basic\nreasoning, but struggle in more complicated reasoning tasks that require a\ncombination of atomic skills, such as math word problem requiring skills like\narithmetic and unit conversion. Previous methods either do not improve the\ninherent atomic skills of models or not attempt to generalize the atomic skills\nto complex reasoning tasks. In this paper, we first propose a probing framework\nto investigate whether the atomic skill can spontaneously generalize to complex\nreasoning tasks. Then, we introduce a hierarchical curriculum learning training\nstrategy to achieve better skill generalization. In our experiments, we find\nthat atomic skills can not spontaneously generalize to compositional tasks. By\nleveraging hierarchical curriculum learning, we successfully induce\ngeneralization, significantly improve the performance of open-source LMs on\ncomplex reasoning tasks. Promisingly, the skill generalization exhibit\neffective in cross-dataset and cross-domain scenarios. Complex reasoning can\nalso help enhance atomic skills. Our findings offer valuable guidance for\ndesigning better training strategies for complex reasoning tasks.\n","authors":["Yuncheng Huang","Qianyu He","Yipei Xu","Jiaqing Liang","Yanghua Xiao"],"pdf_url":"https://arxiv.org/pdf/2403.09479v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09477v1","updated":"2024-03-14T15:19:19Z","published":"2024-03-14T15:19:19Z","title":"VIRUS-NeRF -- Vision, InfraRed and UltraSonic based Neural Radiance\n  Fields","summary":"  Autonomous mobile robots are an increasingly integral part of modern factory\nand warehouse operations. Obstacle detection, avoidance and path planning are\ncritical safety-relevant tasks, which are often solved using expensive LiDAR\nsensors and depth cameras. We propose to use cost-effective low-resolution\nranging sensors, such as ultrasonic and infrared time-of-flight sensors by\ndeveloping VIRUS-NeRF - Vision, InfraRed, and UltraSonic based Neural Radiance\nFields. Building upon Instant Neural Graphics Primitives with a Multiresolution\nHash Encoding (Instant-NGP), VIRUS-NeRF incorporates depth measurements from\nultrasonic and infrared sensors and utilizes them to update the occupancy grid\nused for ray marching. Experimental evaluation in 2D demonstrates that\nVIRUS-NeRF achieves comparable mapping performance to LiDAR point clouds\nregarding coverage. Notably, in small environments, its accuracy aligns with\nthat of LiDAR measurements, while in larger ones, it is bounded by the utilized\nultrasonic sensors. An in-depth ablation study reveals that adding ultrasonic\nand infrared sensors is highly effective when dealing with sparse data and low\nview variation. Further, the proposed occupancy grid of VIRUS-NeRF improves the\nmapping capabilities and increases the training speed by 46% compared to\nInstant-NGP. Overall, VIRUS-NeRF presents a promising approach for\ncost-effective local mapping in mobile robotics, with potential applications in\nsafety and navigation tasks. The code can be found at\nhttps://github.com/ethz-asl/virus nerf.\n","authors":["Nicolaj Schmid","Cornelius von Einem","Cesar Cadena","Roland Siegwart","Lorenz Hruby","Florian Tschopp"],"pdf_url":"https://arxiv.org/pdf/2403.09477v1.pdf","comment":null}],"Computer Science and Game Theory":[{"id":"http://arxiv.org/abs/2403.08082v2","updated":"2024-03-14T06:11:23Z","published":"2024-03-12T21:16:25Z","title":"Data Monetization Pathways and Complex Dynamic Game Equilibrium Analysis\n  in the Energy Industry","summary":"  As the most critical production factor in the era of the digital economy,\ndata will have a significant impact on social production and development.\nEnergy enterprises possess data that is interconnected with multiple\nindustries, characterized by diverse needs, sensitivity, and long-term nature.\nThe path to monetizing energy enterprises' data is challenging yet crucial.\nThis paper explores the game-theoretic aspects of the data monetization process\nin energy enterprises by considering the relationships between enterprises and\ntrading platforms. We construct a class of game decision models and study their\nequilibrium strategies. Our analysis shows that enterprises and platforms can\nadjust respective benefits by regulating the wholesale price of data and the\nintensity of data value mining to form a benign equilibrium state. Furthermore,\nby integrating nonlinear dynamical theory, we discuss the dynamic\ncharacteristics present in multi-period repeated game processes. We find that\ndecision-makers should keep the adjustment parameters and initial states within\nreasonable ranges in multi-period dynamic decision-making to avoid market\nfailure. Finally, based on the theoretical and numerical analysis, we provide\ndecision insights and recommendations for enterprise decision-making to\nfacilitate data monetization through strategic interactions with trading\nplatforms.\n","authors":["Zongxian Wang","Jie Song"],"pdf_url":"https://arxiv.org/pdf/2403.08082v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.15423v3","updated":"2024-03-14T17:53:08Z","published":"2023-09-27T06:20:28Z","title":"Prosumers Participation in Markets: A Scalar-Parameterized Function\n  Bidding Approach","summary":"  In uniform-price markets, suppliers compete to supply a resource to\nconsumers, resulting in a single market price determined by their competition.\nFor sufficient flexibility, producers and consumers prefer to commit to a\nfunction as their strategies, indicating their preferred quantity at any given\nmarket price. Producers and consumers may wish to act as both, i.e., prosumers.\nIn this paper, we examine the behavior of profit-maximizing prosumers in a\nuniform-price market for resource allocation with the objective of maximizing\nthe social welfare. We propose a scalar-parameterized function bidding\nmechanism for the prosumers, in which we establish the existence and uniqueness\nof Nash equilibrium. Furthermore, we provide an efficient way to compute the\nNash equilibrium through the computation of the market allocation at the Nash\nequilibrium. Finally, we present a case study to illustrate the welfare loss\nunder different variations of market parameters, such as the market's supply\ncapacity and inelastic demand.\n","authors":["Abdullah Alawad","Muhammad Aneeq uz Zaman","Khaled Alshehri","Tamer Başar"],"pdf_url":"https://arxiv.org/pdf/2309.15423v3.pdf","comment":"Corrected typos in the figures"},{"id":"http://arxiv.org/abs/2403.09545v1","updated":"2024-03-14T16:32:29Z","published":"2024-03-14T16:32:29Z","title":"Sequential Contracts","summary":"  We study the principal-agent setting, where a principal delegates the\nexecution of a costly project to an agent. In the classical model, the agent\nchooses an action among a set of available actions. Every action is associated\nwith some cost, and leads to a stochastic outcome for the project. The agent's\naction is hidden from the principal, who only observes the outcome. The\nprincipal incentivizes the agent through a payment scheme (a contract) that\nmaps outcomes to payments, with the objective of finding the optimal contract -\nthe contract maximizing the principal's expected utility.\n  In this work, we introduce a sequential variant of the model, capturing many\nreal-life settings, where the agent engages in multiple attempts, incurring the\nsum of costs of the actions taken and being compensated for the best realized\noutcome. We study the contract design problem in this new setting. We first\nobserve that the agent's problem - finding the sequential set of actions that\nmaximizes his utility for a given contract - is equivalent to the well-known\nPandora's Box problem. With this insight at hand, we provide algorithms and\nhardness results for the (principal's) contract design problem, under both\nindependent and correlated actions. For independent actions, we show that the\noptimal linear contract can be computed in polynomial time. Furthermore, this\nresult extends to the optimal arbitrary contract when the number of outcomes is\na constant. For correlated actions we find that approximating the optimal\ncontract within any constant ratio is NP-hard.\n","authors":["Tomer Ezra","Michal Feldman","Maya Schlesinger"],"pdf_url":"https://arxiv.org/pdf/2403.09545v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06689v2","updated":"2024-03-14T16:02:45Z","published":"2023-10-10T15:05:21Z","title":"Approximating Nash Equilibria in Normal-Form Games via Stochastic\n  Optimization","summary":"  We propose the first loss function for approximate Nash equilibria of\nnormal-form games that is amenable to unbiased Monte Carlo estimation. This\nconstruction allows us to deploy standard non-convex stochastic optimization\ntechniques for approximating Nash equilibria, resulting in novel algorithms\nwith provable guarantees. We complement our theoretical analysis with\nexperiments demonstrating that stochastic gradient descent can outperform\nprevious state-of-the-art approaches.\n","authors":["Ian Gemp","Luke Marris","Georgios Piliouras"],"pdf_url":"https://arxiv.org/pdf/2310.06689v2.pdf","comment":"Published at ICLR 2024"},{"id":"http://arxiv.org/abs/2403.09510v1","updated":"2024-03-14T15:56:39Z","published":"2024-03-14T15:56:39Z","title":"Trust AI Regulation? Discerning users are vital to build trust and\n  effective AI regulation","summary":"  There is general agreement that some form of regulation is necessary both for\nAI creators to be incentivised to develop trustworthy systems, and for users to\nactually trust those systems. But there is much debate about what form these\nregulations should take and how they should be implemented. Most work in this\narea has been qualitative, and has not been able to make formal predictions.\nHere, we propose that evolutionary game theory can be used to quantitatively\nmodel the dilemmas faced by users, AI creators, and regulators, and provide\ninsights into the possible effects of different regulatory regimes. We show\nthat creating trustworthy AI and user trust requires regulators to be\nincentivised to regulate effectively. We demonstrate the effectiveness of two\nmechanisms that can achieve this. The first is where governments can recognise\nand reward regulators that do a good job. In that case, if the AI system is not\ntoo risky for users then some level of trustworthy development and user trust\nevolves. We then consider an alternative solution, where users can condition\ntheir trust decision on the effectiveness of the regulators. This leads to\neffective regulation, and consequently the development of trustworthy AI and\nuser trust, provided that the cost of implementing regulations is not too high.\nOur findings highlight the importance of considering the effect of different\nregulatory regimes from an evolutionary game theoretic perspective.\n","authors":["Zainab Alalawi","Paolo Bova","Theodor Cimpeanu","Alessandro Di Stefano","Manh Hong Duong","Elias Fernandez Domingos","The Anh Han","Marcus Krellner","Bianca Ogbo","Simon T. Powers","Filippo Zimmaro"],"pdf_url":"https://arxiv.org/pdf/2403.09510v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09129v1","updated":"2024-03-14T06:29:17Z","published":"2024-03-14T06:29:17Z","title":"All-pay Auction Based Profit Maximization in End-to-End Computation\n  Offloading System","summary":"  Pricing is an important issue in mobile edge computing. How to appropriately\ndetermine the bid of end user (EU) is an incentive factor for edge cloud (EC)\nto offer service. In this letter, we propose an equilibrium pricing scheme\nbased on the all-pay auction model in end-to-end collaboration environment,\nwherein all EUs can acquire the service at a lower price than the own value of\nthe required resource. In addition, we propose a set allocation algorithm to\ndivide all the bidders into different sets according to the price, and the EUs\nin each set get the service, which averts the case of getting no service due to\nthe low price. Extensive simulation results demonstrate that the proposed\nscheme can effectively maximize the total profit of the edge offloading system,\nand guarantee all EUs can access the service.\n","authors":["Hai Xue","Yun Xia","Di Zhang","Honghua Wei","Xiaolong Xu"],"pdf_url":"https://arxiv.org/pdf/2403.09129v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.17757v3","updated":"2024-03-14T02:55:25Z","published":"2023-05-28T15:43:59Z","title":"Diversity-seeking Jump Games in Networks","summary":"  Recently, strategic games inspired by Schelling's influential model of\nresidential segregation have been studied in the TCS and AI literature. In\nthese games, agents of k different types occupy the nodes of a network topology\naiming to maximize their utility, which is a function of the fraction of\nsame-type agents they are adjacent to in the network. As such, the agents\nexhibit similarity seeking strategic behavior. In this paper, we introduce a\nclass of strategic jump games in which the agents are diversity-seeking: The\nutility of an agent is defined as the fraction of its neighbors that are of\ndifferent type than itself. We show that in general it is computationally hard\nto determine the existence of an equilibrium in such games. However, when the\nnetwork is a tree, diversity-seeking jump games always admit an equilibrium\nassignment. For regular graphs and spider graphs with a single empty node, we\nprove a stronger result: The game is potential, that is, the improving response\ndynamics always converge to an equilibrium from any initial placement of the\nagents. We also show (nearly tight) bounds on the price of anarchy and price of\nstability in terms of the social welfare (the total utility of the agents).\n","authors":["Lata Narayanan","Yasaman Sabbagh","Alexandros A. Voudouris"],"pdf_url":"https://arxiv.org/pdf/2305.17757v3.pdf","comment":null}],"Information Theory":[{"id":"http://arxiv.org/abs/2403.06629v3","updated":"2024-03-14T01:48:39Z","published":"2024-03-11T11:39:08Z","title":"Assembly Theory is an approximation to algorithmic complexity based on\n  LZ compression that does not explain selection or evolution","summary":"  We demonstrate that Assembly Theory, pathway complexity, the assembly index,\nand the assembly number are subsumed and constitute a weak version of\nalgorithmic (Kolmogorov-Solomonoff-Chaitin) complexity reliant on an\napproximation method based upon statistical compression, their results obtained\ndue to the use of methods strictly equivalent to the LZ family of compression\nalgorithms used in compressing algorithms such as ZIP, GZIP, or JPEG. Such\npopular algorithms have been shown to empirically reproduce the results of AT\nthat were reported before in successful application to separating organic from\nnon-organic molecules and in the context of the study of selection and\nevolution. We prove the connections and full equivalence of Assembly Theory to\nShannon Entropy and statistical compression, and AT's disconnection as a\nstatistical approach from causality. We demonstrate that formulating a\ntraditional statistically compressed description of molecules, or the theory\nunderlying it, does not imply an explanation or quantification of biases in\ngenerative (physical or biological) processes, including those brought about by\nselection and evolution, when lacking in logical consistency and empirical\nevidence. We argue that in their basic arguments, the authors of AT conflate\nhow objects may assemble with causal directionality, and conclude that Assembly\nTheory does not explain selection or evolution beyond known and previously\nestablished connections, some of which are reviewed.\n","authors":["Felipe S. Abrahão","Santiago Hernández-Orozco","Narsis A. Kiani","Jesper Tegnér","Hector Zenil"],"pdf_url":"https://arxiv.org/pdf/2403.06629v3.pdf","comment":"15 pages + appendix, 2 figures"},{"id":"http://arxiv.org/abs/2403.08339v2","updated":"2024-03-14T04:31:44Z","published":"2024-03-13T08:43:01Z","title":"Low-Complexity Beam Training for Multi-RIS-Assisted Multi-User\n  Communications","summary":"  In this paper, we investigate the beam training problem in the multi-user\nmillimeter wave (mmWave) communication system, where multiple reconfigurable\nintelligent surfaces (RISs) are deployed to improve the coverage and the\nachievable rate. However, existing beam training techniques in mmWave systems\nsuffer from the high complexity (i.e., exponential order) and low\nidentification accuracy. To address these problems, we propose a novel hashing\nmulti-arm beam (HMB) training scheme that reduces the training complexity to\nthe logarithmic order with the high accuracy. Specifically, we first design a\ngeneration mechanism for HMB codebooks. Then, we propose a demultiplexing\nalgorithm based on the soft decision to distinguish signals from different RIS\nreflective links. Finally, we utilize a multi-round voting mechanism to align\nthe beams. Simulation results show that the proposed HMB training scheme\nenables simultaneous training for multiple RISs and multiple users, and reduces\nthe beam training overhead to the logarithmic level. Moreover, it also shows\nthat our proposed scheme can significantly improve the identification accuracy\nby at least 20% compared to existing beam training techniques.\n","authors":["Yuan Xu","Chongwen Huang","Wei Li","Zhaohui Yang","Xiaoming Chen","Zhaoyang Zhang","Chau Yuen","Mérouane Debbah"],"pdf_url":"https://arxiv.org/pdf/2403.08339v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09614v1","updated":"2024-03-14T17:52:17Z","published":"2024-03-14T17:52:17Z","title":"Localization in Digital Twin MIMO Networks: A Case for Massive\n  Fingerprinting","summary":"  Localization in outdoor wireless systems typically requires transmitting\nspecific reference signals to estimate distance (trilateration methods) or\nangle (triangulation methods). These cause overhead on communication, need a\nLoS link to work well, and require multiple base stations, often imposing\nsynchronization or specific hardware requirements. Fingerprinting has none of\nthese drawbacks, but building its database requires high human effort to\ncollect real-world measurements. For a long time, this issue limited the size\nof databases and thus their performance. This work proposes significantly\nreducing human effort in building fingerprinting databases by populating them\nwith \\textit{digital twin RF maps}. These RF maps are built from ray-tracing\nsimulations on a digital replica of the environment across several frequency\nbands and beamforming configurations. Online user fingerprints are then matched\nagainst this spatial database. The approach was evaluated with practical\nsimulations using realistic propagation models and user measurements. Our\nexperiments show sub-meter localization errors on a NLoS location 95\\% of the\ntime using sensible user measurement report sizes. Results highlight the\npromising potential of the proposed digital twin approach for ubiquitous\nwide-area 6G localization.\n","authors":["João Morais","Ahmed Alkhateeb"],"pdf_url":"https://arxiv.org/pdf/2403.09614v1.pdf","comment":"To appear in ICC 2024. The Dataset and code will be available soon on\n  the DeepMIMO and DeepVerse websites https://www.deepmimo.net/\n  https://www.deepverse6g.net/"},{"id":"http://arxiv.org/abs/2403.09570v1","updated":"2024-03-14T17:00:01Z","published":"2024-03-14T17:00:01Z","title":"Multi-Fidelity Bayesian Optimization With Across-Task Transferable\n  Max-Value Entropy Search","summary":"  In many applications, ranging from logistics to engineering, a designer is\nfaced with a sequence of optimization tasks for which the objectives are in the\nform of black-box functions that are costly to evaluate. For example, the\ndesigner may need to tune the hyperparameters of neural network models for\ndifferent learning tasks over time. Rather than evaluating the objective\nfunction for each candidate solution, the designer may have access to\napproximations of the objective functions, for which higher-fidelity\nevaluations entail a larger cost. Existing multi-fidelity black-box\noptimization strategies select candidate solutions and fidelity levels with the\ngoal of maximizing the information accrued about the optimal value or solution\nfor the current task. Assuming that successive optimization tasks are related,\nthis paper introduces a novel information-theoretic acquisition function that\nbalances the need to acquire information about the current task with the goal\nof collecting information transferable to future tasks. The proposed method\nincludes shared inter-task latent variables, which are transferred across tasks\nby implementing particle-based variational Bayesian updates. Experimental\nresults across synthetic and real-world examples reveal that the proposed\nprovident acquisition strategy that caters to future tasks can significantly\nimprove the optimization efficiency as soon as a sufficient number of tasks is\nprocessed.\n","authors":["Yunchuan Zhang","Sangwoo Park","Osvaldo Simeone"],"pdf_url":"https://arxiv.org/pdf/2403.09570v1.pdf","comment":"submitted to IEEE for review"},{"id":"http://arxiv.org/abs/2305.03571v2","updated":"2024-03-14T15:54:10Z","published":"2023-05-05T14:27:58Z","title":"Model-free Reinforcement Learning of Semantic Communication by\n  Stochastic Policy Gradient","summary":"  Following the recent success of Machine Learning tools in wireless\ncommunications, the idea of semantic communication by Weaver from 1949 has\ngained attention. It breaks with Shannon's classic design paradigm by aiming to\ntransmit the meaning, i.e., semantics, of a message instead of its exact\nversion, allowing for information rate savings. In this work, we apply the\nStochastic Policy Gradient (SPG) to design a semantic communication system by\nreinforcement learning, separating transmitter and receiver, and not requiring\na known or differentiable channel model -- a crucial step towards deployment in\npractice. Further, we derive the use of SPG for both classic and semantic\ncommunication from the maximization of the mutual information between received\nand target variables. Numerical results show that our approach achieves\ncomparable performance to a model-aware approach based on the reparametrization\ntrick, albeit with a decreased convergence rate.\n","authors":["Edgar Beck","Carsten Bockelmann","Armin Dekorsy"],"pdf_url":"https://arxiv.org/pdf/2305.03571v2.pdf","comment":"Accepted for publication in IEEE International Conference on Machine\n  Learning for Communication and Networking (ICMLCN 2024), Source Code:\n  https://github.com/ant-uni-bremen/SINFONY"},{"id":"http://arxiv.org/abs/2202.00119v2","updated":"2024-03-14T15:12:59Z","published":"2022-01-31T22:19:49Z","title":"A lower bound on the space overhead of fault-tolerant quantum\n  computation","summary":"  The threshold theorem is a fundamental result in the theory of fault-tolerant\nquantum computation stating that arbitrarily long quantum computations can be\nperformed with a polylogarithmic overhead provided the noise level is below a\nconstant level. A recent work by Fawzi, Grospellier and Leverrier (FOCS 2018)\nbuilding on a result by Gottesman (QIC 2013) has shown that the space overhead\ncan be asymptotically reduced to a constant independent of the circuit provided\nwe only consider circuits with a length bounded by a polynomial in the width.\nIn this work, using a minimal model for quantum fault tolerance, we establish a\ngeneral lower bound on the space overhead required to achieve fault tolerance.\n  For any non-unitary qubit channel $\\mathcal{N}$ and any quantum fault\ntolerance schemes against $\\mathrm{i.i.d.}$ noise modeled by $\\mathcal{N}$, we\nprove a lower bound of\n$\\max\\left\\{\\mathrm{Q}(\\mathcal{N})^{-1}n,\\alpha_\\mathcal{N} \\log T\\right\\}$ on\nthe number of physical qubits, for circuits of length $T$ and width $n$. Here,\n$\\mathrm{Q}(\\mathcal{N})$ denotes the quantum capacity of $\\mathcal{N}$ and\n$\\alpha_\\mathcal{N}>0$ is a constant only depending on the channel\n$\\mathcal{N}$. In our model, we allow for qubits to be replaced by fresh ones\nduring the execution of the circuit and we allow classical computation to be\nfree and perfect. This improves upon results that assumed classical\ncomputations to be also affected by noise, and that sometimes did not allow for\nfresh qubits to be added. Along the way, we prove an exponential upper bound on\nthe maximal length of fault-tolerant quantum computation with amplitude damping\nnoise resolving a conjecture by Ben-Or, Gottesman, and Hassidim (2013).\n","authors":["Omar Fawzi","Alexander Müller-Hermes","Ala Shayeghi"],"pdf_url":"https://arxiv.org/pdf/2202.00119v2.pdf","comment":"22 pages, 2 figures, an earlier version of this paper appeared in\n  proceedings of ITCS 2022. In the current version, Lemma 10 has been\n  simplified and some bounds are improved"},{"id":"http://arxiv.org/abs/2302.02436v3","updated":"2024-03-14T14:06:01Z","published":"2023-02-05T17:21:35Z","title":"Uncertainty-Aware and Reliable Neural MIMO Receivers via Modular\n  Bayesian Deep Learning","summary":"  Deep learning is envisioned to play a key role in the design of future\nwireless receivers. A popular approach to design learning-aided receivers\ncombines deep neural networks (DNNs) with traditional model-based receiver\nalgorithms, realizing hybrid model-based data-driven architectures. Such\narchitectures typically include multiple modules, each carrying out a different\nfunctionality dictated by the model-based receiver workflow. Conventionally\ntrained DNN-based modules are known to produce poorly calibrated, typically\noverconfident, decisions. Consequently, incorrect decisions may propagate\nthrough the architecture without any indication of their insufficient accuracy.\nTo address this problem, we present a novel combination of Bayesian deep\nlearning with hybrid model-based data-driven architectures for wireless\nreceiver design. The proposed methodology, referred to as modular Bayesian deep\nlearning, is designed to yield calibrated modules, which in turn improves both\naccuracy and calibration of the overall receiver. We specialize this approach\nfor two fundamental tasks in multiple-input multiple-output (MIMO) receivers -\nequalization and decoding. In the presence of scarce data, the ability of\nmodular Bayesian deep learning to produce reliable uncertainty measures is\nconsistently shown to directly translate into improved performance of the\noverall MIMO receiver chain.\n","authors":["Tomer Raviv","Sangwoo Park","Osvaldo Simeone","Nir Shlezinger"],"pdf_url":"https://arxiv.org/pdf/2302.02436v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09411v1","updated":"2024-03-14T14:02:10Z","published":"2024-03-14T14:02:10Z","title":"Near-Field Channel Modeling for Holographic MIMO Communications","summary":"  Empowered by the latest progress on innovative metamaterials/metasurfaces and\nadvanced antenna technologies, holographic multiple-input multiple-output\n(H-MIMO) emerges as a promising technology to fulfill the extreme goals of the\nsixth-generation (6G) wireless networks. The antenna arrays utilized in H-MIMO\ncomprise massive (possibly to extreme extent) numbers of antenna elements,\ndensely spaced less than half-a-wavelength and integrated into a compact space,\nrealizing an almost continuous aperture. Thanks to the expected low cost, size,\nweight, and power consumption, such apertures are expected to be largely\nfabricated for near-field communications. In addition, the physical features of\nH-MIMO enable manipulations directly on the electromagnetic (EM) wave domain\nand spatial multiplexing. To fully leverage this potential, near-field H-MIMO\nchannel modeling, especially from the EM perspective, is of paramount\nsignificance. In this article, we overview near-field H-MIMO channel models\nelaborating on the various modeling categories and respective features, as well\nas their challenges and evaluation criteria. We also present EM-domain channel\nmodels that address the inherit computational and measurement complexities.\nFinally, the article is concluded with a set of future research directions on\nthe topic.\n","authors":["Tierui Gong","Li Wei","Chongwen Huang","George C. Alexandropoulos","Mérouane Debbah","Chau Yuen"],"pdf_url":"https://arxiv.org/pdf/2403.09411v1.pdf","comment":"double column, 9 pages, 3 figures, 2 tables, accepted by IEEE\n  Wireless Communications Magazine"},{"id":"http://arxiv.org/abs/2403.09357v1","updated":"2024-03-14T13:04:40Z","published":"2024-03-14T13:04:40Z","title":"Joint Port Selection and Beamforming Design for Fluid Antenna Assisted\n  Integrated Data and Energy Transfer","summary":"  Integrated data and energy transfer (IDET) has been of fundamental importance\nfor providing both wireless data transfer (WDT) and wireless energy transfer\n(WET) services towards low-power devices. Fluid antenna (FA) is capable of\nexploiting the huge spatial diversity of the wireless channel to enhance the\nreceive signal strength, which is more suitable for the tiny-size low-power\ndevices having the IDET requirements. In this letter, a multiuser FA assisted\nIDET system is studied and the weighted energy harvesting power at energy\nreceivers (ERs) is maximized by jointly optimizing the port selection and\ntransmit beamforming design under imperfect channel state information (CSI),\nwhile the signal-to-interference-plus-noise ratio (SINR) constraint for each\ndata receiver (DR) is satisfied. An efficient algorithm is proposed to obtain\nthe suboptimal solutions for the non-convex problem. Simulation results\nevaluate the performance of the FA-IDET system, while also demonstrate that FA\noutperforms the multi-input-multi-output (MIMO) counterpart in terms of the\nIDET performance, as long as the port number is large enough.\n","authors":["Long Zhang","Halvin Yang","Yizhe Zhao","Jie Hu"],"pdf_url":"https://arxiv.org/pdf/2403.09357v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09275v1","updated":"2024-03-14T10:55:36Z","published":"2024-03-14T10:55:36Z","title":"Static Grouping Strategy Design for Beyond Diagonal Reconfigurable\n  Intelligent Surfaces","summary":"  Beyond diagonal reconfigurable intelligent surface (BD-RIS) extends\nconventional RIS through novel architectures, such as group-connected RIS, with\nscattering matrix not restricted to being diagonal. However, it remains\nunexplored how to optimally group the elements in group-connected RISs to\nmaximize the performance while maintaining a low-complexity circuit. In this\nstudy, we propose and model BD-RIS with a static grouping strategy optimized\nbased on the channel statistics. After formulating the corresponding problems,\nwe design the grouping in single- and multi-user systems. Numerical results\nreveal the benefits of grouping optimization, i.e., up to 60% sum rate\nimprovement, especially in highly correlated channels.\n","authors":["Matteo Nerini","Shanpu Shen","Bruno Clerckx"],"pdf_url":"https://arxiv.org/pdf/2403.09275v1.pdf","comment":"Submitted to IEEE for publication"},{"id":"http://arxiv.org/abs/2403.09270v1","updated":"2024-03-14T10:47:01Z","published":"2024-03-14T10:47:01Z","title":"A Deep Reinforcement Learning Approach for Autonomous Reconfigurable\n  Intelligent Surfaces","summary":"  A reconfigurable intelligent surface (RIS) is a prospective wireless\ntechnology that enhances wireless channel quality. An RIS is often equipped\nwith passive array of elements and provides cost and power-efficient solutions\nfor coverage extension of wireless communication systems. Without any radio\nfrequency (RF) chains or computing resources, however, the RIS requires control\ninformation to be sent to it from an external unit, e.g., a base station (BS).\nThe control information can be delivered by wired or wireless channels, and the\nBS must be aware of the RIS and the RIS-related channel conditions in order to\neffectively configure its behavior. Recent works have introduced hybrid RIS\nstructures possessing a few active elements that can sense and digitally\nprocess received data. Here, we propose the operation of an entirely autonomous\nRIS that operates without a control link between the RIS and BS. Using a few\nsensing elements, the autonomous RIS employs a deep Q network (DQN) based on\nreinforcement learning in order to enhance the sum rate of the network. Our\nresults illustrate the potential of deploying autonomous RISs in wireless\nnetworks with essentially no network overhead.\n","authors":["Hyuckjin Choi","Ly V. Nguyen","Junil Choi","A. Lee Swindlehurst"],"pdf_url":"https://arxiv.org/pdf/2403.09270v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06633v2","updated":"2024-03-14T10:45:49Z","published":"2024-03-11T11:45:32Z","title":"Fractal spatio-temporal scale-free messaging: amplitude modulation of\n  self-executable carriers given by the Weierstrass function's components","summary":"  In many communication contexts, the capabilities of the involved actors\ncannot be known beforehand, whether it is a cell, a plant, an insect, or even a\nlife form unknown to Earth. Regardless of the recipient, the message space and\ntime scale could be too fast, too slow, too large, or too small and may never\nbe decoded. Therefore, it pays to devise a way to encode messages agnostic of\nspace and time scales. We propose the use of fractal functions as\nself-executable infinite-frequency carriers for sending messages, given their\nproperties of structural self-similarity and scale invariance. We call it\n`fractal messaging'. Starting from a spatial embedding, we introduce a\nframework for a space-time scale-free messaging approach to this challenge.\nWhen considering a space and time-agnostic framework for message transmission,\nit would be interesting to encode a message such that it could be decoded at\nseveral spatio-temporal scales. Hence, the core idea of the framework proposed\nherein is to encode a binary message as waves along infinitely many frequencies\n(in power-like distributions) and amplitudes, transmit such a message, and then\ndecode and reproduce it. To do so, the components of the Weierstrass function,\na known fractal, are used as carriers of the message. Each component will have\nits amplitude modulated to embed the binary stream, allowing for a\nspace-time-agnostic approach to messaging.\n","authors":["Hector Zenil","Luan Carlos de Sena Monteiro"],"pdf_url":"https://arxiv.org/pdf/2403.06633v2.pdf","comment":"13 pages"}],"Databases":[{"id":"http://arxiv.org/abs/2403.09588v1","updated":"2024-03-14T17:26:00Z","published":"2024-03-14T17:26:00Z","title":"Iterative Forgetting: Online Data Stream Regression Using\n  Database-Inspired Adaptive Granulation","summary":"  Many modern systems, such as financial, transportation, and\ntelecommunications systems, are time-sensitive in the sense that they demand\nlow-latency predictions for real-time decision-making. Such systems often have\nto contend with continuous unbounded data streams as well as concept drift,\nwhich are challenging requirements that traditional regression techniques are\nunable to cater to. There exists a need to create novel data stream regression\nmethods that can handle these scenarios. We present a database-inspired\ndatastream regression model that (a) uses inspiration from R*-trees to create\ngranules from incoming datastreams such that relevant information is retained,\n(b) iteratively forgets granules whose information is deemed to be outdated,\nthus maintaining a list of only recent, relevant granules, and (c) uses the\nrecent data and granules to provide low-latency predictions. The\nR*-tree-inspired approach also makes the algorithm amenable to integration with\ndatabase systems. Our experiments demonstrate that the ability of this method\nto discard data produces a significant order-of-magnitude improvement in\nlatency and training time when evaluated against the most accurate\nstate-of-the-art algorithms, while the R*-tree-inspired granulation technique\nprovides competitively accurate predictions\n","authors":["Niket Kathiriya","Hossein Haeri","Cindy Chen","Kshitij Jerath"],"pdf_url":"https://arxiv.org/pdf/2403.09588v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2110.07917v2","updated":"2024-03-14T11:40:49Z","published":"2021-10-15T07:59:04Z","title":"Improving overlay maps of science: combining overview and detail","summary":"  Overlay maps of science are global base maps over which subsets of\npublications can be projected. Such maps can be used to monitor, explore, and\nstudy research through its publication output. Most maps of science, including\noverlay maps, are flat in the sense that they visualize research fields at one\nsingle level. Such maps generally fail to provide both overview and detail\nabout the research being analyzed. The aim of this study is to improve overlay\nmaps of science to provide both features in a single visualization. I created a\nmap based on a hierarchical classification of publications, including broad\ndisciplines for overview and more granular levels to incorporate detailed\ninformation. The classification was obtained by clustering articles in a\ncitation network of about 17 million publication records in PubMed from 1995\nonwards. The map emphasizes the hierarchical structure of the classification by\nvisualizing both disciplines and the underlying specialties. To show how the\nvisualization methodology can help getting both overview of research and\ndetailed information about its topical structure, I projected two overlay maps\nonto the base map: (1) open access publishing and (2) coronavirus/Covid-19\nresearch.\n","authors":["Peter Sjögårde"],"pdf_url":"https://arxiv.org/pdf/2110.07917v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09060v1","updated":"2024-03-14T02:56:42Z","published":"2024-03-14T02:56:42Z","title":"Query Rewriting via Large Language Models","summary":"  Query rewriting is one of the most effective techniques for coping with\npoorly written queries before passing them down to the query optimizer. Manual\nrewriting is not scalable, as it is error-prone and requires deep expertise.\nSimilarly, traditional query rewriting algorithms can only handle a small\nsubset of queries: rule-based techniques do not generalize to new query\npatterns and synthesis-based techniques cannot handle complex queries.\nFortunately, the rise of Large Language Models (LLMs), equipped with broad\ngeneral knowledge and advanced reasoning capabilities, has created hopes for\nsolving some of these previously open problems.\n  In this paper, we present GenRewrite, the first holistic system that\nleverages LLMs for query rewriting. We introduce the notion of Natural Language\nRewrite Rules (NLR2s), and use them as hints to the LLM but also a means for\ntransferring knowledge from rewriting one query to another, and thus becoming\nsmarter and more effective over time. We present a novel counterexample-guided\ntechnique that iteratively corrects the syntactic and semantic errors in the\nrewritten query, significantly reducing the LLM costs and the manual effort\nrequired for verification. GenRewrite speeds up 22 out of 99 TPC queries (the\nmost complex public benchmark) by more than 2x, which is 2.5x--3.2x higher\ncoverage than state-of-the-art traditional query rewriting and 2.1x higher than\nthe out-of-the-box LLM baseline.\n","authors":["Jie Liu","Barzan Mozafari"],"pdf_url":"https://arxiv.org/pdf/2403.09060v1.pdf","comment":"12 pages"}]},"2024-03-18T00:00:00Z":{"Distributed, Parallel, and Cluster Computing":[{"id":"http://arxiv.org/abs/2403.11603v1","updated":"2024-03-18T09:25:59Z","published":"2024-03-18T09:25:59Z","title":"Fair Distributed Cooperative Bandit Learning on Networks for Intelligent\n  Internet of Things Systems (Technical Report)","summary":"  In intelligent Internet of Things (IoT) systems, edge servers within a\nnetwork exchange information with their neighbors and collect data from sensors\nto complete delivered tasks. In this paper, we propose a multiplayer\nmulti-armed bandit model for intelligent IoT systems to facilitate data\ncollection and incorporate fairness considerations. In our model, we establish\nan effective communication protocol that helps servers cooperate with their\nneighbors. Then we design a distributed cooperative bandit algorithm, DC-ULCB,\nenabling servers to collaboratively select sensors to maximize data rates while\nmaintaining fairness in their choices. We conduct an analysis of the reward\nregret and fairness regret of DC-ULCB, and prove that both regrets have\nlogarithmic instance-dependent upper bounds. Additionally, through extensive\nsimulations, we validate that DC-ULCB outperforms existing algorithms in\nmaximizing reward and ensuring fairness.\n","authors":["Ziqun Chen","Kechao Cai","Jinbei Zhang","Zhigang Yu"],"pdf_url":"https://arxiv.org/pdf/2403.11603v1.pdf","comment":"10 pages, 8 figures, conference technical report"},{"id":"http://arxiv.org/abs/2403.05828v2","updated":"2024-03-18T08:54:10Z","published":"2024-03-09T07:38:45Z","title":"Multi-GPU-Enabled Hybrid Quantum-Classical Workflow in Quantum-HPC\n  Middleware: Applications in Quantum Simulations","summary":"  Achieving high-performance computation on quantum systems presents a\nformidable challenge that necessitates bridging the capabilities between\nquantum hardware and classical computing resources. This study introduces an\ninnovative distribution-aware Quantum-Classical-Quantum (QCQ) architecture,\nwhich integrates cutting-edge quantum software framework works with\nhigh-performance classical computing resources to address challenges in quantum\nsimulation for materials and condensed matter physics. At the heart of this\narchitecture is the seamless integration of VQE algorithms running on QPUs for\nefficient quantum state preparation, Tensor Network states, and QCNNs for\nclassifying quantum states on classical hardware.\n  For benchmarking quantum simulators, the QCQ architecture utilizes the\ncuQuantum SDK to leverage multi-GPU acceleration, integrated with PennyLane's\nLightning plugin, demonstrating up to tenfold increases in computational speed\nfor complex phase transition classification tasks compared to traditional\nCPU-based methods. This significant acceleration enables models such as the\ntransverse field Ising and XXZ systems to accurately predict phase transitions\nwith a 99.5% accuracy. The architecture's ability to distribute computation\nbetween QPUs and classical resources addresses critical bottlenecks in\nQuantum-HPC, paving the way for scalable quantum simulation.\n  The QCQ framework embodies a synergistic combination of quantum algorithms,\nmachine learning, and Quantum-HPC capabilities, enhancing its potential to\nprovide transformative insights into the behavior of quantum systems across\ndifferent scales. As quantum hardware continues to improve, this hybrid\ndistribution-aware framework will play a crucial role in realizing the full\npotential of quantum computing by seamlessly integrating distributed quantum\nresources with the state-of-the-art classical computing infrastructure.\n","authors":["Kuan-Cheng Chen","Xiaoren Li","Xiaotian Xu","Yun-Yuan Wang","Chen-Yu Liu"],"pdf_url":"https://arxiv.org/pdf/2403.05828v2.pdf","comment":"8 pages, 8 figures"},{"id":"http://arxiv.org/abs/2403.11522v1","updated":"2024-03-18T07:22:31Z","published":"2024-03-18T07:22:31Z","title":"LOOPer: A Learned Automatic Code Optimizer For Polyhedral Compilers","summary":"  While polyhedral compilers have shown success in implementing advanced code\ntransformations, they still have challenges in selecting the most profitable\ntransformations that lead to the best speedups. This has motivated the use of\nmachine learning to build cost models to guide the search for polyhedral\noptimizations. State-of-the-art polyhedral compilers have demonstrated a viable\nproof-of-concept of this approach. While such a proof-of-concept has shown\npromise, it still has significant limitations. State-of-the-art polyhedral\ncompilers that use a deep-learning cost model only support a small subset of\naffine transformations, limiting their ability to apply complex code\ntransformations. They also only support simple programs that have a single loop\nnest and a rectangular iteration domain, limiting their applicability to many\nprograms. These limitations significantly impact the generality of such\ncompilers and autoschedulers and put into question the whole approach. In this\npaper, we introduce LOOPer, the first polyhedral autoscheduler that uses a\ndeep-learning based cost model and covers a large set of affine transformations\nand programs. It supports the exploration of a large set of affine\ntransformations, allowing the application of complex sequences of polyhedral\ntransformations. It also supports the optimization of programs with multiple\nloop nests and with rectangular and non-rectangular iteration domains, allowing\nthe optimization of an extensive set of programs. We implement and evaluate\nLOOPer and show that it achieves speedups over the state-of-the-art. On the\nPolybench benchmark, LOOPer achieves a geometric mean speedup of 1.59x over\nTiramisu. LOOPer also achieves competitive speedups with a geometric mean\nspeedup of 1.34x over Pluto, a state-of-the-art polyhedral compiler that does\nnot use a machine-learning based cost model.\n","authors":["Massinissa Merouani","Khaled Afif Boudaoud","Iheb Nassim Aouadj","Nassim Tchoulak","Islam Kara Bernou","Hamza Benyamina","Fatima Benbouzid-Si Tayeb","Karima Benatchba","Hugh Leather","Riyadh Baghdadi"],"pdf_url":"https://arxiv.org/pdf/2403.11522v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11434v1","updated":"2024-03-18T03:08:49Z","published":"2024-03-18T03:08:49Z","title":"Earth+: on-board satellite imagery compression leveraging historical\n  earth observations","summary":"  With the increasing deployment of earth observation satellite constellations,\nthe downlink (satellite-to-ground) capacity often limits the freshness,\nquality, and coverage of the imagery data available to applications on the\nground. To overcome the downlink limitation, we present Earth+, a new satellite\nimagery compression system that, instead of compressing each image\nindividually, pinpoints and downloads only recent imagery changes with respect\nto the history reference images. To minimize the amount of changes, it is\ncritical to make reference images as fresh as possible. Earth+ enables each\nsatellite to choose fresh reference images from not only its own history images\nbut also past images of other satellites from an entire satellite\nconstellation. To share reference images across satellites, Earth+ utilizes the\nlimited capacity of the existing uplink (ground-to-satellite) by judiciously\nselecting and compressing reference images while still allowing accurate change\ndetection. In short, Earth+ is the first to make reference-based compression\nefficient, by enabling constellation-wide sharing of fresh reference images\nacross satellites. Our evaluation shows that Earth+ can reduce the downlink\nusage by a factor of 3.3 compared to state-of-the-art on-board image\ncompression techniques while not sacrificing image quality, or using more\non-board computing or storage resources, or more uplink bandwidth than\ncurrently available.\n","authors":["Kuntai Du","Yihua Cheng","Peder Olsen","Shadi Noghabi","Ranveer Chandra","Junchen Jiang"],"pdf_url":"https://arxiv.org/pdf/2403.11434v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11421v1","updated":"2024-03-18T02:30:23Z","published":"2024-03-18T02:30:23Z","title":"FastDecode: High-Throughput GPU-Efficient LLM Serving using\n  Heterogeneous Pipelines","summary":"  Cost of serving large language models (LLM) is high, but the expensive and\nscarce GPUs are poorly efficient when generating tokens sequentially, unless\nthe batch of sequences is enlarged. However, the batch size is limited by some\nconstantly reused intermediate results, namely KV-Cache. They occupy too much\nmemory to fit more sequences into a GPU simultaneously. While they could be\noffloaded to host memory, the CPU-GPU bandwidth is an inevitable bottleneck.\n  We find a way to decompose the transformer models into two parts of different\ncharacteristics, one of which includes the memory-bound KV-Cache accessing. Our\nkey insight is that the aggregated memory capacity, bandwidth, and computing\npower of CPUs across multiple nodes is an efficient option to process this\npart. Performance improvement comes from reduced data transmission overhead and\nboosted GPU throughput to process the other model part. Moreover, we address\nefficiency challenges brought by heterogeneity at both temporal and\ninter-device scopes using scheduling and performance modeling techniques.\nEvaluation results show that our system achieves 1.88x - 5.04x the throughput\nof vLLM when serving modern LLMs with the same GPU.\n","authors":["Jiaao He","Jidong Zhai"],"pdf_url":"https://arxiv.org/pdf/2403.11421v1.pdf","comment":"15 pages, 15 figures"},{"id":"http://arxiv.org/abs/2403.12313v1","updated":"2024-03-18T23:20:08Z","published":"2024-03-18T23:20:08Z","title":"Improving LoRA in Privacy-preserving Federated Learning","summary":"  Low-rank adaptation (LoRA) is one of the most popular task-specific\nparameter-efficient fine-tuning (PEFT) methods on pre-trained language models\nfor its good performance and computational efficiency. LoRA injects a product\nof two trainable rank decomposition matrices over the top of each frozen\npre-trained model module. However, when applied in the setting of\nprivacy-preserving federated learning (FL), LoRA may become unstable due to the\nfollowing facts: 1) the effects of data heterogeneity and multi-step local\nupdates are non-negligible, 2) additive noise enforced on updating gradients to\nguarantee differential privacy (DP) can be amplified and 3) the final\nperformance is susceptible to hyper-parameters. A key factor leading to these\nphenomena is the discordance between jointly optimizing the two low-rank\nmatrices by local clients and separately aggregating them by the central\nserver. Thus, this paper proposes an efficient and effective version of LoRA,\nFederated Freeze A LoRA (FFA-LoRA), to alleviate these challenges and further\nhalve the communication cost of federated fine-tuning LLMs. The core idea of\nFFA-LoRA is to fix the randomly initialized non-zero matrices and only\nfine-tune the zero-initialized matrices. Compared to LoRA, FFA-LoRA is\nmotivated by practical and theoretical benefits in privacy-preserved FL. Our\nexperiments demonstrate that FFA-LoRA provides more consistent performance with\nbetter computational efficiency over vanilla LoRA in various FL tasks.\n","authors":["Youbang Sun","Zitao Li","Yaliang Li","Bolin Ding"],"pdf_url":"https://arxiv.org/pdf/2403.12313v1.pdf","comment":"published at ICLR 2024, full paper 17 pages"},{"id":"http://arxiv.org/abs/2403.12256v1","updated":"2024-03-18T21:11:09Z","published":"2024-03-18T21:11:09Z","title":"BeRGeR: Byzantine-Robust Geometric Routing","summary":"  We present BeRGeR: the first asynchronous geometric routing algorithm that\nguarantees delivery of a message despite a Byzantine fault without relying on\ncryptographic primitives or randomization. The communication graph is a planar\nembedding that remains three-connected if all edges intersecting the\nsource-target line segment are removed. We prove the algorithm correct and\nestimate its message complexity.\n","authors":["Brown Zaz","Mikhail Nesterenko","Gokarna Sharma"],"pdf_url":"https://arxiv.org/pdf/2403.12256v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12239v1","updated":"2024-03-18T20:39:34Z","published":"2024-03-18T20:39:34Z","title":"Large language models in 6G security: challenges and opportunities","summary":"  The rapid integration of Generative AI (GenAI) and Large Language Models\n(LLMs) in sectors such as education and healthcare have marked a significant\nadvancement in technology. However, this growth has also led to a largely\nunexplored aspect: their security vulnerabilities. As the ecosystem that\nincludes both offline and online models, various tools, browser plugins, and\nthird-party applications continues to expand, it significantly widens the\nattack surface, thereby escalating the potential for security breaches. These\nexpansions in the 6G and beyond landscape provide new avenues for adversaries\nto manipulate LLMs for malicious purposes. We focus on the security aspects of\nLLMs from the viewpoint of potential adversaries. We aim to dissect their\nobjectives and methodologies, providing an in-depth analysis of known security\nweaknesses. This will include the development of a comprehensive threat\ntaxonomy, categorizing various adversary behaviors. Also, our research will\nconcentrate on how LLMs can be integrated into cybersecurity efforts by defense\nteams, also known as blue teams. We will explore the potential synergy between\nLLMs and blockchain technology, and how this combination could lead to the\ndevelopment of next-generation, fully autonomous security solutions. This\napproach aims to establish a unified cybersecurity strategy across the entire\ncomputing continuum, enhancing overall digital security infrastructure.\n","authors":["Tri Nguyen","Huong Nguyen","Ahmad Ijaz","Saeid Sheikhi","Athanasios V. Vasilakos","Panos Kostakos"],"pdf_url":"https://arxiv.org/pdf/2403.12239v1.pdf","comment":"29 pages, 2 figures"},{"id":"http://arxiv.org/abs/2403.12231v1","updated":"2024-03-18T20:23:14Z","published":"2024-03-18T20:23:14Z","title":"Edge-Disjoint Spanning Trees on Star-Product Networks","summary":"  Star-product graphs are a natural extension of the Cartesian product, but\nhave not been well-studied. We show that many important established and\nemerging network topologies, including HyperX, SlimFly, BundleFly, PolarStar,\nmesh, and torus, are in fact star-product graphs. While this connection was\nknown for BundleFly and PolarStar, it was not for the others listed.\n  We extend a method of constructing maximal and near-maximal sets of\nedge-disjoint spanning trees on Cartesian products to the star product, thus\nobtain maximal or near-maximal sets of edge-disjoint spanning trees on new\nnetworks of importance, where such sets can improve bandwidth of collective\noperations and therefore accelerate many important workloads in\nhigh-performance computing.\n","authors":["Aleyah Dawkins","Kelly Isham","Ales Kubicek","Kartik Lakhotia","Laura Monroe"],"pdf_url":"https://arxiv.org/pdf/2403.12231v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12179v1","updated":"2024-03-18T18:50:59Z","published":"2024-03-18T18:50:59Z","title":"AMReX and pyAMReX: Looking Beyond ECP","summary":"  AMReX is a software framework for the development of block-structured mesh\napplications with adaptive mesh refinement (AMR). AMReX was initially developed\nand supported by the AMReX Co-Design Center as part of the U.S. DOE Exascale\nComputing Project, and is continuing to grow post-ECP. In addition to adding\nnew functionality and performance improvements to the core AMReX framework, we\nhave also developed a Python binding, pyAMReX, that provides a bridge between\nAMReX-based application codes and the data science ecosystem. pyAMReX provides\nzero-copy application GPU data access for AI/ML, in situ analysis and\napplication coupling, and enables rapid, massively parallel prototyping. In\nthis paper we review the overall functionality of AMReX and pyAMReX, focusing\non new developments, new functionality, and optimizations of key operations. We\nalso summarize capabilities of ECP projects that used AMReX and provide an\noverview of new, non-ECP applications.\n","authors":["Andrew Myers","Weiqun Zhang","Ann Almgren","Thierry Antoun","John Bell","Axel Huebl","Alexander Sinn"],"pdf_url":"https://arxiv.org/pdf/2403.12179v1.pdf","comment":"12 pages, 1 figure, submitted to the International Journal of High\n  Performance Computing Applications"},{"id":"http://arxiv.org/abs/2403.11892v1","updated":"2024-03-18T15:49:48Z","published":"2024-03-18T15:49:48Z","title":"KnFu: Effective Knowledge Fusion","summary":"  Federated Learning (FL) has emerged as a prominent alternative to the\ntraditional centralized learning approach. Generally speaking, FL is a\ndecentralized approach that allows for collaborative training of Machine\nLearning (ML) models across multiple local nodes, ensuring data privacy and\nsecurity while leveraging diverse datasets. Conventional FL, however, is\nsusceptible to gradient inversion attacks, restrictively enforces a uniform\narchitecture on local models, and suffers from model heterogeneity (model\ndrift) due to non-IID local datasets. To mitigate some of these challenges, the\nnew paradigm of Federated Knowledge Distillation (FKD) has emerged. FDK is\ndeveloped based on the concept of Knowledge Distillation (KD), which involves\nextraction and transfer of a large and well-trained teacher model's knowledge\nto lightweight student models. FKD, however, still faces the model drift issue.\nIntuitively speaking, not all knowledge is universally beneficial due to the\ninherent diversity of data among local nodes. This calls for innovative\nmechanisms to evaluate the relevance and effectiveness of each client's\nknowledge for others, to prevent propagation of adverse knowledge. In this\ncontext, the paper proposes Effective Knowledge Fusion (KnFu) algorithm that\nevaluates knowledge of local models to only fuse semantic neighbors' effective\nknowledge for each client. The KnFu is a personalized effective knowledge\nfusion scheme for each client, that analyzes effectiveness of different local\nmodels' knowledge prior to the aggregation phase. Comprehensive experiments\nwere performed on MNIST and CIFAR10 datasets illustrating effectiveness of the\nproposed KnFu in comparison to its state-of-the-art counterparts. A key\nconclusion of the work is that in scenarios with large and highly heterogeneous\nlocal datasets, local training could be preferable to knowledge fusion-based\nsolutions.\n","authors":["S. Jamal Seyedmohammadi","S. Kawa Atapour","Jamshid Abouei","Arash Mohammadi"],"pdf_url":"https://arxiv.org/pdf/2403.11892v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.02880v2","updated":"2024-03-18T15:27:14Z","published":"2023-12-05T16:52:20Z","title":"PULSAR: Simultaneous Many-Row Activation for Reliable and\n  High-Performance Computing in Off-the-Shelf DRAM Chips","summary":"  Data movement between the processor and the main memory is a first-order\nobstacle against improving performance and energy efficiency in modern systems.\nTo address this obstacle, Processing-using-Memory (PuM) is a promising approach\nwhere bulk-bitwise operations are performed leveraging intrinsic analog\nproperties within the DRAM array and massive parallelism across DRAM columns.\nUnfortunately, 1) modern off-the-shelf DRAM chips do not officially support PuM\noperations, and 2) existing techniques of performing PuM operations on\noff-the-shelf DRAM chips suffer from two key limitations. First, these\ntechniques have low success rates, i.e., only a small fraction of DRAM columns\ncan correctly execute PuM operations because they operate beyond\nmanufacturer-recommended timing constraints, causing these operations to be\nhighly susceptible to noise and process variation. Second, these techniques\nhave limited compute primitives, preventing them from fully leveraging\nparallelism across DRAM columns and thus hindering their performance benefits.\n  We propose PULSAR, a new technique to enable high-success-rate and\nhigh-performance PuM operations in off-the-shelf DRAM chips. PULSAR leverages\nour new observation that a carefully crafted sequence of DRAM commands\nsimultaneously activates up to 32 DRAM rows. PULSAR overcomes the limitations\nof existing techniques by 1) replicating the input data to improve the success\nrate and 2) enabling new bulk bitwise operations (e.g., many-input majority,\nMulti-RowInit, and Bulk-Write) to improve the performance.\n  Our analysis on 120 off-the-shelf DDR4 chips from two major manufacturers\nshows that PULSAR achieves a 24.18% higher success rate and 121% higher\nperformance over seven arithmetic-logic operations compared to FracDRAM, a\nstate-of-the-art off-the-shelf DRAM-based PuM technique.\n","authors":["Ismail Emir Yuksel","Yahya Can Tugrul","F. Nisa Bostanci","Abdullah Giray Yaglikci","Ataberk Olgun","Geraldo F. Oliveira","Melina Soysal","Haocong Luo","Juan Gomez Luna","Mohammad Sadrosadati","Onur Mutlu"],"pdf_url":"https://arxiv.org/pdf/2312.02880v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.17453v2","updated":"2024-03-18T14:21:41Z","published":"2023-06-30T07:57:30Z","title":"Pollen: High-throughput Simulation of Federated Learning via\n  Resource-Aware Client Placement","summary":"  Federated Learning (FL) is a privacy-focused machine learning paradigm that\ncollaboratively trains models directly on edge devices. Simulated environments\nare crucial for large-scale FL research, allowing scientists to quickly test\nnew ideas without acquiring millions of devices. However, current simulators\ncannot match the scale necessary to emulate production systems or push the\nboundaries of research in a time-efficient manner. This work proposes\n\\emph{Pollen}, a novel resource-aware system for speeding up simulations.\n\\emph{Pollen} addresses two limiting factors from previous systems: (a)\ncommunication inefficiency in pull-based client execution and (b) ignoring\nsystem inefficiencies from simulation-hardware diversity. \\emph{Pollen}\nexecutes high-throughput FL simulations at scale by (a) using a push-based\nclient placement system and (b) balancing clients across servers and their GPUs\nwith a novel online machine learning model. Furthermore, \\emph{Pollen}'s\nplacement model reduces GPU idle time by up to 50\\% by providing accurate\ntraining time predictions, allowing researchers to run extensive experiments\nsampling from millions of clients. Our experiments evaluate \\pollen on four\nrepresentative FL tasks. We compare \\emph{Pollen} to ad-hoc FL frameworks,\n\\emph{Flower}, \\emph{Flute}, \\emph{FedScale}, and \\emph{Parrot}, and show\nexperimental speed-ups of days or weeks.\n","authors":["Lorenzo Sani","Pedro Porto Buarque de Gusmão","Alex Iacob","Wanru Zhao","Xinchi Qiu","Yan Gao","Javier Fernandez-Marques","Nicholas Donald Lane"],"pdf_url":"https://arxiv.org/pdf/2306.17453v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11795v1","updated":"2024-03-18T13:53:17Z","published":"2024-03-18T13:53:17Z","title":"Low-Cost Privacy-Aware Decentralized Learning","summary":"  This paper introduces ZIP-DL, a novel privacy-aware decentralized learning\n(DL) algorithm that relies on adding correlated noise to each model update\nduring the model training process. This technique ensures that the added noise\nalmost neutralizes itself during the aggregation process due to its\ncorrelation, thus minimizing the impact on model accuracy. In addition, ZIP-DL\ndoes not require multiple communication rounds for noise cancellation,\naddressing the common trade-off between privacy protection and communication\noverhead. We provide theoretical guarantees for both convergence speed and\nprivacy guarantees, thereby making ZIP-DL applicable to practical scenarios.\nOur extensive experimental study shows that ZIP-DL achieves the best trade-off\nbetween vulnerability and accuracy. In particular, ZIP-DL (i) reduces the\neffectiveness of a linkability attack by up to 52 points compared to baseline\nDL, and (ii) achieves up to 37 more accuracy points for the same vulnerability\nunder membership inference attacks against a privacy-preserving competitor\n","authors":["Sayan Biswas","Davide Frey","Romaric Gaudel","Anne-Marie Kermarrec","Dimitri Lerévérend","Rafael Pires","Rishi Sharma","François Taïani"],"pdf_url":"https://arxiv.org/pdf/2403.11795v1.pdf","comment":null}],"Performance":[{"id":"http://arxiv.org/abs/2403.11421v1","updated":"2024-03-18T02:30:23Z","published":"2024-03-18T02:30:23Z","title":"FastDecode: High-Throughput GPU-Efficient LLM Serving using\n  Heterogeneous Pipelines","summary":"  Cost of serving large language models (LLM) is high, but the expensive and\nscarce GPUs are poorly efficient when generating tokens sequentially, unless\nthe batch of sequences is enlarged. However, the batch size is limited by some\nconstantly reused intermediate results, namely KV-Cache. They occupy too much\nmemory to fit more sequences into a GPU simultaneously. While they could be\noffloaded to host memory, the CPU-GPU bandwidth is an inevitable bottleneck.\n  We find a way to decompose the transformer models into two parts of different\ncharacteristics, one of which includes the memory-bound KV-Cache accessing. Our\nkey insight is that the aggregated memory capacity, bandwidth, and computing\npower of CPUs across multiple nodes is an efficient option to process this\npart. Performance improvement comes from reduced data transmission overhead and\nboosted GPU throughput to process the other model part. Moreover, we address\nefficiency challenges brought by heterogeneity at both temporal and\ninter-device scopes using scheduling and performance modeling techniques.\nEvaluation results show that our system achieves 1.88x - 5.04x the throughput\nof vLLM when serving modern LLMs with the same GPU.\n","authors":["Jiaao He","Jidong Zhai"],"pdf_url":"https://arxiv.org/pdf/2403.11421v1.pdf","comment":"15 pages, 15 figures"}],"Artificial Intelligence":[{"id":"http://arxiv.org/abs/2403.11703v1","updated":"2024-03-18T12:04:11Z","published":"2024-03-18T12:04:11Z","title":"LLaVA-UHD: an LMM Perceiving Any Aspect Ratio and High-Resolution Images","summary":"  Visual encoding constitutes the basis of large multimodal models (LMMs) in\nunderstanding the visual world. Conventional LMMs process images in fixed sizes\nand limited resolutions, while recent explorations in this direction are\nlimited in adaptivity, efficiency, and even correctness. In this work, we first\ntake GPT-4V and LLaVA-1.5 as representative examples and expose systematic\nflaws rooted in their visual encoding strategy. To address the challenges, we\npresent LLaVA-UHD, a large multimodal model that can efficiently perceive\nimages in any aspect ratio and high resolution. LLaVA-UHD includes three key\ncomponents: (1) An image modularization strategy that divides native-resolution\nimages into smaller variable-sized slices for efficient and extensible\nencoding, (2) a compression module that further condenses image tokens from\nvisual encoders, and (3) a spatial schema to organize slice tokens for LLMs.\nComprehensive experiments show that LLaVA-UHD outperforms established LMMs\ntrained with 2-3 orders of magnitude more data on 9 benchmarks. Notably, our\nmodel built on LLaVA-1.5 336x336 supports 6 times larger (i.e., 672x1088)\nresolution images using only 94% inference computation, and achieves 6.4\naccuracy improvement on TextVQA. Moreover, the model can be efficiently trained\nin academic settings, within 23 hours on 8 A100 GPUs (vs. 26 hours of\nLLaVA-1.5). We make the data and code publicly available at\nhttps://github.com/thunlp/LLaVA-UHD.\n","authors":["Ruyi Xu","Yuan Yao","Zonghao Guo","Junbo Cui","Zanlin Ni","Chunjiang Ge","Tat-Seng Chua","Zhiyuan Liu","Maosong Sun","Gao Huang"],"pdf_url":"https://arxiv.org/pdf/2403.11703v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2403.11671v1","updated":"2024-03-18T11:19:37Z","published":"2024-03-18T11:19:37Z","title":"HDLdebugger: Streamlining HDL debugging with Large Language Models","summary":"  In the domain of chip design, Hardware Description Languages (HDLs) play a\npivotal role. However, due to the complex syntax of HDLs and the limited\navailability of online resources, debugging HDL codes remains a difficult and\ntime-intensive task, even for seasoned engineers. Consequently, there is a\npressing need to develop automated HDL code debugging models, which can\nalleviate the burden on hardware engineers. Despite the strong capabilities of\nLarge Language Models (LLMs) in generating, completing, and debugging software\ncode, their utilization in the specialized field of HDL debugging has been\nlimited and, to date, has not yielded satisfactory results. In this paper, we\npropose an LLM-assisted HDL debugging framework, namely HDLdebugger, which\nconsists of HDL debugging data generation via a reverse engineering approach, a\nsearch engine for retrieval-augmented generation, and a retrieval-augmented LLM\nfine-tuning approach. Through the integration of these components, HDLdebugger\ncan automate and streamline HDL debugging for chip design. Our comprehensive\nexperiments, conducted on an HDL code dataset sourced from Huawei, reveal that\nHDLdebugger outperforms 13 cutting-edge LLM baselines, displaying exceptional\neffectiveness in HDL code debugging.\n","authors":["Xufeng Yao","Haoyang Li","Tsz Ho Chan","Wenyi Xiao","Mingxuan Yuan","Yu Huang","Lei Chen","Bei Yu"],"pdf_url":"https://arxiv.org/pdf/2403.11671v1.pdf","comment":"13 pages,5 figures"},{"id":"http://arxiv.org/abs/2403.04325v2","updated":"2024-03-18T11:17:48Z","published":"2024-03-07T08:44:42Z","title":"Measuring Meaning Composition in the Human Brain with Composition Scores\n  from Large Language Models","summary":"  The process of meaning composition, wherein smaller units like morphemes or\nwords combine to form the meaning of phrases and sentences, is essential for\nhuman sentence comprehension. Despite extensive neurolinguistic research into\nthe brain regions involved in meaning composition, a computational metric to\nquantify the extent of composition is still lacking. Drawing on the key-value\nmemory interpretation of transformer feed-forward network blocks, we introduce\nthe Composition Score, a novel model-based metric designed to quantify the\ndegree of meaning composition during sentence comprehension. Experimental\nfindings show that this metric correlates with brain clusters associated with\nword frequency, structural processing, and general sensitivity to words,\nsuggesting the multifaceted nature of meaning composition during human sentence\ncomprehension.\n","authors":["Changjiang Gao","Jixing Li","Jiajun Chen","Shujian Huang"],"pdf_url":"https://arxiv.org/pdf/2403.04325v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.14899v2","updated":"2024-03-18T10:55:36Z","published":"2024-02-22T17:36:34Z","title":"Stop Reasoning! When Multimodal LLMs with Chain-of-Thought Reasoning\n  Meets Adversarial Images","summary":"  Recently, Multimodal LLMs (MLLMs) have shown a great ability to understand\nimages. However, like traditional vision models, they are still vulnerable to\nadversarial images. Meanwhile, Chain-of-Thought (CoT) reasoning has been widely\nexplored on MLLMs, which not only improves model's performance, but also\nenhances model's explainability by giving intermediate reasoning steps.\nNevertheless, there is still a lack of study regarding MLLMs' adversarial\nrobustness with CoT and an understanding of what the rationale looks like when\nMLLMs infer wrong answers with adversarial images. Our research evaluates the\nadversarial robustness of MLLMs when employing CoT reasoning, finding that CoT\nmarginally improves adversarial robustness against existing attack methods.\nMoreover, we introduce a novel stop-reasoning attack technique that effectively\nbypasses the CoT-induced robustness enhancements. Finally, we demonstrate the\nalterations in CoT reasoning when MLLMs confront adversarial images, shedding\nlight on their reasoning process under adversarial attacks.\n","authors":["Zefeng Wang","Zhen Han","Shuo Chen","Fan Xue","Zifeng Ding","Xun Xiao","Volker Tresp","Philip Torr","Jindong Gu"],"pdf_url":"https://arxiv.org/pdf/2402.14899v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11642v1","updated":"2024-03-18T10:34:40Z","published":"2024-03-18T10:34:40Z","title":"Guiding the generation of counterfactual explanations through temporal\n  background knowledge for Predictive Process Monitoring","summary":"  Counterfactual explanations suggest what should be different in the input\ninstance to change the outcome of an AI system. When dealing with\ncounterfactual explanations in the field of Predictive Process Monitoring,\nhowever, control flow relationships among events have to be carefully\nconsidered. A counterfactual, indeed, should not violate control flow\nrelationships among activities (temporal background knowledege). Within the\nfield of Explainability in Predictive Process Monitoring, there have been a\nseries of works regarding counterfactual explanations for outcome-based\npredictions. However, none of them consider the inclusion of temporal\nbackground knowledge when generating these counterfactuals. In this work, we\nadapt state-of-the-art techniques for counterfactual generation in the domain\nof XAI that are based on genetic algorithms to consider a series of temporal\nconstraints at runtime. We assume that this temporal background knowledge is\ngiven, and we adapt the fitness function, as well as the crossover and mutation\noperators, to maintain the satisfaction of the constraints. The proposed\nmethods are evaluated with respect to state-of-the-art genetic algorithms for\ncounterfactual generation and the results are presented. We showcase that the\ninclusion of temporal background knowledge allows the generation of\ncounterfactuals more conformant to the temporal background knowledge, without\nhowever losing in terms of the counterfactual traditional quality metrics.\n","authors":["Andrei Buliga","Chiara Di Francescomarino","Chiara Ghidini","Ivan Donadello","Fabrizio Maria Maggi"],"pdf_url":"https://arxiv.org/pdf/2403.11642v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.01012v3","updated":"2024-03-18T10:32:59Z","published":"2023-10-02T09:03:59Z","title":"Unconstrained Stochastic CCA: Unifying Multiview and Self-Supervised\n  Learning","summary":"  The Canonical Correlation Analysis (CCA) family of methods is foundational in\nmultiview learning. Regularised linear CCA methods can be seen to generalise\nPartial Least Squares (PLS) and be unified with a Generalized Eigenvalue\nProblem (GEP) framework. However, classical algorithms for these linear methods\nare computationally infeasible for large-scale data. Extensions to Deep CCA\nshow great promise, but current training procedures are slow and complicated.\nFirst we propose a novel unconstrained objective that characterizes the top\nsubspace of GEPs. Our core contribution is a family of fast algorithms for\nstochastic PLS, stochastic CCA, and Deep CCA, simply obtained by applying\nstochastic gradient descent (SGD) to the corresponding CCA objectives. Our\nalgorithms show far faster convergence and recover higher correlations than the\nprevious state-of-the-art on all standard CCA and Deep CCA benchmarks. These\nimprovements allow us to perform a first-of-its-kind PLS analysis of an\nextremely large biomedical dataset from the UK Biobank, with over 33,000\nindividuals and 500,000 features. Finally, we apply our algorithms to match the\nperformance of `CCA-family' Self-Supervised Learning (SSL) methods on CIFAR-10\nand CIFAR-100 with minimal hyper-parameter tuning, and also present theory to\nclarify the links between these methods and classical CCA, laying the\ngroundwork for future insights.\n","authors":["James Chapman","Lennie Wells","Ana Lawry Aguila"],"pdf_url":"https://arxiv.org/pdf/2310.01012v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07440v2","updated":"2024-03-18T10:13:05Z","published":"2024-03-12T09:32:25Z","title":"Matrix-Transformation Based Low-Rank Adaptation (MTLoRA): A\n  Brain-Inspired Method for Parameter-Efficient Fine-Tuning","summary":"  Fine-tuning techniques based on Large Pretrained Language Models (LPLMs) have\nbeen proven to significantly enhance model performance on a variety of\ndownstream tasks and effectively control the output behaviors of LPLMs. Recent\nstudies have proposed numerous methods for fine-tuning a small number of\nparameters based on open-source LPLMs, reducing the demand for computational\nand storage resources. Among these, reparameterization fine-tuning methods\nrepresented by LoRA (Low-Rank Adaptation) have gained popularity. We find that\nalthough these methods perform well in many aspects, there is still\nconsiderable room for improvement in terms of complex task adaptability,\nperformance, stability, and algorithm complexity. In response to this, inspired\nby the idea that the functions of the brain are shaped by its geometric\nstructure, this paper integrates this idea into LoRA technology and proposes a\nnew matrix transformation-based reparameterization method for efficient\nfine-tuning, named Matrix-Transformation based Low-Rank Adaptation (MTLoRA).\nMTLoRA aims to dynamically alter its spatial geometric structure by applying a\ntransformation-matrix T to perform linear transformations, such as rotation,\nscaling, and translation, on the task-specific parameter matrix, generating new\nmatrix feature patterns (eigenvectors) to mimic the fundamental influence of\ncomplex geometric structure feature patterns in the brain on functions, thereby\nenhancing the model's performance in downstream tasks. In Natural Language\nUnderstanding (NLU) tasks, it is evaluated using the GLUE benchmark test, and\nthe results reveal that MTLoRA achieves an overall performance increase of\nabout 1.0% across eight tasks; in Natural Language Generation (NLG) tasks,\nMTLoRA improves performance by an average of 0.95% and 0.56% in the DART and\nWebNLG tasks, respectively.\n","authors":["Yao Liang","Yuwei Wang","Yang Li","Yi Zeng"],"pdf_url":"https://arxiv.org/pdf/2403.07440v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11626v1","updated":"2024-03-18T09:58:43Z","published":"2024-03-18T09:58:43Z","title":"QEAN: Quaternion-Enhanced Attention Network for Visual Dance Generation","summary":"  The study of music-generated dance is a novel and challenging Image\ngeneration task. It aims to input a piece of music and seed motions, then\ngenerate natural dance movements for the subsequent music. Transformer-based\nmethods face challenges in time series prediction tasks related to human\nmovements and music due to their struggle in capturing the nonlinear\nrelationship and temporal aspects. This can lead to issues like joint\ndeformation, role deviation, floating, and inconsistencies in dance movements\ngenerated in response to the music. In this paper, we propose a\nQuaternion-Enhanced Attention Network (QEAN) for visual dance synthesis from a\nquaternion perspective, which consists of a Spin Position Embedding (SPE)\nmodule and a Quaternion Rotary Attention (QRA) module. First, SPE embeds\nposition information into self-attention in a rotational manner, leading to\nbetter learning of features of movement sequences and audio sequences, and\nimproved understanding of the connection between music and dance. Second, QRA\nrepresents and fuses 3D motion features and audio features in the form of a\nseries of quaternions, enabling the model to better learn the temporal\ncoordination of music and dance under the complex temporal cycle conditions of\ndance generation. Finally, we conducted experiments on the dataset AIST++, and\nthe results show that our approach achieves better and more robust performance\nin generating accurate, high-quality dance movements. Our source code and\ndataset can be available from https://github.com/MarasyZZ/QEAN and\nhttps://google.github.io/aistplusplus_dataset respectively.\n","authors":["Zhizhen Zhou","Yejing Huo","Guoheng Huang","An Zeng","Xuhang Chen","Lian Huang","Zinuo Li"],"pdf_url":"https://arxiv.org/pdf/2403.11626v1.pdf","comment":"Accepted by The Visual Computer Journal"},{"id":"http://arxiv.org/abs/2402.13602v3","updated":"2024-03-18T09:50:00Z","published":"2024-02-21T08:09:05Z","title":"Hybrid Reasoning Based on Large Language Models for Autonomous Car\n  Driving","summary":"  Large Language Models (LLMs) have garnered significant attention for their\nability to understand text and images, generate human-like text, and perform\ncomplex reasoning tasks. However, their ability to generalize this advanced\nreasoning with a combination of natural language text for decision-making in\ndynamic situations requires further exploration. In this study, we investigate\nhow well LLMs can adapt and apply a combination of arithmetic and common-sense\nreasoning, particularly in autonomous driving scenarios. We hypothesize that\nLLMs hybrid reasoning abilities can improve autonomous driving by enabling them\nto analyze detected object and sensor data, understand driving regulations and\nphysical laws, and offer additional context. This addresses complex scenarios,\nlike decisions in low visibility (due to weather conditions), where traditional\nmethods might fall short. We evaluated Large Language Models (LLMs) based on\naccuracy by comparing their answers with human-generated ground truth inside\nCARLA. The results showed that when a combination of images (detected objects)\nand sensor data is fed into the LLM, it can offer precise information for brake\nand throttle control in autonomous vehicles across various weather conditions.\nThis formulation and answers can assist in decision-making for auto-pilot\nsystems.\n","authors":["Mehdi Azarafza","Mojtaba Nayyeri","Charles Steinmetz","Steffen Staab","Achim Rettberg"],"pdf_url":"https://arxiv.org/pdf/2402.13602v3.pdf","comment":"12 pages, 4 figures"},{"id":"http://arxiv.org/abs/2403.06880v2","updated":"2024-03-18T09:43:20Z","published":"2024-03-11T16:34:23Z","title":"Unveiling the Significance of Toddler-Inspired Reward Transition in\n  Goal-Oriented Reinforcement Learning","summary":"  Toddlers evolve from free exploration with sparse feedback to exploiting\nprior experiences for goal-directed learning with denser rewards. Drawing\ninspiration from this Toddler-Inspired Reward Transition, we set out to explore\nthe implications of varying reward transitions when incorporated into\nReinforcement Learning (RL) tasks. Central to our inquiry is the transition\nfrom sparse to potential-based dense rewards, which share optimal strategies\nregardless of reward changes. Through various experiments, including those in\negocentric navigation and robotic arm manipulation tasks, we found that proper\nreward transitions significantly influence sample efficiency and success rates.\nOf particular note is the efficacy of the toddler-inspired Sparse-to-Dense\n(S2D) transition. Beyond these performance metrics, using Cross-Density\nVisualizer technique, we observed that transitions, especially the S2D, smooth\nthe policy loss landscape, promoting wide minima that enhance generalization in\nRL models.\n","authors":["Junseok Park","Yoonsung Kim","Hee Bin Yoo","Min Whoo Lee","Kibeom Kim","Won-Seok Choi","Minsu Lee","Byoung-Tak Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.06880v2.pdf","comment":"Accepted as a full paper at AAAI 2024 (Oral presentation): 7 pages\n  (main paper), 2 pages (references), 17 pages (appendix) each"},{"id":"http://arxiv.org/abs/2402.16086v2","updated":"2024-03-18T09:33:47Z","published":"2024-02-25T13:22:17Z","title":"Deep Homography Estimation for Visual Place Recognition","summary":"  Visual place recognition (VPR) is a fundamental task for many applications\nsuch as robot localization and augmented reality. Recently, the hierarchical\nVPR methods have received considerable attention due to the trade-off between\naccuracy and efficiency. They usually first use global features to retrieve the\ncandidate images, then verify the spatial consistency of matched local features\nfor re-ranking. However, the latter typically relies on the RANSAC algorithm\nfor fitting homography, which is time-consuming and non-differentiable. This\nmakes existing methods compromise to train the network only in global feature\nextraction. Here, we propose a transformer-based deep homography estimation\n(DHE) network that takes the dense feature map extracted by a backbone network\nas input and fits homography for fast and learnable geometric verification.\nMoreover, we design a re-projection error of inliers loss to train the DHE\nnetwork without additional homography labels, which can also be jointly trained\nwith the backbone network to help it extract the features that are more\nsuitable for local matching. Extensive experiments on benchmark datasets show\nthat our method can outperform several state-of-the-art methods. And it is more\nthan one order of magnitude faster than the mainstream hierarchical VPR methods\nusing RANSAC. The code is released at https://github.com/Lu-Feng/DHE-VPR.\n","authors":["Feng Lu","Shuting Dong","Lijun Zhang","Bingxi Liu","Xiangyuan Lan","Dongmei Jiang","Chun Yuan"],"pdf_url":"https://arxiv.org/pdf/2402.16086v2.pdf","comment":"Accepted by AAAI2024"},{"id":"http://arxiv.org/abs/2403.11598v1","updated":"2024-03-18T09:19:01Z","published":"2024-03-18T09:19:01Z","title":"Optimal Layout Synthesis for Deep Quantum Circuits on NISQ Processors\n  with 100+ Qubits","summary":"  Layout synthesis is mapping a quantum circuit to a quantum processor. SWAP\ngate insertions are needed for scheduling 2-qubit gates only on connected\nphysical qubits. With the ever-increasing number of qubits in NISQ processors,\nscalable layout synthesis is of utmost importance. With large optimality gaps\nobserved in heuristic approaches, scalable exact methods are needed. While\nrecent exact and near-optimal approaches scale to moderate circuits, large deep\ncircuits are still out of scope.\n  In this work, we propose a SAT encoding based on parallel plans that apply 1\nSWAP and a group of CNOTs at each time step. Using domain-specific information,\nwe maintain optimality in parallel plans while scaling to large and deep\ncircuits. From our results, we show the scalability of our approach which\nsignificantly outperforms leading exact and near-optimal approaches (up to\n100x). For the first time, we can optimally map several 8, 14, and 16 qubit\ncircuits onto 54, 80, and 127 qubit platforms with up to 17 SWAPs. While adding\noptimal SWAPs, we also report near-optimal depth in our mapped circuits.\n","authors":["Irfansha Shaik","Jaco van de Pol"],"pdf_url":"https://arxiv.org/pdf/2403.11598v1.pdf","comment":"7 Figures, 4 Tables, 1 Listing"},{"id":"http://arxiv.org/abs/2401.11944v2","updated":"2024-03-18T09:02:03Z","published":"2024-01-22T13:34:34Z","title":"CMMMU: A Chinese Massive Multi-discipline Multimodal Understanding\n  Benchmark","summary":"  As the capabilities of large multimodal models (LMMs) continue to advance,\nevaluating the performance of LMMs emerges as an increasing need. Additionally,\nthere is an even larger gap in evaluating the advanced knowledge and reasoning\nabilities of LMMs in non-English contexts such as Chinese. We introduce CMMMU,\na new Chinese Massive Multi-discipline Multimodal Understanding benchmark\ndesigned to evaluate LMMs on tasks demanding college-level subject knowledge\nand deliberate reasoning in a Chinese context. CMMMU is inspired by and\nstrictly follows the annotation and analysis pattern of MMMU.\n  CMMMU includes 12k manually collected multimodal questions from college\nexams, quizzes, and textbooks, covering six core disciplines: Art & Design,\nBusiness, Science, Health & Medicine, Humanities & Social Science, and Tech &\nEngineering, like its companion, MMMU. These questions span 30 subjects and\ncomprise 39 highly heterogeneous image types, such as charts, diagrams, maps,\ntables, music sheets, and chemical structures.\n  CMMMU focuses on complex perception and reasoning with domain-specific\nknowledge in the Chinese context. We evaluate 11 open-source LLMs and one\nproprietary GPT-4V(ision). Even GPT-4V only achieves accuracies of 42%,\nindicating a large space for improvement. CMMMU will boost the community to\nbuild the next-generation LMMs towards expert artificial intelligence and\npromote the democratization of LMMs by providing diverse language contexts.\n","authors":["Ge Zhang","Xinrun Du","Bei Chen","Yiming Liang","Tongxu Luo","Tianyu Zheng","Kang Zhu","Yuyang Cheng","Chunpu Xu","Shuyue Guo","Haoran Zhang","Xingwei Qu","Junjie Wang","Ruibin Yuan","Yizhi Li","Zekun Wang","Yudong Liu","Yu-Hsuan Tsai","Fengji Zhang","Chenghua Lin","Wenhao Huang","Wenhu Chen","Jie Fu"],"pdf_url":"https://arxiv.org/pdf/2401.11944v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11585v1","updated":"2024-03-18T08:58:47Z","published":"2024-03-18T08:58:47Z","title":"Linguacodus: A Synergistic Framework for Transformative Code Generation\n  in Machine Learning Pipelines","summary":"  In the ever-evolving landscape of machine learning, seamless translation of\nnatural language descriptions into executable code remains a formidable\nchallenge. This paper introduces Linguacodus, an innovative framework designed\nto tackle this challenge by deploying a dynamic pipeline that iteratively\ntransforms natural language task descriptions into code through high-level\ndata-shaping instructions. The core of Linguacodus is a fine-tuned large\nlanguage model (LLM), empowered to evaluate diverse solutions for various\nproblems and select the most fitting one for a given task. This paper details\nthe fine-tuning process, and sheds light on how natural language descriptions\ncan be translated into functional code. Linguacodus represents a substantial\nleap towards automated code generation, effectively bridging the gap between\ntask descriptions and executable code. It holds great promise for advancing\nmachine learning applications across diverse domains. Additionally, we propose\nan algorithm capable of transforming a natural description of an ML task into\ncode with minimal human interaction. In extensive experiments on a vast machine\nlearning code dataset originating from Kaggle, we showcase the effectiveness of\nLinguacodus. The investigations highlight its potential applications across\ndiverse domains, emphasizing its impact on applied machine learning in various\nscientific fields.\n","authors":["Ekaterina Trofimova","Emil Sataev","Andrey E. Ustyuzhanin"],"pdf_url":"https://arxiv.org/pdf/2403.11585v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.16701v2","updated":"2024-03-18T08:55:36Z","published":"2023-08-15T17:38:55Z","title":"Is it Really Negative? Evaluating Natural Language Video Localization\n  Performance on Multiple Reliable Videos Pool","summary":"  With the explosion of multimedia content in recent years, Video Corpus Moment\nRetrieval (VCMR), which aims to detect a video moment that matches a given\nnatural language query from multiple videos, has become a critical problem.\nHowever, existing VCMR studies have a significant limitation since they have\nregarded all videos not paired with a specific query as negative, neglecting\nthe possibility of including false negatives when constructing the negative\nvideo set. In this paper, we propose an MVMR (Massive Videos Moment Retrieval)\ntask that aims to localize video frames within a massive video set, mitigating\nthe possibility of falsely distinguishing positive and negative videos. For\nthis task, we suggest an automatic dataset construction framework by employing\ntextual and visual semantic matching evaluation methods on the existing video\nmoment search datasets and introduce three MVMR datasets. To solve MVMR task,\nwe further propose a strong method, CroCs, which employs cross-directional\ncontrastive learning that selectively identifies the reliable and informative\nnegatives, enhancing the robustness of a model on MVMR task. Experimental\nresults on the introduced datasets reveal that existing video moment search\nmodels are easily distracted by negative video frames, whereas our model shows\nsignificant performance.\n","authors":["Nakyeong Yang","Minsung Kim","Seunghyun Yoon","Joongbo Shin","Kyomin Jung"],"pdf_url":"https://arxiv.org/pdf/2309.16701v2.pdf","comment":"15 pages, 10 figures"},{"id":"http://arxiv.org/abs/2403.05828v2","updated":"2024-03-18T08:54:10Z","published":"2024-03-09T07:38:45Z","title":"Multi-GPU-Enabled Hybrid Quantum-Classical Workflow in Quantum-HPC\n  Middleware: Applications in Quantum Simulations","summary":"  Achieving high-performance computation on quantum systems presents a\nformidable challenge that necessitates bridging the capabilities between\nquantum hardware and classical computing resources. This study introduces an\ninnovative distribution-aware Quantum-Classical-Quantum (QCQ) architecture,\nwhich integrates cutting-edge quantum software framework works with\nhigh-performance classical computing resources to address challenges in quantum\nsimulation for materials and condensed matter physics. At the heart of this\narchitecture is the seamless integration of VQE algorithms running on QPUs for\nefficient quantum state preparation, Tensor Network states, and QCNNs for\nclassifying quantum states on classical hardware.\n  For benchmarking quantum simulators, the QCQ architecture utilizes the\ncuQuantum SDK to leverage multi-GPU acceleration, integrated with PennyLane's\nLightning plugin, demonstrating up to tenfold increases in computational speed\nfor complex phase transition classification tasks compared to traditional\nCPU-based methods. This significant acceleration enables models such as the\ntransverse field Ising and XXZ systems to accurately predict phase transitions\nwith a 99.5% accuracy. The architecture's ability to distribute computation\nbetween QPUs and classical resources addresses critical bottlenecks in\nQuantum-HPC, paving the way for scalable quantum simulation.\n  The QCQ framework embodies a synergistic combination of quantum algorithms,\nmachine learning, and Quantum-HPC capabilities, enhancing its potential to\nprovide transformative insights into the behavior of quantum systems across\ndifferent scales. As quantum hardware continues to improve, this hybrid\ndistribution-aware framework will play a crucial role in realizing the full\npotential of quantum computing by seamlessly integrating distributed quantum\nresources with the state-of-the-art classical computing infrastructure.\n","authors":["Kuan-Cheng Chen","Xiaoren Li","Xiaotian Xu","Yun-Yuan Wang","Chen-Yu Liu"],"pdf_url":"https://arxiv.org/pdf/2403.05828v2.pdf","comment":"8 pages, 8 figures"},{"id":"http://arxiv.org/abs/2403.08293v2","updated":"2024-03-18T08:48:30Z","published":"2024-03-13T06:54:47Z","title":"Generative Pretrained Structured Transformers: Unsupervised Syntactic\n  Language Models at Scale","summary":"  A syntactic language model (SLM) incrementally generates a sentence with its\nsyntactic tree in a left-to-right manner. We present Generative Pretrained\nStructured Transformers (GPST), an unsupervised SLM at scale capable of being\npre-trained from scratch on raw texts with high parallelism. GPST circumvents\nthe limitations of previous SLMs such as relying on gold trees and sequential\ntraining. It consists of two components, a usual SLM supervised by a\nuni-directional language modeling loss, and an additional composition model,\nwhich induces syntactic parse trees and computes constituent representations,\nsupervised by a bi-directional language modeling loss. We propose a\nrepresentation surrogate to enable joint parallel training of the two models in\na hard-EM fashion. We pre-train GPST on OpenWebText, a corpus with $9$ billion\ntokens, and demonstrate the superiority of GPST over GPT-2 with a comparable\nsize in numerous tasks covering both language understanding and language\ngeneration. Meanwhile, GPST also significantly outperforms existing\nunsupervised SLMs on left-to-right grammar induction, while holding a\nsubstantial acceleration on training.\n","authors":["Xiang Hu","Pengyu Ji","Qingyang Zhu","Wei Wu","Kewei Tu"],"pdf_url":"https://arxiv.org/pdf/2403.08293v2.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2401.14405v2","updated":"2024-03-18T08:45:52Z","published":"2024-01-25T18:59:58Z","title":"Multimodal Pathway: Improve Transformers with Irrelevant Data from Other\n  Modalities","summary":"  We propose to improve transformers of a specific modality with irrelevant\ndata from other modalities, e.g., improve an ImageNet model with audio or point\ncloud datasets. We would like to highlight that the data samples of the target\nmodality are irrelevant to the other modalities, which distinguishes our method\nfrom other works utilizing paired (e.g., CLIP) or interleaved data of different\nmodalities. We propose a methodology named Multimodal Pathway - given a target\nmodality and a transformer designed for it, we use an auxiliary transformer\ntrained with data of another modality and construct pathways to connect\ncomponents of the two models so that data of the target modality can be\nprocessed by both models. In this way, we utilize the universal\nsequence-to-sequence modeling abilities of transformers obtained from two\nmodalities. As a concrete implementation, we use a modality-specific tokenizer\nand task-specific head as usual but utilize the transformer blocks of the\nauxiliary model via a proposed method named Cross-Modal Re-parameterization,\nwhich exploits the auxiliary weights without any inference costs. On the image,\npoint cloud, video, and audio recognition tasks, we observe significant and\nconsistent performance improvements with irrelevant data from other modalities.\nThe code and models are available at https://github.com/AILab-CVC/M2PT.\n","authors":["Yiyuan Zhang","Xiaohan Ding","Kaixiong Gong","Yixiao Ge","Ying Shan","Xiangyu Yue"],"pdf_url":"https://arxiv.org/pdf/2401.14405v2.pdf","comment":"CVPR 2024. Code and models are available at\n  https://github.com/AILab-CVC/M2PT"},{"id":"http://arxiv.org/abs/2403.04164v2","updated":"2024-03-18T08:40:48Z","published":"2024-03-07T02:48:42Z","title":"ProMISe: Promptable Medical Image Segmentation using SAM","summary":"  With the proposal of the Segment Anything Model (SAM), fine-tuning SAM for\nmedical image segmentation (MIS) has become popular. However, due to the large\nsize of the SAM model and the significant domain gap between natural and\nmedical images, fine-tuning-based strategies are costly with potential risk of\ninstability, feature damage and catastrophic forgetting. Furthermore, some\nmethods of transferring SAM to a domain-specific MIS through fine-tuning\nstrategies disable the model's prompting capability, severely limiting its\nutilization scenarios. In this paper, we propose an Auto-Prompting Module\n(APM), which provides SAM-based foundation model with Euclidean adaptive\nprompts in the target domain. Our experiments demonstrate that such adaptive\nprompts significantly improve SAM's non-fine-tuned performance in MIS. In\naddition, we propose a novel non-invasive method called Incremental Pattern\nShifting (IPS) to adapt SAM to specific medical domains. Experimental results\nshow that the IPS enables SAM to achieve state-of-the-art or competitive\nperformance in MIS without the need for fine-tuning. By coupling these two\nmethods, we propose ProMISe, an end-to-end non-fine-tuned framework for\nPromptable Medical Image Segmentation. Our experiments demonstrate that both\nusing our methods individually or in combination achieves satisfactory\nperformance in low-cost pattern shifting, with all of SAM's parameters frozen.\n","authors":["Jinfeng Wang","Sifan Song","Xinkun Wang","Yiyi Wang","Yiyi Miao","Jionglong Su","S. Kevin Zhou"],"pdf_url":"https://arxiv.org/pdf/2403.04164v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.15599v2","updated":"2024-03-18T08:37:24Z","published":"2023-11-27T07:48:50Z","title":"UniRepLKNet: A Universal Perception Large-Kernel ConvNet for Audio,\n  Video, Point Cloud, Time-Series and Image Recognition","summary":"  Large-kernel convolutional neural networks (ConvNets) have recently received\nextensive research attention, but two unresolved and critical issues demand\nfurther investigation. 1) The architectures of existing large-kernel ConvNets\nlargely follow the design principles of conventional ConvNets or transformers,\nwhile the architectural design for large-kernel ConvNets remains\nunder-addressed. 2) As transformers have dominated multiple modalities, it\nremains to be investigated whether ConvNets also have a strong universal\nperception ability in domains beyond vision. In this paper, we contribute from\ntwo aspects. 1) We propose four architectural guidelines for designing\nlarge-kernel ConvNets, the core of which is to exploit the essential\ncharacteristics of large kernels that distinguish them from small kernels -\nthey can see wide without going deep. Following such guidelines, our proposed\nlarge-kernel ConvNet shows leading performance in image recognition (ImageNet\naccuracy of 88.0%, ADE20K mIoU of 55.6%, and COCO box AP of 56.4%),\ndemonstrating better performance and higher speed than the recent powerful\ncompetitors. 2) We discover large kernels are the key to unlocking the\nexceptional performance of ConvNets in domains where they were originally not\nproficient. With certain modality-related preprocessing approaches, the\nproposed model achieves state-of-the-art performance on time-series forecasting\nand audio recognition tasks even without modality-specific customization to the\narchitecture. All the code and models are publicly available on GitHub and\nHuggingface.\n","authors":["Xiaohan Ding","Yiyuan Zhang","Yixiao Ge","Sijie Zhao","Lin Song","Xiangyu Yue","Ying Shan"],"pdf_url":"https://arxiv.org/pdf/2311.15599v2.pdf","comment":"CVPR 2024. Code, all the models, reproducible training scripts at\n  https://github.com/AILab-CVC/UniRepLKNet"},{"id":"http://arxiv.org/abs/2403.11558v1","updated":"2024-03-18T08:18:37Z","published":"2024-03-18T08:18:37Z","title":"Reinforcement Learning with Token-level Feedback for Controllable Text\n  Generation","summary":"  To meet the requirements of real-world applications, it is essential to\ncontrol generations of large language models (LLMs). Prior research has tried\nto introduce reinforcement learning (RL) into controllable text generation\nwhile most existing methods suffer from overfitting issues (finetuning-based\nmethods) or semantic collapse (post-processing methods). However, current RL\nmethods are generally guided by coarse-grained (sentence/paragraph-level)\nfeedback, which may lead to suboptimal performance owing to semantic twists or\nprogressions within sentences. To tackle that, we propose a novel reinforcement\nlearning algorithm named TOLE which formulates TOken-LEvel rewards for\ncontrollable text generation, and employs a \"first-quantize-then-noise\"\nparadigm to enhance the robustness of the RL algorithm.Furthermore, TOLE can be\nflexibly extended to multiple constraints with little computational expense.\nExperimental results show that our algorithm can achieve superior performance\non both single-attribute and multi-attribute control tasks. We have released\nour codes at https://github.com/WindyLee0822/CTG\n","authors":["Wendi Li","Wei Wei","Kaihe Xu","Wenfeng Xie","Dangyang Chen","Yu Cheng"],"pdf_url":"https://arxiv.org/pdf/2403.11558v1.pdf","comment":"Accepted to NAACL 2024 Findings"},{"id":"http://arxiv.org/abs/2403.11552v1","updated":"2024-03-18T08:03:47Z","published":"2024-03-18T08:03:47Z","title":"LLM^3:Large Language Model-based Task and Motion Planning with Motion\n  Failure Reasoning","summary":"  Conventional Task and Motion Planning (TAMP) approaches rely on manually\ncrafted interfaces connecting symbolic task planning with continuous motion\ngeneration. These domain-specific and labor-intensive modules are limited in\naddressing emerging tasks in real-world settings. Here, we present LLM^3, a\nnovel Large Language Model (LLM)-based TAMP framework featuring a\ndomain-independent interface. Specifically, we leverage the powerful reasoning\nand planning capabilities of pre-trained LLMs to propose symbolic action\nsequences and select continuous action parameters for motion planning.\nCrucially, LLM^3 incorporates motion planning feed- back through prompting,\nallowing the LLM to iteratively refine its proposals by reasoning about motion\nfailure. Consequently, LLM^3 interfaces between task planning and motion\nplanning, alleviating the intricate design process of handling domain- specific\nmessages between them. Through a series of simulations in a box-packing domain,\nwe quantitatively demonstrate the effectiveness of LLM^3 in solving TAMP\nproblems and the efficiency in selecting action parameters. Ablation studies\nun- derscore the significant contribution of motion failure reasoning to the\nsuccess of LLM^3. Furthermore, we conduct qualitative experiments on a physical\nmanipulator, demonstrating the practical applicability of our approach in\nreal-world settings.\n","authors":["Shu Wang","Muzhi Han","Ziyuan Jiao","Zeyu Zhang","Ying Nian Wu","Song-Chun Zhu","Hangxin Liu"],"pdf_url":"https://arxiv.org/pdf/2403.11552v1.pdf","comment":"Submitted to IROS 2024. Codes available:\n  https://github.com/AssassinWS/LLM-TAMP"},{"id":"http://arxiv.org/abs/2403.09629v2","updated":"2024-03-18T07:56:48Z","published":"2024-03-14T17:58:16Z","title":"Quiet-STaR: Language Models Can Teach Themselves to Think Before\n  Speaking","summary":"  When writing and talking, people sometimes pause to think. Although\nreasoning-focused works have often framed reasoning as a method of answering\nquestions or completing agentic tasks, reasoning is implicit in almost all\nwritten text. For example, this applies to the steps not stated between the\nlines of a proof or to the theory of mind underlying a conversation. In the\nSelf-Taught Reasoner (STaR, Zelikman et al. 2022), useful thinking is learned\nby inferring rationales from few-shot examples in question-answering and\nlearning from those that lead to a correct answer. This is a highly constrained\nsetting -- ideally, a language model could instead learn to infer unstated\nrationales in arbitrary text. We present Quiet-STaR, a generalization of STaR\nin which LMs learn to generate rationales at each token to explain future text,\nimproving their predictions. We address key challenges, including 1) the\ncomputational cost of generating continuations, 2) the fact that the LM does\nnot initially know how to generate or use internal thoughts, and 3) the need to\npredict beyond individual next tokens. To resolve these, we propose a tokenwise\nparallel sampling algorithm, using learnable tokens indicating a thought's\nstart and end, and an extended teacher-forcing technique. Encouragingly,\ngenerated rationales disproportionately help model difficult-to-predict tokens\nand improve the LM's ability to directly answer difficult questions. In\nparticular, after continued pretraining of an LM on a corpus of internet text\nwith Quiet-STaR, we find zero-shot improvements on GSM8K\n(5.9%$\\rightarrow$10.9%) and CommonsenseQA (36.3%$\\rightarrow$47.2%) and\nobserve a perplexity improvement of difficult tokens in natural text.\nCrucially, these improvements require no fine-tuning on these tasks. Quiet-STaR\nmarks a step towards LMs that can learn to reason in a more general and\nscalable way.\n","authors":["Eric Zelikman","Georges Harik","Yijia Shao","Varuna Jayasiri","Nick Haber","Noah D. Goodman"],"pdf_url":"https://arxiv.org/pdf/2403.09629v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.15487v3","updated":"2024-03-18T07:51:52Z","published":"2023-11-27T02:12:02Z","title":"Global $\\mathcal{L}^2$ minimization at uniform exponential rate via\n  geometrically adapted gradient descent in Deep Learning","summary":"  We consider the gradient descent flow widely used for the minimization of the\n$\\mathcal{L}^2$ cost function in Deep Learning networks, and introduce two\nmodified versions; one adapted for the overparametrized setting, and the other\nfor the underparametrized setting. Both have a clear and natural invariant\ngeometric meaning, taking into account the pullback vector bundle structure in\nthe overparametrized, and the pushforward vector bundle structure in the\nunderparametrized setting. In the overparametrized case, we prove that,\nprovided that a rank condition holds, all orbits of the modified gradient\ndescent drive the $\\mathcal{L}^2$ cost to its global minimum at a uniform\nexponential convergence rate; one thereby obtains an a priori stopping time for\nany prescribed proximity to the global minimum. We point out relations of the\nlatter to sub-Riemannian geometry.\n","authors":["Thomas Chen"],"pdf_url":"https://arxiv.org/pdf/2311.15487v3.pdf","comment":"AMS Latex, 16 pages. Section 2.1 on rank condition, and Section 2.4\n  on the trapping of orbits in the standard gradient descent flow added. Title\n  changed"},{"id":"http://arxiv.org/abs/2403.11536v1","updated":"2024-03-18T07:41:39Z","published":"2024-03-18T07:41:39Z","title":"OCR is All you need: Importing Multi-Modality into Image-based Defect\n  Detection System","summary":"  Automatic optical inspection (AOI) plays a pivotal role in the manufacturing\nprocess, predominantly leveraging high-resolution imaging instruments for\nscanning purposes. It detects anomalies by analyzing image textures or\npatterns, making it an essential tool in industrial manufacturing and quality\ncontrol. Despite its importance, the deployment of models for AOI often faces\nchallenges. These include limited sample sizes, which hinder effective feature\nlearning, variations among source domains, and sensitivities to changes in\nlighting and camera positions during imaging. These factors collectively\ncompromise the accuracy of model predictions. Traditional AOI often fails to\ncapitalize on the rich mechanism-parameter information from machines or inside\nimages, including statistical parameters, which typically benefit AOI\nclassification. To address this, we introduce an external modality-guided data\nmining framework, primarily rooted in optical character recognition (OCR), to\nextract statistical features from images as a second modality to enhance\nperformance, termed OANet (Ocr-Aoi-Net). A key aspect of our approach is the\nalignment of external modality features, extracted using a single\nmodality-aware model, with image features encoded by a convolutional neural\nnetwork. This synergy enables a more refined fusion of semantic representations\nfrom different modalities. We further introduce feature refinement and a gating\nfunction in our OANet to optimize the combination of these features, enhancing\ninference and decision-making capabilities. Experimental outcomes show that our\nmethodology considerably boosts the recall rate of the defect detection model\nand maintains high robustness even in challenging scenarios.\n","authors":["Chih-Chung Hsu","Chia-Ming Lee","Chun-Hung Sun","Kuang-Ming Wu"],"pdf_url":"https://arxiv.org/pdf/2403.11536v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08281v3","updated":"2024-03-18T07:21:28Z","published":"2024-03-13T06:18:48Z","title":"Mastering Text, Code and Math Simultaneously via Fusing Highly\n  Specialized Language Models","summary":"  Underlying data distributions of natural language, programming code, and\nmathematical symbols vary vastly, presenting a complex challenge for large\nlanguage models (LLMs) that strive to achieve high performance across all three\ndomains simultaneously. Achieving a very high level of proficiency for an LLM\nwithin a specific domain often requires extensive training with relevant\ncorpora, which is typically accompanied by a sacrifice in performance in other\ndomains. In this paper, we propose to fuse models that are already\nhighly-specialized directly. The proposed fusing framework, UltraFuser,\nconsists of three distinct specialists that are already sufficiently trained on\nlanguage, coding, and mathematics. A token-level gating mechanism is introduced\nto blend the specialists' outputs. A two-stage training strategy accompanied by\nbalanced sampling is designed to ensure stability. To effectively train the\nfused model, we further construct a high-quality supervised instruction tuning\ndataset, UltraChat 2, which includes text, code, and mathematical content. This\ndataset comprises approximately 300,000 instructions and covers a wide range of\ntopics in each domain. Experiments show that our model could simultaneously\nachieve mastery of the three crucial domains.\n","authors":["Ning Ding","Yulin Chen","Ganqu Cui","Xingtai Lv","Weilin Zhao","Ruobing Xie","Bowen Zhou","Zhiyuan Liu","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2403.08281v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.04306v2","updated":"2024-03-18T07:21:01Z","published":"2024-03-07T08:25:27Z","title":"Effectiveness Assessment of Recent Large Vision-Language Models","summary":"  The advent of large vision-language models (LVLMs) represents a noteworthy\nadvancement towards the pursuit of artificial general intelligence. However,\nthe extent of their efficacy across both specialized and general tasks warrants\nfurther investigation. This article endeavors to evaluate the competency of\npopular LVLMs in specialized and general tasks, respectively, aiming to offer a\ncomprehensive comprehension of these innovative methodologies. To gauge their\nefficacy in specialized tasks, we tailor a comprehensive testbed comprising\nthree distinct scenarios: natural, healthcare, and industrial, encompassing six\nchallenging tasks. These tasks include salient, camouflaged, and transparent\nobject detection, as well as polyp and skin lesion detection, alongside\nindustrial anomaly detection. We examine the performance of three recent\nopen-source LVLMs -- MiniGPT-v2, LLaVA-1.5, and Shikra -- in the realm of\nvisual recognition and localization. Moreover, we conduct empirical\ninvestigations utilizing the aforementioned models alongside GPT-4V, assessing\ntheir multi-modal understanding capacities in general tasks such as object\ncounting, absurd question answering, affordance reasoning, attribute\nrecognition, and spatial relation reasoning. Our investigations reveal that\nthese models demonstrate limited proficiency not only in specialized tasks but\nalso in general tasks. We delve deeper into this inadequacy and suggest several\npotential factors, including limited cognition in specialized tasks, object\nhallucination, text-to-image interference, and decreased robustness in complex\nproblems. We hope this study would provide valuable insights for the future\ndevelopment of LVLMs, augmenting their power in coping with both general and\nspecialized applications.\n","authors":["Yao Jiang","Xinyu Yan","Ge-Peng Ji","Keren Fu","Meijun Sun","Huan Xiong","Deng-Ping Fan","Fahad Shahbaz Khan"],"pdf_url":"https://arxiv.org/pdf/2403.04306v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.17500v2","updated":"2024-03-18T07:10:02Z","published":"2024-01-30T23:18:35Z","title":"LeTO: Learning Constrained Visuomotor Policy with Differentiable\n  Trajectory Optimization","summary":"  This paper introduces LeTO, a method for learning constrained visuomotor\npolicy via differentiable trajectory optimization. Our approach uniquely\nintegrates a differentiable optimization layer into the neural network. By\nformulating the optimization layer as a trajectory optimization problem, we\nenable the model to end-to-end generate actions in a safe and controlled\nfashion without extra modules. Our method allows for the introduction of\nconstraints information during the training process, thereby balancing the\ntraining objectives of satisfying constraints, smoothing the trajectories, and\nminimizing errors with demonstrations. This \"gray box\" method marries the\noptimization-based safety and interpretability with the powerful\nrepresentational abilities of neural networks. We quantitatively evaluate LeTO\nin simulation and on the real robot. In simulation, LeTO achieves a success\nrate comparable to state-of-the-art imitation learning methods, but the\ngenerated trajectories are of less uncertainty, higher quality, and smoother.\nIn real-world experiments, we deployed LeTO to handle constraints-critical\ntasks. The results show the effectiveness of LeTO comparing with\nstate-of-the-art imitation learning approaches. We release our code at\nhttps://github.com/ZhengtongXu/LeTO.\n","authors":["Zhengtong Xu","Yu She"],"pdf_url":"https://arxiv.org/pdf/2401.17500v2.pdf","comment":"8 pages, 5 figures"},{"id":"http://arxiv.org/abs/2403.11506v1","updated":"2024-03-18T06:24:46Z","published":"2024-03-18T06:24:46Z","title":"End-To-End Underwater Video Enhancement: Dataset and Model","summary":"  Underwater video enhancement (UVE) aims to improve the visibility and frame\nquality of underwater videos, which has significant implications for marine\nresearch and exploration. However, existing methods primarily focus on\ndeveloping image enhancement algorithms to enhance each frame independently.\nThere is a lack of supervised datasets and models specifically tailored for UVE\ntasks. To fill this gap, we construct the Synthetic Underwater Video\nEnhancement (SUVE) dataset, comprising 840 diverse underwater-style videos\npaired with ground-truth reference videos. Based on this dataset, we train a\nnovel underwater video enhancement model, UVENet, which utilizes inter-frame\nrelationships to achieve better enhancement performance. Through extensive\nexperiments on both synthetic and real underwater videos, we demonstrate the\neffectiveness of our approach. This study represents the first comprehensive\nexploration of UVE to our knowledge. The code is available at\nhttps://anonymous.4open.science/r/UVENet.\n","authors":["Dazhao Du","Enhan Li","Lingyu Si","Fanjiang Xu","Jianwei Niu"],"pdf_url":"https://arxiv.org/pdf/2403.11506v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11504v1","updated":"2024-03-18T06:19:37Z","published":"2024-03-18T06:19:37Z","title":"MLVICX: Multi-Level Variance-Covariance Exploration for Chest X-ray\n  Self-Supervised Representation Learning","summary":"  Self-supervised learning (SSL) is potentially useful in reducing the need for\nmanual annotation and making deep learning models accessible for medical image\nanalysis tasks. By leveraging the representations learned from unlabeled data,\nself-supervised models perform well on tasks that require little to no\nfine-tuning. However, for medical images, like chest X-rays, which are\ncharacterized by complex anatomical structures and diverse clinical conditions,\nthere arises a need for representation learning techniques that can encode\nfine-grained details while preserving the broader contextual information. In\nthis context, we introduce MLVICX (Multi-Level Variance-Covariance Exploration\nfor Chest X-ray Self-Supervised Representation Learning), an approach to\ncapture rich representations in the form of embeddings from chest X-ray images.\nCentral to our approach is a novel multi-level variance and covariance\nexploration strategy that empowers the model to detect diagnostically\nmeaningful patterns while reducing redundancy effectively. By enhancing the\nvariance and covariance of the learned embeddings, MLVICX promotes the\nretention of critical medical insights by adapting both global and local\ncontextual details. We demonstrate the performance of MLVICX in advancing\nself-supervised chest X-ray representation learning through comprehensive\nexperiments. The performance enhancements we observe across various downstream\ntasks highlight the significance of the proposed approach in enhancing the\nutility of chest X-ray embeddings for precision medical diagnosis and\ncomprehensive image analysis. For pertaining, we used the NIH-Chest X-ray\ndataset, while for downstream tasks, we utilized NIH-Chest X-ray, Vinbig-CXR,\nRSNA pneumonia, and SIIM-ACR Pneumothorax datasets. Overall, we observe more\nthan 3% performance gains over SOTA SSL approaches in various downstream tasks.\n","authors":["Azad Singh","Vandan Gorade","Deepak Mishra"],"pdf_url":"https://arxiv.org/pdf/2403.11504v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.09542v6","updated":"2024-03-18T06:06:48Z","published":"2022-07-19T20:44:42Z","title":"Controllable Data Generation by Deep Learning: A Review","summary":"  Designing and generating new data under targeted properties has been\nattracting various critical applications such as molecule design, image editing\nand speech synthesis. Traditional hand-crafted approaches heavily rely on\nexpertise experience and intensive human efforts, yet still suffer from the\ninsufficiency of scientific knowledge and low throughput to support effective\nand efficient data generation. Recently, the advancement of deep learning has\ncreated the opportunity for expressive methods to learn the underlying\nrepresentation and properties of data. Such capability provides new ways of\ndetermining the mutual relationship between the structural patterns and\nfunctional properties of the data and leveraging such relationships to generate\nstructural data, given the desired properties. This article is a systematic\nreview that explains this promising research area, commonly known as\ncontrollable deep data generation. First, the article raises the potential\nchallenges and provides preliminaries. Then the article formally defines\ncontrollable deep data generation, proposes a taxonomy on various techniques\nand summarizes the evaluation metrics in this specific domain. After that, the\narticle introduces exciting applications of controllable deep data generation,\nexperimentally analyzes and compares existing works. Finally, this article\nhighlights the promising future directions of controllable deep data generation\nand identifies five potential challenges.\n","authors":["Shiyu Wang","Yuanqi Du","Xiaojie Guo","Bo Pan","Zhaohui Qin","Liang Zhao"],"pdf_url":"https://arxiv.org/pdf/2207.09542v6.pdf","comment":"This survey has been accepted by ACM Computing Surveys"},{"id":"http://arxiv.org/abs/2403.11496v1","updated":"2024-03-18T06:00:38Z","published":"2024-03-18T06:00:38Z","title":"MCD: Diverse Large-Scale Multi-Campus Dataset for Robot Perception","summary":"  Perception plays a crucial role in various robot applications. However,\nexisting well-annotated datasets are biased towards autonomous driving\nscenarios, while unlabelled SLAM datasets are quickly over-fitted, and often\nlack environment and domain variations. To expand the frontier of these fields,\nwe introduce a comprehensive dataset named MCD (Multi-Campus Dataset),\nfeaturing a wide range of sensing modalities, high-accuracy ground truth, and\ndiverse challenging environments across three Eurasian university campuses. MCD\ncomprises both CCS (Classical Cylindrical Spinning) and NRE (Non-Repetitive\nEpicyclic) lidars, high-quality IMUs (Inertial Measurement Units), cameras, and\nUWB (Ultra-WideBand) sensors. Furthermore, in a pioneering effort, we introduce\nsemantic annotations of 29 classes over 59k sparse NRE lidar scans across three\ndomains, thus providing a novel challenge to existing semantic segmentation\nresearch upon this largely unexplored lidar modality. Finally, we propose, for\nthe first time to the best of our knowledge, continuous-time ground truth based\non optimization-based registration of lidar-inertial data on large survey-grade\nprior maps, which are also publicly released, each several times the size of\nexisting ones. We conduct a rigorous evaluation of numerous state-of-the-art\nalgorithms on MCD, report their performance, and highlight the challenges\nawaiting solutions from the research community.\n","authors":["Thien-Minh Nguyen","Shenghai Yuan","Thien Hoang Nguyen","Pengyu Yin","Haozhi Cao","Lihua Xie","Maciej Wozniak","Patric Jensfelt","Marko Thiel","Justin Ziegenbein","Noel Blunder"],"pdf_url":"https://arxiv.org/pdf/2403.11496v1.pdf","comment":"Accepted by The IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition 2024"},{"id":"http://arxiv.org/abs/2403.11495v1","updated":"2024-03-18T05:59:56Z","published":"2024-03-18T05:59:56Z","title":"Semantic-Enhanced Representation Learning for Road Networks with\n  Temporal Dynamics","summary":"  In this study, we introduce a novel framework called Toast for learning\ngeneral-purpose representations of road networks, along with its advanced\ncounterpart DyToast, designed to enhance the integration of temporal dynamics\nto boost the performance of various time-sensitive downstream tasks.\nSpecifically, we propose to encode two pivotal semantic characteristics\nintrinsic to road networks: traffic patterns and traveling semantics. To\nachieve this, we refine the skip-gram module by incorporating auxiliary\nobjectives aimed at predicting the traffic context associated with a target\nroad segment. Moreover, we leverage trajectory data and design pre-training\nstrategies based on Transformer to distill traveling semantics on road\nnetworks. DyToast further augments this framework by employing unified\ntrigonometric functions characterized by their beneficial properties, enabling\nthe capture of temporal evolution and dynamic nature of road networks more\neffectively. With these proposed techniques, we can obtain representations that\nencode multi-faceted aspects of knowledge within road networks, applicable\nacross both road segment-based applications and trajectory-based applications.\nExtensive experiments on two real-world datasets across three tasks demonstrate\nthat our proposed framework consistently outperforms the state-of-the-art\nbaselines by a significant margin.\n","authors":["Yile Chen","Xiucheng Li","Gao Cong","Zhifeng Bao","Cheng Long"],"pdf_url":"https://arxiv.org/pdf/2403.11495v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.04578v3","updated":"2024-03-18T05:56:42Z","published":"2024-02-07T04:36:31Z","title":"S-Agents: Self-organizing Agents in Open-ended Environments","summary":"  Leveraging large language models (LLMs), autonomous agents have significantly\nimproved, gaining the ability to handle a variety of tasks. In open-ended\nsettings, optimizing collaboration for efficiency and effectiveness demands\nflexible adjustments. Despite this, current research mainly emphasizes fixed,\ntask-oriented workflows and overlooks agent-centric organizational structures.\nDrawing inspiration from human organizational behavior, we introduce a\nself-organizing agent system (S-Agents) with a \"tree of agents\" structure for\ndynamic workflow, an \"hourglass agent architecture\" for balancing information\npriorities, and a \"non-obstructive collaboration\" method to allow asynchronous\ntask execution among agents. This structure can autonomously coordinate a group\nof agents, efficiently addressing the challenges of open and dynamic\nenvironments without human intervention. Our experiments demonstrate that\nS-Agents proficiently execute collaborative building tasks and resource\ncollection in the Minecraft environment, validating their effectiveness.\n","authors":["Jiaqi Chen","Yuxian Jiang","Jiachen Lu","Li Zhang"],"pdf_url":"https://arxiv.org/pdf/2402.04578v3.pdf","comment":"Preview, 15 pages, 12 figure"},{"id":"http://arxiv.org/abs/2403.11492v1","updated":"2024-03-18T05:53:20Z","published":"2024-03-18T05:53:20Z","title":"SmartRefine: An Scenario-Adaptive Refinement Framework for Efficient\n  Motion Prediction","summary":"  Predicting the future motion of surrounding agents is essential for\nautonomous vehicles (AVs) to operate safely in dynamic, human-robot-mixed\nenvironments. Context information, such as road maps and surrounding agents'\nstates, provides crucial geometric and semantic information for motion behavior\nprediction. To this end, recent works explore two-stage prediction frameworks\nwhere coarse trajectories are first proposed, and then used to select critical\ncontext information for trajectory refinement. However, they either incur a\nlarge amount of computation or bring limited improvement, if not both. In this\npaper, we introduce a novel scenario-adaptive refinement strategy, named\nSmartRefine, to refine prediction with minimal additional computation.\nSpecifically, SmartRefine can comprehensively adapt refinement configurations\nbased on each scenario's properties, and smartly chooses the number of\nrefinement iterations by introducing a quality score to measure the prediction\nquality and remaining refinement potential of each scenario. SmartRefine is\ndesigned as a generic and flexible approach that can be seamlessly integrated\ninto most state-of-the-art motion prediction models. Experiments on Argoverse\n(1 & 2) show that our method consistently improves the prediction accuracy of\nmultiple state-of-the-art prediction models. Specifically, by adding\nSmartRefine to QCNet, we outperform all published ensemble-free works on the\nArgoverse 2 leaderboard (single agent track) at submission. Comprehensive\nstudies are also conducted to ablate design choices and explore the mechanism\nbehind multi-iteration refinement. Codes are available at\nhttps://github.com/opendilab/SmartRefine/\n","authors":["Yang Zhou","Hao Shao","Letian Wang","Steven L. Waslander","Hongsheng Li","Yu Liu"],"pdf_url":"https://arxiv.org/pdf/2403.11492v1.pdf","comment":"Camera-ready version for CVPR 2024"},{"id":"http://arxiv.org/abs/2403.11487v1","updated":"2024-03-18T05:38:07Z","published":"2024-03-18T05:38:07Z","title":"Can LLMs Generate Human-Like Wayfinding Instructions? Towards\n  Platform-Agnostic Embodied Instruction Synthesis","summary":"  We present a novel approach to automatically synthesize \"wayfinding\ninstructions\" for an embodied robot agent. In contrast to prior approaches that\nare heavily reliant on human-annotated datasets designed exclusively for\nspecific simulation platforms, our algorithm uses in-context learning to\ncondition an LLM to generate instructions using just a few references. Using an\nLLM-based Visual Question Answering strategy, we gather detailed information\nabout the environment which is used by the LLM for instruction synthesis. We\nimplement our approach on multiple simulation platforms including Matterport3D,\nAI Habitat and ThreeDWorld, thereby demonstrating its platform-agnostic nature.\nWe subjectively evaluate our approach via a user study and observe that 83.3%\nof users find the synthesized instructions accurately capture the details of\nthe environment and show characteristics similar to those of human-generated\ninstructions. Further, we conduct zero-shot navigation with multiple approaches\non the REVERIE dataset using the generated instructions, and observe very close\ncorrelation with the baseline on standard success metrics (< 1% change in SR),\nquantifying the viability of generated instructions in replacing\nhuman-annotated data. To the best of our knowledge, ours is the first\nLLM-driven approach capable of generating \"human-like\" instructions in a\nplatform-agnostic manner, without requiring any form of training.\n","authors":["Vishnu Sashank Dorbala","Sanjoy Chowdhury","Dinesh Manocha"],"pdf_url":"https://arxiv.org/pdf/2403.11487v1.pdf","comment":"13 Pages"},{"id":"http://arxiv.org/abs/2403.11483v1","updated":"2024-03-18T05:12:54Z","published":"2024-03-18T05:12:54Z","title":"Open-World Semi-Supervised Learning for Node Classification","summary":"  Open-world semi-supervised learning (Open-world SSL) for node classification,\nthat classifies unlabeled nodes into seen classes or multiple novel classes, is\na practical but under-explored problem in the graph community. As only seen\nclasses have human labels, they are usually better learned than novel classes,\nand thus exhibit smaller intra-class variances within the embedding space\n(named as imbalance of intra-class variances between seen and novel classes).\nBased on empirical and theoretical analysis, we find the variance imbalance can\nnegatively impact the model performance. Pre-trained feature encoders can\nalleviate this issue via producing compact representations for novel classes.\nHowever, creating general pre-trained encoders for various types of graph data\nhas been proven to be challenging. As such, there is a demand for an effective\nmethod that does not rely on pre-trained graph encoders. In this paper, we\npropose an IMbalance-Aware method named OpenIMA for Open-world semi-supervised\nnode classification, which trains the node classification model from scratch\nvia contrastive learning with bias-reduced pseudo labels. Extensive experiments\non seven popular graph benchmarks demonstrate the effectiveness of OpenIMA, and\nthe source code has been available on GitHub.\n","authors":["Yanling Wang","Jing Zhang","Lingxi Zhang","Lixin Liu","Yuxiao Dong","Cuiping Li","Hong Chen","Hongzhi Yin"],"pdf_url":"https://arxiv.org/pdf/2403.11483v1.pdf","comment":"Accepted by ICDE 2024"},{"id":"http://arxiv.org/abs/2403.03835v2","updated":"2024-03-18T05:09:15Z","published":"2024-03-06T16:26:40Z","title":"Cobweb: An Incremental and Hierarchical Model of Human-Like Category\n  Learning","summary":"  Cobweb, a human like category learning system, differs from other incremental\ncategorization models in constructing hierarchically organized cognitive\ntree-like structures using the category utility measure. Prior studies have\nshown that Cobweb can capture psychological effects such as the basic level,\ntypicality, and fan effects. However, a broader evaluation of Cobweb as a model\nof human categorization remains lacking. The current study addresses this gap.\nIt establishes Cobweb's alignment with classical human category learning\neffects. It also explores Cobweb's flexibility to exhibit both exemplar and\nprototype like learning within a single model. These findings set the stage for\nfuture research on Cobweb as a comprehensive model of human category learning.\n","authors":["Xin Lian","Sashank Varma","Christopher J. MacLellan"],"pdf_url":"https://arxiv.org/pdf/2403.03835v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11473v1","updated":"2024-03-18T04:45:44Z","published":"2024-03-18T04:45:44Z","title":"Word Order's Impacts: Insights from Reordering and Generation Analysis","summary":"  Existing works have studied the impacts of the order of words within natural\ntext. They usually analyze it by destroying the original order of words to\ncreate a scrambled sequence, and then comparing the models' performance between\nthe original and scrambled sequences. The experimental results demonstrate\nmarginal drops. Considering this findings, different hypothesis about word\norder is proposed, including ``the order of words is redundant with lexical\nsemantics'', and ``models do not rely on word order''. In this paper, we\nrevisit the aforementioned hypotheses by adding a order reconstruction\nperspective, and selecting datasets of different spectrum. Specifically, we\nfirst select four different datasets, and then design order reconstruction and\ncontinuing generation tasks. Empirical findings support that ChatGPT relies on\nword order to infer, but cannot support or negate the redundancy relations\nbetween word order lexical semantics.\n","authors":["Qinghua Zhao","Jiaang Li","Lei Li","Zenghui Zhou","Junfeng Liu"],"pdf_url":"https://arxiv.org/pdf/2403.11473v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11468v1","updated":"2024-03-18T04:41:38Z","published":"2024-03-18T04:41:38Z","title":"Collage Prompting: Budget-Friendly Visual Recognition with GPT-4V","summary":"  Recent advancements in generative AI have suggested that by taking visual\nprompt, GPT-4V can demonstrate significant proficiency in image recognition\ntask. Despite its impressive capabilities, the financial cost associated with\nGPT-4V's inference presents a substantial barrier for its wide use. To address\nthis challenge, our work introduces Collage Prompting, a budget-friendly\nprompting approach that concatenates multiple images into a single visual\ninput. With collage prompt, GPT-4V is able to perform image recognition on\nseveral images simultaneously. Based on the observation that the accuracy of\nGPT-4V's image recognition varies significantly with the order of images within\nthe collage prompt, our method further learns to optimize the arrangement of\nimages for maximum recognition accuracy. A graph predictor is trained to\nindicate the accuracy of each collage prompt, then we propose an optimization\nmethod to navigate the search space of possible image arrangements. Experiment\nresults across various datasets demonstrate the cost-efficiency score of\ncollage prompt is much larger than standard prompt. Additionally, collage\nprompt with learned arrangement achieves clearly better accuracy than collage\nprompt with random arrangement in GPT-4V's visual recognition.\n","authors":["Siyu Xu","Yunke Wang","Daochang Liu","Chang Xu"],"pdf_url":"https://arxiv.org/pdf/2403.11468v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.10228v3","updated":"2024-03-18T04:22:17Z","published":"2024-02-05T07:07:30Z","title":"HyperAgent: A Simple, Scalable, Efficient and Provable Reinforcement\n  Learning Framework for Complex Environments","summary":"  To solve complex tasks under resource constraints, reinforcement learning\n(RL) agents need to be simple, efficient, and scalable, addressing (1) large\nstate spaces and (2) the continuous accumulation of interaction data. We\npropose HyperAgent, an RL framework featuring the hypermodel and index sampling\nschemes that enable computation-efficient incremental approximation for the\nposteriors associated with general value functions without the need for\nconjugacy, and data-efficient action selection. Implementing HyperAgent is\nstraightforward, requiring only one additional module beyond what is necessary\nfor Double-DQN. HyperAgent stands out as the first method to offer robust\nperformance in large-scale deep RL benchmarks while achieving provably scalable\nper-step computational complexity and attaining sublinear regret under tabular\nassumptions. HyperAgent can solve Deep Sea hard exploration problems with\nepisodes that optimally scale with problem size and exhibits significant\nefficiency gains in both data and computation under the Atari benchmark. The\ncore of our theoretical analysis is the sequential posterior approximation\nargument, enabled by the first analytical tool for sequential random projection\n-- a non-trivial martingale extension of the Johnson-Lindenstrauss. This work\nbridges the theoretical and practical realms of RL, establishing a new\nbenchmark for RL algorithm design.\n","authors":["Yingru Li","Jiawei Xu","Lei Han","Zhi-Quan Luo"],"pdf_url":"https://arxiv.org/pdf/2402.10228v3.pdf","comment":"Bridging the theory and practice! Invited talk in Informs\n  Optimization Conference 2024 and International Symposium on Mathematical\n  Programming 2024!"},{"id":"http://arxiv.org/abs/2306.10900v2","updated":"2024-03-18T04:14:50Z","published":"2023-06-19T12:58:17Z","title":"MotionGPT: Finetuned LLMs Are General-Purpose Motion Generators","summary":"  Generating realistic human motion from given action descriptions has\nexperienced significant advancements because of the emerging requirement of\ndigital humans. While recent works have achieved impressive results in\ngenerating motion directly from textual action descriptions, they often support\nonly a single modality of the control signal, which limits their application in\nthe real digital human industry. This paper presents a Motion General-Purpose\ngeneraTor (MotionGPT) that can use multimodal control signals, e.g., text and\nsingle-frame poses, for generating consecutive human motions by treating\nmultimodal signals as special input tokens in large language models (LLMs).\nSpecifically, we first quantize multimodal control signals into discrete codes\nand then formulate them in a unified prompt instruction to ask the LLMs to\ngenerate the motion answer. Our MotionGPT demonstrates a unified human motion\ngeneration model with multimodal control signals by tuning a mere 0.4% of LLM\nparameters. To the best of our knowledge, MotionGPT is the first method to\ngenerate human motion by multimodal control signals, which we hope can shed\nlight on this new direction. Visit our webpage at\nhttps://qiqiapink.github.io/MotionGPT/.\n","authors":["Yaqi Zhang","Di Huang","Bin Liu","Shixiang Tang","Yan Lu","Lu Chen","Lei Bai","Qi Chu","Nenghai Yu","Wanli Ouyang"],"pdf_url":"https://arxiv.org/pdf/2306.10900v2.pdf","comment":"18 pages, 8 figures, accepted by AAAI 2024"},{"id":"http://arxiv.org/abs/2403.11456v1","updated":"2024-03-18T04:12:35Z","published":"2024-03-18T04:12:35Z","title":"HateCOT: An Explanation-Enhanced Dataset for Generalizable Offensive\n  Speech Detection via Large Language Models","summary":"  The ubiquitousness of social media has led to the need for reliable and\nefficient detection of offensive content to limit harmful effects. This has led\nto a proliferation of datasets and models related to detecting offensive\ncontent. While sophisticated models have attained strong performance on\nindividual datasets, these models often do not generalize due to differences\nbetween how \"offensive content\" is conceptualized, and the resulting\ndifferences in how these datasets are labeled. In this paper, we introduce\nHateCOT, a dataset of 52,000 samples drawn from diverse existing sources with\nexplanations generated by GPT-3.5-Turbo and human-curated. We show that\npre-training models for the detection of offensive content on HateCOT\nsignificantly boots open-sourced Language Models on three benchmark datasets in\nboth zero and few-shot settings, despite differences in domain and task.} We\nfurther find that HateCOT enables effective K-shot fine-tuning in the\nlow-resource settings.\n","authors":["Huy Nghiem","Hal Daumé III"],"pdf_url":"https://arxiv.org/pdf/2403.11456v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2311.10319v3","updated":"2024-03-18T04:01:53Z","published":"2023-11-17T04:04:29Z","title":"Shifting to Machine Supervision: Annotation-Efficient Semi and\n  Self-Supervised Learning for Automatic Medical Image Segmentation and\n  Classification","summary":"  Advancements in clinical treatment are increasingly constrained by the\nlimitations of supervised learning techniques, which depend heavily on large\nvolumes of annotated data. The annotation process is not only costly but also\ndemands substantial time from clinical specialists. Addressing this issue, we\nintroduce the S4MI (Self-Supervision and Semi-Supervision for Medical Imaging)\npipeline, a novel approach that leverages the advancements in self-supervised\nand semi-supervised learning. These techniques engage in auxiliary tasks that\ndo not require labeling, thus simplifying the scaling of machine supervision\ncompared to fully-supervised methods. Our study benchmarks these techniques on\nthree distinct medical imaging datasets to evaluate their effectiveness in\nclassification and segmentation tasks. Notably, we observed that\nself-supervised learning significantly surpassed the performance of supervised\nmethods in the classification of all evaluated datasets. Remarkably, the\nsemi-supervised approach demonstrated superior outcomes in segmentation,\noutperforming fully-supervised methods while using 50% fewer labels across all\ndatasets. In line with our commitment to contributing to the scientific\ncommunity, we have made the S4MI code openly accessible, allowing for broader\napplication and further development of these methods.\n","authors":["Pranav Singh","Raviteja Chukkapalli","Shravan Chaudhari","Luoyao Chen","Mei Chen","Jinqian Pan","Craig Smuda","Jacopo Cirrone"],"pdf_url":"https://arxiv.org/pdf/2311.10319v3.pdf","comment":"Seventeen pages (incl. references), five figures, and one table.\n  (Under Review)"},{"id":"http://arxiv.org/abs/2305.11490v5","updated":"2024-03-18T03:41:09Z","published":"2023-05-19T07:44:39Z","title":"LLM-CXR: Instruction-Finetuned LLM for CXR Image Understanding and\n  Generation","summary":"  Following the impressive development of LLMs, vision-language alignment in\nLLMs is actively being researched to enable multimodal reasoning and visual IO.\nThis direction of research is particularly relevant to medical imaging because\nmedical image analysis and generation consist of reasoning based on a\ncombination of visual features and prior knowledge. Many recent works have\nfocused on training adapter networks that serve as an information bridge\nbetween image processing networks and LLMs; but presumably, in order to achieve\nmaximum reasoning potential of LLMs on visual information as well, visual and\nlanguage features should be allowed to interact more freely. This is especially\nimportant in the medical domain because understanding and generating medical\nimages such as chest X-rays (CXR) require not only accurate visual and\nlanguage-based reasoning but also a more intimate mapping between the two\nmodalities. Thus, taking inspiration from previous work on the transformer and\nVQ-GAN combination for bidirectional image and text generation, we build upon\nthis approach and develop a method for instruction-tuning an LLM pre-trained\nonly on text to gain vision-language capabilities for medical images.\nSpecifically, we leverage a pretrained LLM's existing question-answering and\ninstruction-following abilities to teach it to understand visual inputs by\ninstructing it to answer questions about image inputs and, symmetrically,\noutput both text and image responses appropriate to a given query by tuning the\nLLM with diverse tasks that encompass image-based text-generation and\ntext-based image-generation. We show that our model, LLM-CXR, trained in this\napproach shows better image-text alignment in both CXR understanding and\ngeneration tasks while being smaller in size compared to previously developed\nmodels that perform a narrower range of tasks. The code is at\nhttps://github.com/hyn2028/llm-cxr.\n","authors":["Suhyeon Lee","Won Jun Kim","Jinho Chang","Jong Chul Ye"],"pdf_url":"https://arxiv.org/pdf/2305.11490v5.pdf","comment":"21 pages, 8 figures; ICLR 2024 (poster)"},{"id":"http://arxiv.org/abs/2310.14402v2","updated":"2024-03-18T03:35:28Z","published":"2023-10-22T20:25:08Z","title":"Value of Assistance for Grasping","summary":"  In multiple realistic settings, a robot is tasked with grasping an object\nwithout knowing its exact pose and relies on a probabilistic estimation of the\npose to decide how to attempt the grasp. We support settings in which it is\npossible to provide the robot with an observation of the object before a grasp\nis attempted but this possibility is limited and there is a need to decide\nwhich sensing action would be most beneficial. We support this decision by\noffering a novel Value of Assistance (VOA) measure for assessing the expected\neffect a specific observation will have on the robot's ability to complete its\ntask. We evaluate our suggested measure in simulated and real-world\ncollaborative grasping settings.\n","authors":["Mohammad Masarwy","Yuval Goshen","David Dovrat","Sarah Keren"],"pdf_url":"https://arxiv.org/pdf/2310.14402v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05155v2","updated":"2024-03-18T03:19:33Z","published":"2023-10-08T13:07:42Z","title":"Toolink: Linking Toolkit Creation and Using through Chain-of-Solving on\n  Open-Source Model","summary":"  Large Language Models (LLMs) have demonstrated remarkable progress in\nutilizing tools, but their closed-source nature and high inference costs pose\nlimitations on their adaptability, necessitating a valid method that leverages\nsmaller, open-sourced models. In this paper, we introduce Toolink, a\ncomprehensive framework that performs task-solving by first creating a toolkit\nand then integrating the planning and calling of tools through a\nchain-of-solving (CoS) approach. We first validate the efficacy of Toolink in\nharnessing the model's creativity and CoS ability on ChatGPT. Subsequently, we\ncurate CoS-GPT, a chain-of-solving dataset designed for tool-using, and\nfinetune the LLaMA-7B model. It results in LLaMA-CoS, a powerful open-source\nmodel with advanced tool-planning and tool-calling capabilities. Evaluation of\ndiverse tasks from BIG-bench demonstrates its CoS ability matches that of\nChatGPT while its performance surpasses the chain-of-thought approach. Further\nstudies highlight the generalization of LLaMA-CoS to unseen tasks and showcase\nits capability in using toolkits not explicitly tailored for the target task,\naffirming its robustness in real-world scenarios.\n","authors":["Cheng Qian","Chenyan Xiong","Zhenghao Liu","Zhiyuan Liu"],"pdf_url":"https://arxiv.org/pdf/2310.05155v2.pdf","comment":"NAACL 2024 Main"},{"id":"http://arxiv.org/abs/2403.11432v1","updated":"2024-03-18T02:59:13Z","published":"2024-03-18T02:59:13Z","title":"Demystifying Deep Reinforcement Learning-Based Autonomous Vehicle\n  Decision-Making","summary":"  With the advent of universal function approximators in the domain of\nreinforcement learning, the number of practical applications leveraging deep\nreinforcement learning (DRL) has exploded. Decision-making in automated driving\ntasks has emerged as a chief application among them, taking the sensor data or\nthe higher-order kinematic variables as the input and providing a discrete\nchoice or continuous control output. However, the black-box nature of the\nmodels presents an overwhelming limitation that restricts the real-world\ndeployment of DRL in autonomous vehicles (AVs). Therefore, in this research\nwork, we focus on the interpretability of an attention-based DRL framework. We\nuse a continuous proximal policy optimization-based DRL algorithm as the\nbaseline model and add a multi-head attention framework in an open-source AV\nsimulation environment. We provide some analytical techniques for discussing\nthe interpretability of the trained models in terms of explainability and\ncausality for spatial and temporal correlations. We show that the weights in\nthe first head encode the positions of the neighboring vehicles while the\nsecond head focuses on the leader vehicle exclusively. Also, the ego vehicle's\naction is causally dependent on the vehicles in the target lane spatially and\ntemporally. Through these findings, we reliably show that these techniques can\nhelp practitioners decipher the results of the DRL algorithms.\n","authors":["Hanxi Wan","Pei Li","Arpan Kusari"],"pdf_url":"https://arxiv.org/pdf/2403.11432v1.pdf","comment":"Submitted for peer-review"},{"id":"http://arxiv.org/abs/2403.11420v1","updated":"2024-03-18T02:20:22Z","published":"2024-03-18T02:20:22Z","title":"Neural network representation of quantum systems","summary":"  It has been proposed that random wide neural networks near Gaussian process\nare quantum field theories around Gaussian fixed points. In this paper, we\nprovide a novel map with which a wide class of quantum mechanical systems can\nbe cast into the form of a neural network with a statistical summation over\nnetwork parameters. Our simple idea is to use the universal approximation\ntheorem of neural networks to generate arbitrary paths in the Feynman's path\nintegral. The map can be applied to interacting quantum systems / field\ntheories, even away from the Gaussian limit. Our findings bring machine\nlearning closer to the quantum world.\n","authors":["Koji Hashimoto","Yuji Hirono","Jun Maeda","Jojiro Totsuka-Yoshinaka"],"pdf_url":"https://arxiv.org/pdf/2403.11420v1.pdf","comment":"24 pages, 6 figures"},{"id":"http://arxiv.org/abs/2310.17513v3","updated":"2024-03-18T02:13:24Z","published":"2023-10-26T16:08:33Z","title":"The Expressive Power of Low-Rank Adaptation","summary":"  Low-Rank Adaptation (LoRA), a parameter-efficient fine-tuning method that\nleverages low-rank adaptation of weight matrices, has emerged as a prevalent\ntechnique for fine-tuning pre-trained models such as large language models and\ndiffusion models. Despite its huge success in practice, the theoretical\nunderpinnings of LoRA have largely remained unexplored. This paper takes the\nfirst step to bridge this gap by theoretically analyzing the expressive power\nof LoRA. We prove that, for fully connected neural networks, LoRA can adapt any\nmodel $f$ to accurately represent any smaller target model $\\overline{f}$ if\nLoRA-rank $\\geq(\\text{width of }f) \\times \\frac{\\text{depth of\n}\\overline{f}}{\\text{depth of }f}$. We also quantify the approximation error\nwhen LoRA-rank is lower than the threshold. For Transformer networks, we show\nany model can be adapted to a target model of the same size with\nrank-$(\\frac{\\text{embedding size}}{2})$ LoRA adapters.\n","authors":["Yuchen Zeng","Kangwook Lee"],"pdf_url":"https://arxiv.org/pdf/2310.17513v3.pdf","comment":"40 pages, 5 figures"}],"Databases":[{"id":"http://arxiv.org/abs/2403.11716v1","updated":"2024-03-18T12:18:51Z","published":"2024-03-18T12:18:51Z","title":"Models for Storage in Database Backends","summary":"  This paper describes ongoing work on developing a formal specification of a\ndatabase backend. We present the formalisation of the expected behaviour of a\nbasic transactional system that calls into a simple store API, and instantiate\nin two semantic models. The first one is a map-based, classical versioned\nkey-value store; the second one, journal-based, appends individual transaction\neffects to a journal. We formalise a significant part of the specification in\nthe Coq proof assistant. This work will form the basis for a formalisation of a\nfull-fledged backend store with features such as caching or write-ahead\nlogging, as variations on maps and journals.\n","authors":["Edgard Schiebelbein","Saalik Hatia","Annette Bieniusa","Gustavo Petri","Carla Ferreira","Marc Shapiro"],"pdf_url":"https://arxiv.org/pdf/2403.11716v1.pdf","comment":"Workshop on Principles and Practice of Consistency for Distributed\n  Data (PaPoC), EuroSys (ACM), Apr 2024, Ath{\\`e}nes, Greece"},{"id":"http://arxiv.org/abs/2403.07331v2","updated":"2024-03-18T09:55:30Z","published":"2024-03-12T05:32:33Z","title":"LIST: Learning to Index Spatio-Textual Data for Embedding based Spatial\n  Keyword Queries","summary":"  With the proliferation of spatio-textual data, Top-k KNN spatial keyword\nqueries (TkQs), which return a list of objects based on a ranking function that\nevaluates both spatial and textual relevance, have found many real-life\napplications. Existing geo-textual indexes for TkQs use traditional retrieval\nmodels like BM25 to compute text relevance and usually exploit a simple linear\nfunction to compute spatial relevance, but its effectiveness is limited. To\nimprove effectiveness, several deep learning models have recently been\nproposed, but they suffer severe efficiency issues. To the best of our\nknowledge, there are no efficient indexes specifically designed to accelerate\nthe top-k search process for these deep learning models.\n  To tackle these issues, we propose a novel technique, which Learns to Index\nthe Spatio-Textual data for answering embedding based spatial keyword queries\n(called LIST). LIST is featured with two novel components. Firstly, we propose\na lightweight and effective relevance model that is capable of learning both\ntextual and spatial relevance. Secondly, we introduce a novel machine learning\nbased Approximate Nearest Neighbor Search (ANNS) index, which utilizes a new\nlearning-to-cluster technique to group relevant queries and objects together\nwhile separating irrelevant queries and objects. Two key challenges in building\nan effective and efficient index are the absence of high-quality labels and\nunbalanced clustering results. We develop a novel pseudo-label generation\ntechnique to address the two challenges. Experimental results show that LIST\nsignificantly outperforms state-of-the-art methods on effectiveness, with\nimprovements up to 19.21% and 12.79% in terms of NDCG@1 and Recall@10, and is\nthree orders of magnitude faster than the most effective baseline.\n","authors":["Ziqi Yin","Shanshan Feng","Shang Liu","Gao Cong","Yew Soon Ong","Bin Cui"],"pdf_url":"https://arxiv.org/pdf/2403.07331v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11472v1","updated":"2024-03-18T04:44:00Z","published":"2024-03-18T04:44:00Z","title":"Accelerating String-Key Learned Index Structures via Memoization-based\n  Incremental Training","summary":"  Learned indexes use machine learning models to learn the mappings between\nkeys and their corresponding positions in key-value indexes. These indexes use\nthe mapping information as training data. Learned indexes require frequent\nretrainings of their models to incorporate the changes introduced by update\nqueries. To efficiently retrain the models, existing learned index systems\noften harness a linear algebraic QR factorization technique that performs\nmatrix decomposition. This factorization approach processes all key-position\npairs during each retraining, resulting in compute operations that grow\nlinearly with the total number of keys and their lengths. Consequently, the\nretrainings create a severe performance bottleneck, especially for\nvariable-length string keys, while the retrainings are crucial for maintaining\nhigh prediction accuracy and in turn, ensuring low query service latency.\n  To address this performance problem, we develop an algorithm-hardware\nco-designed string-key learned index system, dubbed SIA. In designing SIA, we\nleverage a unique algorithmic property of the matrix decomposition-based\ntraining method. Exploiting the property, we develop a memoization-based\nincremental training scheme, which only requires computation over updated keys,\nwhile decomposition results of non-updated keys from previous computations can\nbe reused. We further enhance SIA to offload a portion of this training process\nto an FPGA accelerator to not only relieve CPU resources for serving index\nqueries (i.e., inference), but also accelerate the training itself. Our\nevaluation shows that compared to ALEX, LIPP, and SIndex, a state-of-the-art\nlearned index systems, SIA-accelerated learned indexes offer 2.6x and 3.4x\nhigher throughput on the two real-world benchmark suites, YCSB and Twitter\ncache trace, respectively.\n","authors":["Minsu Kim","Jinwoo Hwang","Guseul Heo","Seiyeon Cho","Divya Mahajan","Jongse Park"],"pdf_url":"https://arxiv.org/pdf/2403.11472v1.pdf","comment":"Accepted at VLDB '24; 12 pages + 2 pages (ref), 18 figures, 2 tables"},{"id":"http://arxiv.org/abs/2403.11395v1","updated":"2024-03-18T01:07:48Z","published":"2024-03-18T01:07:48Z","title":"Automated data processing and feature engineering for deep learning and\n  big data applications: a survey","summary":"  Modern approach to artificial intelligence (AI) aims to design algorithms\nthat learn directly from data. This approach has achieved impressive results\nand has contributed significantly to the progress of AI, particularly in the\nsphere of supervised deep learning. It has also simplified the design of\nmachine learning systems as the learning process is highly automated. However,\nnot all data processing tasks in conventional deep learning pipelines have been\nautomated. In most cases data has to be manually collected, preprocessed and\nfurther extended through data augmentation before they can be effective for\ntraining. Recently, special techniques for automating these tasks have emerged.\nThe automation of data processing tasks is driven by the need to utilize large\nvolumes of complex, heterogeneous data for machine learning and big data\napplications. Today, end-to-end automated data processing systems based on\nautomated machine learning (AutoML) techniques are capable of taking raw data\nand transforming them into useful features for Big Data tasks by automating all\nintermediate processing stages. In this work, we present a thorough review of\napproaches for automating data processing tasks in deep learning pipelines,\nincluding automated data preprocessing--e.g., data cleaning, labeling, missing\ndata imputation, and categorical data encoding--as well as data augmentation\n(including synthetic data generation using generative AI methods) and feature\nengineering--specifically, automated feature extraction, feature construction\nand feature selection. In addition to automating specific data processing\ntasks, we discuss the use of AutoML methods and tools to simultaneously\noptimize all stages of the machine learning pipeline.\n","authors":["Alhassan Mumuni amd Fuseini Mumuni"],"pdf_url":"https://arxiv.org/pdf/2403.11395v1.pdf","comment":"Journal of Information and Intelligence (2024)"},{"id":"http://arxiv.org/abs/2312.04282v2","updated":"2024-03-18T19:40:52Z","published":"2023-12-07T13:06:27Z","title":"Adaptive Recursive Query Optimization","summary":"  Performance-critical industrial applications, including large-scale program,\nnetwork, and distributed system analyses, are increasingly reliant on recursive\nqueries for data analysis. Yet traditional relational algebra-based query\noptimization techniques do not scale well to recursive query processing due to\nthe iterative nature of query evaluation, where relation cardinalities can\nchange unpredictably during the course of a single query execution. To avoid\nerror-prone cardinality estimation, adaptive query processing techniques use\nruntime information to inform query optimization, but these systems are not\noptimized for the specific needs of recursive query processing. In this paper,\nwe introduce Adaptive Metaprogramming, an innovative technique that shifts\nrecursive query optimization and code generation from compile-time to runtime\nusing principled metaprogramming, enabling dynamic optimization and\nre-optimization before and after query execution has begun. We present a custom\njoin-ordering optimization applicable at multiple stages during query\ncompilation and execution. Through Carac, a custom Datalog engine, we evaluate\nthe optimization potential of Adaptive Metaprogramming and show unoptimized\nrecursive query execution time can be improved by three orders of magnitude and\nhand-optimized queries by 6x.\n","authors":["Anna Herlihy","Guillaume Martres","Anastasia Ailamaki","Martin Odersky"],"pdf_url":"https://arxiv.org/pdf/2312.04282v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11952v1","updated":"2024-03-18T16:50:05Z","published":"2024-03-18T16:50:05Z","title":"Exploring Estonia's Open Government Data Development as a Journey\n  towards Excellence: Unveiling the Progress of Local Governments in Open Data\n  Provision","summary":"  Estonia has a global reputation of a digital state or e-country. However,\ndespite the success in digital governance, the country has faced challenges in\nthe realm of Open Government Data (OGD) area, with significant advancements in\nits OGD ecosystem, as reflected in various open data rankings from 2020 and\nonwards, in the recent years being recognized among trend-setters. This paper\naims to explore the evolution and positioning of Estonia's OGD development,\nencompassing national and local levels, through an integrated analysis of\nvarious indices, primary data from the Estonian OGD portal, and a thorough\nliterature review. The research shows that Estonia has made progress in the\nnational level open data ecosystem, primarily due to improvements in the OGD\nportal usability and legislation amendments. However, the local level is not as\ndeveloped, with local governments lagging behind in OGD provision. The\nliterature review highlights the lack of previous research focusing on Estonian\nand European local open data, emphasizing the need for future studies to\nexplore the barriers and enablers of municipal OGD. This study contributes to a\nnuanced understanding of Estonia's dynamic journey in the OGD landscape,\nshedding light on both achievements and areas warranting further attention for\nestablishing a sustainable open data ecosystem.\n","authors":["Katrin Rajamäe-Soosaar","Anastasija Nikiforova"],"pdf_url":"https://arxiv.org/pdf/2403.11952v1.pdf","comment":"This paper has been accepted for publication in Proceedings of the\n  25th Annual International Conference on Digital Government Research and this\n  is a pre-print version of the manuscript. It is posted here for your personal\n  use. Not for redistribution"},{"id":"http://arxiv.org/abs/2403.11874v1","updated":"2024-03-18T15:27:35Z","published":"2024-03-18T15:27:35Z","title":"Benchmarking Analytical Query Processing in Intel SGXv2","summary":"  The recently introduced second generation of Intel SGX (SGXv2) lifts memory\nsize limitations of the first generation. Theoretically, this promises to\nenable secure and highly efficient analytical DBMSs in the cloud. To validate\nthis promise, in this paper, we conduct the first in-depth evaluation study of\nrunning analytical query processing algorithms inside SGXv2. Our study reveals\nthat state-of-the-art query operators like radix joins and SIMD-based scans can\nindeed achieve high performance inside SGXv2 enclaves. These operations are\norders of magnitude faster than joins optimized for the discontinued SGXv1\nhardware. However, substantial performance overheads are still caused by subtle\nhardware and software differences influencing code execution inside an SGX\nenclave. We investigate these differences and propose new optimizations to\nbring the performance inside the enclave on par with native code execution\noutside an enclave.\n","authors":["Adrian Lutsch","Muhammad El-Hindi","Matthias Heinrich","Daniel Ritter","Zsolt István","Carsten Binnig"],"pdf_url":"https://arxiv.org/pdf/2403.11874v1.pdf","comment":"14 pages, 17 figures, submitted for VLDB 2024 in the EA&B category,\n  associated code is available under\n  https://github.com/DataManagementLab/sgxv2-analytical-query-processing-benchmarks"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2403.11722v1","updated":"2024-03-18T12:22:11Z","published":"2024-03-18T12:22:11Z","title":"Time Series Compression using Quaternion Valued Neural Networks and\n  Quaternion Backpropagation","summary":"  We propose a novel quaternionic time-series compression methodology where we\ndivide a long time-series into segments of data, extract the min, max, mean and\nstandard deviation of these chunks as representative features and encapsulate\nthem in a quaternion, yielding a quaternion valued time-series. This\ntime-series is processed using quaternion valued neural network layers, where\nwe aim to preserve the relation between these features through the usage of the\nHamilton product. To train this quaternion neural network, we derive quaternion\nbackpropagation employing the GHR calculus, which is required for a valid\nproduct and chain rule in quaternion space. Furthermore, we investigate the\nconnection between the derived update rules and automatic differentiation. We\napply our proposed compression method on the Tennessee Eastman Dataset, where\nwe perform fault classification using the compressed data in two settings: a\nfully supervised one and in a semi supervised, contrastive learning setting.\nBoth times, we were able to outperform real valued counterparts as well as two\nbaseline models: one with the uncompressed time-series as the input and the\nother with a regular downsampling using the mean. Further, we could improve the\nclassification benchmark set by SimCLR-TS from 81.43% to 83.90%.\n","authors":["Johannes Pöppelbaum","Andreas Schwung"],"pdf_url":"https://arxiv.org/pdf/2403.11722v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.14169v3","updated":"2024-03-18T12:17:50Z","published":"2024-02-21T23:18:03Z","title":"A Temporal Bias Correction using a Machine Learning Attention model","summary":"  Climate models are biased with respect to real world observations and usually\nneed to be calibrated prior to impact studies. The suite of statistical methods\nthat enable such calibrations is called bias correction (BC). However, current\nBC methods struggle to adjust for temporal biases, because they disregard the\ndependence between consecutive time-points. As a result, climate statistics\nwith long-range temporal properties, such as heatwave duration and frequency,\ncannot be corrected accurately, making it more difficult to produce reliable\nimpact studies on such climate statistics. In this paper, we offer a novel BC\nmethodology to correct for temporal biases. This is made possible by i)\nre-thinking BC as a probability model rather than an algorithmic procedure, and\nii) adapting state-of-the-art machine-learning (ML) probabilistic attention\nmodels to fit the BC task. With a case study of heatwave duration statistics in\nAbuja, Nigeria, and Tokyo, Japan, we show striking results compared to current\nclimate model outputs and alternative BC methods.\n","authors":["Omer Nivron","Damon J. Wischik","Mathieu Vrac","Emily Shuckburgh"],"pdf_url":"https://arxiv.org/pdf/2402.14169v3.pdf","comment":"19 pages, 15 figures"},{"id":"http://arxiv.org/abs/2403.11706v1","updated":"2024-03-18T12:08:01Z","published":"2024-03-18T12:08:01Z","title":"Generalized Multi-Source Inference for Text Conditioned Music Diffusion\n  Models","summary":"  Multi-Source Diffusion Models (MSDM) allow for compositional musical\ngeneration tasks: generating a set of coherent sources, creating\naccompaniments, and performing source separation. Despite their versatility,\nthey require estimating the joint distribution over the sources, necessitating\npre-separated musical data, which is rarely available, and fixing the number\nand type of sources at training time. This paper generalizes MSDM to arbitrary\ntime-domain diffusion models conditioned on text embeddings. These models do\nnot require separated data as they are trained on mixtures, can parameterize an\narbitrary number of sources, and allow for rich semantic control. We propose an\ninference procedure enabling the coherent generation of sources and\naccompaniments. Additionally, we adapt the Dirac separator of MSDM to perform\nsource separation. We experiment with diffusion models trained on Slakh2100 and\nMTG-Jamendo, showcasing competitive generation and separation results in a\nrelaxed data setting.\n","authors":["Emilian Postolache","Giorgio Mariani","Luca Cosmo","Emmanouil Benetos","Emanuele Rodolà"],"pdf_url":"https://arxiv.org/pdf/2403.11706v1.pdf","comment":"Accepted at ICASSP 2024"},{"id":"http://arxiv.org/abs/2403.11705v1","updated":"2024-03-18T12:07:46Z","published":"2024-03-18T12:07:46Z","title":"Coarsening of chiral domains in itinerant electron magnets: A machine\n  learning force field approach","summary":"  Frustrated itinerant magnets often exhibit complex noncollinear or\nnoncoplanar magnetic orders which support topological electronic structures. A\ncanonical example is the anomalous quantum Hall state with a chiral spin order\nstabilized by electron-spin interactions on a triangular lattice. While a\nlong-range magnetic order cannot survive thermal fluctuations in two\ndimensions, the chiral order which results from the breaking of a discrete\nIsing symmetry persists even at finite temperatures. We present a scalable\nmachine learning (ML) framework to model the complex electron-mediated\nspin-spin interactions that stabilize the chiral magnetic domains in a\ntriangular lattice. Large-scale dynamical simulations, enabled by the ML\nforce-field models, are performed to investigate the coarsening of chiral\ndomains after a thermal quench. While the chiral phase is described by a broken\n$Z_2$ Ising-type symmetry, we find that the characteristic size of chiral\ndomains increases linearly with time, in stark contrast to the expected\nAllen-Cahn domain growth law for a non-conserved Ising order parameter field.\nThe linear growth of the chiral domains is attributed to the orientational\nanisotropy of domain boundaries. Our work also demonstrates the promising\npotential of ML models for large-scale spin dynamics of itinerant magnets.\n","authors":["Yunhao Fan","Sheng Zhang","Gia-Wei Chern"],"pdf_url":"https://arxiv.org/pdf/2403.11705v1.pdf","comment":"16 pages, 8 figures"},{"id":"http://arxiv.org/abs/2403.11696v1","updated":"2024-03-18T11:52:33Z","published":"2024-03-18T11:52:33Z","title":"Generalization error of spectral algorithms","summary":"  The asymptotically precise estimation of the generalization of kernel methods\nhas recently received attention due to the parallels between neural networks\nand their associated kernels. However, prior works derive such estimates for\ntraining by kernel ridge regression (KRR), whereas neural networks are\ntypically trained with gradient descent (GD). In the present work, we consider\nthe training of kernels with a family of $\\textit{spectral algorithms}$\nspecified by profile $h(\\lambda)$, and including KRR and GD as special cases.\nThen, we derive the generalization error as a functional of learning profile\n$h(\\lambda)$ for two data models: high-dimensional Gaussian and low-dimensional\ntranslation-invariant model. Under power-law assumptions on the spectrum of the\nkernel and target, we use our framework to (i) give full loss asymptotics for\nboth noisy and noiseless observations (ii) show that the loss localizes on\ncertain spectral scales, giving a new perspective on the KRR saturation\nphenomenon (iii) conjecture, and demonstrate for the considered data models,\nthe universality of the loss w.r.t. non-spectral details of the problem, but\nonly in case of noisy observation.\n","authors":["Maksim Velikanov","Maxim Panov","Dmitry Yarotsky"],"pdf_url":"https://arxiv.org/pdf/2403.11696v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.15776v2","updated":"2024-03-18T11:51:18Z","published":"2024-02-24T09:47:46Z","title":"Truly No-Regret Learning in Constrained MDPs","summary":"  Constrained Markov decision processes (CMDPs) are a common way to model\nsafety constraints in reinforcement learning. State-of-the-art methods for\nefficiently solving CMDPs are based on primal-dual algorithms. For these\nalgorithms, all currently known regret bounds allow for error cancellations --\none can compensate for a constraint violation in one round with a strict\nconstraint satisfaction in another. This makes the online learning process\nunsafe since it only guarantees safety for the final (mixture) policy but not\nduring learning. As Efroni et al. (2020) pointed out, it is an open question\nwhether primal-dual algorithms can provably achieve sublinear regret if we do\nnot allow error cancellations. In this paper, we give the first affirmative\nanswer. We first generalize a result on last-iterate convergence of regularized\nprimal-dual schemes to CMDPs with multiple constraints. Building upon this\ninsight, we propose a model-based primal-dual algorithm to learn in an unknown\nCMDP. We prove that our algorithm achieves sublinear regret without error\ncancellations.\n","authors":["Adrian Müller","Pragnya Alatur","Volkan Cevher","Giorgia Ramponi","Niao He"],"pdf_url":"https://arxiv.org/pdf/2402.15776v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11687v1","updated":"2024-03-18T11:37:53Z","published":"2024-03-18T11:37:53Z","title":"Nonsmooth Implicit Differentiation: Deterministic and Stochastic\n  Convergence Rates","summary":"  We study the problem of efficiently computing the derivative of the\nfixed-point of a parametric non-differentiable contraction map. This problem\nhas wide applications in machine learning, including hyperparameter\noptimization, meta-learning and data poisoning attacks. We analyze two popular\napproaches: iterative differentiation (ITD) and approximate implicit\ndifferentiation (AID). A key challenge behind the nonsmooth setting is that the\nchain rule does not hold anymore. Building upon the recent work by Bolte et al.\n(2022), who proved the linear convergence of non-differentiable ITD, we provide\nrefined linear convergence rates for both ITD and AID in the deterministic\ncase. We further introduce NSID, a new method to compute the implicit\nderivative when the fixed point is defined as the composition of an outer map\nand an inner map which is accessible only through a stochastic unbiased\nestimator. We establish rates for the convergence of NSID to the true\nderivative, encompassing the best available rates in the smooth setting. We\npresent illustrative experiments confirming our analysis.\n","authors":["Riccardo Grazzi","Massimiliano Pontil","Saverio Salzo"],"pdf_url":"https://arxiv.org/pdf/2403.11687v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11686v1","updated":"2024-03-18T11:37:42Z","published":"2024-03-18T11:37:42Z","title":"Crystalformer: Infinitely Connected Attention for Periodic Structure\n  Encoding","summary":"  Predicting physical properties of materials from their crystal structures is\na fundamental problem in materials science. In peripheral areas such as the\nprediction of molecular properties, fully connected attention networks have\nbeen shown to be successful. However, unlike these finite atom arrangements,\ncrystal structures are infinitely repeating, periodic arrangements of atoms,\nwhose fully connected attention results in infinitely connected attention. In\nthis work, we show that this infinitely connected attention can lead to a\ncomputationally tractable formulation, interpreted as neural potential\nsummation, that performs infinite interatomic potential summations in a deeply\nlearned feature space. We then propose a simple yet effective Transformer-based\nencoder architecture for crystal structures called Crystalformer. Compared to\nan existing Transformer-based model, the proposed model requires only 29.4% of\nthe number of parameters, with minimal modifications to the original\nTransformer architecture. Despite the architectural simplicity, the proposed\nmethod outperforms state-of-the-art methods for various property regression\ntasks on the Materials Project and JARVIS-DFT datasets.\n","authors":["Tatsunori Taniai","Ryo Igarashi","Yuta Suzuki","Naoya Chiba","Kotaro Saito","Yoshitaka Ushiku","Kanta Ono"],"pdf_url":"https://arxiv.org/pdf/2403.11686v1.pdf","comment":"13 main pages, 3 figures, 4 tables, 10 appendix pages. Published as a\n  conference paper at ICLR 2024. For more information, see\n  https://omron-sinicx.github.io/crystalformer/"},{"id":"http://arxiv.org/abs/2403.11678v1","updated":"2024-03-18T11:29:43Z","published":"2024-03-18T11:29:43Z","title":"Exploring 3D-aware Latent Spaces for Efficiently Learning Numerous\n  Scenes","summary":"  We present a method enabling the scaling of NeRFs to learn a large number of\nsemantically-similar scenes. We combine two techniques to improve the required\ntraining time and memory cost per scene. First, we learn a 3D-aware latent\nspace in which we train Tri-Plane scene representations, hence reducing the\nresolution at which scenes are learned. Moreover, we present a way to share\ncommon information across scenes, hence allowing for a reduction of model\ncomplexity to learn a particular scene. Our method reduces effective per-scene\nmemory costs by 44% and per-scene time costs by 86% when training 1000 scenes.\nOur project page can be found at https://3da-ae.github.io .\n","authors":["Antoine Schnepf","Karim Kassab","Jean-Yves Franceschi","Laurent Caraffa","Flavian Vasile","Jeremie Mary","Andrew Comport","Valérie Gouet-Brunet"],"pdf_url":"https://arxiv.org/pdf/2403.11678v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11671v1","updated":"2024-03-18T11:19:37Z","published":"2024-03-18T11:19:37Z","title":"HDLdebugger: Streamlining HDL debugging with Large Language Models","summary":"  In the domain of chip design, Hardware Description Languages (HDLs) play a\npivotal role. However, due to the complex syntax of HDLs and the limited\navailability of online resources, debugging HDL codes remains a difficult and\ntime-intensive task, even for seasoned engineers. Consequently, there is a\npressing need to develop automated HDL code debugging models, which can\nalleviate the burden on hardware engineers. Despite the strong capabilities of\nLarge Language Models (LLMs) in generating, completing, and debugging software\ncode, their utilization in the specialized field of HDL debugging has been\nlimited and, to date, has not yielded satisfactory results. In this paper, we\npropose an LLM-assisted HDL debugging framework, namely HDLdebugger, which\nconsists of HDL debugging data generation via a reverse engineering approach, a\nsearch engine for retrieval-augmented generation, and a retrieval-augmented LLM\nfine-tuning approach. Through the integration of these components, HDLdebugger\ncan automate and streamline HDL debugging for chip design. Our comprehensive\nexperiments, conducted on an HDL code dataset sourced from Huawei, reveal that\nHDLdebugger outperforms 13 cutting-edge LLM baselines, displaying exceptional\neffectiveness in HDL code debugging.\n","authors":["Xufeng Yao","Haoyang Li","Tsz Ho Chan","Wenyi Xiao","Mingxuan Yuan","Yu Huang","Lei Chen","Bei Yu"],"pdf_url":"https://arxiv.org/pdf/2403.11671v1.pdf","comment":"13 pages,5 figures"},{"id":"http://arxiv.org/abs/2402.14899v2","updated":"2024-03-18T10:55:36Z","published":"2024-02-22T17:36:34Z","title":"Stop Reasoning! When Multimodal LLMs with Chain-of-Thought Reasoning\n  Meets Adversarial Images","summary":"  Recently, Multimodal LLMs (MLLMs) have shown a great ability to understand\nimages. However, like traditional vision models, they are still vulnerable to\nadversarial images. Meanwhile, Chain-of-Thought (CoT) reasoning has been widely\nexplored on MLLMs, which not only improves model's performance, but also\nenhances model's explainability by giving intermediate reasoning steps.\nNevertheless, there is still a lack of study regarding MLLMs' adversarial\nrobustness with CoT and an understanding of what the rationale looks like when\nMLLMs infer wrong answers with adversarial images. Our research evaluates the\nadversarial robustness of MLLMs when employing CoT reasoning, finding that CoT\nmarginally improves adversarial robustness against existing attack methods.\nMoreover, we introduce a novel stop-reasoning attack technique that effectively\nbypasses the CoT-induced robustness enhancements. Finally, we demonstrate the\nalterations in CoT reasoning when MLLMs confront adversarial images, shedding\nlight on their reasoning process under adversarial attacks.\n","authors":["Zefeng Wang","Zhen Han","Shuo Chen","Fan Xue","Zifeng Ding","Xun Xiao","Volker Tresp","Philip Torr","Jindong Gu"],"pdf_url":"https://arxiv.org/pdf/2402.14899v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.09493v3","updated":"2024-03-18T10:40:00Z","published":"2024-01-17T01:17:12Z","title":"Identifying Three-Dimensional Radiative Patterns Associated with Early\n  Tropical Cyclone Intensification","summary":"  Cloud radiative feedback impacts early tropical cyclone (TC) intensification,\nbut limitations in existing diagnostic frameworks make them unsuitable for\nstudying asymmetric or transient radiative heating. We propose a linear\nVariational Encoder-Decoder (VED) to learn the hidden relationship between\nradiation and the surface intensification of realistic simulated TCs. Limiting\nVED model inputs enables using its uncertainty to identify periods when\nradiation has more importance for intensification. A close examination of the\nextracted 3D radiative structures suggests that longwave radiative forcing from\ninner core deep convection and shallow clouds both contribute to\nintensification, with the deep convection having the most impact overall. We\nfind that deep convection downwind of the shallow clouds is critical to the\nintensification of Haiyan. Our work demonstrates that machine learning can\ndiscover thermodynamic-kinematic relationships without relying on axisymmetric\nor deterministic assumptions, paving the way towards the objective discovery of\nprocesses leading to TC intensification in realistic conditions.\n","authors":["Frederick Iat-Hin Tam","Tom Beucler","James H. Ruppert Jr"],"pdf_url":"https://arxiv.org/pdf/2401.09493v3.pdf","comment":"15 pages, 6 figures (main text)"},{"id":"http://arxiv.org/abs/2403.11643v1","updated":"2024-03-18T10:35:15Z","published":"2024-03-18T10:35:15Z","title":"Diffusion-Based Environment-Aware Trajectory Prediction","summary":"  The ability to predict the future trajectories of traffic participants is\ncrucial for the safe and efficient operation of autonomous vehicles. In this\npaper, a diffusion-based generative model for multi-agent trajectory prediction\nis proposed. The model is capable of capturing the complex interactions between\ntraffic participants and the environment, accurately learning the multimodal\nnature of the data. The effectiveness of the approach is assessed on\nlarge-scale datasets of real-world traffic scenarios, showing that our model\noutperforms several well-established methods in terms of prediction accuracy.\nBy the incorporation of differential motion constraints on the model output, we\nillustrate that our model is capable of generating a diverse set of realistic\nfuture trajectories. Through the use of an interaction-aware guidance signal,\nwe further demonstrate that the model can be adapted to predict the behavior of\nless cooperative agents, emphasizing its practical applicability under\nuncertain traffic conditions.\n","authors":["Theodor Westny","Björn Olofsson","Erik Frisk"],"pdf_url":"https://arxiv.org/pdf/2403.11643v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11642v1","updated":"2024-03-18T10:34:40Z","published":"2024-03-18T10:34:40Z","title":"Guiding the generation of counterfactual explanations through temporal\n  background knowledge for Predictive Process Monitoring","summary":"  Counterfactual explanations suggest what should be different in the input\ninstance to change the outcome of an AI system. When dealing with\ncounterfactual explanations in the field of Predictive Process Monitoring,\nhowever, control flow relationships among events have to be carefully\nconsidered. A counterfactual, indeed, should not violate control flow\nrelationships among activities (temporal background knowledege). Within the\nfield of Explainability in Predictive Process Monitoring, there have been a\nseries of works regarding counterfactual explanations for outcome-based\npredictions. However, none of them consider the inclusion of temporal\nbackground knowledge when generating these counterfactuals. In this work, we\nadapt state-of-the-art techniques for counterfactual generation in the domain\nof XAI that are based on genetic algorithms to consider a series of temporal\nconstraints at runtime. We assume that this temporal background knowledge is\ngiven, and we adapt the fitness function, as well as the crossover and mutation\noperators, to maintain the satisfaction of the constraints. The proposed\nmethods are evaluated with respect to state-of-the-art genetic algorithms for\ncounterfactual generation and the results are presented. We showcase that the\ninclusion of temporal background knowledge allows the generation of\ncounterfactuals more conformant to the temporal background knowledge, without\nhowever losing in terms of the counterfactual traditional quality metrics.\n","authors":["Andrei Buliga","Chiara Di Francescomarino","Chiara Ghidini","Ivan Donadello","Fabrizio Maria Maggi"],"pdf_url":"https://arxiv.org/pdf/2403.11642v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.01012v3","updated":"2024-03-18T10:32:59Z","published":"2023-10-02T09:03:59Z","title":"Unconstrained Stochastic CCA: Unifying Multiview and Self-Supervised\n  Learning","summary":"  The Canonical Correlation Analysis (CCA) family of methods is foundational in\nmultiview learning. Regularised linear CCA methods can be seen to generalise\nPartial Least Squares (PLS) and be unified with a Generalized Eigenvalue\nProblem (GEP) framework. However, classical algorithms for these linear methods\nare computationally infeasible for large-scale data. Extensions to Deep CCA\nshow great promise, but current training procedures are slow and complicated.\nFirst we propose a novel unconstrained objective that characterizes the top\nsubspace of GEPs. Our core contribution is a family of fast algorithms for\nstochastic PLS, stochastic CCA, and Deep CCA, simply obtained by applying\nstochastic gradient descent (SGD) to the corresponding CCA objectives. Our\nalgorithms show far faster convergence and recover higher correlations than the\nprevious state-of-the-art on all standard CCA and Deep CCA benchmarks. These\nimprovements allow us to perform a first-of-its-kind PLS analysis of an\nextremely large biomedical dataset from the UK Biobank, with over 33,000\nindividuals and 500,000 features. Finally, we apply our algorithms to match the\nperformance of `CCA-family' Self-Supervised Learning (SSL) methods on CIFAR-10\nand CIFAR-100 with minimal hyper-parameter tuning, and also present theory to\nclarify the links between these methods and classical CCA, laying the\ngroundwork for future insights.\n","authors":["James Chapman","Lennie Wells","Ana Lawry Aguila"],"pdf_url":"https://arxiv.org/pdf/2310.01012v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.00502v2","updated":"2024-03-18T10:32:01Z","published":"2023-12-01T11:06:00Z","title":"A Comprehensive Evaluation of Augmentations for Robust OOD\n  Self-Supervised Contrastive Phonocardiogram Representation Learning","summary":"  Despite the recent increase in research activity, deep-learning models have\nnot yet been widely accepted in several real-world settings, such as medicine.\nThe shortage of high-quality annotated data often hinders the development of\nrobust and generalizable models, which do not suffer from degraded\neffectiveness when presented with newly-collected, out-of-distribution (OOD)\ndatasets. Contrastive Self-Supervised Learning (SSL) offers a potential\nsolution to labeled data scarcity, as it takes advantage of unlabeled data to\nincrease model effectiveness and robustness. In this research, we propose\napplying contrastive SSL for detecting abnormalities in 1D phonocardiogram\n(PCG) samples by learning a generalized representation of the signal.\nSpecifically, we perform an extensive comparative evaluation of a wide range of\naudio-based augmentations, evaluate trained classifiers on multiple datasets\nacross different downstream tasks, and finally report on the impact of each\naugmentation in model training. We experimentally demonstrate that, depending\non its training distribution, the effectiveness of a fully-supervised model can\ndegrade up to 32% when evaluated on unseen data, while SSL models only lose up\nto 10% or even improve in some cases. We argue and experimentally demonstrate\nthat, contrastive SSL pretraining can assist in providing robust classifiers\nwhich can generalize to unseen, OOD data, without relying on time- and\nlabor-intensive annotation processes by medical experts. Furthermore, the\nproposed extensive evaluation protocol sheds light on the most promising and\nappropriate augmentations for robust PCG signal processing, by calculating\ntheir effect size on model training. Finally, we provide researchers and\npractitioners with a roadmap towards producing robust models for PCG\nclassification, in addition to an open-source codebase for developing novel\napproaches.\n","authors":["Aristotelis Ballas","Vasileios Papapanagiotou","Christos Diou"],"pdf_url":"https://arxiv.org/pdf/2312.00502v2.pdf","comment":"PREPRINT Manuscript under review"},{"id":"http://arxiv.org/abs/2403.11637v1","updated":"2024-03-18T10:19:52Z","published":"2024-03-18T10:19:52Z","title":"The Value of Reward Lookahead in Reinforcement Learning","summary":"  In reinforcement learning (RL), agents sequentially interact with changing\nenvironments while aiming to maximize the obtained rewards. Usually, rewards\nare observed only after acting, and so the goal is to maximize the expected\ncumulative reward. Yet, in many practical settings, reward information is\nobserved in advance -- prices are observed before performing transactions;\nnearby traffic information is partially known; and goals are oftentimes given\nto agents prior to the interaction. In this work, we aim to quantifiably\nanalyze the value of such future reward information through the lens of\ncompetitive analysis. In particular, we measure the ratio between the value of\nstandard RL agents and that of agents with partial future-reward lookahead. We\ncharacterize the worst-case reward distribution and derive exact ratios for the\nworst-case reward expectations. Surprisingly, the resulting ratios relate to\nknown quantities in offline RL and reward-free exploration. We further provide\ntight bounds for the ratio given the worst-case dynamics. Our results cover the\nfull spectrum between observing the immediate rewards before acting to\nobserving all the rewards before the interaction starts.\n","authors":["Nadav Merlis","Dorian Baudry","Vianney Perchet"],"pdf_url":"https://arxiv.org/pdf/2403.11637v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.10352v3","updated":"2024-03-18T10:07:40Z","published":"2023-07-19T21:21:18Z","title":"Properties of Discrete Sliced Wasserstein Losses","summary":"  The Sliced Wasserstein (SW) distance has become a popular alternative to the\nWasserstein distance for comparing probability measures. Widespread\napplications include image processing, domain adaptation and generative\nmodelling, where it is common to optimise some parameters in order to minimise\nSW, which serves as a loss function between discrete probability measures\n(since measures admitting densities are numerically unattainable). All these\noptimisation problems bear the same sub-problem, which is minimising the Sliced\nWasserstein energy. In this paper we study the properties of $\\mathcal{E}: Y\n\\longmapsto \\mathrm{SW}_2^2(\\gamma_Y, \\gamma_Z)$, i.e. the SW distance between\ntwo uniform discrete measures with the same amount of points as a function of\nthe support $Y \\in \\mathbb{R}^{n \\times d}$ of one of the measures. We\ninvestigate the regularity and optimisation properties of this energy, as well\nas its Monte-Carlo approximation $\\mathcal{E}_p$ (estimating the expectation in\nSW using only $p$ samples) and show convergence results on the critical points\nof $\\mathcal{E}_p$ to those of $\\mathcal{E}$, as well as an almost-sure uniform\nconvergence. Finally, we show that in a certain sense, Stochastic Gradient\nDescent methods minimising $\\mathcal{E}$ and $\\mathcal{E}_p$ converge towards\n(Clarke) critical points of these energies.\n","authors":["Eloi Tanguy","Rémi Flamary","Julie Delon"],"pdf_url":"https://arxiv.org/pdf/2307.10352v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11624v1","updated":"2024-03-18T09:56:00Z","published":"2024-03-18T09:56:00Z","title":"Dual-Channel Multiplex Graph Neural Networks for Recommendation","summary":"  Efficient recommender systems play a crucial role in accurately capturing\nuser and item attributes that mirror individual preferences. Some existing\nrecommendation techniques have started to shift their focus towards modeling\nvarious types of interaction relations between users and items in real-world\nrecommendation scenarios, such as clicks, marking favorites, and purchases on\nonline shopping platforms. Nevertheless, these approaches still grapple with\ntwo significant shortcomings: (1) Insufficient modeling and exploitation of the\nimpact of various behavior patterns formed by multiplex relations between users\nand items on representation learning, and (2) ignoring the effect of different\nrelations in the behavior patterns on the target relation in recommender system\nscenarios. In this study, we introduce a novel recommendation framework,\nDual-Channel Multiplex Graph Neural Network (DCMGNN), which addresses the\naforementioned challenges. It incorporates an explicit behavior pattern\nrepresentation learner to capture the behavior patterns composed of multiplex\nuser-item interaction relations, and includes a relation chain representation\nlearning and a relation chain-aware encoder to discover the impact of various\nauxiliary relations on the target relation, the dependencies between different\nrelations, and mine the appropriate order of relations in a behavior pattern.\nExtensive experiments on three real-world datasets demonstrate that our \\model\nsurpasses various state-of-the-art recommendation methods. It outperforms the\nbest baselines by 10.06\\% and 12.15\\% on average across all datasets in terms\nof R@10 and N@10 respectively.\n","authors":["Xiang Li","Chaofan Fu","Zhongying Zhao","Guanjie Zheng","Chao Huang","Junyu Dong","Yanwei Yu"],"pdf_url":"https://arxiv.org/pdf/2403.11624v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.11714v3","updated":"2024-03-18T09:55:08Z","published":"2023-07-21T17:19:01Z","title":"Convergence of SGD for Training Neural Networks with Sliced Wasserstein\n  Losses","summary":"  Optimal Transport has sparked vivid interest in recent years, in particular\nthanks to the Wasserstein distance, which provides a geometrically sensible and\nintuitive way of comparing probability measures. For computational reasons, the\nSliced Wasserstein (SW) distance was introduced as an alternative to the\nWasserstein distance, and has seen uses for training generative Neural Networks\n(NNs). While convergence of Stochastic Gradient Descent (SGD) has been observed\npractically in such a setting, there is to our knowledge no theoretical\nguarantee for this observation. Leveraging recent works on convergence of SGD\non non-smooth and non-convex functions by Bianchi et al. (2022), we aim to\nbridge that knowledge gap, and provide a realistic context under which\nfixed-step SGD trajectories for the SW loss on NN parameters converge. More\nprecisely, we show that the trajectories approach the set of (sub)-gradient\nflow equations as the step decreases. Under stricter assumptions, we show a\nmuch stronger convergence result for noised and projected SGD schemes, namely\nthat the long-run limits of the trajectories approach a set of generalised\ncritical points of the loss function.\n","authors":["Eloi Tanguy"],"pdf_url":"https://arxiv.org/pdf/2307.11714v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.06604v3","updated":"2024-03-18T09:51:00Z","published":"2024-01-12T14:40:55Z","title":"Identifying Policy Gradient Subspaces","summary":"  Policy gradient methods hold great potential for solving complex continuous\ncontrol tasks. Still, their training efficiency can be improved by exploiting\nstructure within the optimization problem. Recent work indicates that\nsupervised learning can be accelerated by leveraging the fact that gradients\nlie in a low-dimensional and slowly-changing subspace. In this paper, we\nconduct a thorough evaluation of this phenomenon for two popular deep policy\ngradient methods on various simulated benchmark tasks. Our results demonstrate\nthe existence of such gradient subspaces despite the continuously changing data\ndistribution inherent to reinforcement learning. These findings reveal\npromising directions for future work on more efficient reinforcement learning,\ne.g., through improving parameter-space exploration or enabling second-order\noptimization.\n","authors":["Jan Schneider","Pierre Schumacher","Simon Guist","Le Chen","Daniel Häufle","Bernhard Schölkopf","Dieter Büchler"],"pdf_url":"https://arxiv.org/pdf/2401.06604v3.pdf","comment":"Published as conference paper at ICLR 2024"},{"id":"http://arxiv.org/abs/2402.09288v3","updated":"2024-03-18T09:47:08Z","published":"2024-02-14T16:21:47Z","title":"EcoVal: An Efficient Data Valuation Framework for Machine Learning","summary":"  Quantifying the value of data within a machine learning workflow can play a\npivotal role in making more strategic decisions in machine learning\ninitiatives. The existing Shapley value based frameworks for data valuation in\nmachine learning are computationally expensive as they require considerable\namount of repeated training of the model to obtain the Shapley value. In this\npaper, we introduce an efficient data valuation framework EcoVal, to estimate\nthe value of data for machine learning models in a fast and practical manner.\nInstead of directly working with individual data sample, we determine the value\nof a cluster of similar data points. This value is further propagated amongst\nall the member cluster points. We show that the overall data value can be\ndetermined by estimating the intrinsic and extrinsic value of each data. This\nis enabled by formulating the performance of a model as a \\textit{production\nfunction}, a concept which is popularly used to estimate the amount of output\nbased on factors like labor and capital in a traditional free economic market.\nWe provide a formal proof of our valuation technique and elucidate the\nprinciples and mechanisms that enable its accelerated performance. We\ndemonstrate the real-world applicability of our method by showcasing its\neffectiveness for both in-distribution and out-of-sample data. This work\naddresses one of the core challenges of efficient data valuation at scale in\nmachine learning models.\n","authors":["Ayush K Tarun","Vikram S Chundawat","Murari Mandal","Hong Ming Tan","Bowei Chen","Mohan Kankanhalli"],"pdf_url":"https://arxiv.org/pdf/2402.09288v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06880v2","updated":"2024-03-18T09:43:20Z","published":"2024-03-11T16:34:23Z","title":"Unveiling the Significance of Toddler-Inspired Reward Transition in\n  Goal-Oriented Reinforcement Learning","summary":"  Toddlers evolve from free exploration with sparse feedback to exploiting\nprior experiences for goal-directed learning with denser rewards. Drawing\ninspiration from this Toddler-Inspired Reward Transition, we set out to explore\nthe implications of varying reward transitions when incorporated into\nReinforcement Learning (RL) tasks. Central to our inquiry is the transition\nfrom sparse to potential-based dense rewards, which share optimal strategies\nregardless of reward changes. Through various experiments, including those in\negocentric navigation and robotic arm manipulation tasks, we found that proper\nreward transitions significantly influence sample efficiency and success rates.\nOf particular note is the efficacy of the toddler-inspired Sparse-to-Dense\n(S2D) transition. Beyond these performance metrics, using Cross-Density\nVisualizer technique, we observed that transitions, especially the S2D, smooth\nthe policy loss landscape, promoting wide minima that enhance generalization in\nRL models.\n","authors":["Junseok Park","Yoonsung Kim","Hee Bin Yoo","Min Whoo Lee","Kibeom Kim","Won-Seok Choi","Minsu Lee","Byoung-Tak Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.06880v2.pdf","comment":"Accepted as a full paper at AAAI 2024 (Oral presentation): 7 pages\n  (main paper), 2 pages (references), 17 pages (appendix) each"},{"id":"http://arxiv.org/abs/2403.11603v1","updated":"2024-03-18T09:25:59Z","published":"2024-03-18T09:25:59Z","title":"Fair Distributed Cooperative Bandit Learning on Networks for Intelligent\n  Internet of Things Systems (Technical Report)","summary":"  In intelligent Internet of Things (IoT) systems, edge servers within a\nnetwork exchange information with their neighbors and collect data from sensors\nto complete delivered tasks. In this paper, we propose a multiplayer\nmulti-armed bandit model for intelligent IoT systems to facilitate data\ncollection and incorporate fairness considerations. In our model, we establish\nan effective communication protocol that helps servers cooperate with their\nneighbors. Then we design a distributed cooperative bandit algorithm, DC-ULCB,\nenabling servers to collaboratively select sensors to maximize data rates while\nmaintaining fairness in their choices. We conduct an analysis of the reward\nregret and fairness regret of DC-ULCB, and prove that both regrets have\nlogarithmic instance-dependent upper bounds. Additionally, through extensive\nsimulations, we validate that DC-ULCB outperforms existing algorithms in\nmaximizing reward and ensuring fairness.\n","authors":["Ziqun Chen","Kechao Cai","Jinbei Zhang","Zhigang Yu"],"pdf_url":"https://arxiv.org/pdf/2403.11603v1.pdf","comment":"10 pages, 8 figures, conference technical report"},{"id":"http://arxiv.org/abs/2403.06645v2","updated":"2024-03-18T09:22:01Z","published":"2024-03-11T12:07:33Z","title":"Ricci flow-based brain surface covariance descriptors for diagnosing\n  Alzheimer's disease","summary":"  Automated feature extraction from MRI brain scans and diagnosis of\nAlzheimer's disease are ongoing challenges. With advances in 3D imaging\ntechnology, 3D data acquisition is becoming more viable and efficient than its\n2D counterpart. Rather than using feature-based vectors, in this paper, for the\nfirst time, we suggest a pipeline to extract novel covariance-based descriptors\nfrom the cortical surface using the Ricci energy optimization. The covariance\ndescriptors are components of the nonlinear manifold of symmetric\npositive-definite matrices, thus we focus on using the Gaussian radial basis\nfunction to apply manifold-based classification to the 3D shape problem.\nApplying this novel signature to the analysis of abnormal cortical brain\nmorphometry allows for diagnosing Alzheimer's disease. Experimental studies\nperformed on about two hundred 3D MRI brain models, gathered from Alzheimer's\nDisease Neuroimaging Initiative (ADNI) dataset demonstrate the effectiveness of\nour descriptors in achieving remarkable classification accuracy.\n","authors":["Fatemeh Ahmadi","Mohamad Ebrahim Shiri","Behroz Bidabad","Maral Sedaghat","Pooran Memari"],"pdf_url":"https://arxiv.org/pdf/2403.06645v2.pdf","comment":"Accepted for publication in Biomedical Signal Processing and Control\n  journal"},{"id":"http://arxiv.org/abs/2403.11591v1","updated":"2024-03-18T09:10:39Z","published":"2024-03-18T09:10:39Z","title":"A physics-informed neural network method for the approximation of slow\n  invariant manifolds for the general class of stiff systems of ODEs","summary":"  We present a physics-informed neural network (PINN) approach for the\ndiscovery of slow invariant manifolds (SIMs), for the most general class of\nfast/slow dynamical systems of ODEs. In contrast to other machine learning (ML)\napproaches that construct reduced order black box surrogate models using simple\nregression, and/or require a priori knowledge of the fast and slow variables,\nour approach, simultaneously decomposes the vector field into fast and slow\ncomponents and provides a functional of the underlying SIM in a closed form.\nThe decomposition is achieved by finding a transformation of the state\nvariables to the fast and slow ones, which enables the derivation of an\nexplicit, in terms of fast variables, SIM functional. The latter is obtained by\nsolving a PDE corresponding to the invariance equation within the Geometric\nSingular Perturbation Theory (GSPT) using a single-layer feedforward neural\nnetwork with symbolic differentiation. The performance of the proposed\nphysics-informed ML framework is assessed via three benchmark problems: the\nMichaelis-Menten, the target mediated drug disposition (TMDD) reaction model\nand a fully competitive substrate-inhibitor(fCSI) mechanism. We also provide a\ncomparison with other GPST methods, namely the quasi steady state approximation\n(QSSA), the partial equilibrium approximation (PEA) and CSP with one and two\niterations. We show that the proposed PINN scheme provides SIM approximations,\nof equivalent or even higher accuracy, than those provided by QSSA, PEA and\nCSP, especially close to the boundaries of the underlying SIMs.\n","authors":["Dimitrios G. Patsatzis","Lucia Russo","Constantinos Siettos"],"pdf_url":"https://arxiv.org/pdf/2403.11591v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.10207v5","updated":"2024-03-18T09:05:12Z","published":"2023-10-16T09:19:18Z","title":"Bongard-OpenWorld: Few-Shot Reasoning for Free-form Visual Concepts in\n  the Real World","summary":"  We introduce Bongard-OpenWorld, a new benchmark for evaluating real-world\nfew-shot reasoning for machine vision. It originates from the classical Bongard\nProblems (BPs): Given two sets of images (positive and negative), the model\nneeds to identify the set that query images belong to by inducing the visual\nconcepts, which is exclusively depicted by images from the positive set. Our\nbenchmark inherits the few-shot concept induction of the original BPs while\nadding the two novel layers of challenge: 1) open-world free-form concepts, as\nthe visual concepts in Bongard-OpenWorld are unique compositions of terms from\nan open vocabulary, ranging from object categories to abstract visual\nattributes and commonsense factual knowledge; 2) real-world images, as opposed\nto the synthetic diagrams used by many counterparts. In our exploration,\nBongard-OpenWorld already imposes a significant challenge to current few-shot\nreasoning algorithms. We further investigate to which extent the recently\nintroduced Large Language Models (LLMs) and Vision-Language Models (VLMs) can\nsolve our task, by directly probing VLMs, and combining VLMs and LLMs in an\ninteractive reasoning scheme. We even conceived a neuro-symbolic reasoning\napproach that reconciles LLMs & VLMs with logical reasoning to emulate the\nhuman problem-solving process for Bongard Problems. However, none of these\napproaches manage to close the human-machine gap, as the best learner achieves\n64% accuracy while human participants easily reach 91%. We hope\nBongard-OpenWorld can help us better understand the limitations of current\nvisual intelligence and facilitate future research on visual agents with\nstronger few-shot visual reasoning capabilities.\n","authors":["Rujie Wu","Xiaojian Ma","Zhenliang Zhang","Wei Wang","Qing Li","Song-Chun Zhu","Yizhou Wang"],"pdf_url":"https://arxiv.org/pdf/2310.10207v5.pdf","comment":"Accepted to ICLR 2024"},{"id":"http://arxiv.org/abs/2401.15603v2","updated":"2024-03-18T09:00:41Z","published":"2024-01-28T08:12:00Z","title":"Improving Expressive Power of Spectral Graph Neural Networks with\n  Eigenvalue Correction","summary":"  In recent years, spectral graph neural networks, characterized by polynomial\nfilters, have garnered increasing attention and have achieved remarkable\nperformance in tasks such as node classification. These models typically assume\nthat eigenvalues for the normalized Laplacian matrix are distinct from each\nother, thus expecting a polynomial filter to have a high fitting ability.\nHowever, this paper empirically observes that normalized Laplacian matrices\nfrequently possess repeated eigenvalues. Moreover, we theoretically establish\nthat the number of distinguishable eigenvalues plays a pivotal role in\ndetermining the expressive power of spectral graph neural networks. In light of\nthis observation, we propose an eigenvalue correction strategy that can free\npolynomial filters from the constraints of repeated eigenvalue inputs.\nConcretely, the proposed eigenvalue correction strategy enhances the uniform\ndistribution of eigenvalues, thus mitigating repeated eigenvalues, and\nimproving the fitting capacity and expressive power of polynomial filters.\nExtensive experimental results on both synthetic and real-world datasets\ndemonstrate the superiority of our method.\n","authors":["Kangkang Lu","Yanhua Yu","Hao Fei","Xuan Li","Zixuan Yang","Zirui Guo","Meiyu Liang","Mengran Yin","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2401.15603v2.pdf","comment":"Accepted by AAAI-24"},{"id":"http://arxiv.org/abs/2403.11585v1","updated":"2024-03-18T08:58:47Z","published":"2024-03-18T08:58:47Z","title":"Linguacodus: A Synergistic Framework for Transformative Code Generation\n  in Machine Learning Pipelines","summary":"  In the ever-evolving landscape of machine learning, seamless translation of\nnatural language descriptions into executable code remains a formidable\nchallenge. This paper introduces Linguacodus, an innovative framework designed\nto tackle this challenge by deploying a dynamic pipeline that iteratively\ntransforms natural language task descriptions into code through high-level\ndata-shaping instructions. The core of Linguacodus is a fine-tuned large\nlanguage model (LLM), empowered to evaluate diverse solutions for various\nproblems and select the most fitting one for a given task. This paper details\nthe fine-tuning process, and sheds light on how natural language descriptions\ncan be translated into functional code. Linguacodus represents a substantial\nleap towards automated code generation, effectively bridging the gap between\ntask descriptions and executable code. It holds great promise for advancing\nmachine learning applications across diverse domains. Additionally, we propose\nan algorithm capable of transforming a natural description of an ML task into\ncode with minimal human interaction. In extensive experiments on a vast machine\nlearning code dataset originating from Kaggle, we showcase the effectiveness of\nLinguacodus. The investigations highlight its potential applications across\ndiverse domains, emphasizing its impact on applied machine learning in various\nscientific fields.\n","authors":["Ekaterina Trofimova","Emil Sataev","Andrey E. Ustyuzhanin"],"pdf_url":"https://arxiv.org/pdf/2403.11585v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11574v1","updated":"2024-03-18T08:50:30Z","published":"2024-03-18T08:50:30Z","title":"Offline Multitask Representation Learning for Reinforcement Learning","summary":"  We study offline multitask representation learning in reinforcement learning\n(RL), where a learner is provided with an offline dataset from different tasks\nthat share a common representation and is asked to learn the shared\nrepresentation. We theoretically investigate offline multitask low-rank RL, and\npropose a new algorithm called MORL for offline multitask representation\nlearning. Furthermore, we examine downstream RL in reward-free, offline and\nonline scenarios, where a new task is introduced to the agent that shares the\nsame representation as the upstream offline tasks. Our theoretical results\ndemonstrate the benefits of using the learned representation from the upstream\noffline task instead of directly learning the representation of the low-rank\nmodel.\n","authors":["Haque Ishfaq","Thanh Nguyen-Tang","Songtao Feng","Raman Arora","Mengdi Wang","Ming Yin","Doina Precup"],"pdf_url":"https://arxiv.org/pdf/2403.11574v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.14405v2","updated":"2024-03-18T08:45:52Z","published":"2024-01-25T18:59:58Z","title":"Multimodal Pathway: Improve Transformers with Irrelevant Data from Other\n  Modalities","summary":"  We propose to improve transformers of a specific modality with irrelevant\ndata from other modalities, e.g., improve an ImageNet model with audio or point\ncloud datasets. We would like to highlight that the data samples of the target\nmodality are irrelevant to the other modalities, which distinguishes our method\nfrom other works utilizing paired (e.g., CLIP) or interleaved data of different\nmodalities. We propose a methodology named Multimodal Pathway - given a target\nmodality and a transformer designed for it, we use an auxiliary transformer\ntrained with data of another modality and construct pathways to connect\ncomponents of the two models so that data of the target modality can be\nprocessed by both models. In this way, we utilize the universal\nsequence-to-sequence modeling abilities of transformers obtained from two\nmodalities. As a concrete implementation, we use a modality-specific tokenizer\nand task-specific head as usual but utilize the transformer blocks of the\nauxiliary model via a proposed method named Cross-Modal Re-parameterization,\nwhich exploits the auxiliary weights without any inference costs. On the image,\npoint cloud, video, and audio recognition tasks, we observe significant and\nconsistent performance improvements with irrelevant data from other modalities.\nThe code and models are available at https://github.com/AILab-CVC/M2PT.\n","authors":["Yiyuan Zhang","Xiaohan Ding","Kaixiong Gong","Yixiao Ge","Ying Shan","Xiangyu Yue"],"pdf_url":"https://arxiv.org/pdf/2401.14405v2.pdf","comment":"CVPR 2024. Code and models are available at\n  https://github.com/AILab-CVC/M2PT"},{"id":"http://arxiv.org/abs/2309.16883v4","updated":"2024-03-18T08:43:46Z","published":"2023-09-28T22:41:47Z","title":"The Lipschitz-Variance-Margin Tradeoff for Enhanced Randomized Smoothing","summary":"  Real-life applications of deep neural networks are hindered by their unsteady\npredictions when faced with noisy inputs and adversarial attacks. The certified\nradius in this context is a crucial indicator of the robustness of models.\nHowever how to design an efficient classifier with an associated certified\nradius? Randomized smoothing provides a promising framework by relying on noise\ninjection into the inputs to obtain a smoothed and robust classifier. In this\npaper, we first show that the variance introduced by the Monte-Carlo sampling\nin the randomized smoothing procedure estimate closely interacts with two other\nimportant properties of the classifier, \\textit{i.e.} its Lipschitz constant\nand margin. More precisely, our work emphasizes the dual impact of the\nLipschitz constant of the base classifier, on both the smoothed classifier and\nthe empirical variance. To increase the certified robust radius, we introduce a\ndifferent way to convert logits to probability vectors for the base classifier\nto leverage the variance-margin trade-off. We leverage the use of Bernstein's\nconcentration inequality along with enhanced Lipschitz bounds for randomized\nsmoothing. Experimental results show a significant improvement in certified\naccuracy compared to current state-of-the-art methods. Our novel certification\nprocedure allows us to use pre-trained models with randomized smoothing,\neffectively improving the current certification radius in a zero-shot manner.\n","authors":["Blaise Delattre","Alexandre Araujo","Quentin Barthélemy","Alexandre Allauzen"],"pdf_url":"https://arxiv.org/pdf/2309.16883v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.15599v2","updated":"2024-03-18T08:37:24Z","published":"2023-11-27T07:48:50Z","title":"UniRepLKNet: A Universal Perception Large-Kernel ConvNet for Audio,\n  Video, Point Cloud, Time-Series and Image Recognition","summary":"  Large-kernel convolutional neural networks (ConvNets) have recently received\nextensive research attention, but two unresolved and critical issues demand\nfurther investigation. 1) The architectures of existing large-kernel ConvNets\nlargely follow the design principles of conventional ConvNets or transformers,\nwhile the architectural design for large-kernel ConvNets remains\nunder-addressed. 2) As transformers have dominated multiple modalities, it\nremains to be investigated whether ConvNets also have a strong universal\nperception ability in domains beyond vision. In this paper, we contribute from\ntwo aspects. 1) We propose four architectural guidelines for designing\nlarge-kernel ConvNets, the core of which is to exploit the essential\ncharacteristics of large kernels that distinguish them from small kernels -\nthey can see wide without going deep. Following such guidelines, our proposed\nlarge-kernel ConvNet shows leading performance in image recognition (ImageNet\naccuracy of 88.0%, ADE20K mIoU of 55.6%, and COCO box AP of 56.4%),\ndemonstrating better performance and higher speed than the recent powerful\ncompetitors. 2) We discover large kernels are the key to unlocking the\nexceptional performance of ConvNets in domains where they were originally not\nproficient. With certain modality-related preprocessing approaches, the\nproposed model achieves state-of-the-art performance on time-series forecasting\nand audio recognition tasks even without modality-specific customization to the\narchitecture. All the code and models are publicly available on GitHub and\nHuggingface.\n","authors":["Xiaohan Ding","Yiyuan Zhang","Yixiao Ge","Sijie Zhao","Lin Song","Xiangyu Yue","Ying Shan"],"pdf_url":"https://arxiv.org/pdf/2311.15599v2.pdf","comment":"CVPR 2024. Code, all the models, reproducible training scripts at\n  https://github.com/AILab-CVC/UniRepLKNet"},{"id":"http://arxiv.org/abs/2403.11565v1","updated":"2024-03-18T08:35:17Z","published":"2024-03-18T08:35:17Z","title":"Decentralized Stochastic Subgradient Methods for Nonsmooth Nonconvex\n  Optimization","summary":"  In this paper, we concentrate on decentralized optimization problems with\nnonconvex and nonsmooth objective functions, especially on the decentralized\ntraining of nonsmooth neural networks. We introduce a unified framework, named\nDSM, to analyze the global convergence of decentralized stochastic subgradient\nmethods. We prove the global convergence of our proposed framework under mild\nconditions, by establishing that the generated sequence asymptotically\napproximates the trajectories of its associated differential inclusion.\nFurthermore, we establish that our proposed framework encompasses a wide range\nof existing efficient decentralized subgradient methods, including\ndecentralized stochastic subgradient descent (DSGD), DSGD with\ngradient-tracking technique (DSGD-T), and DSGD with momentum (DSGDm). In\naddition, we introduce SignSGD employing the sign map to regularize the update\ndirections in DSGDm, and show it is enclosed in our proposed framework.\nConsequently, our convergence results establish, for the first time, global\nconvergence of these methods when applied to nonsmooth nonconvex objectives.\nPreliminary numerical experiments demonstrate that our proposed framework\nyields highly efficient decentralized subgradient methods with convergence\nguarantees in the training of nonsmooth neural networks.\n","authors":["Siyuan Zhang","Nachuan Xiao","Xin Liu"],"pdf_url":"https://arxiv.org/pdf/2403.11565v1.pdf","comment":"23 pages"},{"id":"http://arxiv.org/abs/2403.11563v1","updated":"2024-03-18T08:33:56Z","published":"2024-03-18T08:33:56Z","title":"Advancing Neuromorphic Computing: Mixed-Signal Design Techniques\n  Leveraging Brain Code Units and Fundamental Code Units","summary":"  This paper introduces a groundbreaking digital neuromorphic architecture that\ninnovatively integrates Brain Code Unit (BCU) and Fundamental Code Unit (FCU)\nusing mixedsignal design methodologies. Leveraging open-source datasets and the\nlatest advances in materials science, our research focuses on enhancing the\ncomputational efficiency, accuracy, and adaptability of neuromorphic systems.\nThe core of our approach lies in harmonizing the precision and scalability of\ndigital systems with the robustness and energy efficiency of analog processing.\nThrough experimentation, we demonstrate the effectiveness of our system across\nvarious metrics. The BCU achieved an accuracy of 88.0% and a power efficiency\nof 20.0 GOP/s/W, while the FCU recorded an accuracy of 86.5% and a power\nefficiency of 18.5 GOP/s/W. Our mixed-signal design approach significantly\nimproved latency and throughput, achieving a latency as low as 0.75 ms and\nthroughput up to 213 TOP/s. These results firmly establish the potential of our\narchitecture in neuromorphic computing, providing a solid foundation for future\ndevelopments in this domain. Our study underscores the feasibility of\nmixedsignal neuromorphic systems and their promise in advancing the field,\nparticularly in applications requiring high efficiency and adaptability\n","authors":["Murat Isik","Sols Miziev","Wiktoria Pawlak","Newton Howard"],"pdf_url":"https://arxiv.org/pdf/2403.11563v1.pdf","comment":"Accepted at 2024 International Joint Conference on Neural Networks"},{"id":"http://arxiv.org/abs/2311.14272v2","updated":"2024-03-18T08:15:48Z","published":"2023-11-24T04:16:32Z","title":"CRISP: Hybrid Structured Sparsity for Class-aware Model Pruning","summary":"  Machine learning pipelines for classification tasks often train a universal\nmodel to achieve accuracy across a broad range of classes. However, a typical\nuser encounters only a limited selection of classes regularly. This disparity\nprovides an opportunity to enhance computational efficiency by tailoring models\nto focus on user-specific classes. Existing works rely on unstructured pruning,\nwhich introduces randomly distributed non-zero values in the model, making it\nunsuitable for hardware acceleration. Alternatively, some approaches employ\nstructured pruning, such as channel pruning, but these tend to provide only\nminimal compression and may lead to reduced model accuracy. In this work, we\npropose CRISP, a novel pruning framework leveraging a hybrid structured\nsparsity pattern that combines both fine-grained N:M structured sparsity and\ncoarse-grained block sparsity. Our pruning strategy is guided by a\ngradient-based class-aware saliency score, allowing us to retain weights\ncrucial for user-specific classes. CRISP achieves high accuracy with minimal\nmemory consumption for popular models like ResNet-50, VGG-16, and MobileNetV2\non ImageNet and CIFAR-100 datasets. Moreover, CRISP delivers up to 14$\\times$\nreduction in latency and energy consumption compared to existing pruning\nmethods while maintaining comparable accuracy. Our code is available at\nhttps://github.com/shivmgg/CRISP/.\n","authors":["Shivam Aggarwal","Kuluhan Binici","Tulika Mitra"],"pdf_url":"https://arxiv.org/pdf/2311.14272v2.pdf","comment":"6 pages, accepted in Design, Automation & Test in Europe Conference &\n  Exhibition (DATE) 2024"},{"id":"http://arxiv.org/abs/2304.06094v4","updated":"2024-03-18T08:11:08Z","published":"2023-04-12T18:20:58Z","title":"Energy-guided Entropic Neural Optimal Transport","summary":"  Energy-based models (EBMs) are known in the Machine Learning community for\ndecades. Since the seminal works devoted to EBMs dating back to the noughties,\nthere have been a lot of efficient methods which solve the generative modelling\nproblem by means of energy potentials (unnormalized likelihood functions). In\ncontrast, the realm of Optimal Transport (OT) and, in particular, neural OT\nsolvers is much less explored and limited by few recent works (excluding\nWGAN-based approaches which utilize OT as a loss function and do not model OT\nmaps themselves). In our work, we bridge the gap between EBMs and\nEntropy-regularized OT. We present a novel methodology which allows utilizing\nthe recent developments and technical improvements of the former in order to\nenrich the latter. From the theoretical perspective, we prove generalization\nbounds for our technique. In practice, we validate its applicability in toy 2D\nand image domains. To showcase the scalability, we empower our method with a\npre-trained StyleGAN and apply it to high-res AFHQ $512\\times 512$ unpaired I2I\ntranslation. For simplicity, we choose simple short- and long-run EBMs as a\nbackbone of our Energy-guided Entropic OT approach, leaving the application of\nmore sophisticated EBMs for future research. Our code is available at:\nhttps://github.com/PetrMokrov/Energy-guided-Entropic-OT\n","authors":["Petr Mokrov","Alexander Korotin","Alexander Kolesov","Nikita Gushchin","Evgeny Burnaev"],"pdf_url":"https://arxiv.org/pdf/2304.06094v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09629v2","updated":"2024-03-18T07:56:48Z","published":"2024-03-14T17:58:16Z","title":"Quiet-STaR: Language Models Can Teach Themselves to Think Before\n  Speaking","summary":"  When writing and talking, people sometimes pause to think. Although\nreasoning-focused works have often framed reasoning as a method of answering\nquestions or completing agentic tasks, reasoning is implicit in almost all\nwritten text. For example, this applies to the steps not stated between the\nlines of a proof or to the theory of mind underlying a conversation. In the\nSelf-Taught Reasoner (STaR, Zelikman et al. 2022), useful thinking is learned\nby inferring rationales from few-shot examples in question-answering and\nlearning from those that lead to a correct answer. This is a highly constrained\nsetting -- ideally, a language model could instead learn to infer unstated\nrationales in arbitrary text. We present Quiet-STaR, a generalization of STaR\nin which LMs learn to generate rationales at each token to explain future text,\nimproving their predictions. We address key challenges, including 1) the\ncomputational cost of generating continuations, 2) the fact that the LM does\nnot initially know how to generate or use internal thoughts, and 3) the need to\npredict beyond individual next tokens. To resolve these, we propose a tokenwise\nparallel sampling algorithm, using learnable tokens indicating a thought's\nstart and end, and an extended teacher-forcing technique. Encouragingly,\ngenerated rationales disproportionately help model difficult-to-predict tokens\nand improve the LM's ability to directly answer difficult questions. In\nparticular, after continued pretraining of an LM on a corpus of internet text\nwith Quiet-STaR, we find zero-shot improvements on GSM8K\n(5.9%$\\rightarrow$10.9%) and CommonsenseQA (36.3%$\\rightarrow$47.2%) and\nobserve a perplexity improvement of difficult tokens in natural text.\nCrucially, these improvements require no fine-tuning on these tasks. Quiet-STaR\nmarks a step towards LMs that can learn to reason in a more general and\nscalable way.\n","authors":["Eric Zelikman","Georges Harik","Yijia Shao","Varuna Jayasiri","Nick Haber","Noah D. Goodman"],"pdf_url":"https://arxiv.org/pdf/2403.09629v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11544v1","updated":"2024-03-18T07:54:11Z","published":"2024-03-18T07:54:11Z","title":"RL en Markov Games with Independent Function Approximation: Improved\n  Sample Complexity Bound under the Local Access Model","summary":"  Efficiently learning equilibria with large state and action spaces in\ngeneral-sum Markov games while overcoming the curse of multi-agency is a\nchallenging problem. Recent works have attempted to solve this problem by\nemploying independent linear function classes to approximate the marginal\n$Q$-value for each agent. However, existing sample complexity bounds under such\na framework have a suboptimal dependency on the desired accuracy $\\varepsilon$\nor the action space. In this work, we introduce a new algorithm,\nLin-Confident-FTRL, for learning coarse correlated equilibria (CCE) with local\naccess to the simulator, i.e., one can interact with the underlying environment\non the visited states. Up to a logarithmic dependence on the size of the state\nspace, Lin-Confident-FTRL learns $\\epsilon$-CCE with a provable optimal\naccuracy bound $O(\\epsilon^{-2})$ and gets rids of the linear dependency on the\naction space, while scaling polynomially with relevant problem parameters (such\nas the number of agents and time horizon). Moreover, our analysis of\nLinear-Confident-FTRL generalizes the virtual policy iteration technique in the\nsingle-agent local planning literature, which yields a new computationally\nefficient algorithm with a tighter sample complexity bound when assuming random\naccess to the simulator.\n","authors":["Junyi Fan","Yuxuan Han","Jialin Zeng","Jian-Feng Cai","Yang Wang","Yang Xiang","Jiheng Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.11544v1.pdf","comment":"Accepted at the 27th International Conference on Artificial\n  Intelligence and Statistics (AISTATS 2024)"},{"id":"http://arxiv.org/abs/2311.15487v3","updated":"2024-03-18T07:51:52Z","published":"2023-11-27T02:12:02Z","title":"Global $\\mathcal{L}^2$ minimization at uniform exponential rate via\n  geometrically adapted gradient descent in Deep Learning","summary":"  We consider the gradient descent flow widely used for the minimization of the\n$\\mathcal{L}^2$ cost function in Deep Learning networks, and introduce two\nmodified versions; one adapted for the overparametrized setting, and the other\nfor the underparametrized setting. Both have a clear and natural invariant\ngeometric meaning, taking into account the pullback vector bundle structure in\nthe overparametrized, and the pushforward vector bundle structure in the\nunderparametrized setting. In the overparametrized case, we prove that,\nprovided that a rank condition holds, all orbits of the modified gradient\ndescent drive the $\\mathcal{L}^2$ cost to its global minimum at a uniform\nexponential convergence rate; one thereby obtains an a priori stopping time for\nany prescribed proximity to the global minimum. We point out relations of the\nlatter to sub-Riemannian geometry.\n","authors":["Thomas Chen"],"pdf_url":"https://arxiv.org/pdf/2311.15487v3.pdf","comment":"AMS Latex, 16 pages. Section 2.1 on rank condition, and Section 2.4\n  on the trapping of orbits in the standard gradient descent flow added. Title\n  changed"},{"id":"http://arxiv.org/abs/2403.11537v1","updated":"2024-03-18T07:43:14Z","published":"2024-03-18T07:43:14Z","title":"Semantic Prompting with Image-Token for Continual Learning","summary":"  Continual learning aims to refine model parameters for new tasks while\nretaining knowledge from previous tasks. Recently, prompt-based learning has\nemerged to leverage pre-trained models to be prompted to learn subsequent tasks\nwithout the reliance on the rehearsal buffer. Although this approach has\ndemonstrated outstanding results, existing methods depend on preceding\ntask-selection process to choose appropriate prompts. However, imperfectness in\ntask-selection may lead to negative impacts on the performance particularly in\nthe scenarios where the number of tasks is large or task distributions are\nimbalanced. To address this issue, we introduce I-Prompt, a task-agnostic\napproach focuses on the visual semantic information of image tokens to\neliminate task prediction. Our method consists of semantic prompt matching,\nwhich determines prompts based on similarities between tokens, and image\ntoken-level prompting, which applies prompts directly to image tokens in the\nintermediate layers. Consequently, our method achieves competitive performance\non four benchmarks while significantly reducing training time compared to\nstate-of-the-art methods. Moreover, we demonstrate the superiority of our\nmethod across various scenarios through extensive experiments.\n","authors":["Jisu Han","Jaemin Na","Wonjun Hwang"],"pdf_url":"https://arxiv.org/pdf/2403.11537v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11536v1","updated":"2024-03-18T07:41:39Z","published":"2024-03-18T07:41:39Z","title":"OCR is All you need: Importing Multi-Modality into Image-based Defect\n  Detection System","summary":"  Automatic optical inspection (AOI) plays a pivotal role in the manufacturing\nprocess, predominantly leveraging high-resolution imaging instruments for\nscanning purposes. It detects anomalies by analyzing image textures or\npatterns, making it an essential tool in industrial manufacturing and quality\ncontrol. Despite its importance, the deployment of models for AOI often faces\nchallenges. These include limited sample sizes, which hinder effective feature\nlearning, variations among source domains, and sensitivities to changes in\nlighting and camera positions during imaging. These factors collectively\ncompromise the accuracy of model predictions. Traditional AOI often fails to\ncapitalize on the rich mechanism-parameter information from machines or inside\nimages, including statistical parameters, which typically benefit AOI\nclassification. To address this, we introduce an external modality-guided data\nmining framework, primarily rooted in optical character recognition (OCR), to\nextract statistical features from images as a second modality to enhance\nperformance, termed OANet (Ocr-Aoi-Net). A key aspect of our approach is the\nalignment of external modality features, extracted using a single\nmodality-aware model, with image features encoded by a convolutional neural\nnetwork. This synergy enables a more refined fusion of semantic representations\nfrom different modalities. We further introduce feature refinement and a gating\nfunction in our OANet to optimize the combination of these features, enhancing\ninference and decision-making capabilities. Experimental outcomes show that our\nmethodology considerably boosts the recall rate of the defect detection model\nand maintains high robustness even in challenging scenarios.\n","authors":["Chih-Chung Hsu","Chia-Ming Lee","Chun-Hung Sun","Kuang-Ming Wu"],"pdf_url":"https://arxiv.org/pdf/2403.11536v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11532v1","updated":"2024-03-18T07:35:25Z","published":"2024-03-18T07:35:25Z","title":"Out-of-Distribution Detection Should Use Conformal Prediction (and\n  Vice-versa?)","summary":"  Research on Out-Of-Distribution (OOD) detection focuses mainly on building\nscores that efficiently distinguish OOD data from In Distribution (ID) data. On\nthe other hand, Conformal Prediction (CP) uses non-conformity scores to\nconstruct prediction sets with probabilistic coverage guarantees. In this work,\nwe propose to use CP to better assess the efficiency of OOD scores.\nSpecifically, we emphasize that in standard OOD benchmark settings, evaluation\nmetrics can be overly optimistic due to the finite sample size of the test\ndataset. Based on the work of (Bates et al., 2022), we define new conformal\nAUROC and conformal FRP@TPR95 metrics, which are corrections that provide\nprobabilistic conservativeness guarantees on the variability of these metrics.\nWe show the effect of these corrections on two reference OOD and anomaly\ndetection benchmarks, OpenOOD (Yang et al., 2022) and ADBench (Han et al.,\n2022). We also show that the benefits of using OOD together with CP apply the\nother way around by using OOD scores as non-conformity scores, which results in\nimproving upon current CP methods. One of the key messages of these\ncontributions is that since OOD is concerned with designing scores and CP with\ninterpreting these scores, the two fields may be inherently intertwined.\n","authors":["Paul Novello","Joseba Dalmau","Léo Andeol"],"pdf_url":"https://arxiv.org/pdf/2403.11532v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11522v1","updated":"2024-03-18T07:22:31Z","published":"2024-03-18T07:22:31Z","title":"LOOPer: A Learned Automatic Code Optimizer For Polyhedral Compilers","summary":"  While polyhedral compilers have shown success in implementing advanced code\ntransformations, they still have challenges in selecting the most profitable\ntransformations that lead to the best speedups. This has motivated the use of\nmachine learning to build cost models to guide the search for polyhedral\noptimizations. State-of-the-art polyhedral compilers have demonstrated a viable\nproof-of-concept of this approach. While such a proof-of-concept has shown\npromise, it still has significant limitations. State-of-the-art polyhedral\ncompilers that use a deep-learning cost model only support a small subset of\naffine transformations, limiting their ability to apply complex code\ntransformations. They also only support simple programs that have a single loop\nnest and a rectangular iteration domain, limiting their applicability to many\nprograms. These limitations significantly impact the generality of such\ncompilers and autoschedulers and put into question the whole approach. In this\npaper, we introduce LOOPer, the first polyhedral autoscheduler that uses a\ndeep-learning based cost model and covers a large set of affine transformations\nand programs. It supports the exploration of a large set of affine\ntransformations, allowing the application of complex sequences of polyhedral\ntransformations. It also supports the optimization of programs with multiple\nloop nests and with rectangular and non-rectangular iteration domains, allowing\nthe optimization of an extensive set of programs. We implement and evaluate\nLOOPer and show that it achieves speedups over the state-of-the-art. On the\nPolybench benchmark, LOOPer achieves a geometric mean speedup of 1.59x over\nTiramisu. LOOPer also achieves competitive speedups with a geometric mean\nspeedup of 1.34x over Pluto, a state-of-the-art polyhedral compiler that does\nnot use a machine-learning based cost model.\n","authors":["Massinissa Merouani","Khaled Afif Boudaoud","Iheb Nassim Aouadj","Nassim Tchoulak","Islam Kara Bernou","Hamza Benyamina","Fatima Benbouzid-Si Tayeb","Karima Benatchba","Hugh Leather","Riyadh Baghdadi"],"pdf_url":"https://arxiv.org/pdf/2403.11522v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.00318v2","updated":"2024-03-18T07:21:27Z","published":"2023-11-01T06:02:59Z","title":"Flooding Regularization for Stable Training of Generative Adversarial\n  Networks","summary":"  Generative Adversarial Networks (GANs) have shown remarkable performance in\nimage generation. However, GAN training suffers from the problem of\ninstability. One of the main approaches to address this problem is to modify\nthe loss function, often using regularization terms in addition to changing the\ntype of adversarial losses. This paper focuses on directly regularizing the\nadversarial loss function. We propose a method that applies flooding, an\noverfitting suppression method in supervised learning, to GANs to directly\nprevent the discriminator's loss from becoming excessively low. Flooding\nrequires tuning the flood level, but when applied to GANs, we propose that the\nappropriate range of flood level settings is determined by the adversarial loss\nfunction, supported by theoretical analysis of GANs using the binary cross\nentropy loss. We experimentally verify that flooding stabilizes GAN training\nand can be combined with other stabilization techniques. We also show that by\nrestricting the discriminator's loss to be no less than the flood level, the\ntraining proceeds stably even when the flood level is somewhat high.\n","authors":["Iu Yahiro","Takashi Ishida","Naoto Yokoya"],"pdf_url":"https://arxiv.org/pdf/2311.00318v2.pdf","comment":"25 pages, 9 figures, 18 tables"},{"id":"http://arxiv.org/abs/2403.04306v2","updated":"2024-03-18T07:21:01Z","published":"2024-03-07T08:25:27Z","title":"Effectiveness Assessment of Recent Large Vision-Language Models","summary":"  The advent of large vision-language models (LVLMs) represents a noteworthy\nadvancement towards the pursuit of artificial general intelligence. However,\nthe extent of their efficacy across both specialized and general tasks warrants\nfurther investigation. This article endeavors to evaluate the competency of\npopular LVLMs in specialized and general tasks, respectively, aiming to offer a\ncomprehensive comprehension of these innovative methodologies. To gauge their\nefficacy in specialized tasks, we tailor a comprehensive testbed comprising\nthree distinct scenarios: natural, healthcare, and industrial, encompassing six\nchallenging tasks. These tasks include salient, camouflaged, and transparent\nobject detection, as well as polyp and skin lesion detection, alongside\nindustrial anomaly detection. We examine the performance of three recent\nopen-source LVLMs -- MiniGPT-v2, LLaVA-1.5, and Shikra -- in the realm of\nvisual recognition and localization. Moreover, we conduct empirical\ninvestigations utilizing the aforementioned models alongside GPT-4V, assessing\ntheir multi-modal understanding capacities in general tasks such as object\ncounting, absurd question answering, affordance reasoning, attribute\nrecognition, and spatial relation reasoning. Our investigations reveal that\nthese models demonstrate limited proficiency not only in specialized tasks but\nalso in general tasks. We delve deeper into this inadequacy and suggest several\npotential factors, including limited cognition in specialized tasks, object\nhallucination, text-to-image interference, and decreased robustness in complex\nproblems. We hope this study would provide valuable insights for the future\ndevelopment of LVLMs, augmenting their power in coping with both general and\nspecialized applications.\n","authors":["Yao Jiang","Xinyu Yan","Ge-Peng Ji","Keren Fu","Meijun Sun","Huan Xiong","Deng-Ping Fan","Fahad Shahbaz Khan"],"pdf_url":"https://arxiv.org/pdf/2403.04306v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11521v1","updated":"2024-03-18T07:15:01Z","published":"2024-03-18T07:15:01Z","title":"A Data-driven Approach for Rapid Detection of Aeroelastic Modes from\n  Flutter Flight Test Based on Limited Sensor Measurements","summary":"  Flutter flight test involves the evaluation of the airframes aeroelastic\nstability by applying artificial excitation on the aircraft lifting surfaces.\nThe subsequent responses are captured and analyzed to extract the frequencies\nand damping characteristics of the system. However, noise contamination,\nturbulence, non-optimal excitation of modes, and sensor malfunction in one or\nmore sensors make it time-consuming and corrupt the extraction process. In\norder to expedite the process of identifying and analyzing aeroelastic modes,\nthis study implements a time-delay embedded Dynamic Mode Decomposition\ntechnique. This approach is complemented by Robust Principal Component Analysis\nmethodology, and a sparsity promoting criterion which enables the automatic and\noptimal selection of sparse modes. The anonymized flutter flight test data,\nprovided by the fifth author of this research paper, is utilized in this\nimplementation. The methodology assumes no knowledge of the input excitation,\nonly deals with the responses captured by accelerometer channels, and rapidly\nidentifies the aeroelastic modes. By incorporating a compressed sensing\nalgorithm, the methodology gains the ability to identify aeroelastic modes,\neven when the number of available sensors is limited. This augmentation greatly\nenhances the methodology's robustness and effectiveness, making it an excellent\nchoice for real-time implementation during flutter test campaigns.\n","authors":["Arpan Das","Pier Marzocca","Giuliano Coppotelli","Oleg Levinski","Paul Taylor"],"pdf_url":"https://arxiv.org/pdf/2403.11521v1.pdf","comment":"31 pages, 12 figures, submitted in 'Mechanical Systems and Signal\n  processing' journal"},{"id":"http://arxiv.org/abs/2403.11520v1","updated":"2024-03-18T07:14:21Z","published":"2024-03-18T07:14:21Z","title":"State-Separated SARSA: A Practical Sequential Decision-Making Algorithm\n  with Recovering Rewards","summary":"  While many multi-armed bandit algorithms assume that rewards for all arms are\nconstant across rounds, this assumption does not hold in many real-world\nscenarios. This paper considers the setting of recovering bandits (Pike-Burke &\nGrunewalder, 2019), where the reward depends on the number of rounds elapsed\nsince the last time an arm was pulled. We propose a new reinforcement learning\n(RL) algorithm tailored to this setting, named the State-Separate SARSA\n(SS-SARSA) algorithm, which treats rounds as states. The SS-SARSA algorithm\nachieves efficient learning by reducing the number of state combinations\nrequired for Q-learning/SARSA, which often suffers from combinatorial issues\nfor large-scale RL problems. Additionally, it makes minimal assumptions about\nthe reward structure and offers lower computational complexity. Furthermore, we\nprove asymptotic convergence to an optimal policy under mild assumptions.\nSimulation studies demonstrate the superior performance of our algorithm across\nvarious settings.\n","authors":["Yuto Tanimoto","Kenji Fukumizu"],"pdf_url":"https://arxiv.org/pdf/2403.11520v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.16681v2","updated":"2024-03-18T07:10:46Z","published":"2024-02-26T15:59:38Z","title":"Enhancing Continuous Domain Adaptation with Multi-Path Transfer\n  Curriculum","summary":"  Addressing the large distribution gap between training and testing data has\nlong been a challenge in machine learning, giving rise to fields such as\ntransfer learning and domain adaptation. Recently, Continuous Domain Adaptation\n(CDA) has emerged as an effective technique, closing this gap by utilizing a\nseries of intermediate domains. This paper contributes a novel CDA method,\nW-MPOT, which rigorously addresses the domain ordering and error accumulation\nproblems overlooked by previous studies. Specifically, we construct a transfer\ncurriculum over the source and intermediate domains based on Wasserstein\ndistance, motivated by theoretical analysis of CDA. Then we transfer the source\nmodel to the target domain through multiple valid paths in the curriculum using\na modified version of continuous optimal transport. A bidirectional path\nconsistency constraint is introduced to mitigate the impact of accumulated\nmapping errors during continuous transfer. We extensively evaluate W-MPOT on\nmultiple datasets, achieving up to 54.1\\% accuracy improvement on multi-session\nAlzheimer MR image classification and 94.7\\% MSE reduction on battery capacity\nestimation.\n","authors":["Hanbing Liu","Jingge Wang","Xuan Zhang","Ye Guo","Yang Li"],"pdf_url":"https://arxiv.org/pdf/2402.16681v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.09362v2","updated":"2024-03-18T07:08:31Z","published":"2023-10-13T19:09:31Z","title":"From Words and Exercises to Wellness: Farsi Chatbot for Self-Attachment\n  Technique","summary":"  In the wake of the post-pandemic era, marked by social isolation and surging\nrates of depression and anxiety, conversational agents based on digital\npsychotherapy can play an influential role compared to traditional therapy\nsessions. In this work, we develop a voice-capable chatbot in Farsi to guide\nusers through Self-Attachment (SAT), a novel, self-administered, holistic\npsychological technique based on attachment theory. Our chatbot uses a dynamic\narray of rule-based and classification-based modules to comprehend user input\nthroughout the conversation and navigates a dialogue flowchart accordingly,\nrecommending appropriate SAT exercises that depend on the user's emotional and\nmental state. In particular, we collect a dataset of over 6,000 utterances and\ndevelop a novel sentiment-analysis module that classifies user sentiment into\n12 classes, with accuracy above 92%. To keep the conversation novel and\nengaging, the chatbot's responses are retrieved from a large dataset of\nutterances created with the aid of Farsi GPT-2 and a reinforcement learning\napproach, thus requiring minimal human annotation. Our chatbot also offers a\nquestion-answering module, called SAT Teacher, to answer users' questions about\nthe principles of Self-Attachment. Finally, we design a cross-platform\napplication as the bot's user interface. We evaluate our platform in a ten-day\nhuman study with N=52 volunteers from the non-clinical population, who have had\nover 2,000 dialogues in total with the chatbot. The results indicate that the\nplatform was engaging to most users (75%), 72% felt better after the\ninteractions, and 74% were satisfied with the SAT Teacher's performance.\n","authors":["Sina Elahimanesh","Shayan Salehi","Sara Zahedi Movahed","Lisa Alazraki","Ruoyu Hu","Abbas Edalat"],"pdf_url":"https://arxiv.org/pdf/2310.09362v2.pdf","comment":null}],"Computer Science and Game Theory":[{"id":"http://arxiv.org/abs/2403.11633v1","updated":"2024-03-18T10:13:51Z","published":"2024-03-18T10:13:51Z","title":"Cooperative Agri-Food Export under Minimum Quantity Commitments","summary":"  International trade can be a profitable business for agri-food communities.\nHowever, access to international markets can be costly and thus unattainable\nfor small and medium sized enterprises (SMEs). This problem is exacerbated\nunder trade policies which require minimum quantity commitments (MQCs) on\nexport volumes, e.g., licensing tariff rate quota (TRQ) mechanisms.\n  We show how cooperative exporting among agri-food SMEs can tackle the\nbarriers posed by the MQCs, and give market access to a broader range of SMEs.\nWe formulate a class of cooperative games associated with these situations and\nfind a gain-sharing mechanism that result in allocations in their corresponding\ncores. Thus, grand coalitions of cooperative exporting SMEs can form in stable\nmanners.\n  This allocation rule shares the export surplus only among the \"essential\" SME\nexporters, that is, the players who are sufficiently cost efficient. Thus, less\ncost efficient \"complimentary\" SMEs whose capacities are needed to maintain\nMQCs receive no benefit from collaborative exporting and their participation\nhave to be altruistic. We propose two modifications to our original allocation\nrule to share a portion of export surplus among the complementary SMEs through\ntaxing the essential SMEs: the first through egalitarian, and the second\nthrough revenue-based rates. We compare the performance of these allocations\nwith the numerical examples and discuss their practical implications.\n","authors":["Luis A. Guardiola","Behzad Hezarkhani","Ana Meca"],"pdf_url":"https://arxiv.org/pdf/2403.11633v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11486v1","updated":"2024-03-18T05:32:31Z","published":"2024-03-18T05:32:31Z","title":"Expanding the Resolution Boundary of Outcome-Based Imperfect-Recall\n  Abstraction in Games with Ordered Signals","summary":"  In the development of advanced Texas Hold'em AI systems, abstraction\ntechnology has garnered widespread attention due to its significant effect in\nsimplifying game complexity. This study adopts a more specific model, the games\nof ordered signal, to describe Texas Hold'em-style games and optimizes this\nmodel to streamline its mathematical representation and broaden its\napplicability. By transitioning from a broad imperfect information game model\nto a game with ordered signals model, we have separated the previously\nintertwined infoset abstraction and action abstraction into independent signal\nabstraction and action abstraction. Importantly, this signal abstraction\nprovides a mathematical framework for the hand abstraction task, which is\nemphatically discussed in this paper. Additionally, a novel common refinement\nprinciple is introduced, revealing the limit performance of hand abstraction\nalgorithms. We introduce potential outcome isomorphism (POI) and pinpoint that\nit suffers from the issue of excessive abstraction. Futher, We demonstrate that\nPOI serves as a common refinement for leading outcome-based hand abstraction\nalgorithms, such as E[HS] and PA\\&PAEMD. Consequently, excessive abstraction\nalso inherently affects these algorithms, leading to suboptimal performance.\nOur investigation reveals the omission of historical data as a primary\ncontributor to excessive abstraction. To remedy this, we propose the K-Recall\nOutcome Isomorphism (KROI) to incorporate the missing information. Compared\nwith POI, KROI more accurately mirrors lossless isomorphism (LI), the ground\ntruth, offering enhanced signal abstraction resolution. Experimental results in\nthe Numeral211 Hold'em indicate that strategies developed through KROI\napproximate the exploitability of those developed through LI more closely than\nthose trained through POI.\n","authors":["Yanchang Fu","Junge Zhang","Dongdong Bai","Lingyun Zhao","Jialu Song","Kaiqi Huang"],"pdf_url":"https://arxiv.org/pdf/2403.11486v1.pdf","comment":"35 pages, 7 figures"},{"id":"http://arxiv.org/abs/2403.12204v1","updated":"2024-03-18T19:40:16Z","published":"2024-03-18T19:40:16Z","title":"Information Compression in Dynamic Information Disclosure Games","summary":"  We consider a two-player dynamic information design problem between a\nprincipal and a receiver -- a game is played between the two agents on top of a\nMarkovian system controlled by the receiver's actions, where the principal\nobtains and strategically shares some information about the underlying system\nwith the receiver in order to influence their actions. In our setting, both\nplayers have long-term objectives, and the principal sequentially commits to\ntheir strategies instead of committing at the beginning. Further, the principal\ncannot directly observe the system state, but at every turn they can choose\nrandomized experiments to observe the system partially. The principal can share\ndetails about the experiments to the receiver. For our analysis we impose the\ntruthful disclosure rule: the principal is required to truthfully announce the\ndetails and the result of each experiment to the receiver immediately after the\nexperiment result is revealed. Based on the received information, the receiver\ntakes an action when its their turn, with the action influencing the state of\nthe underlying system. We show that there exist Perfect Bayesian equilibria in\nthis game where both agents play Canonical Belief Based (CBB) strategies using\na compressed version of their information, rather than full information, to\nchoose experiments (for the principal) or actions (for the receiver). We also\nprovide a backward inductive procedure to solve for an equilibrium in CBB\nstrategies.\n","authors":["Dengwang Tang","Vijay G. Subramanian"],"pdf_url":"https://arxiv.org/pdf/2403.12204v1.pdf","comment":"14 pages, 5 figures"},{"id":"http://arxiv.org/abs/2403.12195v1","updated":"2024-03-18T19:09:20Z","published":"2024-03-18T19:09:20Z","title":"PackIt! Gamified Rectangle Packing","summary":"  We present and analyze PackIt!, a turn-based game consisting of packing\nrectangles on an $n \\times n$ grid. PackIt! can be easily played on paper,\neither as a competitive two-player game or in \\emph{solitaire} fashion. On the\n$t$-th turn, a rectangle of area $t$ or $t+1$ must be placed in the grid. In\nthe two-player format of PackIt! whichever player places a rectangle last wins,\nwhereas the goal in the solitaire variant is to perfectly pack the $n \\times n$\ngrid. We analyze conditions for the existence of a perfect packing over $n\n\\times n$, then present an automated reasoning approach that allows finding\nperfect games of PackIt! up to $n = 50$ which includes a novel SAT-encoding\ntechnique of independent interest, and conclude by proving an NP-hardness\nresult.\n","authors":["Thomas Garrison","Marijn J. H. Heule","Bernardo Subercaseaux"],"pdf_url":"https://arxiv.org/pdf/2403.12195v1.pdf","comment":"28 pages, 10 figures, Submitted to Fun with Algorithms"},{"id":"http://arxiv.org/abs/2403.12183v1","updated":"2024-03-18T18:55:46Z","published":"2024-03-18T18:55:46Z","title":"Fragile Stable Matchings","summary":"  We show how fragile stable matchings are in a decentralized one-to-one\nmatching setting. The classical work of Roth and Vande Vate (1990) suggests\nsimple decentralized dynamics in which randomly-chosen blocking pairs match\nsuccessively. Such decentralized interactions guarantee convergence to a stable\nmatching. Our first theorem shows that, under mild conditions, any unstable\nmatching -- including a small perturbation of a stable matching -- can\nculminate in any stable matching through these dynamics. Our second theorem\nhighlights another aspect of fragility: stabilization may take a long time.\nEven in markets with a unique stable matching, where the dynamics always\nconverge to the same matching, decentralized interactions can require an\nexponentially long duration to converge. A small perturbation of a stable\nmatching may lead the market away from stability and involve a sizable\nproportion of mismatched participants for extended periods. Our results hold\nfor a broad class of dynamics.\n","authors":["Kirill Rudov"],"pdf_url":"https://arxiv.org/pdf/2403.12183v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12181v1","updated":"2024-03-18T18:52:04Z","published":"2024-03-18T18:52:04Z","title":"MAC Advice for Facility Location Mechanism Design","summary":"  Algorithms with predictions have attracted much attention in the last years\nacross various domains, including variants of facility location, as a way to\nsurpass traditional worst-case analyses. We study the $k$-facility location\nmechanism design problem, where the $n$ agents are strategic and might\nmisreport their location.\n  Unlike previous models, where predictions are for the $k$ optimal facility\nlocations, we receive $n$ predictions for the locations of each of the agents.\nHowever, these predictions are only \"mostly\" and \"approximately\" correct (or\nMAC for short) -- i.e., some $\\delta$-fraction of the predicted locations are\nallowed to be arbitrarily incorrect, and the remainder of the predictions are\nallowed to be correct up to an $\\varepsilon$-error. We make no assumption on\nthe independence of the errors. Can such predictions allow us to beat the\ncurrent best bounds for strategyproof facility location?\n  We show that the $1$-median (geometric median) of a set of points is\nnaturally robust under corruptions, which leads to an algorithm for\nsingle-facility location with MAC predictions. We extend the robustness result\nto a \"balanced\" variant of the $k$ facilities case. Without balancedness, we\nshow that robustness completely breaks down, even for the setting of $k=2$\nfacilities on a line. For this \"unbalanced\" setting, we devise a truthful\nrandom mechanism that outperforms the best known result of Lu et al. [2010],\nwhich does not use predictions. En route, we introduce the problem of \"second\"\nfacility location (when the first facility's location is already fixed). Our\nfindings on the robustness of the $1$-median and more generally $k$-medians may\nbe of independent interest, as quantitative versions of classic breakdown-point\nresults in robust statistics.\n","authors":["Zohar Barak","Anupam Gupta","Inbal Talgam-Cohen"],"pdf_url":"https://arxiv.org/pdf/2403.12181v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11811v1","updated":"2024-03-18T14:09:47Z","published":"2024-03-18T14:09:47Z","title":"A Simple 2-Approximation Algorithm For Minimum Manhattan Network Problem","summary":"  Given a n points in two dimensional space, a Manhattan Network G is a network\nthat connects all n points with either horizontal or vertical edges, with the\nproperty that for any two point in G should be connected by a Manhattan path\nand distance between this two points is equal to Manhattan Distance. The\nMinimum Manhattan Network problem is to find a Manhattan network with minimum\nnetwork length, i.e., summation of all line segment in network should be\nminimize. In this paper, we proposed a 2-approximation algorithm with time\ncomplexity O(|E|lgN) where |E| is the number of edges and N is the number of\nnodes. Using randomly generated datasets, we compare our result with the\noptimal one.\n","authors":["Md. Musfiqur Rahman Sanim","Safrunnesa Saira","Fatin Faiaz Ahsan","Rajon Bardhan","S. M. Ferdous"],"pdf_url":"https://arxiv.org/pdf/2403.11811v1.pdf","comment":"ARSSS International Conference, Dhaka, Bangladesh"},{"id":"http://arxiv.org/abs/2307.06212v3","updated":"2024-03-18T10:16:49Z","published":"2023-07-12T15:00:16Z","title":"Contract-Based Distributed Synthesis in Two-Objective Parity Games","summary":"  We present a novel method to compute $\\textit{assume-guarantee contracts}$ in\nnon-zerosum two-player games over finite graphs where each player has a\ndifferent $ \\omega $-regular winning condition. Given a game graph $G$ and two\nparity winning conditions $\\Phi_0$ and $\\Phi_1$ over $G$, we compute\n$\\textit{contracted strategy-masks}$ ($\\texttt{csm}$) $(\\Psi_{i},\\Phi_{i})$ for\neach Player $i$. Within a $\\texttt{csm}$, $\\Phi_{i}$ is a $\\textit{permissive\nstrategy template}$ which collects an infinite number of winning strategies for\nPlayer $i$ under the assumption that Player $1-i$ chooses any strategy from the\n$\\textit{permissive assumption template}$ $\\Psi_{i}$. The main feature of\n$\\texttt{csm}$'s is their power to $\\textit{fully decentralize all remaining\nstrategy choices}$ -- if the two player's $\\texttt{csm}$'s are compatible, they\nprovide a pair of new local specifications $\\Phi_0^\\bullet$ and\n$\\Phi_1^\\bullet$ such that Player $i$ can locally and fully independently\nchoose any strategy satisfying $\\Phi_i^\\bullet$ and the resulting strategy\nprofile is ensured to be winning in the original two-objective game\n$(G,\\Phi_0,\\Phi_1)$.\n  In addition, the new specifications $\\Phi_i^\\bullet$ are $\\textit{maximally\ncooperative}$, i.e., allow for the distributed synthesis of any cooperative\nsolution. Further, our algorithmic computation of $\\texttt{csm}$'s is complete\nand ensured to terminate.\n  We illustrate how the unique features of our synthesis framework effectively\naddress multiple challenges in the context of \\enquote{correct-by-design}\nlogical control software synthesis for cyber-physical systems and provide\nempirical evidence that our approach possess desirable structural and\ncomputational properties compared to state-of-the-art techniques.\n","authors":["Ashwani Anand","Satya Prakash Nayak","Anne-Kathrin Schmuck"],"pdf_url":"https://arxiv.org/pdf/2307.06212v3.pdf","comment":"HSCC 2024"}],"Information Theory":[{"id":"http://arxiv.org/abs/2403.11693v1","updated":"2024-03-18T11:48:02Z","published":"2024-03-18T11:48:02Z","title":"Beamforming Design for Semantic-Bit Coexisting Communication System","summary":"  Semantic communication (SemCom) is emerging as a key technology for future\nsixth-generation (6G) systems. Unlike traditional bit-level communication\n(BitCom), SemCom directly optimizes performance at the semantic level, leading\nto supe- rior communication efficiency. Nevertheless, the task-oriented nature\nof SemCom renders it challenging to completely replace BitCom. Consequently, it\nis desired to consider a semantic-bit coexisting communication system, where a\nbase station (BS) serves SemCom users (sem-users) and BitCom users (bit-users)\nsimultaneously. Such a system faces severe and heterogeneous inter-user\ninterference. In this context, this paper provides a new semantic-bit\ncoexisting communication framework and proposes a spatial beamforming scheme to\naccommodate both types of users. Specifically, we consider maximizing the\nsemantic rate for semantic users while ensuring the quality-of-service (QoS)\nrequirements for bit-users. Due to the intractability of obtaining the exact\nclosed-form expression of the semantic rate, a data driven method is first\napplied to attain an approximated expression via data fitting. With the\nresulting complex transcendental function, majorization minimization (MM) is\nadopted to convert the original formulated problem into a multiple-ratio\nproblem, which allows fractional programming (FP) to be used to further\ntransform the problem into an inhomogeneous quadratically constrained quadratic\nprograms (QCQP) problem. Solving the problem leads to a semi-closed form\nsolution with undetermined Lagrangian factors that can be updated by a fixed\npoint algorithm. Extensive simulation results demonstrate that the proposed\nbeamforming scheme significantly outperforms conventional beamforming\nalgorithms such as zero-forcing (ZF), maximum ratio transmission (MRT), and\nweighted minimum mean-square error (WMMSE).\n","authors":["Maojun Zhang","Guangxu Zhu","Richeng Jin","Xiaoming Chen","Qingjiang Shi","Caijun Zhong","Kaibing Huang"],"pdf_url":"https://arxiv.org/pdf/2403.11693v1.pdf","comment":"Submitted to IEEE for possible publication"},{"id":"http://arxiv.org/abs/2401.16414v2","updated":"2024-03-18T11:34:18Z","published":"2024-01-29T18:51:02Z","title":"A Causal Model for Quantifying Multipartite Classical and Quantum\n  Correlations","summary":"  We give an operational definition of information-theoretic resources within a\ngiven multipartite classical or quantum correlation. We present our causal\nmodel that serves as the source coding side of this correlation and introduce a\nnovel concept of resource rate. We argue that, beyond classical secrecy,\nadditional resources exist that are useful for the security of distributed\ncomputing problems, which can be captured by the resource rate. Furthermore, we\nestablish a relationship between resource rate and an extension of Shannon's\nlogarithmic information measure, namely, total correlation. Subsequently, we\npresent a novel quantum secrecy monotone and investigate a quantum hybrid key\ndistribution system as an extension of our causal model. Finally, we discuss\nsome connections to optimal transport (OT) problem.\n","authors":["Shuchan Wang","Gerhard Wunder"],"pdf_url":"https://arxiv.org/pdf/2401.16414v2.pdf","comment":"22 pages, 13 figures. Changes in v2: Appendix B added; some notations\n  changed"},{"id":"http://arxiv.org/abs/2403.11551v1","updated":"2024-03-18T08:03:02Z","published":"2024-03-18T08:03:02Z","title":"New Constructions of Reversible DNA Codes","summary":"  DNA codes have many applications, such as in data storage, DNA computing,\netc. Good DNA codes have large sizes and satisfy some certain constraints. In\nthis paper, we present a new construction method for reversible DNA codes. We\nshow that the DNA codes obtained using our construction method can satisfy some\ndesired constraints and the lower bounds of the sizes of some DNA codes are\nbetter than the known results. We also give new lower bounds on the sizes of\nsome DNA codes of lengths $80$, $96$ and $160$ for some fixed Hamming distance\n$d$.\n","authors":["Xueyan Chen","Whan-Hyuk Choi","Hongwei Liu"],"pdf_url":"https://arxiv.org/pdf/2403.11551v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11477v1","updated":"2024-03-18T04:52:11Z","published":"2024-03-18T04:52:11Z","title":"Span-Based Optimal Sample Complexity for Weakly Communicating and\n  General Average Reward MDPs","summary":"  We study the sample complexity of learning an $\\epsilon$-optimal policy in an\naverage-reward Markov decision process (MDP) under a generative model. For\nweakly communicating MDPs, we establish the complexity bound\n$\\tilde{O}(SA\\frac{H}{\\epsilon^2})$, where $H$ is the span of the bias function\nof the optimal policy and $SA$ is the cardinality of the state-action space.\nOur result is the first that is minimax optimal (up to log factors) in all\nparameters $S,A,H$ and $\\epsilon$, improving on existing work that either\nassumes uniformly bounded mixing times for all policies or has suboptimal\ndependence on the parameters. We further investigate sample complexity in\ngeneral (non-weakly-communicating) average-reward MDPs. We argue a new\ntransient time parameter $B$ is necessary, establish an\n$\\tilde{O}(SA\\frac{B+H}{\\epsilon^2})$ complexity bound, and prove a matching\n(up to log factors) minimax lower bound. Both results are based on reducing the\naverage-reward MDP to a discounted MDP, which requires new ideas in the general\nsetting. To establish the optimality of this reduction, we develop improved\nbounds for $\\gamma$-discounted MDPs, showing that\n$\\tilde{\\Omega}\\left(SA\\frac{H}{(1-\\gamma)^2\\epsilon^2}\\right)$ samples suffice\nto learn an $\\epsilon$-optimal policy in weakly communicating MDPs under the\nregime that $\\gamma\\geq 1-1/H$, and\n$\\tilde{\\Omega}\\left(SA\\frac{B+H}{(1-\\gamma)^2\\epsilon^2}\\right)$ samples\nsuffice in general MDPs when $\\gamma\\geq 1-\\frac{1}{B+H}$. Both these results\ncircumvent the well-known lower bound of\n$\\tilde{\\Omega}\\left(SA\\frac{1}{(1-\\gamma)^3\\epsilon^2}\\right)$ for arbitrary\n$\\gamma$-discounted MDPs. Our analysis develops upper bounds on certain\ninstance-dependent variance parameters in terms of the span and transient time\nparameters. The weakly communicating bounds are tighter than those based on the\nmixing time or diameter of the MDP and may be of broader use.\n","authors":["Matthew Zurek","Yudong Chen"],"pdf_url":"https://arxiv.org/pdf/2403.11477v1.pdf","comment":"42 pages, 2 figures; this article supersedes arXiv:2311.13469"},{"id":"http://arxiv.org/abs/2403.11436v1","updated":"2024-03-18T03:12:40Z","published":"2024-03-18T03:12:40Z","title":"Deep Holes of Twisted Reed-Solomon Codes","summary":"  The deep holes of a linear code are the vectors that achieve the maximum\nerror distance to the code. There has been extensive research on the topic of\ndeep holes in Reed-Solomon codes. As a generalization of Reed-Solomon codes, we\ninvestigate the problem of deep holes of a class of twisted Reed-Solomon codes\nin this paper. The covering radius and a standard class of deep holes of\ntwisted Reed-Solomon codes ${\\rm TRS}_k(\\mathcal{A}, \\theta)$ are obtained for\na general evaluation set $\\mathcal{A} \\subseteq \\mathbb{F}_q$. Furthermore, we\nconsider the problem of determining all deep holes of the full-length twisted\nReed-Solomon codes ${\\rm TRS}_k(\\mathbb{F}_q, \\theta)$. Specifically, we prove\nthat there are no other deep holes of ${\\rm TRS}_k(\\mathbb{F}_q, \\theta)$ for\n$\\frac{3q-8}{4} \\leq k\\leq q-4$ when $q$ is even, and $\\frac{3q+2\\sqrt{q}-7}{4}\n\\leq k\\leq q-4$ when $q$ is odd. We also completely determine their deep holes\nfor $q-3 \\leq k \\leq q-1$.\n","authors":["Weijun Fang","Jingke Xu","Ruiqi Zhu"],"pdf_url":"https://arxiv.org/pdf/2403.11436v1.pdf","comment":"This work has been submitted to the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2403.11433v1","updated":"2024-03-18T03:07:09Z","published":"2024-03-18T03:07:09Z","title":"Measuring Quantum Information Leakage Under Detection Threat","summary":"  Gentle quantum leakage is proposed as a measure of information leakage to\narbitrary eavesdroppers that aim to avoid detection. Gentle (also sometimes\nreferred to as weak or non-demolition) measurements are used to encode the\ndesire of the eavesdropper to evade detection. The gentle quantum leakage meets\nimportant axioms proposed for measures of information leakage including\npositivity, independence, and unitary invariance. Global depolarizing noise, an\nimportant family of physical noise in quantum devices, is shown to reduce\ngentle quantum leakage (and hence can be used as a mechanism to ensure privacy\nor security). A lower bound for the gentle quantum leakage based on asymmetric\napproximate cloning is presented. This lower bound relates information leakage\nto mutual incompatibility of quantum states. A numerical example, based on the\nencoding in the celebrated BB84 quantum key distribution algorithm, is used to\ndemonstrate the results.\n","authors":["Farhad Farokhi","Sejeong Kim"],"pdf_url":"https://arxiv.org/pdf/2403.11433v1.pdf","comment":null}],"Operating Systems":[{"id":"http://arxiv.org/abs/2403.11805v1","updated":"2024-03-18T14:03:23Z","published":"2024-03-18T14:03:23Z","title":"LLM as a System Service on Mobile Devices","summary":"  Being more powerful and intrusive into user-device interactions, LLMs are\neager for on-device execution to better preserve user privacy. In this work, we\npropose a new paradigm of mobile AI: LLM as a system service on mobile devices\n(LLMaaS). Unlike traditional DNNs that execute in a stateless manner, such a\nsystem service is stateful: LLMs execution often needs to maintain persistent\nstates (mainly KV cache) across multiple invocations. To minimize the LLM\ncontext switching overhead under tight device memory budget, this work presents\nLLMS, which decouples the memory management of app and LLM contexts with a key\nidea of fine-grained, chunk-wise, globally-optimized KV cache compression and\nswapping. By fully leveraging KV cache's unique characteristics, it proposes\nthree novel techniques: (1) Tolerance-Aware Compression: it compresses chunks\nbased on their measured accuracy tolerance to compression. (2) IO-Recompute\nPipelined Loading: it introduces recompute to swapping-in for acceleration. (3)\nChunk Lifecycle Management: it optimizes the memory activities of chunks with\nan ahead-of-time swapping-out and an LCTRU (Least Compression-Tolerable and\nRecently-Used) queue based eviction. In evaluations conducted on\nwell-established traces and various edge devices, \\sys reduces context\nswitching latency by up to 2 orders of magnitude when compared to competitive\nbaseline solutions.\n","authors":["Wangsong Yin","Mengwei Xu","Yuanchun Li","Xuanzhe Liu"],"pdf_url":"https://arxiv.org/pdf/2403.11805v1.pdf","comment":"Technical Report"}]},"2024-03-17T00:00:00Z":{"Distributed, Parallel, and Cluster Computing":[{"id":"http://arxiv.org/abs/2403.11366v1","updated":"2024-03-17T23:02:04Z","published":"2024-03-17T23:02:04Z","title":"JORA: JAX Tensor-Parallel LoRA Library for Retrieval Augmented\n  Fine-Tuning","summary":"  The scaling of Large Language Models (LLMs) for retrieval-based tasks,\nparticularly in Retrieval Augmented Generation (RAG), faces significant memory\nconstraints, especially when fine-tuning extensive prompt sequences. Current\nopen-source libraries support full-model inference and fine-tuning across\nmultiple GPUs but fall short of accommodating the efficient parameter\ndistribution required for retrieved context. Addressing this gap, we introduce\na novel framework for PEFT-compatible fine-tuning of Llama-2 models, leveraging\ndistributed training. Our framework uniquely utilizes JAX's just-in-time (JIT)\ncompilation and tensor-sharding for efficient resource management, thereby\nenabling accelerated fine-tuning with reduced memory requirements. This\nadvancement significantly improves the scalability and feasibility of\nfine-tuning LLMs for complex RAG applications, even on systems with limited GPU\nresources. Our experiments show more than 12x improvement in runtime compared\nto Hugging Face/DeepSpeed implementation with four GPUs while consuming less\nthan half the VRAM per GPU. Our library will be open-sourced in due course.\n","authors":["Anique Tahir","Lu Cheng","Huan Liu"],"pdf_url":"https://arxiv.org/pdf/2403.11366v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11259v1","updated":"2024-03-17T16:23:00Z","published":"2024-03-17T16:23:00Z","title":"A learning-based solution approach to the application placement problem\n  in mobile edge computing under uncertainty","summary":"  Placing applications in mobile edge computing servers presents a complex\nchallenge involving many servers, users, and their requests. Existing\nalgorithms take a long time to solve high-dimensional problems with significant\nuncertainty scenarios. Therefore, an efficient approach is required to maximize\nthe quality of service while considering all technical constraints. One of\nthese approaches is machine learning, which emulates optimal solutions for\napplication placement in edge servers. Machine learning models are expected to\nlearn how to allocate user requests to servers based on the spatial positions\nof users and servers. In this study, the problem is formulated as a two-stage\nstochastic programming. A sufficient amount of training records is generated by\nvarying parameters such as user locations, their request rates, and solving the\noptimization model. Then, based on the distance features of each user from the\navailable servers and their request rates, machine learning models generate\ndecision variables for the first stage of the stochastic optimization model,\nwhich is the user-to-server request allocation, and are employed as independent\ndecision agents that reliably mimic the optimization model. Support Vector\nMachines (SVM) and Multi-layer Perceptron (MLP) are used in this research to\nachieve practical decisions from the stochastic optimization models. The\nperformance of each model has shown an execution effectiveness of over 80%.\nThis research aims to provide a more efficient approach for tackling\nhigh-dimensional problems and scenarios with uncertainties in mobile edge\ncomputing by leveraging machine learning models for optimal decision-making in\nrequest allocation to edge servers. These results suggest that machine-learning\nmodels can significantly improve solution times compared to conventional\napproaches.\n","authors":["Taha-Hossein Hejazi","Zahra Ghadimkhani","Arezoo Borji"],"pdf_url":"https://arxiv.org/pdf/2403.11259v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11238v1","updated":"2024-03-17T14:53:38Z","published":"2024-03-17T14:53:38Z","title":"JUMBO: Fully Asynchronous BFT Consensus Made Truly Scalable","summary":"  Recent progresses in asynchronous Byzantine fault-tolerant (BFT) consensus,\ne.g. Dumbo-NG (CCS' 22) and Tusk (EuroSys' 22), show promising performance\nthrough decoupling transaction dissemination and block agreement. However, when\nexecuted with a larger number $n$ of nodes, like several hundreds, they would\nsuffer from significant degradation in performance. Their dominating\nscalability bottleneck is the huge authenticator complexity: each node has to\nmulticast $\\bigO(n)$ quorum certificates (QCs) and subsequently verify them for\neach block.\n  This paper systematically investigates and resolves the above scalability\nissue. We first propose a signature-free asynchronous BFT consensus FIN-NG that\nadapts a recent signature-free asynchronous common subset protocol FIN (CCS'\n23) into the state-of-the-art framework of concurrent broadcast and agreement.\nThe liveness of FIN-NG relies on our non-trivial redesign of FIN's multi-valued\nvalidated Byzantine agreement towards achieving optimal quality. FIN-NG greatly\nimproves the performance of FIN and already outperforms Dumbo-NG in most\ndeployment settings. To further overcome the scalability limit of FIN-NG due to\n$\\bigO(n^3)$ messages, we propose JUMBO, a scalable instantiation of Dumbo-NG,\nwith only $\\bigO(n^2)$ complexities for both authenticators and messages. We\nuse various aggregation and dispersal techniques for QCs to significantly\nreduce the authenticator complexity of original Dumbo-NG implementations by up\nto $\\bigO(n^2)$ orders. We also propose a ``fairness'' patch for JUMBO, thus\npreventing a flooding adversary from controlling an overwhelming portion of\ntransactions in its output.\n","authors":["Hao Cheng","Yuan Lu","Zhenliang Lu","Qiang Tang","Yuxuan Zhang","Zhenfeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.11238v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11221v1","updated":"2024-03-17T13:45:28Z","published":"2024-03-17T13:45:28Z","title":"Lion: Minimizing Distributed Transactions through Adaptive Replica\n  Provision (Extended Version)","summary":"  Distributed transaction processing often involves multiple rounds of\ncross-node communications, and therefore tends to be slow. To improve\nperformance, existing approaches convert distributed transactions into\nsingle-node transactions by either migrating co-accessed partitions onto the\nsame nodes or establishing a super node housing replicas of the entire\ndatabase. However, migration-based methods might cause transactions to be\nblocked due to waiting for data migration, while the super node can become a\nbottleneck. In this paper, we present Lion, a novel transaction processing\nprotocol that utilizes partition-based replication to reduce the occurrence of\ndistributed transactions. Lion aims to assign a node with one replica from each\npartition involved in a given transaction's read or write operations. To ensure\nsuch a node is available, we propose an adaptive replica provision mechanism,\nenhanced with an LSTM-based workload prediction algorithm, to determine the\nappropriate node for locating replicas of co-accessed partitions. The\nadaptation of replica placement is conducted preemptively and asynchronously,\nthereby minimizing its impact on performance. By employing this adaptive\nreplica placement strategy, we ensure that the majority of transactions can be\nefficiently processed on a single node without additional overhead. Only a\nsmall fraction of transactions will need to be treated as regular distributed\ntransactions when such a node is unavailable. Consequently, Lion effectively\nminimizes distributed transactions while avoiding any disruption caused by data\nmigration or the creation of a super node. We conduct extensive experiments to\ncompare Lion against various transaction processing protocols. The results show\nthat Lion achieves up to 2.7x higher throughput and 76.4% better scalability\nagainst these state-of-the-art approaches.\n","authors":["Qiushi Zheng","Zhanhao Zhao","Wei Lu","Chang Yao","Yuxing Chen","Anqun Pan","Xiaoyong Du"],"pdf_url":"https://arxiv.org/pdf/2403.11221v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11204v1","updated":"2024-03-17T13:06:29Z","published":"2024-03-17T13:06:29Z","title":"Partitioned Neural Network Training via Synthetic Intermediate Labels","summary":"  The proliferation of extensive neural network architectures, particularly\ndeep learning models, presents a challenge in terms of resource-intensive\ntraining. GPU memory constraints have become a notable bottleneck in training\nsuch sizable models. Existing strategies, including data parallelism, model\nparallelism, pipeline parallelism, and fully sharded data parallelism, offer\npartial solutions. Model parallelism, in particular, enables the distribution\nof the entire model across multiple GPUs, yet the ensuing data communication\nbetween these partitions slows down training. Additionally, the substantial\nmemory overhead required to store auxiliary parameters on each GPU compounds\ncomputational demands. Instead of using the entire model for training, this\nstudy advocates partitioning the model across GPUs and generating synthetic\nintermediate labels to train individual segments. These labels, produced\nthrough a random process, mitigate memory overhead and computational load. This\napproach results in a more efficient training process that minimizes data\ncommunication while maintaining model accuracy. To validate this method, a\n6-layer fully connected neural network is partitioned into two parts and its\nperformance is assessed on the extended MNIST dataset. Experimental results\nindicate that the proposed approach achieves similar testing accuracies to\nconventional training methods, while significantly reducing memory and\ncomputational requirements. This work contributes to mitigating the\nresource-intensive nature of training large neural networks, paving the way for\nmore efficient deep learning model development.\n","authors":["Cevat Volkan Karadağ","Nezih Topaloğlu"],"pdf_url":"https://arxiv.org/pdf/2403.11204v1.pdf","comment":"12 pages, 10 figures"}],"Performance":[{"id":"http://arxiv.org/abs/2403.11097v1","updated":"2024-03-17T05:35:09Z","published":"2024-03-17T05:35:09Z","title":"Secrecy Outage Probability Analysis for Downlink RIS-NOMA Networks with\n  On-Off Control","summary":"  Reconfigurable intelligent surface (RIS) has been regarded as a promising\ntechnology since it has ability to create the favorable channel conditions.\nThis paper investigates the secure communications of RIS assisted\nnon-orthogonal multiple access (NOMA) networks, where both external and\ninternal eavesdropping scenarios are taken into consideration. More\nspecifically, novel approximate and asymptotic expressions of secrecy outage\nprobability (SOP) for the k-th legitimate user (LU) are derived by invoking\nimperfect successive interference cancellation (ipSIC) and perfect successive\ninterference cancellation (pSIC). To characterize the secrecy performance of\nRIS-NOMA networks, the diversity order of the k-th LU with ipSIC/pSIC is\nobtained in the high signal-to-noise ratio region. The secrecy system\nthroughput of RIS-NOMA networks is discussed in delay-limited transmission\nmode. Numerical results are presented to verify theoretical analysis that: i)\nThe SOP of RIS-NOMA networks is superior to that of RIS assisted orthogonal\nmultiple access (OMA) and conventional cooperative communication schemes; ii)\nAs the number of reflecting elements increases, the RIS-NOMA networks are\ncapable of achieving the enhanced secrecy performance; and iii) The RIS-NOMA\nnetworks have better secrecy system throughput than that of RIS-OMA networks\nand conventional cooperative communication schemes.\n","authors":["Yingjie Pei","Xinwei Yue","Wenqiang Yi","Yuanwei Liu","Xuehua Li","Zhiguo Ding"],"pdf_url":"https://arxiv.org/pdf/2403.11097v1.pdf","comment":"This paper has been published in IEEE Transactions on Vehicular\n  Technology"}],"Databases":[{"id":"http://arxiv.org/abs/2403.11361v1","updated":"2024-03-17T22:38:08Z","published":"2024-03-17T22:38:08Z","title":"Graph Theory for Consent Management: A New Approach for Complex Data\n  Flows","summary":"  Through legislation and technical advances users gain more control over how\ntheir data is processed, and they expect online services to respect their\nprivacy choices and preferences. However, data may be processed for many\ndifferent purposes by several layers of algorithms that create complex data\nworkflows. To date, there is no existing approach to automatically satisfy\nfine-grained privacy constraints of a user in a way which optimises the service\nprovider's gains from processing. In this article, we propose a solution to\nthis problem by modelling a data flow as a graph. User constraints and\nprocessing purposes are pairs of vertices which need to be disconnected in this\ngraph. In general, this problem is NP-hard, thus, we propose several heuristics\nand algorithms. We discuss the optimality versus efficiency of our algorithms\nand evaluate them using synthetically generated data. On the practical side,\nour algorithms can provide nearly optimal solutions for tens of constraints and\ngraphs of thousands of nodes, in a few seconds.\n","authors":["Dorota Filipczuk","Enrico H. Gerding","George Konstantinidis"],"pdf_url":"https://arxiv.org/pdf/2403.11361v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11246v1","updated":"2024-03-17T15:39:18Z","published":"2024-03-17T15:39:18Z","title":"Exploring Distance Query Processing in Edge Computing Environments","summary":"  In the context of changing travel behaviors and the expanding user base of\nGeographic Information System (GIS) services, conventional centralized\narchitectures responsible for handling shortest distance queries are facing\nincreasing challenges, such as heightened load pressure and longer response\ntimes. To mitigate these concerns, this study is the first to develop an edge\ncomputing framework specially tailored for processing distance queries. In\nconjunction with this innovative system, we have developed a straightforward,\nyet effective, labeling technique termed Border Labeling. Furthermore, we have\ndevised and implemented a range of query strategies intended to capitalize on\nthe capabilities of the edge computing infrastructure. Our experiments\ndemonstrate that our solution surpasses other methods in terms of both indexing\ntime and query speed across various road network datasets. The empirical\nevidence from our experiments supports the claim that our edge computing\narchitecture significantly reduces the latency encountered by end-users, thus\nmarkedly decreasing their waiting times.\n","authors":["Xiubo Zhang","Yujie He","Ye Li","Yan Li","Zijie Zhou","Dongyao Wei"," Ryan"],"pdf_url":"https://arxiv.org/pdf/2403.11246v1.pdf","comment":"15pages, 18(sub)pages"},{"id":"http://arxiv.org/abs/2403.11221v1","updated":"2024-03-17T13:45:28Z","published":"2024-03-17T13:45:28Z","title":"Lion: Minimizing Distributed Transactions through Adaptive Replica\n  Provision (Extended Version)","summary":"  Distributed transaction processing often involves multiple rounds of\ncross-node communications, and therefore tends to be slow. To improve\nperformance, existing approaches convert distributed transactions into\nsingle-node transactions by either migrating co-accessed partitions onto the\nsame nodes or establishing a super node housing replicas of the entire\ndatabase. However, migration-based methods might cause transactions to be\nblocked due to waiting for data migration, while the super node can become a\nbottleneck. In this paper, we present Lion, a novel transaction processing\nprotocol that utilizes partition-based replication to reduce the occurrence of\ndistributed transactions. Lion aims to assign a node with one replica from each\npartition involved in a given transaction's read or write operations. To ensure\nsuch a node is available, we propose an adaptive replica provision mechanism,\nenhanced with an LSTM-based workload prediction algorithm, to determine the\nappropriate node for locating replicas of co-accessed partitions. The\nadaptation of replica placement is conducted preemptively and asynchronously,\nthereby minimizing its impact on performance. By employing this adaptive\nreplica placement strategy, we ensure that the majority of transactions can be\nefficiently processed on a single node without additional overhead. Only a\nsmall fraction of transactions will need to be treated as regular distributed\ntransactions when such a node is unavailable. Consequently, Lion effectively\nminimizes distributed transactions while avoiding any disruption caused by data\nmigration or the creation of a super node. We conduct extensive experiments to\ncompare Lion against various transaction processing protocols. The results show\nthat Lion achieves up to 2.7x higher throughput and 76.4% better scalability\nagainst these state-of-the-art approaches.\n","authors":["Qiushi Zheng","Zhanhao Zhao","Wei Lu","Chang Yao","Yuxing Chen","Anqun Pan","Xiaoyong Du"],"pdf_url":"https://arxiv.org/pdf/2403.11221v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11099v1","updated":"2024-03-17T06:07:23Z","published":"2024-03-17T06:07:23Z","title":"Wait to be Faster: a Smart Pooling Framework for Dynamic Ridesharing","summary":"  Ridesharing services, such as Uber or Didi, have attracted considerable\nattention in recent years due to their positive impact on environmental\nprotection and the economy. Existing studies require quick responses to orders,\nwhich lack the flexibility to accommodate longer wait times for better grouping\nopportunities. In this paper, we address a NP-hard ridesharing problem, called\nMinimal Extra Time RideSharing (METRS), which balances waiting time and group\nquality (i.e., detour time) to improve riders' satisfaction. To tackle this\nproblem, we propose a novel approach called WATTER (WAit To be fasTER), which\nleverages an order pooling management algorithm allowing orders to wait until\nthey can be matched with suitable groups. The key challenge is to customize the\nextra time threshold for each order by reducing the original optimization\nobjective into a convex function of threshold, thus offering a theoretical\nguarantee to be optimized efficiently. We model the dispatch process using a\nMarkov Decision Process (MDP) with a carefully designed value function to learn\nthe threshold. Through extensive experiments on three real datasets, we\ndemonstrate the efficiency and effectiveness of our proposed approaches.\n","authors":["Xiaoyao Zhong","Jiabao Jin","Peng Cheng","Wangze Ni","Libin Zheng","Lei Chen","Xuemin Lin"],"pdf_url":"https://arxiv.org/pdf/2403.11099v1.pdf","comment":"IEEE ICDE 2024"},{"id":"http://arxiv.org/abs/2403.11088v1","updated":"2024-03-17T04:44:48Z","published":"2024-03-17T04:44:48Z","title":"Programming Frameworks for Differential Privacy","summary":"  Many programming frameworks have been introduced to support the development\nof differentially private software applications. In this chapter, we survey\nsome of the conceptual ideas underlying these frameworks in a way that we hope\nwill be helpful for both practitioners and researchers. For practitioners, the\nsurvey can provide a starting point for understanding what features may be\nvaluable when selecting a programming framework. For researchers, it can help\norganize existing work in a unified way and provide context for understanding\nnew features in future frameworks.\n","authors":["Marco Gaboardi","Michael Hay","Salil Vadhan"],"pdf_url":"https://arxiv.org/pdf/2403.11088v1.pdf","comment":"To appear as a chapter in the book \"Differential Privacy for\n  Artificial Intelligence,\" edited by Ferdinando Fioretto and Pascal van\n  Hentenryck and to be published by now publishers"}],"Computer Science and Game Theory":[{"id":"http://arxiv.org/abs/2403.11345v1","updated":"2024-03-17T21:11:55Z","published":"2024-03-17T21:11:55Z","title":"Independent RL for Cooperative-Competitive Agents: A Mean-Field\n  Perspective","summary":"  We address in this paper Reinforcement Learning (RL) among agents that are\ngrouped into teams such that there is cooperation within each team but\ngeneral-sum (non-zero sum) competition across different teams. To develop an RL\nmethod that provably achieves a Nash equilibrium, we focus on a\nlinear-quadratic structure. Moreover, to tackle the non-stationarity induced by\nmulti-agent interactions in the finite population setting, we consider the case\nwhere the number of agents within each team is infinite, i.e., the mean-field\nsetting. This results in a General-Sum LQ Mean-Field Type Game (GS-MFTGs). We\ncharacterize the Nash equilibrium (NE) of the GS-MFTG, under a standard\ninvertibility condition. This MFTG NE is then shown to be $\\mathcal{O}(1/M)$-NE\nfor the finite population game where $M$ is a lower bound on the number of\nagents in each team. These structural results motivate an algorithm called\nMulti-player Receding-horizon Natural Policy Gradient (MRPG), where each team\nminimizes its cumulative cost independently in a receding-horizon manner.\nDespite the non-convexity of the problem, we establish that the resulting\nalgorithm converges to a global NE through a novel problem decomposition into\nsub-problems using backward recursive discrete-time Hamilton-Jacobi-Isaacs\n(HJI) equations, in which independent natural policy gradient is shown to\nexhibit linear convergence under time-independent diagonal dominance.\nExperiments illuminate the merits of this approach in practice.\n","authors":["Muhammad Aneeq uz Zaman","Alec Koppel","Mathieu Laurière","Tamer Başar"],"pdf_url":"https://arxiv.org/pdf/2403.11345v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.14510v3","updated":"2024-03-17T21:10:24Z","published":"2023-12-22T08:18:25Z","title":"Strategic Bidding Wars in On-chain Auctions","summary":"  The Ethereum block-building process has changed significantly since the\nemergence of Proposer-Builder Separation. Validators access blocks through a\nmarketplace, where block builders bid for the right to construct the block and\nearn MEV (Maximal Extractable Value) rewards in an on-chain competition, known\nas the MEV-boost auction. While more than 90% of blocks are currently built via\nMEV-Boost, trade-offs between builders' strategic behaviors and auction design\nremain poorly understood. In this paper we address this gap. We introduce a\ngame-theoretic model for MEV-Boost auctions and use simulations to study\ndifferent builders' bidding strategies observed in practice. We study various\nstrategic interactions and auction setups and evaluate how the interplay\nbetween critical elements such as access to MEV opportunities and improved\nconnectivity to relays impact bidding performance. Our results demonstrate the\nimportance of latency on the effectiveness of builders' strategies and the\noverall auction outcome from the proposer's perspective.\n","authors":["Fei Wu","Thomas Thiery","Stefanos Leonardos","Carmine Ventre"],"pdf_url":"https://arxiv.org/pdf/2312.14510v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11307v1","updated":"2024-03-17T19:05:40Z","published":"2024-03-17T19:05:40Z","title":"An upper bound of the mutation probability in the genetic algorithm for\n  general 0-1 knapsack problem","summary":"  As an important part of genetic algorithms (GAs), mutation operators is\nwidely used in evolutionary algorithms to solve $\\mathcal{NP}$-hard problems\nbecause it can increase the population diversity of individual. Due to\nlimitations in mathematical tools, the mutation probability of the mutation\noperator is primarily empirically set in practical applications.\n  In this paper, we propose a novel reduction method for the 0-1 knapsack\nproblem(0-1 KP) and an improved mutation operator (IMO) based on the assumption\n$\\mathcal{NP}\\neq\\mathcal{P}$, along with the utilization of linear relaxation\ntechniques and a recent result by Dey et al. (Math. Prog., pp 569-587, 2022).\nWe employ this method to calculate an upper bound of the mutation probability\nin general instances of the 0-1 KP, and construct an instance where the\nmutation probability does not tend towards 0 as the problem size increases.\nFinally, we prove that the probability of the IMO hitting the optimal solution\nwithin only a single iteration in large-scale instances is superior to that of\nthe traditional mutation operator.\n","authors":["Yang Yang"],"pdf_url":"https://arxiv.org/pdf/2403.11307v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11267v1","updated":"2024-03-17T16:43:15Z","published":"2024-03-17T16:43:15Z","title":"Barely Random Algorithms for Metrical Task Systems","summary":"  We consider metrical task systems on general metric spaces with $n$ points,\nand show that any fully randomized algorithm can be turned into a randomized\nalgorithm that uses only $2\\log n$ random bits, and achieves the same\ncompetitive ratio up to a factor $2$. This provides the first order-optimal\nbarely random algorithms for metrical task systems, i.e. which use a number of\nrandom bits that does not depend on the number of requests addressed to the\nsystem. We put forward an equivalent view that we call collective metrical task\nsystems where $k$ agents in a metrical task system team up, and suffer the\naverage cost paid by each agent. Our results imply that such team can be\n$O(\\log n^2)$-competitive, as soon as $k\\geq n^2$ (in comparison, a single\nagent is $\\Omega(n)$-competitive at best). We discuss implications on various\naspects of online decision making such as: distributed systems, transaction\ncosts, and advice complexity, suggesting broad applicability.\n","authors":["Romain Cosson","Laurent Massoulié"],"pdf_url":"https://arxiv.org/pdf/2403.11267v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11054v1","updated":"2024-03-17T01:51:09Z","published":"2024-03-17T01:51:09Z","title":"A Novel Mutual Insurance Model for Hedging Against Cyber Risks in Power\n  Systems Deploying Smart Technologies","summary":"  In this paper, a novel cyber-insurance model design is proposed based on\nsystem risk evaluation with smart technology applications. The cyber insurance\npolicy for power systems is tailored via cyber risk modeling, reliability\nimpact analysis, and insurance premium calculation. A stochastic Epidemic\nNetwork Model is developed to evaluate the cyber risk by propagating\ncyberattacks among graphical vulnerabilities. Smart technologies deployed in\nrisk modeling include smart monitoring and job thread assignment. Smart\nmonitoring boosts the substation availability against cyberattacks with\npreventive and corrective measures. The job thread assignment solution reduces\nthe execution failures by distributing the control and monitoring tasks to\nmultiple threads. Reliability assessment is deployed to estimate load losses\nconvertible to monetary losses. These monetary losses would be shared through a\nmutual insurance plan. To ensure a fair distribution of indemnity, a new\nShapley mutual insurance principle is devised. Effectiveness of the proposed\nShapley mutual insurance design is validated via case studies. The Shapley\npremium is compared with existent premium designs. It is shown that the Shapley\npremium has high indemnity levels closer to those of Tail Conditional\nExpectation premium. Meanwhile, the Shapley premium is nearly as affordable as\nthe coalitional premium and keeps a relatively low insolvency probability.\n","authors":["Pikkin Lau","Lingfeng Wang","Wei Wei","Zhaoxi Liu","Chee-Wooi Ten"],"pdf_url":"https://arxiv.org/pdf/2403.11054v1.pdf","comment":"Power system reliability, cyber-insurance, power system security,\n  cyber-physical systems, cyber risk modeling, actuarial design, tail risk"}],"Information Theory":[{"id":"http://arxiv.org/abs/2403.11272v1","updated":"2024-03-17T17:01:25Z","published":"2024-03-17T17:01:25Z","title":"Interference Cancellation for OTFS-Based Over-the-Air Computation","summary":"  This paper investigates over-the-air computation (AirComp) in the context of\nmultiple-access time-varying multipath channels. We focus on a scenario where\ndevices with high mobility transmit their sensing data to a fusion center (FC)\nfor averaging. To combat the time-varying channel and Doppler effect, each\ndevice adopts orthogonal time frequency space (OTFS) modulation. After signals\nare received by the FC, the aggregated data undergoes demodulation and\nestimation within the delay-Doppler domain. We leverage the mean squared error\n(MSE) as a metric for the computational error of OTFS-based AirComp. We then\nderive the optimal transmit power at each device and signal scaling factor at\nFC for minimizing MSE. Notably, the performance of OTFS-based AirComp is not\nonly affected by the noise but also by the inter-symbol interference and\ninter-link interference arising from the multipath channel. To counteract the\ninterference-induced computational errors, we incorporate zero-padding\n(ZP)-assisted OTFS into AirComp and propose algorithms for interference\ncancellation. Numerical results underscore the enhanced performance of\nZP-assisted OTFS-based AirComp over naive OTFS-based AirComp.\n","authors":["Xinyu Huang","Henrik Hellstrom","Carlo Fischione"],"pdf_url":"https://arxiv.org/pdf/2403.11272v1.pdf","comment":"6 pages, 4 figures. Accepted by IEEE ICC 2024 Workshop"},{"id":"http://arxiv.org/abs/2403.11187v1","updated":"2024-03-17T12:15:56Z","published":"2024-03-17T12:15:56Z","title":"Task-Based Quantizer Design for Sensing With Random Signals","summary":"  In integrated sensing and communication (ISAC) systems, random signaling is\nused to convey useful information as well as sense the environment. Such\nrandomness poses challenges in various components in sensing signal processing.\nIn this paper, we investigate quantizer design for sensing in ISAC systems.\nUnlike quantizers for channel estimation in massive multiple-input-multiple-out\n(MIMO) communication systems, sensing in ISAC systems needs to deal with random\nnonorthogonal transmitted signals rather than a fixed orthogonal pilot.\nConsidering sensing performance and hardware implementation, we focus on\ntask-based hardware-limited quantization with spatial analog combining. We\npropose two strategies of quantizer optimization, i.e., data-dependent (DD) and\ndata-independent (DI). The former achieves optimized sensing performance with\nhigh implementation overhead. To reduce hardware complexity, the latter\noptimizes the quantizer with respect to the random signal from a stochastic\nperspective. We derive the optimal quantizers for both strategies and formulate\nan algorithm based on sample average approximation (SAA) to solve the\noptimization in the DI strategy. Numerical results show that the optimized\nquantizers outperform digital-only quantizers in terms of sensing performance.\nAdditionally, the DI strategy, despite its lower computational complexity\ncompared to the DD strategy, achieves near-optimal sensing performance.\n","authors":["Hang Ruan","Fan Liu"],"pdf_url":"https://arxiv.org/pdf/2403.11187v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11175v1","updated":"2024-03-17T11:23:51Z","published":"2024-03-17T11:23:51Z","title":"Prior-dependent analysis of posterior sampling reinforcement learning\n  with function approximation","summary":"  This work advances randomized exploration in reinforcement learning (RL) with\nfunction approximation modeled by linear mixture MDPs. We establish the first\nprior-dependent Bayesian regret bound for RL with function approximation; and\nrefine the Bayesian regret analysis for posterior sampling reinforcement\nlearning (PSRL), presenting an upper bound of ${\\mathcal{O}}(d\\sqrt{H^3 T \\log\nT})$, where $d$ represents the dimensionality of the transition kernel, $H$ the\nplanning horizon, and $T$ the total number of interactions. This signifies a\nmethodological enhancement by optimizing the $\\mathcal{O}(\\sqrt{\\log T})$\nfactor over the previous benchmark (Osband and Van Roy, 2014) specified to\nlinear mixture MDPs. Our approach, leveraging a value-targeted model learning\nperspective, introduces a decoupling argument and a variance reduction\ntechnique, moving beyond traditional analyses reliant on confidence sets and\nconcentration inequalities to formalize Bayesian regret bounds more\neffectively.\n","authors":["Yingru Li","Zhi-Quan Luo"],"pdf_url":"https://arxiv.org/pdf/2403.11175v1.pdf","comment":"Published in the 27th International Conference on Artificial\n  Intelligence and Statistics (AISTATS)"},{"id":"http://arxiv.org/abs/1911.11246v2","updated":"2024-03-17T10:10:02Z","published":"2019-11-25T21:42:19Z","title":"The mean and variance of the reciprocal merit factor of four classes of\n  binary sequences","summary":"  The merit factor of a $\\{-1, 1\\}$ binary sequence measures the collective\nsmallness of its non-trivial aperiodic autocorrelations. Binary sequences with\nlarge merit factor are important in digital communications because they allow\nthe efficient separation of signals from noise. It is a longstanding open\nquestion whether the maximum merit factor is asymptotically unbounded and, if\nso, what is its limiting value. Attempts to answer this question over almost\nsixty years have identified certain classes of binary sequences as particularly\nimportant: skew-symmetric sequences, symmetric sequences, and anti-symmetric\nsequences. Using only elementary methods, we find an exact formula for the mean\nand variance of the reciprocal merit factor of sequences in each of these\nclasses, and in the class of all binary sequences. This provides a much deeper\nunderstanding of the distribution of the merit factor in these four classes\nthan was previously available. A consequence is that, for each of the four\nclasses, the merit factor of a sequence drawn uniformly at random from the\nclass converges in probability to a constant as the sequence length increases.\n","authors":["Jonathan Jedwab"],"pdf_url":"https://arxiv.org/pdf/1911.11246v2.pdf","comment":"Removed discussion of $L_4$ norm to focus on reciprocal merit factor.\n  The underlying calculations have not changed but the title, abstract,\n  terminology, and introduction have"}]},"2024-03-16T00:00:00Z":{"Distributed, Parallel, and Cluster Computing":[{"id":"http://arxiv.org/abs/2403.10977v1","updated":"2024-03-16T17:16:01Z","published":"2024-03-16T17:16:01Z","title":"An Open-Source Experimentation Framework for the Edge Cloud Continuum","summary":"  The CODECO Experimentation Framework is an open-source solution designed for\nthe rapid experimentation of Kubernetes-based edge cloud deployments. It adopts\na microservice-based architecture and introduces innovative abstractions for\n(i) the holistic deployment of Kubernetes clusters and associated applications,\nstarting from the VM allocation level; (ii) declarative cross-layer experiment\nconfiguration; and (iii) automation features covering the entire experimental\nprocess, from the configuration up to the results visualization. We present\nproof-of-concept results that demonstrate the above capabilities in three\ndistinct contexts: (i) a comparative evaluation of various network fabrics\nacross different edge-oriented Kubernetes distributions; (ii) the automated\ndeployment of EdgeNet, which is a complex edge cloud orchestration system; and\n(iii) an assessment of anomaly detection (AD) workflows tailored for edge\nenvironments.\n","authors":["Georgios Koukis","Sotiris Skaperas","Ioanna Angeliki Kapetanidou","Vassilis Tsaoussidis","Lefteris Mamatas"],"pdf_url":"https://arxiv.org/pdf/2403.10977v1.pdf","comment":"To be published in IEEE INFOCOM CNERT Workshop"},{"id":"http://arxiv.org/abs/2403.10954v1","updated":"2024-03-16T15:42:06Z","published":"2024-03-16T15:42:06Z","title":"ClusterSlice: A Zero-touch Deployment Platform for the Edge Cloud\n  Continuum","summary":"  We demonstrate ClusterSlice, an open-source solution for automated\nKubernetes-center deployments for the edge continuum. ClusterSlice is an\ninfrastructure-as-a-service, platform-as-a-service, and\napplication-as-a-service solution, supporting: (i) declarative deployment slice\ndefinitions; (ii) infrastructure-on-demand capabilities over multiple\nheterogeneous domains; (iii) composable Kubernetes deployments, supporting\nmulti-clustering as well as various Kubernetes flavors and\nintra-cluster/inter-cluster network plugins; (iv) configurable application\ndeployment; and (v) experimentation automation.\n","authors":["Lefteris Mamatas","Sotiris Skaperas","Ilias Sakellariou"],"pdf_url":"https://arxiv.org/pdf/2403.10954v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08091v2","updated":"2024-03-16T09:37:40Z","published":"2023-11-14T11:25:34Z","title":"Lumiere: Making Optimal BFT for Partial Synchrony Practical","summary":"  The view synchronization problem lies at the heart of many Byzantine Fault\nTolerant (BFT) State Machine Replication (SMR) protocols in the partial\nsynchrony model, since these protocols are usually based on views. Liveness is\nguaranteed if honest processors spend a sufficiently long time in the same view\nduring periods of synchrony, and if the leader of the view is honest. Ensuring\nthat these conditions occur, known as Byzantine View Synchronization (BVS), has\nturned out to be the performance bottleneck of many BFT SMR protocols.\n  A recent line of work has shown that, by using an appropriate view\nsynchronization protocol, BFT SMR protocols can achieve $O(n^2)$ communication\ncomplexity in the worst case after GST, thereby finally matching the lower\nbound established by Dolev and Reischuk in 1985. However, these protocols\nsuffer from two major issues:\n  (1) When implemented so as to be optimistically responsive, even a single\nByzantine processor may infinitely often cause $\\Omega(n\\Delta)$ latency\nbetween consecutive consensus decisions.\n  (2) Even in the absence of Byzantine action, infinitely many views require\nhonest processors to send $\\Omega(n^2)$ messages.\n  Here, we present Lumiere, an optimistically responsive BVS protocol which\nmaintains optimal worst-case communication complexity while simultaneously\naddressing the two issues above: for the first time, Lumiere enables BFT\nconsensus solutions in the partial synchrony setting that have $O(n^2)$\nworst-case communication complexity, and that eventually always (i.e., except\nfor a small constant number of \"warmup\" decisions) have communication\ncomplexity and latency which is linear in the number of actual faults in the\nexecution.\n","authors":["Andrew Lewis-Pye","Dahlia Malkhi","Oded Naor","Kartik Nayak"],"pdf_url":"https://arxiv.org/pdf/2311.08091v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.16694v2","updated":"2024-03-16T03:55:50Z","published":"2024-01-30T02:41:05Z","title":"EdgeOL: Efficient in-situ Online Learning on Edge Devices","summary":"  Emerging applications, such as robot-assisted eldercare and object\nrecognition, generally employ deep learning neural networks (DNNs) and\nnaturally require: i) handling streaming-in inference requests and ii) adapting\nto possible deployment scenario changes. Online model fine-tuning is widely\nadopted to satisfy these needs. However, an inappropriate fine-tuning scheme\ncould involve significant energy consumption, making it challenging to deploy\non edge devices. In this paper, we propose EdgeOL, an edge online learning\nframework that optimizes inference accuracy, fine-tuning execution time, and\nenergy efficiency through both inter-tuning and intra-tuning optimizations.\nExperimental results show that, on average, EdgeOL reduces overall fine-tuning\nexecution time by 64%, energy consumption by 52%, and improves average\ninference accuracy by 1.75% over the immediate online learning strategy.\n","authors":["Sheng Li","Geng Yuan","Yawen Wu","Yue Dai","Chao Wu","Alex K. Jones","Jingtong Hu","Yanzhi Wang","Xulong Tang"],"pdf_url":"https://arxiv.org/pdf/2401.16694v2.pdf","comment":null},{"id":"http://arxiv.org/abs/1708.05264v3","updated":"2024-03-16T21:15:27Z","published":"2017-08-17T13:43:26Z","title":"Design, Configuration, Implementation, and Performance of a Simple 32\n  Core Raspberry Pi Cluster","summary":"  In this report, I describe the design and implementation of an inexpensive,\neight node, 32 core, cluster of raspberry pi single board computers, as well as\nthe performance of this cluster on two computational tasks, one that requires\nsignificant data transfer relative to computational time requirements, and one\nthat does not. We have two use-cases for the cluster: (a) as an educational\ntool for classroom usage, such as covering parallel algorithms in an algorithms\ncourse; and (b) as a test system for use during the development of parallel\nmetaheuristics, essentially serving as a personal desktop parallel computing\ncluster. Our preliminary results show that the slow 100 Mbps networking of the\nraspberry pi significantly limits such clusters to parallel computational tasks\nthat are either long running relative to data communications requirements, or\nthat which requires very little internode communications. Additionally,\nalthough the raspberry pi 3 has a quad-core processor, parallel speedup\ndegrades during attempts to utilize all four cores of all cluster nodes for a\nparallel computation, likely due to resource contention with operating system\nlevel processes. However, distributing a task across three cores of each\ncluster node does enable linear (or near linear) speedup.\n","authors":["Vincent A. Cicirello"],"pdf_url":"https://arxiv.org/pdf/1708.05264v3.pdf","comment":"Minor edits to the text (e.g., fixing typos, etc)"},{"id":"http://arxiv.org/abs/2403.15439v1","updated":"2024-03-16T14:35:03Z","published":"2024-03-16T14:35:03Z","title":"Federated Learning based on Pruning and Recovery","summary":"  A novel federated learning training framework for heterogeneous environments\nis presented, taking into account the diverse network speeds of clients in\nrealistic settings. This framework integrates asynchronous learning algorithms\nand pruning techniques, effectively addressing the inefficiencies of\ntraditional federated learning algorithms in scenarios involving heterogeneous\ndevices, as well as tackling the staleness issue and inadequate training of\ncertain clients in asynchronous algorithms. Through the incremental restoration\nof model size during training, the framework expedites model training while\npreserving model accuracy. Furthermore, enhancements to the federated learning\naggregation process are introduced, incorporating a buffering mechanism to\nenable asynchronous federated learning to operate akin to synchronous learning.\nAdditionally, optimizations in the process of the server transmitting the\nglobal model to clients reduce communication overhead. Our experiments across\nvarious datasets demonstrate that: (i) significant reductions in training time\nand improvements in convergence accuracy are achieved compared to conventional\nasynchronous FL and HeteroFL; (ii) the advantages of our approach are more\npronounced in scenarios with heterogeneous clients and non-IID client data.\n","authors":["Chengjie Ma"],"pdf_url":"https://arxiv.org/pdf/2403.15439v1.pdf","comment":null}],"Databases":[{"id":"http://arxiv.org/abs/2308.03276v3","updated":"2024-03-16T08:39:17Z","published":"2023-08-07T03:35:47Z","title":"Spatialyze: A Geospatial Video Analytics System with Spatial-Aware\n  Optimizations","summary":"  Videos that are shot using commodity hardware such as phones and surveillance\ncameras record various metadata such as time and location. We encounter such\ngeospatial videos on a daily basis and such videos have been growing in volume\nsignificantly. Yet, we do not have data management systems that allow users to\ninteract with such data effectively.\n  In this paper, we describe Spatialyze, a new framework for end-to-end\nquerying of geospatial videos. Spatialyze comes with a domain-specific language\nwhere users can construct geospatial video analytic workflows using a 3-step,\ndeclarative, build-filter-observe paradigm. Internally, Spatialyze leverages\nthe declarative nature of such workflows, the temporal-spatial metadata stored\nwith videos, and physical behavior of real-world objects to optimize the\nexecution of workflows. Our results using real-world videos and workflows show\nthat Spatialyze can reduce execution time by up to 5.3x, while maintaining up\nto 97.1% accuracy compared to unoptimized execution.\n","authors":["Chanwut Kittivorawong","Yongming Ge","Yousef Helal","Alvin Cheung"],"pdf_url":"https://arxiv.org/pdf/2308.03276v3.pdf","comment":"GitHub Repository: https://github.com/apperception-db/spatialyze"},{"id":"http://arxiv.org/abs/2403.10746v1","updated":"2024-03-16T00:34:25Z","published":"2024-03-16T00:34:25Z","title":"Vector search with small radiuses","summary":"  In recent years, the dominant accuracy metric for vector search is the recall\nof a result list of fixed size (top-k retrieval), considering as ground truth\nthe exact vector retrieval results. Although convenient to compute, this metric\nis distantly related to the end-to-end accuracy of a full system that\nintegrates vector search. In this paper we focus on the common case where a\nhard decision needs to be taken depending on the vector retrieval results, for\nexample, deciding whether a query image matches a database image or not. We\nsolve this as a range search task, where all vectors within a certain radius\nfrom the query are returned.\n  We show that the value of a range search result can be modeled rigorously\nbased on the query-to-vector distance. This yields a metric for range search,\nRSM, that is both principled and easy to compute without running an end-to-end\nevaluation. We apply this metric to the case of image retrieval. We show that\nindexing methods that are adapted for top-k retrieval do not necessarily\nmaximize the RSM. In particular, for inverted file based indexes, we show that\nvisiting a limited set of clusters and encoding vectors compactly yields near\noptimal results.\n","authors":["Gergely Szilvasy","Pierre-Emmanuel Mazaré","Matthijs Douze"],"pdf_url":"https://arxiv.org/pdf/2403.10746v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.03584v2","updated":"2024-03-16T00:12:15Z","published":"2023-08-07T13:42:38Z","title":"A Polystore Architecture Using Knowledge Graphs to Support Queries on\n  Heterogeneous Data Stores","summary":"  Modern applications commonly need to manage dataset types composed of\nheterogeneous data and schemas, making it difficult to access them in an\nintegrated way. A single data store to manage heterogeneous data using a common\ndata model is not effective in such a scenario, which results in the domain\ndata being fragmented in the data stores that best fit their storage and access\nrequirements (e.g., NoSQL, relational DBMS, or HDFS). Besides, organization\nworkflows independently consume these fragments, and usually, there is no\nexplicit link among the fragments that would be useful to support an integrated\nview. The research challenge tackled by this work is to provide the means to\nquery heterogeneous data residing on distinct data repositories that are not\nexplicitly connected. We propose a federated database architecture by providing\na single abstract global conceptual schema to users, allowing them to write\ntheir queries, encapsulating data heterogeneity, location, and linkage by\nemploying: (i) meta-models to represent the global conceptual schema, the\nremote data local conceptual schemas, and mappings among them; (ii) provenance\nto create explicit links among the consumed and generated data residing in\nseparate datasets. We evaluated the architecture through its implementation as\na polystore service, following a microservice architecture approach, in a\nscenario that simulates a real case in Oil \\& Gas industry. Also, we compared\nthe proposed architecture to a relational multidatabase system based on foreign\ndata wrappers, measuring the user's cognitive load to write a query (or query\ncomplexity) and the query processing time. The results demonstrated that the\nproposed architecture allows query writing two times less complex than the one\nwritten for the relational multidatabase system, adding an excess of no more\nthan 30% in query processing time.\n","authors":["Leonardo Guerreiro Azevedo","Renan Francisco Santos Souza","Elton F. de S. Soares","Raphael M. Thiago","Julio Cesar Cardoso Tesolin","Ann C. Oliveira","Marcio Ferreira Moreno"],"pdf_url":"https://arxiv.org/pdf/2308.03584v2.pdf","comment":"Reference the paper as L. G. Azevedo, R. Souza, E. F. de S. Soares,\n  R. M. Thiago, J. C. D. Tesolin, A. C. Oliveira, M. F. Moreno, A Polystore\n  Architecture Using Knowledge Graphs to Support Queries on Heterogeneous Data\n  Stores. Proceedings of 20th Brazilian Symposium in Information Systems, 2024\n  (to be published)"},{"id":"http://arxiv.org/abs/2403.14701v1","updated":"2024-03-16T10:35:34Z","published":"2024-03-16T10:35:34Z","title":"Rule based Complex Event Processing for an Air Quality Monitoring System\n  in Smart City","summary":"  In recent years, smart city-based development has gained momentum due to its\nversatile nature in architecture and planning for the systematic habitation of\nhuman beings. According to World Health Organization (WHO) report, air\npollution causes serious respiratory diseases. Hence, it becomes necessary to\nreal-time monitoring of air quality to minimize effect by taking time-bound\ndecisions by the stakeholders. The air pollution comprises various compositions\nsuch as NH3, O3, SO2, NO2, etc., and their concentrations vary from location to\nlocation.The research work proposes an integrated framework for monitoring air\nquality using rule-based Complex Event Processing (CEP) and SPARQL queries. CEP\nworks with the data stream based on predefined rules to detect the complex\npattern, which helps in decision support for stakeholders. Initially, the\ndataset was collected from the Central Pollution Control Board (CPCB) of India\nand this data was then preprocessed and passed through Apache Kafka. Then a\nknowledge graph developed based on the air quality paradigm. Consequently,\nconvert preprocessed data into Resource Description Framework (RDF) data, and\nintegrate with Knowledge graph which is ingested to CEP engine using Apache\nJena for enhancing the decision support . Simultaneously, rules are extracted\nusing a decision tree, and some ground truth parameters of CPCB are added and\ningested to the CEP engine to determine the complex patterns. Consequently, the\nSPARQL query is used on real-time RDF dataset for fetching the condition of air\nquality as good, poor, severe, hazardous etc based on complex events detection.\nFor validating the proposed approach various chunks of RDF are used for the\ndeployment of events to the CEP engine, and its performance is examined over\ntime while performing simple and complex queries.\n","authors":["Shashi Shekhar Kumar","Ritesh Chandra","Sonali Agarwal"],"pdf_url":"https://arxiv.org/pdf/2403.14701v1.pdf","comment":null}],"Computer Science and Game Theory":[{"id":"http://arxiv.org/abs/2403.11022v1","updated":"2024-03-16T21:54:13Z","published":"2024-03-16T21:54:13Z","title":"Auctions with Dynamic Scoring","summary":"  We study the design of auctions with dynamic scoring, which allocate a single\nitem according to a given scoring rule. We are motivated by online advertising\nauctions when users interact with a platform over the course of a session. The\nplatform ranks ads based on a combination of bids and quality scores, and\nupdates the quality scores throughout the session based on the user's online\nactivity. The platform must decide when to show an ad during the session. By\ndelaying the auction, the auctioneer acquires information about an ad's\nquality, improving her chances of selecting a high quality ad. However\ninformation is costly, because delay reduces market thickness and in turn\nrevenue. When should the auctioneer allocate the impression to balance these\nforces?\n  We develop a theoretical model to study the effect of market design on the\ntrade-off between market thickness and information. In particular, we focus on\nfirst- and second-price auctions. The auctioneer can commit to the auction\nformat, but not to its timing: her decision can thus be cast as a real options\nproblem. We show that under optimal stopping the first-price auction allocates\nefficiently but with delay. Instead, the second-price auction generates more\nrevenue by avoiding delay. The auctioneer benefits from introducing reserve\nprices, more so in a first-price auction.\n","authors":["Martino Banchio","Aranyak Mehta","Andres Perlroth"],"pdf_url":"https://arxiv.org/pdf/2403.11022v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10980v1","updated":"2024-03-16T17:23:20Z","published":"2024-03-16T17:23:20Z","title":"Inverse learning of black-box aggregator for robust Nash equilibrium","summary":"  In this note, we investigate the robustness of Nash equilibria (NE) in\nmulti-player aggregative games with coupling constraints. There are many\nalgorithms for computing an NE of an aggregative game given a known aggregator.\nWhen the coupling parameters are affected by uncertainty, robust NE need to be\ncomputed. We consider a scenario where players' weight in the aggregator is\nunknown, making the aggregator kind of \"a black box\". We pursue a suitable\nlearning approach to estimate the unknown aggregator by proposing an inverse\nvariational inequality-based relationship. We then utilize the counterpart to\nreconstruct the game and obtain first-order conditions for robust NE in the\nworst case. Furthermore, we characterize the generalization property of the\nlearning methodology via an upper bound on the violation probability.\nSimulation experiments show the effectiveness of the proposed inverse learning\napproach.\n","authors":["Guanpu Chen","Gehui Xu","Fengxiang He","Dacheng Tao","Thomas Parisini","Karl Henrik Johansson"],"pdf_url":"https://arxiv.org/pdf/2403.10980v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.15935v2","updated":"2024-03-16T02:55:59Z","published":"2023-10-24T15:32:54Z","title":"Mediator Interpretation and Faster Learning Algorithms for Linear\n  Correlated Equilibria in General Extensive-Form Games","summary":"  A recent paper by Farina & Pipis (2023) established the existence of\nuncoupled no-linear-swap regret dynamics with polynomial-time iterations in\nextensive-form games. The equilibrium points reached by these dynamics, known\nas linear correlated equilibria, are currently the tightest known relaxation of\ncorrelated equilibrium that can be learned in polynomial time in any finite\nextensive-form game. However, their properties remain vastly unexplored, and\ntheir computation is onerous. In this paper, we provide several contributions\nshedding light on the fundamental nature of linear-swap regret. First, we show\na connection between linear deviations and a generalization of communication\ndeviations in which the player can make queries to a \"mediator\" who replies\nwith action recommendations, and, critically, the player is not constrained to\nmatch the timing of the game as would be the case for communication deviations.\nWe coin this latter set the untimed communication (UTC) deviations. We show\nthat the UTC deviations coincide precisely with the linear deviations, and\ntherefore that any player minimizing UTC regret also minimizes linear-swap\nregret. We then leverage this connection to develop state-of-the-art no-regret\nalgorithms for computing linear correlated equilibria, both in theory and in\npractice. In theory, our algorithms achieve polynomially better per-iteration\nruntimes; in practice, our algorithms represent the state of the art by several\norders of magnitude.\n","authors":["Brian Hu Zhang","Gabriele Farina","Tuomas Sandholm"],"pdf_url":"https://arxiv.org/pdf/2310.15935v2.pdf","comment":null}]},"2024-03-15T00:00:00Z":{"Distributed, Parallel, and Cluster Computing":[{"id":"http://arxiv.org/abs/2403.10726v1","updated":"2024-03-15T23:17:24Z","published":"2024-03-15T23:17:24Z","title":"Strict Partitioning for Sporadic Rigid Gang Tasks","summary":"  The rigid gang task model is based on the idea of executing multiple threads\nsimultaneously on a fixed number of processors to increase efficiency and\nperformance. Although there is extensive literature on global rigid gang\nscheduling, partitioned approaches have several practical advantages (e.g.,\ntask isolation and reduced scheduling overheads). In this paper, we propose a\nnew partitioned scheduling strategy for rigid gang tasks, named strict\npartitioning. The method creates disjoint partitions of tasks and processors to\navoid inter-partition interference. Moreover, it tries to assign tasks with\nsimilar volumes (i.e., parallelisms) to the same partition so that the\nintra-partition interference can be reduced. Within each partition, the tasks\ncan be scheduled using any type of scheduler, which allows the use of a less\npessimistic schedulability test. Extensive synthetic experiments and a case\nstudy based on Edge TPU benchmarks show that strict partitioning achieves\nbetter schedulability performance than state-of-the-art global gang\nschedulability analyses for both preemptive and non-preemptive rigid gang task\nsets.\n","authors":["Binqi Sun","Tomasz Kloda","Marco Caccamo"],"pdf_url":"https://arxiv.org/pdf/2403.10726v1.pdf","comment":"to be published in IEEE Real-Time and Embedded Technology and\n  Applications Symposium (RTAS 2024)"},{"id":"http://arxiv.org/abs/2304.12576v2","updated":"2024-03-15T21:30:14Z","published":"2023-04-25T05:04:44Z","title":"Harnessing Deep Learning and HPC Kernels via High-Level Loop and Tensor\n  Abstractions on CPU Architectures","summary":"  During the past decade, Deep Learning (DL) algorithms, programming systems\nand hardware have converged with the High Performance Computing (HPC)\ncounterparts. Nevertheless, the programming methodology of DL and HPC systems\nis stagnant, relying on highly-optimized, yet platform-specific and inflexible\nvendor-optimized libraries. Such libraries provide close-to-peak performance on\nspecific platforms, kernels and shapes thereof that vendors have dedicated\noptimizations efforts, while they underperform in the remaining use-cases,\nyielding non-portable codes with performance glass-jaws. This work introduces a\nframework to develop efficient, portable DL and HPC kernels for modern CPU\narchitectures. We decompose the kernel development in two steps: 1) Expressing\nthe computational core using Tensor Processing Primitives (TPPs): a compact,\nversatile set of 2D-tensor operators, 2) Expressing the logical loops around\nTPPs in a high-level, declarative fashion whereas the exact instantiation\n(ordering, tiling, parallelization) is determined via simple knobs. We\ndemonstrate the efficacy of our approach using standalone kernels and\nend-to-end workloads that outperform state-of-the-art implementations on\ndiverse CPU platforms.\n","authors":["Evangelos Georganas","Dhiraj Kalamkar","Kirill Voronin","Abhisek Kundu","Antonio Noack","Hans Pabst","Alexander Breuer","Alexander Heinecke"],"pdf_url":"https://arxiv.org/pdf/2304.12576v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.01001v2","updated":"2024-03-15T19:27:09Z","published":"2024-02-01T20:31:08Z","title":"Ensuring Data Privacy in AC Optimal Power Flow with a Distributed\n  Co-Simulation Framework","summary":"  During the energy transition, the significance of collaborative management\namong institutions is rising, confronting challenges posed by data privacy\nconcerns. Prevailing research on distributed approaches, as an alternative to\ncentralized management, often lacks numerical convergence guarantees or is\nlimited to single-machine numerical simulation. To address this, we present a\ndistributed approach for solving AC Optimal Power Flow (OPF) problems within a\ngeographically distributed environment. This involves integrating the energy\nsystem Co-Simulation (eCoSim) module in the eASiMOV framework with the\nconvergence-guaranteed distributed optimization algorithm, i.e., the Augmented\nLagrangian based Alternating Direction Inexact Newton method (ALADIN).\nComprehensive evaluations across multiple system scenarios reveal a marginal\nperformance slowdown compared to the centralized approach and the distributed\napproach executed on single machines -- a justified trade-off for enhanced data\nprivacy. This investigation serves as empirical validation of the successful\nexecution of distributed AC OPF within a geographically distributed\nenvironment, highlighting potential directions for future research.\n","authors":["Xinliang Dai","Alexander Kocher","Jovana Kovačević","Burak Dindar","Yuning Jiang","Colin N. Jones","Hüseyin Çakmak","Veit Hagenmeyer"],"pdf_url":"https://arxiv.org/pdf/2402.01001v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10504v1","updated":"2024-03-15T17:43:43Z","published":"2024-03-15T17:43:43Z","title":"ATOM: Asynchronous Training of Massive Models for Deep Learning in a\n  Decentralized Environment","summary":"  The advent of the Transformer architecture has propelled the growth of\nnatural language processing (NLP) models, leading to remarkable achievements in\nnumerous NLP tasks. Yet, the absence of specialized hardware like expansive GPU\nmemory and high-speed interconnects poses challenges for training large-scale\nmodels. This makes it daunting for many users to experiment with pre-training\nand fine-tuning large language models (LLMs). In this study, we introduce\n\\atom, a resilient distributed training framework designed for asynchronous\ntraining of vast models in a decentralized setting using cost-effective\nhardware, including consumer-grade GPUs and Ethernet. Unlike conventional model\npartitioning methods that distribute sub-models across GPUs, \\atom aims to\naccommodate a complete LLM on one host (peer) through seamlessly model swapping\nand concurrently trains multiple copies across various peers to optimize\ntraining throughput. Through static analysis, \\atom identifies the best model\npartitioning strategy and flawlessly merges model execution with swapping. Key\nbenefits of \\atom include: Avoiding the central point of failure found in\npipeline parallelism methods. Demonstrating superior performance and\nscalability compared to closely-integrated pipeline parallelism in slower\nnetworks. Our experiments using different GPT-3 model configurations reveal\nthat, in scenarios with suboptimal network connections, \\atom can enhance\ntraining efficiency up to $20 \\times$ when juxtaposed with the state-of-the-art\ndecentralized pipeline parallelism approaches.\n","authors":["Xiaofeng Wu","Jia Rao","Wei Chen"],"pdf_url":"https://arxiv.org/pdf/2403.10504v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10455v1","updated":"2024-03-15T16:43:21Z","published":"2024-03-15T16:43:21Z","title":"Evaluation of Quantum and Hybrid Solvers for Combinatorial Optimization","summary":"  Academic and industrial sectors have been engaged in a fierce competition to\ndevelop quantum technologies, fueled by the explosive advancements in quantum\nhardware. While universal quantum computers have been shown to support up to\nhundreds of qubits, the scale of quantum annealers has reached three orders of\nmagnitude (i.e., thousands of qubits). Therefore, quantum algorithms are\nbecoming increasingly popular in a variety of fields, with optimization being\none of the most prominent. This work aims to explore the topic of quantum\noptimization by comprehensively evaluating the technologies provided by D-Wave\nSystems. To do so, a model for the energy optimization of data centers is\nproposed as a benchmark. D-Wave quantum and hybrid solvers are compared, in\norder to identify the most suitable one for the considered application. To\nhighlight its advantageous performance capabilities and associated solving\npotential, the selected D-Wave hybrid solver is then contrasted with CPLEX, a\nhighly efficient classical solver.\n","authors":["Amedeo Bertuzzi","Davide Ferrari","Antonio Manzalini","Michele Amoretti"],"pdf_url":"https://arxiv.org/pdf/2403.10455v1.pdf","comment":"10 pages, 4 figures"},{"id":"http://arxiv.org/abs/2403.10332v1","updated":"2024-03-15T14:19:09Z","published":"2024-03-15T14:19:09Z","title":"GreedyML: A Parallel Algorithm for Maximizing Submodular Functions","summary":"  We describe a parallel approximation algorithm for maximizing monotone\nsubmodular functions subject to hereditary constraints on distributed memory\nmultiprocessors. Our work is motivated by the need to solve submodular\noptimization problems on massive data sets, for practical applications in areas\nsuch as data summarization, machine learning, and graph sparsification. Our\nwork builds on the randomized distributed RandGreedI algorithm, proposed by\nBarbosa, Ene, Nguyen, and Ward (2015). This algorithm computes a distributed\nsolution by randomly partitioning the data among all the processors and then\nemploying a single accumulation step in which all processors send their partial\nsolutions to one processor. However, for large problems, the accumulation step\ncould exceed the memory available on a processor, and the processor which\nperforms the accumulation could become a computational bottleneck.\n  Here, we propose a generalization of the RandGreedI algorithm that employs\nmultiple accumulation steps to reduce the memory required. We analyze the\napproximation ratio and the time complexity of the algorithm (in the BSP\nmodel). We also evaluate the new GreedyML algorithm on three classes of\nproblems, and report results from massive data sets with millions of elements.\nThe results show that the GreedyML algorithm can solve problems where the\nsequential Greedy and distributed RandGreedI algorithms fail due to memory\nconstraints. For certain computationally intensive problems, the GreedyML\nalgorithm can be faster than the RandGreedI algorithm. The observed\napproximation quality of the solutions computed by the GreedyML algorithm\nclosely matches those obtained by the RandGreedI algorithm on these problems.\n","authors":["Shivaram Gopal","S M Ferdous","Hemanta K. Maji","Alex Pothen"],"pdf_url":"https://arxiv.org/pdf/2403.10332v1.pdf","comment":"22 pages, 7 figures"},{"id":"http://arxiv.org/abs/2112.07303v3","updated":"2024-03-15T14:09:15Z","published":"2021-12-14T11:21:24Z","title":"MMO: Meta Multi-Objectivization for Software Configuration Tuning","summary":"  Software configuration tuning is essential for optimizing a given performance\nobjective (e.g., minimizing latency). Yet, due to the software's intrinsically\ncomplex configuration landscape and expensive measurement, there has been a\nrather mild success, particularly in preventing the search from being trapped\nin local optima. To address this issue, in this paper we take a different\nperspective. Instead of focusing on improving the optimizer, we work on the\nlevel of optimization model and propose a meta multi-objectivization (MMO)\nmodel that considers an auxiliary performance objective (e.g., throughput in\naddition to latency). What makes this model distinct is that we do not optimize\nthe auxiliary performance objective, but rather use it to make\nsimilarly-performing while different configurations less comparable (i.e.\nPareto nondominated to each other), thus preventing the search from being\ntrapped in local optima. Importantly, by designing a new normalization method,\nwe show how to effectively use the MMO model without worrying about its weight\n-- the only yet highly sensitive parameter that can affect its effectiveness.\nExperiments on 22 cases from 11 real-world software systems/environments\nconfirm that our MMO model with the new normalization performs better than its\nstate-of-the-art single-objective counterparts on 82% cases while achieving up\nto 2.09x speedup. For 68% of the cases, the new normalization also enables the\nMMO model to outperform the instance when using it with the normalization from\nour prior FSE work under pre-tuned best weights, saving a great amount of\nresources which would be otherwise necessary to find a good weight. We also\ndemonstrate that the MMO model with the new normalization can consolidate\nrecent model-based tuning tools on 68% of the cases with up to 1.22x speedup in\ngeneral.\n","authors":["Pengzhou Chen","Tao Chen","Miqing Li"],"pdf_url":"https://arxiv.org/pdf/2112.07303v3.pdf","comment":"20 figures, 4 tables. journal extension at TSE. arXiv admin note:\n  text overlap with arXiv:2106.01331"},{"id":"http://arxiv.org/abs/2403.10266v1","updated":"2024-03-15T12:53:50Z","published":"2024-03-15T12:53:50Z","title":"DSP: Dynamic Sequence Parallelism for Multi-Dimensional Transformers","summary":"  Scaling large models with long sequences across applications like language\ngeneration, video generation and multimodal tasks requires efficient sequence\nparallelism. However, existing sequence parallelism methods all assume a single\nsequence dimension and fail to adapt to multi-dimensional transformer\narchitectures that perform attention calculations across different dimensions.\nThis paper introduces Dynamic Sequence Parallelism (DSP), a novel approach to\nenable efficient sequence parallelism for multi-dimensional transformer models.\nThe key idea is to dynamically switch the parallelism dimension according to\nthe current computation stage, leveraging the potential characteristics of\nmulti-dimensional attention. This dynamic dimension switching allows sequence\nparallelism with minimal communication overhead compared to applying\ntraditional single-dimension parallelism to multi-dimensional models.\nExperiments show DSP improves end-to-end throughput by 42.0% to 216.8% over\nprior sequence parallelism methods.\n","authors":["Xuanlei Zhao","Shenggan Cheng","Zangwei Zheng","Zheming Yang","Ziming Liu","Yang You"],"pdf_url":"https://arxiv.org/pdf/2403.10266v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.16772v4","updated":"2024-03-15T11:59:00Z","published":"2023-05-26T09:33:18Z","title":"Real-Time Scheduling for 802.1Qbv Time-Sensitive Networking (TSN): A\n  Systematic Review and Experimental Study","summary":"  Time-Sensitive Networking (TSN) has been recognized as one of the key\nenabling technologies for Industry 4.0 and has been deployed in many mission-\nand safety-critical applications e.g., automotive and aerospace systems. Given\nthe stringent real-time requirements of these applications, the Time-Aware\nShaper (TAS) draws special attention among TSN's many traffic shapers due to\nits ability to achieve deterministic timing guarantees. Many scheduling methods\nfor TAS shapers have been recently developed that claim to improve system\nschedulability. However, these scheduling methods have yet to be thoroughly\nevaluated, especially through experimental comparisons, to provide a\nsystematical understanding of their performance using different evaluation\nmetrics in diverse application scenarios. In this paper, we fill this gap by\npresenting a systematic review and experimental study on existing TAS-based\nscheduling methods for TSN. We first categorize the system models employed in\nthese works along with the specific problems they aim to solve, and outline the\nfundamental considerations in the designs of TAS-based scheduling methods. We\nthen perform an extensive evaluation on 17 representative solutions using both\nhigh-fidelity simulations and a real-life TSN testbed, and compare their\nperformance under both synthetic scenarios and real-life industrial use cases.\nThrough these experimental studies, we identify the limitations of individual\nscheduling methods and highlight several important findings. We expect this\nwork will provide foundational knowledge and performance benchmarks needed for\nfuture studies on real-time TSN scheduling.\n","authors":["Chuanyu Xue","Tianyu Zhang","Yuanbin Zhou","Mark Nixon","Andrew Loveless","Song Han"],"pdf_url":"https://arxiv.org/pdf/2305.16772v4.pdf","comment":"21 pages, 6 authors, RTAS24 tech report"},{"id":"http://arxiv.org/abs/2312.13107v2","updated":"2024-03-15T11:21:01Z","published":"2023-12-20T15:25:56Z","title":"Quick Order Fairness: Implementation and Evaluation","summary":"  Decentralized finance revolutionizes traditional financial systems by\nleveraging blockchain technology to reduce trust. However, some vulnerabilities\npersist, notably front-running by malicious actors who exploit transaction\ninformation to gain financial advantage. Consensus with a fair order aims at\npreventing such attacks, and in particular, the differential order fairness\nproperty addresses this problem and connects fair ordering to the validity of\nconsensus. The notion is implemented by the Quick Order-Fair Atomic Broadcast\n(QOF) protocol (Cachin et al., FC '22). This paper revisits the QOF protocol\nand describes a modular implementation that uses a generic consensus component.\nMoreover, an empirical evaluation is performed to compare the performance of\nQOF to a consensus protocol without fairness. Measurements show that the\nincreased complexity comes at a cost, throughput decreases by at most 5%, and\nlatency increases by roughly 50ms, using an emulated ideal network. This paper\ncontributes to a comprehensive understanding of practical aspects regarding\ndifferential order fairness with the QOF protocol and also connects this with\nsimilar fairness-imposing protocols like Themis and Pompe.\n","authors":["Christian Cachin","Jovana Micic"],"pdf_url":"https://arxiv.org/pdf/2312.13107v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14693v1","updated":"2024-03-15T08:28:38Z","published":"2024-03-15T08:28:38Z","title":"A2CI: A Cloud-based, Service-oriented Geospatial Cyberinfrastructure to\n  Support Atmospheric Research","summary":"  Big earth science data offers the scientific community great opportunities.\nMany more studies at large-scales, over long-terms and at high resolution can\nnow be conducted using the rich information collected by remote sensing\nsatellites, ground-based sensor networks, and even social media input. However,\nthe hundreds of terabytes of information collected and compiled on an hourly\nbasis by NASA and other government agencies present a significant challenge for\natmospheric scientists seeking to improve the understanding of the Earth\natmospheric system. These challenges include effective discovery, organization,\nanalysis and visualization of large amounts of data. This paper reports the\noutcomes of an NSF-funded project that developed a geospatial\ncyberinfrastructure -- the A2CI (Atmospheric Analysis Cyberinfrastructure) --\nto support atmospheric research. We first introduce the service-oriented system\nframework then describe in detail the implementation of the data discovery\nmodule, data management module, data integration module, data analysis and\nvisualization modules following the cloud computing\nprinciples-Data-as-a-Service, Software-as-a-Service, Platform-as-a-Service and\nInfrastructure-as-a-Service. We demonstrate the graphic user interface by\nperforming an analysis between Sea Surface Temperature and the intensity of\ntropical storms in the North Atlantic and Pacific oceans. We expect this work\nto contribute to the technical advancement of cyberinfrastructure research as\nwell as to the development of an online, collaborative scientific analysis\nsystem for atmospheric science.\n","authors":["Wenwen Li","Hu Shao","Sizhe Wang","Xiran Zhou","Sheng Wu"],"pdf_url":"https://arxiv.org/pdf/2403.14693v1.pdf","comment":null}],"Performance":[{"id":"http://arxiv.org/abs/2401.01851v2","updated":"2024-03-15T21:43:10Z","published":"2024-01-03T17:44:17Z","title":"The Power of Training: How Different Neural Network Setups Influence the\n  Energy Demand","summary":"  This work offers a heuristic evaluation of the effects of variations in\nmachine learning training regimes and learning paradigms on the energy\nconsumption of computing, especially HPC hardware with a life-cycle aware\nperspective. While increasing data availability and innovation in\nhigh-performance hardware fuels the training of sophisticated models, it also\nfosters the fading perception of energy consumption and carbon emission.\nTherefore, the goal of this work is to raise awareness about the energy impact\nof general training parameters and processes, from learning rate over batch\nsize to knowledge transfer. Multiple setups with different hyperparameter\nconfigurations are evaluated on three different hardware systems. Among many\nresults, we have found out that even with the same model and hardware to reach\nthe same accuracy, improperly set training hyperparameters consume up to 5\ntimes the energy of the optimal setup. We also extensively examined the\nenergy-saving benefits of learning paradigms including recycling knowledge\nthrough pretraining and sharing knowledge through multitask training.\n","authors":["Daniel Geißler","Bo Zhou","Mengxi Liu","Sungho Suh","Paul Lukowicz"],"pdf_url":"https://arxiv.org/pdf/2401.01851v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10455v1","updated":"2024-03-15T16:43:21Z","published":"2024-03-15T16:43:21Z","title":"Evaluation of Quantum and Hybrid Solvers for Combinatorial Optimization","summary":"  Academic and industrial sectors have been engaged in a fierce competition to\ndevelop quantum technologies, fueled by the explosive advancements in quantum\nhardware. While universal quantum computers have been shown to support up to\nhundreds of qubits, the scale of quantum annealers has reached three orders of\nmagnitude (i.e., thousands of qubits). Therefore, quantum algorithms are\nbecoming increasingly popular in a variety of fields, with optimization being\none of the most prominent. This work aims to explore the topic of quantum\noptimization by comprehensively evaluating the technologies provided by D-Wave\nSystems. To do so, a model for the energy optimization of data centers is\nproposed as a benchmark. D-Wave quantum and hybrid solvers are compared, in\norder to identify the most suitable one for the considered application. To\nhighlight its advantageous performance capabilities and associated solving\npotential, the selected D-Wave hybrid solver is then contrasted with CPLEX, a\nhighly efficient classical solver.\n","authors":["Amedeo Bertuzzi","Davide Ferrari","Antonio Manzalini","Michele Amoretti"],"pdf_url":"https://arxiv.org/pdf/2403.10455v1.pdf","comment":"10 pages, 4 figures"}],"Databases":[{"id":"http://arxiv.org/abs/2403.03193v2","updated":"2024-03-15T21:40:22Z","published":"2024-03-05T18:30:29Z","title":"VeriEQL: Bounded Equivalence Verification for Complex SQL Queries with\n  Integrity Constraints","summary":"  The task of SQL query equivalence checking is important in various real-world\napplications (including query rewriting and automated grading) that involve\ncomplex queries with integrity constraints; yet, state-of-the-art techniques\nare very limited in their capability of reasoning about complex features (e.g.,\nthose that involve sorting, case statement, rich integrity constraints, etc.)\nin real-life queries. To the best of our knowledge, we propose the first\nSMT-based approach and its implementation, VeriEQL, capable of proving and\ndisproving bounded equivalence of complex SQL queries. VeriEQL is based on a\nnew logical encoding that models query semantics over symbolic tuples using the\ntheory of integers with uninterpreted functions. It is simple yet highly\npractical -- our comprehensive evaluation on over 20,000 benchmarks shows that\nVeriEQL outperforms all state-of-the-art techniques by more than one order of\nmagnitude in terms of the number of benchmarks that can be proved or disproved.\nVeriEQL can also generate counterexamples that facilitate many downstream tasks\n(such as finding serious bugs in systems like MySQL and Apache Calcite).\n","authors":["Yang He","Pinhan Zhao","Xinyu Wang","Yuepeng Wang"],"pdf_url":"https://arxiv.org/pdf/2403.03193v2.pdf","comment":"OOPSLA 2024"},{"id":"http://arxiv.org/abs/2403.10313v1","updated":"2024-03-15T13:59:05Z","published":"2024-03-15T13:59:05Z","title":"Interactive Trimming against Evasive Online Data Manipulation Attacks: A\n  Game-Theoretic Approach","summary":"  With the exponential growth of data and its crucial impact on our lives and\ndecision-making, the integrity of data has become a significant concern.\nMalicious data poisoning attacks, where false values are injected into the\ndata, can disrupt machine learning processes and lead to severe consequences.\nTo mitigate these attacks, distance-based defenses, such as trimming, have been\nproposed, but they can be easily evaded by white-box attackers. The evasiveness\nand effectiveness of poisoning attack strategies are two sides of the same\ncoin, making game theory a promising approach. However, existing\ngame-theoretical models often overlook the complexities of online data\npoisoning attacks, where strategies must adapt to the dynamic process of data\ncollection.\n  In this paper, we present an interactive game-theoretical model to defend\nonline data manipulation attacks using the trimming strategy. Our model\naccommodates a complete strategy space, making it applicable to strong evasive\nand colluding adversaries. Leveraging the principle of least action and the\nEuler-Lagrange equation from theoretical physics, we derive an analytical model\nfor the game-theoretic process. To demonstrate its practical usage, we present\na case study in a privacy-preserving data collection system under local\ndifferential privacy where a non-deterministic utility function is adopted. Two\nstrategies are devised from this analytical model, namely, Tit-for-tat and\nElastic. We conduct extensive experiments on real-world datasets, which\nshowcase the effectiveness and accuracy of these two strategies.\n","authors":["Yue Fu","Qingqing Ye","Rong Du","Haibo Hu"],"pdf_url":"https://arxiv.org/pdf/2403.10313v1.pdf","comment":"This manuscript is accepted by ICDE '24"},{"id":"http://arxiv.org/abs/2403.10304v1","updated":"2024-03-15T13:46:36Z","published":"2024-03-15T13:46:36Z","title":"KIF: A Framework for Virtual Integration of Heterogeneous Knowledge\n  Bases using Wikidata","summary":"  We present a knowledge integration framework (called KIF) that uses Wikidata\nas a lingua franca to integrate heterogeneous knowledge bases. These can be\ntriplestores, relational databases, CSV files, etc., which may or may not use\nthe Wikidata dialect of RDF. KIF leverages Wikidata's data model and vocabulary\nplus user-defined mappings to expose a unified view of the integrated bases\nwhile keeping track of the context and provenance of their statements. The\nresult is a virtual knowledge base which behaves like an \"extended Wikidata\"\nand which can be queried either through an efficient filter interface or using\nSPARQL. We present the design and implementation of KIF, discuss how we have\nused it to solve a real integration problem in the domain of chemistry\n(involving Wikidata, PubChem, and IBM CIRCA), and present experimental results\non the performance and overhead of KIF.\n","authors":["Guilherme Lima","Marcelo Machado","Elton Soares","Sandro R. Fiorini","Raphael Thiago","Leonardo G. Azevedo","Viviane T. da Silva","Renato Cerqueira"],"pdf_url":"https://arxiv.org/pdf/2403.10304v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10051v1","updated":"2024-03-15T06:46:00Z","published":"2024-03-15T06:46:00Z","title":"Accelerating Regular Path Queries over Graph Database with\n  Processing-in-Memory","summary":"  Regular path queries (RPQs) in graph databases are bottlenecked by the memory\nwall. Emerging processing-in-memory (PIM) technologies offer a promising\nsolution to dispatch and execute path matching tasks in parallel within PIM\nmodules. We present Moctopus, a PIM-based data management system for graph\ndatabases that supports efficient batch RPQs and graph updates. Moctopus\nemploys a PIM-friendly dynamic graph partitioning algorithm, which tackles\ngraph skewness and preserves graph locality with low overhead for RPQ\nprocessing. Moctopus enables efficient graph update by amortizing the host\nCPU's update overhead to PIM modules. Evaluation of Moctopus demonstrates\nsuperiority over the state-of-the-art traditional graph database.\n","authors":["Ruoyan Ma","Shengan Zheng","Guifeng Wang","Jin Pu","Yifan Hua","Wentao Wang","Linpeng Huang"],"pdf_url":"https://arxiv.org/pdf/2403.10051v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.18157v2","updated":"2024-03-15T00:54:12Z","published":"2023-11-30T00:06:58Z","title":"Finding Smallest Witnesses for Conjunctive Queries","summary":"  A witness is a sub-database that preserves the query results of the original\ndatabase but of much smaller size. It has wide applications in query rewriting\nand debugging, query explanation, IoT analytics, multi-layer network routing,\netc. In this paper, we study the smallest witness problem (SWP) for the class\nof conjunctive queries (CQs) without self-joins.\n  We first establish the dichotomy that SWP for a CQ can be computed in\npolynomial time if and only if it has {\\em head-cluster property}, unless\n$\\texttt{P} = \\texttt{NP}$. We next turn to the approximated version by\nrelaxing the size of a witness from being minimum. We surprisingly find that\nthe {\\em head-domination} property - that has been identified for the deletion\npropagation problem \\cite{kimelfeld2012maximizing} - can also precisely capture\nthe hardness of the approximated smallest witness problem. In polynomial time,\nSWP for any CQ with head-domination property can be approximated within a\nconstant factor, while SWP for any CQ without such a property cannot be\napproximated within a logarithmic factor, unless $\\texttt{P} = \\texttt{NP}$.\n  We further explore efficient approximation algorithms for CQs without\nhead-domination property: (1) we show a trivial algorithm which achieves a\npolynomially large approximation ratio for general CQs; (2) for any CQ with\nonly one non-output attribute, such as star CQs, we show a greedy algorithm\nwith a logarithmic approximation ratio; (3) for line CQs, which contain at\nleast two non-output attributes, we relate SWP problem to the directed steiner\nforest problem, whose algorithms can be applied to line CQs directly.\nMeanwhile, we establish a much higher lower bound, exponentially larger than\nthe logarithmic lower bound obtained above. It remains open to close the gap\nbetween the lower and upper bound of the approximated SWP for CQs without\nhead-domination property.\n","authors":["Xiao Hu","Stavros Sintos"],"pdf_url":"https://arxiv.org/pdf/2311.18157v2.pdf","comment":null}]},"2024-03-19T00:00:00Z":{"Distributed, Parallel, and Cluster Computing":[{"id":"http://arxiv.org/abs/2311.04417v3","updated":"2024-03-19T06:31:52Z","published":"2023-11-08T01:06:25Z","title":"Evaluating Emerging AI/ML Accelerators: IPU, RDU, and NVIDIA/AMD GPUs","summary":"  The relentless advancement of artificial intelligence (AI) and machine\nlearning (ML) applications necessitates the development of specialized hardware\naccelerators capable of handling the increasing complexity and computational\ndemands. Traditional computing architectures, based on the von Neumann model,\nare being outstripped by the requirements of contemporary AI/ML algorithms,\nleading to a surge in the creation of accelerators like the Graphcore\nIntelligence Processing Unit (IPU), Sambanova Reconfigurable Dataflow Unit\n(RDU), and enhanced GPU platforms. These hardware accelerators are\ncharacterized by their innovative data-flow architectures and other design\noptimizations that promise to deliver superior performance and energy\nefficiency for AI/ML tasks.\n  This research provides a preliminary evaluation and comparison of these\ncommercial AI/ML accelerators, delving into their hardware and software design\nfeatures to discern their strengths and unique capabilities. By conducting a\nseries of benchmark evaluations on common DNN operators and other AI/ML\nworkloads, we aim to illuminate the advantages of data-flow architectures over\nconventional processor designs and offer insights into the performance\ntrade-offs of each platform. The findings from our study will serve as a\nvaluable reference for the design and performance expectations of research\nprototypes, thereby facilitating the development of next-generation hardware\naccelerators tailored for the ever-evolving landscape of AI/ML applications.\nThrough this analysis, we aspire to contribute to the broader understanding of\ncurrent accelerator technologies and to provide guidance for future innovations\nin the field.\n","authors":["Hongwu Peng","Caiwen Ding","Tong Geng","Sutanay Choudhury","Kevin Barker","Ang Li"],"pdf_url":"https://arxiv.org/pdf/2311.04417v3.pdf","comment":"ICPE 2024 accepted publication"},{"id":"http://arxiv.org/abs/2401.07931v2","updated":"2024-03-19T17:07:40Z","published":"2024-01-15T19:47:14Z","title":"Vertical Federated Image Segmentation","summary":"  With the popularization of AI solutions for image based problems, there has\nbeen a growing concern for both data privacy and acquisition. In a large number\nof cases, information is located on separate data silos and it can be difficult\nfor a developer to consolidate all of it in a fashion that is appropriate for\nmachine learning model development. Alongside this, a portion of these\nlocalized data regions may not have access to a labelled ground truth. This\nindicates that they have the capacity to reach conclusions numerically, but are\nnot able to assign classifications amid a lack of pertinent information. Such a\ndetermination is often negligible, especially when attempting to develop image\nbased solutions that often necessitate this capability. With this being the\ncase, we propose an innovative vertical federated learning (VFL) model\narchitecture that can operate under this common set of conditions. This is the\nfirst (and currently the only) implementation of a system that can work under\nthe constraints of a VFL environment and perform image segmentation while\nmaintaining nominal accuracies. We achieved this by utilizing an FCN that\nboasts the ability to operate on federates that lack labelled data and\nprivately share the respective weights with a central server, that of which\nhosts the necessary features for classification. Tests were conducted on the\nCamVid dataset in order to determine the impact of heavy feature compression\nrequired for the transfer of information between federates, as well as to reach\nnominal conclusions about the overall performance metrics when working under\nsuch constraints.\n","authors":["Paul K. Mandal","Cole Leo"],"pdf_url":"https://arxiv.org/pdf/2401.07931v2.pdf","comment":"11 pages, 5 figures"},{"id":"http://arxiv.org/abs/2403.12900v1","updated":"2024-03-19T16:53:53Z","published":"2024-03-19T16:53:53Z","title":"Toward Sustainable GenAI using Generation Directives for Carbon-Friendly\n  Large Language Model Inference","summary":"  The rapid advancement of Generative Artificial Intelligence (GenAI) across\ndiverse sectors raises significant environmental concerns, notably the carbon\nemissions from their cloud and high performance computing (HPC) infrastructure.\nThis paper presents Sprout, an innovative framework designed to address these\nconcerns by reducing the carbon footprint of generative Large Language Model\n(LLM) inference services. Sprout leverages the innovative concept of\n\"generation directives\" to guide the autoregressive generation process, thereby\nenhancing carbon efficiency. Our proposed method meticulously balances the need\nfor ecological sustainability with the demand for high-quality generation\noutcomes. Employing a directive optimizer for the strategic assignment of\ngeneration directives to user prompts and an original offline quality\nevaluator, Sprout demonstrates a significant reduction in carbon emissions by\nover 40% in real-world evaluations using the Llama2 LLM and global electricity\ngrid data. This research marks a critical step toward aligning AI technology\nwith sustainable practices, highlighting the potential for mitigating\nenvironmental impacts in the rapidly expanding domain of generative artificial\nintelligence.\n","authors":["Baolin Li","Yankai Jiang","Vijay Gadepally","Devesh Tiwari"],"pdf_url":"https://arxiv.org/pdf/2403.12900v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11366v2","updated":"2024-03-19T16:19:49Z","published":"2024-03-17T23:02:04Z","title":"JORA: JAX Tensor-Parallel LoRA Library for Retrieval Augmented\n  Fine-Tuning","summary":"  The scaling of Large Language Models (LLMs) for retrieval-based tasks,\nparticularly in Retrieval Augmented Generation (RAG), faces significant memory\nconstraints, especially when fine-tuning extensive prompt sequences. Current\nopen-source libraries support full-model inference and fine-tuning across\nmultiple GPUs but fall short of accommodating the efficient parameter\ndistribution required for retrieved context. Addressing this gap, we introduce\na novel framework for PEFT-compatible fine-tuning of Llama-2 models, leveraging\ndistributed training. Our framework uniquely utilizes JAX's just-in-time (JIT)\ncompilation and tensor-sharding for efficient resource management, thereby\nenabling accelerated fine-tuning with reduced memory requirements. This\nadvancement significantly improves the scalability and feasibility of\nfine-tuning LLMs for complex RAG applications, even on systems with limited GPU\nresources. Our experiments show more than 12x improvement in runtime compared\nto Hugging Face/DeepSpeed implementation with four GPUs while consuming less\nthan half the VRAM per GPU.\n","authors":["Anique Tahir","Lu Cheng","Huan Liu"],"pdf_url":"https://arxiv.org/pdf/2403.11366v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12849v1","updated":"2024-03-19T15:54:56Z","published":"2024-03-19T15:54:56Z","title":"Optimizing Service Placement in Edge-to-Cloud AR/VR Systems using a\n  Multi-Objective Genetic Algorithm","summary":"  Augmented Reality (AR) and Virtual Reality (VR) systems involve\ncomputationally intensive image processing algorithms that can burden\nend-devices with limited resources, leading to poor performance in providing\nlow latency services. Edge-to-cloud computing overcomes the limitations of\nend-devices by offloading their computations to nearby edge devices or remote\ncloud servers. Although this proves to be sufficient for many applications,\noptimal placement of latency sensitive AR/VR services in edge-to-cloud\ninfrastructures (to provide desirable service response times and reliability)\nremain a formidable challenging. To address this challenge, this paper develops\na Multi-Objective Genetic Algorithm (MOGA) to optimize the placement of\nAR/VR-based services in multi-tier edge-to-cloud environments. The primary\nobjective of the proposed MOGA is to minimize the response time of all running\nservices, while maximizing the reliability of the underlying system from both\nsoftware and hardware perspectives. To evaluate its performance, we\nmathematically modeled all components and developed a tailor-made simulator to\nassess its effectiveness on various scales. MOGA was compared with several\nheuristics to prove that intuitive solutions, which are usually assumed\nsufficient, are not efficient enough for the stated problem. The experimental\nresults indicated that MOGA can significantly reduce the response time of\ndeployed services by an average of 67\\% on different scales, compared to other\nheuristic methods. MOGA also ensures reliability of the 97\\% infrastructure\n(hardware) and 95\\% services (software).\n","authors":["Mohammadsadeq Garshasbi Herabad","Javid Taheri","Bestoun S. Ahmed","Calin Curescu"],"pdf_url":"https://arxiv.org/pdf/2403.12849v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08185v2","updated":"2024-03-19T15:12:48Z","published":"2023-11-14T14:14:51Z","title":"Predicting Dynamic Memory Requirements for Scientific Workflow Tasks","summary":"  With the increasing amount of data available to scientists in disciplines as\ndiverse as bioinformatics, physics, and remote sensing, scientific workflow\nsystems are becoming increasingly important for composing and executing\nscalable data analysis pipelines. When writing such workflows, users need to\nspecify the resources to be reserved for tasks so that sufficient resources are\nallocated on the target cluster infrastructure. Crucially, underestimating a\ntask's memory requirements can result in task failures. Therefore, users often\nresort to overprovisioning, resulting in significant resource wastage and\ndecreased throughput. In this paper, we propose a novel online method that uses\nmonitoring time series data to predict task memory usage in order to reduce the\nmemory wastage of scientific workflow tasks. Our method predicts a task's\nruntime, divides it into k equally-sized segments, and learns the peak memory\nvalue for each segment depending on the total file input size. We evaluate the\nprototype implementation of our method using workflows from the publicly\navailable nf-core repository, showing an average memory wastage reduction of\n29.48% compared to the best state-of-the-art approach.\n","authors":["Jonathan Bader","Nils Diedrich","Lauritz Thamsen","Odej Kao"],"pdf_url":"https://arxiv.org/pdf/2311.08185v2.pdf","comment":"Paper accepted in 2023 IEEE International Conference on Big Data"},{"id":"http://arxiv.org/abs/2403.12797v1","updated":"2024-03-19T15:00:39Z","published":"2024-03-19T15:00:39Z","title":"Parallel Gaussian process with kernel approximation in CUDA","summary":"  This paper introduces a parallel implementation in CUDA/C++ of the Gaussian\nprocess with a decomposed kernel. This recent formulation, introduced by Joukov\nand Kuli\\'c (2022), is characterized by an approximated -- but much smaller --\nmatrix to be inverted compared to plain Gaussian process. However, it exhibits\na limitation when dealing with higher-dimensional samples which degrades\nexecution times. The solution presented in this paper relies on parallelizing\nthe computation of the predictive posterior statistics on a GPU using CUDA and\nits libraries. The CPU code and GPU code are then benchmarked on different\nCPU-GPU configurations to show the benefits of the parallel implementation on\nGPU over the CPU.\n","authors":["Davide Carminati"],"pdf_url":"https://arxiv.org/pdf/2403.12797v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00865v2","updated":"2024-03-19T11:23:00Z","published":"2023-12-29T21:49:56Z","title":"Xorbits: Automating Operator Tiling for Distributed Data Science","summary":"  Data science pipelines commonly utilize dataframe and array operations for\ntasks such as data preprocessing, analysis, and machine learning. The most\npopular tools for these tasks are pandas and NumPy. However, these tools are\nlimited to executing on a single node, making them unsuitable for processing\nlarge-scale data. Several systems have attempted to distribute data science\napplications to clusters while maintaining interfaces similar to single-node\nlibraries, enabling data scientists to scale their workloads without\nsignificant effort. However, existing systems often struggle with processing\nlarge datasets due to Out-of-Memory (OOM) problems caused by poor data\npartitioning. To overcome these challenges, we develop Xorbits, a\nhigh-performance, scalable data science framework specifically designed to\ndistribute data science workloads across clusters while retaining familiar\nAPIs. The key differentiator of Xorbits is its ability to dynamically switch\nbetween graph construction and graph execution. Xorbits has been successfully\ndeployed in production environments with up to 5k CPU cores. Its applications\nspan various domains, including user behavior analysis and recommendation\nsystems in the e-commerce sector, as well as credit assessment and risk\nmanagement in the finance industry. Users can easily scale their data science\nworkloads by simply changing the import line of their pandas and NumPy code.\nOur experiments demonstrate that Xorbits can effectively process very large\ndatasets without encountering OOM or data-skewing problems. Over the fastest\nstate-of-the-art solutions, Xorbits achieves an impressive 2.66* speedup on\naverage. In terms of API coverage, Xorbits attains a compatibility rate of\n96.7%, surpassing the fastest framework by an impressive margin of 60\npercentage points. Xorbits is available at\nhttps://github.com/xorbitsai/xorbits.\n","authors":["Weizheng Lu","Kaisheng He","Xuye Qin","Chengjie Li","Zhong Wang","Tao Yuan","Xia Liao","Feng Zhang","Yueguo Chen","Xiaoyong Du"],"pdf_url":"https://arxiv.org/pdf/2401.00865v2.pdf","comment":"ICDE 2024 Industrial and Application Track"},{"id":"http://arxiv.org/abs/2401.09670v2","updated":"2024-03-19T06:20:25Z","published":"2024-01-18T01:03:38Z","title":"DistServe: Disaggregating Prefill and Decoding for Goodput-optimized\n  Large Language Model Serving","summary":"  DistServe improves the performance of large language models (LLMs) serving by\ndisaggregating the prefill and decoding computation. Existing LLM serving\nsystems colocate the two phases and batch the computation of prefill and\ndecoding across all users and requests. We find that this strategy not only\nleads to strong prefill-decoding interferences but also couples the resource\nallocation and parallelism plans for both phases. LLM applications often\nemphasize individual latency for each phase: time to first token (TTFT) for the\nprefill phase and time per output token (TPOT) of each request for the decoding\nphase. In the presence of stringent latency requirements, existing systems have\nto prioritize one latency over the other, or over-provision compute resources\nto meet both.\n  DistServe assigns prefill and decoding computation to different GPUs, hence\neliminating prefill-decoding interferences. Given the application's TTFT and\nTPOT requirements, DistServe co-optimizes the resource allocation and\nparallelism strategy tailored for each phase. DistServe also places the two\nphases according to the serving cluster's bandwidth to minimize the\ncommunication caused by disaggregation. As a result, DistServe significantly\nimproves LLM serving performance in terms of the maximum rate that can be\nserved within both TTFT and TPOT constraints on each GPU. Our evaluations show\nthat on various popular LLMs, applications, and latency requirements, DistServe\ncan serve 4.48x more requests or 10.2x tighter SLO, compared to\nstate-of-the-art systems, while staying within latency constraints for > 90% of\nrequests.\n","authors":["Yinmin Zhong","Shengyu Liu","Junda Chen","Jianbo Hu","Yibo Zhu","Xuanzhe Liu","Xin Jin","Hao Zhang"],"pdf_url":"https://arxiv.org/pdf/2401.09670v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.08656v5","updated":"2024-03-19T02:17:43Z","published":"2023-12-14T05:00:49Z","title":"MaxK-GNN: Extremely Fast GPU Kernel Design for Accelerating Graph Neural\n  Networks Training","summary":"  In the acceleration of deep neural network training, the GPU has become the\nmainstream platform. GPUs face substantial challenges on GNNs, such as workload\nimbalance and memory access irregularities, leading to underutilized hardware.\nExisting solutions such as PyG, DGL with cuSPARSE, and GNNAdvisor frameworks\npartially address these challenges but memory traffic is still significant.\n  We argue that drastic performance improvements can only be achieved by the\nvertical optimization of algorithm and system innovations, rather than treating\nthe speedup optimization as an \"after-thought\" (i.e., (i) given a GNN\nalgorithm, designing an accelerator, or (ii) given hardware, mainly optimizing\nthe GNN algorithm). In this paper, we present MaxK-GNN, an advanced\nhigh-performance GPU training system integrating algorithm and system\ninnovation. (i) We introduce the MaxK nonlinearity and provide a theoretical\nanalysis of MaxK nonlinearity as a universal approximator, and present the\nCompressed Balanced Sparse Row (CBSR) format, designed to store the data and\nindex of the feature matrix after nonlinearity; (ii) We design a coalescing\nenhanced forward computation with row-wise product-based SpGEMM Kernel using\nCBSR for input feature matrix fetching and strategic placement of a sparse\noutput accumulation buffer in shared memory; (iii) We develop an optimized\nbackward computation with outer product-based and SSpMM Kernel.\n  We conduct extensive evaluations of MaxK-GNN and report the end-to-end system\nrun-time. Experiments show that MaxK-GNN system could approach the theoretical\nspeedup limit according to Amdahl's law. We achieve comparable accuracy to SOTA\nGNNs, but at a significantly increased speed: 3.22/4.24 times speedup (vs.\ntheoretical limits, 5.52/7.27 times) on Reddit compared to DGL and GNNAdvisor\nimplementations.\n","authors":["Hongwu Peng","Xi Xie","Kaustubh Shivdikar","MD Amit Hasan","Jiahui Zhao","Shaoyi Huang","Omer Khan","David Kaeli","Caiwen Ding"],"pdf_url":"https://arxiv.org/pdf/2312.08656v5.pdf","comment":"ASPLOS 2024 accepted publication"},{"id":"http://arxiv.org/abs/2403.12345v1","updated":"2024-03-19T01:28:01Z","published":"2024-03-19T01:28:01Z","title":"Performance Portable Monte Carlo Particle Transport on Intel, NVIDIA,\n  and AMD GPUs","summary":"  OpenMC is an open source Monte Carlo neutral particle transport application\nthat has recently been ported to GPU using the OpenMP target offloading model.\nWe examine the performance of OpenMC at scale on the Frontier, Polaris, and\nAurora supercomputers, demonstrating that performance portability has been\nachieved by OpenMC across all three major GPU vendors (AMD, NVIDIA, and Intel).\nOpenMC's GPU performance is compared to both the traditional CPU-based version\nof OpenMC as well as several other state-of-the-art CPU-based Monte Carlo\nparticle transport applications. We also provide historical context by\nanalyzing OpenMC's performance on several legacy GPU and CPU architectures.\nThis work includes some of the first published results for a scientific\nsimulation application at scale on a supercomputer featuring Intel's Max series\n\"Ponte Vecchio\" GPUs. It is also one of the first demonstrations of a large\nscientific production application using the OpenMP target offloading model to\nachieve high performance on all three major GPU platforms.\n","authors":["John Tramm","Paul Romano","Patrick Shriwise","Amanda Lund","Johannes Doerfert","Patrick Steinbrecher","Andrew Siegel","Gavin Ridley"],"pdf_url":"https://arxiv.org/pdf/2403.12345v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12329v1","updated":"2024-03-19T00:03:40Z","published":"2024-03-19T00:03:40Z","title":"FedFisher: Leveraging Fisher Information for One-Shot Federated Learning","summary":"  Standard federated learning (FL) algorithms typically require multiple rounds\nof communication between the server and the clients, which has several\ndrawbacks, including requiring constant network connectivity, repeated\ninvestment of computational resources, and susceptibility to privacy attacks.\nOne-Shot FL is a new paradigm that aims to address this challenge by enabling\nthe server to train a global model in a single round of communication. In this\nwork, we present FedFisher, a novel algorithm for one-shot FL that makes use of\nFisher information matrices computed on local client models, motivated by a\nBayesian perspective of FL. First, we theoretically analyze FedFisher for\ntwo-layer over-parameterized ReLU neural networks and show that the error of\nour one-shot FedFisher global model becomes vanishingly small as the width of\nthe neural networks and amount of local training at clients increases. Next, we\npropose practical variants of FedFisher using the diagonal Fisher and K-FAC\napproximation for the full Fisher and highlight their communication and compute\nefficiency for FL. Finally, we conduct extensive experiments on various\ndatasets, which show that these variants of FedFisher consistently improve over\ncompeting baselines.\n","authors":["Divyansh Jhunjhunwala","Shiqiang Wang","Gauri Joshi"],"pdf_url":"https://arxiv.org/pdf/2403.12329v1.pdf","comment":"Accepted to AISTATS 2024"},{"id":"http://arxiv.org/abs/2403.13199v1","updated":"2024-03-19T23:23:35Z","published":"2024-03-19T23:23:35Z","title":"DecentNeRFs: Decentralized Neural Radiance Fields from Crowdsourced\n  Images","summary":"  Neural radiance fields (NeRFs) show potential for transforming images\ncaptured worldwide into immersive 3D visual experiences. However, most of this\ncaptured visual data remains siloed in our camera rolls as these images contain\npersonal details. Even if made public, the problem of learning 3D\nrepresentations of billions of scenes captured daily in a centralized manner is\ncomputationally intractable. Our approach, DecentNeRF, is the first attempt at\ndecentralized, crowd-sourced NeRFs that require $\\sim 10^4\\times$ less server\ncomputing for a scene than a centralized approach. Instead of sending the raw\ndata, our approach requires users to send a 3D representation, distributing the\nhigh computation cost of training centralized NeRFs between the users. It\nlearns photorealistic scene representations by decomposing users' 3D views into\npersonal and global NeRFs and a novel optimally weighted aggregation of only\nthe latter. We validate the advantage of our approach to learn NeRFs with\nphotorealism and minimal server computation cost on structured synthetic and\nreal-world photo tourism datasets. We further analyze how secure aggregation of\nglobal NeRFs in DecentNeRF minimizes the undesired reconstruction of personal\ncontent by the server.\n","authors":["Zaid Tasneem","Akshat Dave","Abhishek Singh","Kushagra Tiwary","Praneeth Vepakomma","Ashok Veeraraghavan","Ramesh Raskar"],"pdf_url":"https://arxiv.org/pdf/2403.13199v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.14007v2","updated":"2024-03-19T21:05:16Z","published":"2023-11-23T13:48:52Z","title":"Extending JSON CRDTs with Move Operations","summary":"  Conflict-Free Replicated Data Types (CRDTs) for JSON allow users to\nconcurrently update a JSON document and automatically merge the updates into a\nconsistent state. Moving a subtree in a map or reordering elements in a list\nwithin a JSON CRDT is challenging: naive merge algorithms may introduce\nunexpected results such as duplicates or cycles. In this paper, we introduce an\nalgorithm for move operations in a JSON CRDT that handles the interaction with\nconcurrent non-move operations, and uses novel optimisations to improve\nperformance. We plan to integrate this algorithm into the Automerge CRDT\nlibrary.\n","authors":["Liangrun Da","Martin Kleppmann"],"pdf_url":"https://arxiv.org/pdf/2311.14007v2.pdf","comment":"7 pages, 4 figures"},{"id":"http://arxiv.org/abs/2403.13135v1","updated":"2024-03-19T20:10:50Z","published":"2024-03-19T20:10:50Z","title":"A Parallel Workflow for Polar Sea-Ice Classification using Auto-labeling\n  of Sentinel-2 Imagery","summary":"  The observation of the advancing and retreating pattern of polar sea ice\ncover stands as a vital indicator of global warming. This research aims to\ndevelop a robust, effective, and scalable system for classifying polar sea ice\nas thick/snow-covered, young/thin, or open water using Sentinel-2 (S2) images.\nSince the S2 satellite is actively capturing high-resolution imagery over the\nearth's surface, there are lots of images that need to be classified. One major\nobstacle is the absence of labeled S2 training data (images) to act as the\nground truth. We demonstrate a scalable and accurate method for segmenting and\nautomatically labeling S2 images using carefully determined color thresholds.\nWe employ a parallel workflow using PySpark to scale and achieve 9-fold data\nloading and 16-fold map-reduce speedup on auto-labeling S2 images based on thin\ncloud and shadow-filtered color-based segmentation to generate label data. The\nauto-labeled data generated from this process are then employed to train a\nU-Net machine learning model, resulting in good classification accuracy. As\ntraining the U-Net classification model is computationally heavy and\ntime-consuming, we distribute the U-Net model training to scale it over 8 GPUs\nusing the Horovod framework over a DGX cluster with a 7.21x speedup without\naffecting the accuracy of the model. Using the Antarctic's Ross Sea region as\nan example, the U-Net model trained on auto-labeled data achieves a\nclassification accuracy of 98.97% for auto-labeled training datasets when the\nthin clouds and shadows from the S2 images are filtered out.\n","authors":["Jurdana Masuma Iqrah","Wei Wang","Hongjie Xie","Sushil Prasad"],"pdf_url":"https://arxiv.org/pdf/2403.13135v1.pdf","comment":"Accepted in the 25th IEEE International Workshop on Parallel and\n  Distributed Scientific and Engineering Computing (PDSEC 2024), May 2024.\n  arXiv admin note: substantial text overlap with arXiv:2303.12719"},{"id":"http://arxiv.org/abs/2403.13108v1","updated":"2024-03-19T19:15:38Z","published":"2024-03-19T19:15:38Z","title":"Analyzing the Impact of Partial Sharing on the Resilience of Online\n  Federated Learning Against Model Poisoning Attacks","summary":"  We scrutinize the resilience of the partial-sharing online federated learning\n(PSO-Fed) algorithm against model-poisoning attacks. PSO-Fed reduces the\ncommunication load by enabling clients to exchange only a fraction of their\nmodel estimates with the server at each update round. Partial sharing of model\nestimates also enhances the robustness of the algorithm against model-poisoning\nattacks. To gain better insights into this phenomenon, we analyze the\nperformance of the PSO-Fed algorithm in the presence of Byzantine clients,\nmalicious actors who may subtly tamper with their local models by adding noise\nbefore sharing them with the server. Through our analysis, we demonstrate that\nPSO-Fed maintains convergence in both mean and mean-square senses, even under\nthe strain of model-poisoning attacks. We further derive the theoretical mean\nsquare error (MSE) of PSO-Fed, linking it to various parameters such as\nstepsize, attack probability, number of Byzantine clients, client participation\nrate, partial-sharing ratio, and noise variance. We also show that there is a\nnon-trivial optimal stepsize for PSO-Fed when faced with model-poisoning\nattacks. The results of our extensive numerical experiments affirm our\ntheoretical assertions and highlight the superior ability of PSO-Fed to\ncounteract Byzantine attacks, outperforming other related leading algorithms.\n","authors":["Ehsan Lari","Vinay Chakravarthi Gogineni","Reza Arablouei","Stefan Werner"],"pdf_url":"https://arxiv.org/pdf/2403.13108v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13101v1","updated":"2024-03-19T19:05:24Z","published":"2024-03-19T19:05:24Z","title":"AdaptSFL: Adaptive Split Federated Learning in Resource-constrained Edge\n  Networks","summary":"  The increasing complexity of deep neural networks poses significant barriers\nto democratizing them to resource-limited edge devices. To address this\nchallenge, split federated learning (SFL) has emerged as a promising solution\nby of floading the primary training workload to a server via model partitioning\nwhile enabling parallel training among edge devices. However, although system\noptimization substantially influences the performance of SFL under\nresource-constrained systems, the problem remains largely uncharted. In this\npaper, we provide a convergence analysis of SFL which quantifies the impact of\nmodel splitting (MS) and client-side model aggregation (MA) on the learning\nperformance, serving as a theoretical foundation. Then, we propose AdaptSFL, a\nnovel resource-adaptive SFL framework, to expedite SFL under\nresource-constrained edge computing systems. Specifically, AdaptSFL adaptively\ncontrols client-side MA and MS to balance communication-computing latency and\ntraining convergence. Extensive simulations across various datasets validate\nthat our proposed AdaptSFL framework takes considerably less time to achieve a\ntarget accuracy than benchmarks, demonstrating the effectiveness of the\nproposed strategies.\n","authors":["Zheng Lin","Guanqiao Qu","Wei Wei","Xianhao Chen","Kin K. Leung"],"pdf_url":"https://arxiv.org/pdf/2403.13101v1.pdf","comment":"15 pages, 10 figures"},{"id":"http://arxiv.org/abs/2403.14718v1","updated":"2024-03-19T09:34:01Z","published":"2024-03-19T09:34:01Z","title":"FedSR: A Semi-Decentralized Federated Learning Algorithm for Non-IIDness\n  in IoT System","summary":"  In the Industrial Internet of Things (IoT), a large amount of data will be\ngenerated every day. Due to privacy and security issues, it is difficult to\ncollect all these data together to train deep learning models, thus the\nfederated learning, a distributed machine learning paradigm that protects data\nprivacy, has been widely used in IoT. However, in practical federated learning,\nthe data distributions usually have large differences across devices, and the\nheterogeneity of data will deteriorate the performance of the model. Moreover,\nfederated learning in IoT usually has a large number of devices involved in\ntraining, and the limited communication resource of cloud servers become a\nbottleneck for training. To address the above issues, in this paper, we combine\ncentralized federated learning with decentralized federated learning to design\na semi-decentralized cloud-edge-device hierarchical federated learning\nframework, which can mitigate the impact of data heterogeneity, and can be\ndeployed at lage scale in IoT. To address the effect of data heterogeneity, we\nuse an incremental subgradient optimization algorithm in each ring cluster to\nimprove the generalization ability of the ring cluster models. Our extensive\nexperiments show that our approach can effectively mitigate the impact of data\nheterogeneity and alleviate the communication bottleneck in cloud servers.\n","authors":["Jianjun Huang","Lixin Ye","Li Kang"],"pdf_url":"https://arxiv.org/pdf/2403.14718v1.pdf","comment":"11 pages, 10 figures"},{"id":"http://arxiv.org/abs/2403.14716v1","updated":"2024-03-19T06:48:40Z","published":"2024-03-19T06:48:40Z","title":"Distributed Learning based on 1-Bit Gradient Coding in the Presence of\n  Stragglers","summary":"  This paper considers the problem of distributed learning (DL) in the presence\nof stragglers. For this problem, DL methods based on gradient coding have been\nwidely investigated, which redundantly distribute the training data to the\nworkers to guarantee convergence when some workers are stragglers. However,\nthese methods require the workers to transmit real-valued vectors during the\nprocess of learning, which induces very high communication burden. To overcome\nthis drawback, we propose a novel DL method based on 1-bit gradient coding\n(1-bit GCDL), where 1-bit data encoded from the locally computed gradients are\ntransmitted by the workers to reduce the communication overhead. We\ntheoretically provide the convergence guarantees of the proposed method for\nboth the convex loss functions and nonconvex loss functions. It is shown\nempirically that 1-bit GC-DL outperforms the baseline methods, which attains\nbetter learning performance under the same communication overhead.\n","authors":["Chengxi Li","Mikael Skoglund"],"pdf_url":"https://arxiv.org/pdf/2403.14716v1.pdf","comment":"Accepted by IEEE TCOM"}],"Performance":[{"id":"http://arxiv.org/abs/2311.04417v3","updated":"2024-03-19T06:31:52Z","published":"2023-11-08T01:06:25Z","title":"Evaluating Emerging AI/ML Accelerators: IPU, RDU, and NVIDIA/AMD GPUs","summary":"  The relentless advancement of artificial intelligence (AI) and machine\nlearning (ML) applications necessitates the development of specialized hardware\naccelerators capable of handling the increasing complexity and computational\ndemands. Traditional computing architectures, based on the von Neumann model,\nare being outstripped by the requirements of contemporary AI/ML algorithms,\nleading to a surge in the creation of accelerators like the Graphcore\nIntelligence Processing Unit (IPU), Sambanova Reconfigurable Dataflow Unit\n(RDU), and enhanced GPU platforms. These hardware accelerators are\ncharacterized by their innovative data-flow architectures and other design\noptimizations that promise to deliver superior performance and energy\nefficiency for AI/ML tasks.\n  This research provides a preliminary evaluation and comparison of these\ncommercial AI/ML accelerators, delving into their hardware and software design\nfeatures to discern their strengths and unique capabilities. By conducting a\nseries of benchmark evaluations on common DNN operators and other AI/ML\nworkloads, we aim to illuminate the advantages of data-flow architectures over\nconventional processor designs and offer insights into the performance\ntrade-offs of each platform. The findings from our study will serve as a\nvaluable reference for the design and performance expectations of research\nprototypes, thereby facilitating the development of next-generation hardware\naccelerators tailored for the ever-evolving landscape of AI/ML applications.\nThrough this analysis, we aspire to contribute to the broader understanding of\ncurrent accelerator technologies and to provide guidance for future innovations\nin the field.\n","authors":["Hongwu Peng","Caiwen Ding","Tong Geng","Sutanay Choudhury","Kevin Barker","Ang Li"],"pdf_url":"https://arxiv.org/pdf/2311.04417v3.pdf","comment":"ICPE 2024 accepted publication"},{"id":"http://arxiv.org/abs/2305.05249v3","updated":"2024-03-19T09:26:16Z","published":"2023-05-09T08:14:28Z","title":"Enhanced Scalability in Assessing Quantum Integer Factorization\n  Performance","summary":"  With the advancement of quantum technologies, there is a potential threat to\ntraditional encryption systems based on integer factorization. Therefore,\ndeveloping techniques for accurately measuring the performance of associated\nquantum algorithms is crucial, as it can provide insights into the practical\nfeasibility from the current perspective. In this chapter, we aim to analyze\nthe time required for integer factorization tasks using Shor's algorithm within\na gate-based quantum circuit simulator of the matrix product state type.\nAdditionally, we observe the impact of parameter pre-selection in Shor's\nalgorithm. Specifically, this pre-selection is expected to increase the success\nrate of integer factorization by reducing the number of iterations and\nfacilitating performance measurement under fixed conditions, thus enabling\nscalable performance evaluation even on real quantum hardware.\n","authors":["Junseo Lee"],"pdf_url":"https://arxiv.org/pdf/2305.05249v3.pdf","comment":"13 pages, 3 figures, 2 tables"}],"Artificial Intelligence":[{"id":"http://arxiv.org/abs/2403.12961v1","updated":"2024-03-19T17:59:09Z","published":"2024-03-19T17:59:09Z","title":"TexTile: A Differentiable Metric for Texture Tileability","summary":"  We introduce TexTile, a novel differentiable metric to quantify the degree\nupon which a texture image can be concatenated with itself without introducing\nrepeating artifacts (i.e., the tileability). Existing methods for tileable\ntexture synthesis focus on general texture quality, but lack explicit analysis\nof the intrinsic repeatability properties of a texture. In contrast, our\nTexTile metric effectively evaluates the tileable properties of a texture,\nopening the door to more informed synthesis and analysis of tileable textures.\nUnder the hood, TexTile is formulated as a binary classifier carefully built\nfrom a large dataset of textures of different styles, semantics, regularities,\nand human annotations.Key to our method is a set of architectural modifications\nto baseline pre-train image classifiers to overcome their shortcomings at\nmeasuring tileability, along with a custom data augmentation and training\nregime aimed at increasing robustness and accuracy. We demonstrate that TexTile\ncan be plugged into different state-of-the-art texture synthesis methods,\nincluding diffusion-based strategies, and generate tileable textures while\nkeeping or even improving the overall texture quality. Furthermore, we show\nthat TexTile can objectively evaluate any tileable texture synthesis method,\nwhereas the current mix of existing metrics produces uncorrelated scores which\nheavily hinders progress in the field.\n","authors":["Carlos Rodriguez-Pardo","Dan Casas","Elena Garces","Jorge Lopez-Moreno"],"pdf_url":"https://arxiv.org/pdf/2403.12961v1.pdf","comment":"CVPR 2024. Project page: https://mslab.es/projects/TexTile/"},{"id":"http://arxiv.org/abs/2403.12959v1","updated":"2024-03-19T17:58:02Z","published":"2024-03-19T17:58:02Z","title":"WHAC: World-grounded Humans and Cameras","summary":"  Estimating human and camera trajectories with accurate scale in the world\ncoordinate system from a monocular video is a highly desirable yet challenging\nand ill-posed problem. In this study, we aim to recover expressive parametric\nhuman models (i.e., SMPL-X) and corresponding camera poses jointly, by\nleveraging the synergy between three critical players: the world, the human,\nand the camera. Our approach is founded on two key observations. Firstly,\ncamera-frame SMPL-X estimation methods readily recover absolute human depth.\nSecondly, human motions inherently provide absolute spatial cues. By\nintegrating these insights, we introduce a novel framework, referred to as\nWHAC, to facilitate world-grounded expressive human pose and shape estimation\n(EHPS) alongside camera pose estimation, without relying on traditional\noptimization techniques. Additionally, we present a new synthetic dataset,\nWHAC-A-Mole, which includes accurately annotated humans and cameras, and\nfeatures diverse interactive human motions as well as realistic camera\ntrajectories. Extensive experiments on both standard and newly established\nbenchmarks highlight the superiority and efficacy of our framework. We will\nmake the code and dataset publicly available.\n","authors":["Wanqi Yin","Zhongang Cai","Ruisi Wang","Fanzhou Wang","Chen Wei","Haiyi Mei","Weiye Xiao","Zhitao Yang","Qingping Sun","Atsushi Yamashita","Ziwei Liu","Lei Yang"],"pdf_url":"https://arxiv.org/pdf/2403.12959v1.pdf","comment":"Homepage: https://wqyin.github.io/projects/WHAC/"},{"id":"http://arxiv.org/abs/2403.12952v1","updated":"2024-03-19T17:54:34Z","published":"2024-03-19T17:54:34Z","title":"Just Shift It: Test-Time Prototype Shifting for Zero-Shot Generalization\n  with Vision-Language Models","summary":"  Advancements in vision-language models (VLMs) have propelled the field of\ncomputer vision, particularly in the zero-shot learning setting. Despite their\npromise, the effectiveness of these models often diminishes due to domain\nshifts in test environments. To address this, we introduce the Test-Time\nPrototype Shifting (TPS) framework, a pioneering approach designed to adapt\nVLMs to test datasets using unlabeled test inputs. Our method is based on the\nnotion of modulating per-class prototypes in the shared embedding space. By\npre-computing and caching prototypes generated with the pre-trained text\nencoder, TPS not only facilitates optimization-free prototype reuse for\nsubsequent predictions but also enables seamless integration with current\nadvancements in prompt engineering. At test-time, TPS dynamically learns shift\nvectors for each prototype based solely on the given test sample, effectively\nbridging the domain gap and enhancing classification accuracy. A notable aspect\nof our framework is its significantly reduced memory and computational demands\nwhen compared to conventional text-prompt tuning methods. Extensive evaluations\nacross 15 datasets involving natural distribution shifts and cross-dataset\ngeneralization demonstrate TPS's superior performance, achieving\nstate-of-the-art results while reducing resource requirements.\n","authors":["Elaine Sui","Xiaohan Wang","Serena Yeung-Levy"],"pdf_url":"https://arxiv.org/pdf/2403.12952v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06420v2","updated":"2024-03-19T17:52:09Z","published":"2024-03-11T04:13:26Z","title":"RLingua: Improving Reinforcement Learning Sample Efficiency in Robotic\n  Manipulations With Large Language Models","summary":"  Reinforcement learning (RL) has demonstrated its capability in solving\nvarious tasks but is notorious for its low sample efficiency. In this paper, we\npropose RLingua, a framework that can leverage the internal knowledge of large\nlanguage models (LLMs) to reduce the sample complexity of RL in robotic\nmanipulations. To this end, we first present a method for extracting the prior\nknowledge of LLMs by prompt engineering so that a preliminary rule-based robot\ncontroller for a specific task can be generated in a user-friendly manner.\nDespite being imperfect, the LLM-generated robot controller is utilized to\nproduce action samples during rollouts with a decaying probability, thereby\nimproving RL's sample efficiency. We employ TD3, the widely-used RL baseline\nmethod, and modify the actor loss to regularize the policy learning towards the\nLLM-generated controller. RLingua also provides a novel method of improving the\nimperfect LLM-generated robot controllers by RL. We demonstrate that RLingua\ncan significantly reduce the sample complexity of TD3 in four robot tasks of\npanda_gym and achieve high success rates in 12 sampled sparsely rewarded robot\ntasks in RLBench, where the standard TD3 fails. Additionally, We validated\nRLingua's effectiveness in real-world robot experiments through Sim2Real,\ndemonstrating that the learned policies are effectively transferable to real\nrobot tasks. Further details about our work are available at our project\nwebsite https://rlingua.github.io.\n","authors":["Liangliang Chen","Yutian Lei","Shiyu Jin","Ying Zhang","Liangjun Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.06420v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12943v1","updated":"2024-03-19T17:47:37Z","published":"2024-03-19T17:47:37Z","title":"Vid2Robot: End-to-end Video-conditioned Policy Learning with\n  Cross-Attention Transformers","summary":"  While large-scale robotic systems typically rely on textual instructions for\ntasks, this work explores a different approach: can robots infer the task\ndirectly from observing humans? This shift necessitates the robot's ability to\ndecode human intent and translate it into executable actions within its\nphysical constraints and environment. We introduce Vid2Robot, a novel\nend-to-end video-based learning framework for robots. Given a video\ndemonstration of a manipulation task and current visual observations, Vid2Robot\ndirectly produces robot actions. This is achieved through a unified\nrepresentation model trained on a large dataset of human video and robot\ntrajectory. The model leverages cross-attention mechanisms to fuse prompt video\nfeatures to the robot's current state and generate appropriate actions that\nmimic the observed task. To further improve policy performance, we propose\nauxiliary contrastive losses that enhance the alignment between human and robot\nvideo representations. We evaluate Vid2Robot on real-world robots,\ndemonstrating a 20% improvement in performance compared to other\nvideo-conditioned policies when using human demonstration videos. Additionally,\nour model exhibits emergent capabilities, such as successfully transferring\nobserved motions from one object to another, and long-horizon composition, thus\nshowcasing its potential for real-world applications. Project website:\nvid2robot.github.io\n","authors":["Vidhi Jain","Maria Attarian","Nikhil J Joshi","Ayzaan Wahid","Danny Driess","Quan Vuong","Pannag R Sanketi","Pierre Sermanet","Stefan Welker","Christine Chan","Igor Gilitschenski","Yonatan Bisk","Debidatta Dwibedi"],"pdf_url":"https://arxiv.org/pdf/2403.12943v1.pdf","comment":"Robot learning: Imitation Learning, Robot Perception, Sensing &\n  Vision, Grasping & Manipulation"},{"id":"http://arxiv.org/abs/2403.12936v1","updated":"2024-03-19T17:43:08Z","published":"2024-03-19T17:43:08Z","title":"Automatic Information Extraction From Employment Tribunal Judgements\n  Using Large Language Models","summary":"  Court transcripts and judgments are rich repositories of legal knowledge,\ndetailing the intricacies of cases and the rationale behind judicial decisions.\nThe extraction of key information from these documents provides a concise\noverview of a case, crucial for both legal experts and the public. With the\nadvent of large language models (LLMs), automatic information extraction has\nbecome increasingly feasible and efficient. This paper presents a comprehensive\nstudy on the application of GPT-4, a large language model, for automatic\ninformation extraction from UK Employment Tribunal (UKET) cases. We\nmeticulously evaluated GPT-4's performance in extracting critical information\nwith a manual verification process to ensure the accuracy and relevance of the\nextracted data. Our research is structured around two primary extraction tasks:\nthe first involves a general extraction of eight key aspects that hold\nsignificance for both legal specialists and the general public, including the\nfacts of the case, the claims made, references to legal statutes, references to\nprecedents, general case outcomes and corresponding labels, detailed order and\nremedies and reasons for the decision. The second task is more focused, aimed\nat analysing three of those extracted features, namely facts, claims and\noutcomes, in order to facilitate the development of a tool capable of\npredicting the outcome of employment law disputes. Through our analysis, we\ndemonstrate that LLMs like GPT-4 can obtain high accuracy in legal information\nextraction, highlighting the potential of LLMs in revolutionising the way legal\ninformation is processed and utilised, offering significant implications for\nlegal research and practice.\n","authors":["Joana Ribeiro de Faria","Huiyuan Xie","Felix Steffek"],"pdf_url":"https://arxiv.org/pdf/2403.12936v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12918v1","updated":"2024-03-19T17:21:29Z","published":"2024-03-19T17:21:29Z","title":"Generalizable and Stable Finetuning of Pretrained Language Models on\n  Low-Resource Texts","summary":"  Pretrained Language Models (PLMs) have advanced Natural Language Processing\n(NLP) tasks significantly, but finetuning PLMs on low-resource datasets poses\nsignificant challenges such as instability and overfitting. Previous methods\ntackle these issues by finetuning a strategically chosen subnetwork on a\ndownstream task, while keeping the remaining weights fixed to the pretrained\nweights. However, they rely on a suboptimal criteria for sub-network selection,\nleading to suboptimal solutions. To address these limitations, we propose a\nregularization method based on attention-guided weight mixup for finetuning\nPLMs. Our approach represents each network weight as a mixup of task-specific\nweight and pretrained weight, controlled by a learnable attention parameter,\nproviding finer control over sub-network selection. Furthermore, we employ a\nbi-level optimization (BLO) based framework on two separate splits of the\ntraining dataset, improving generalization and combating overfitting. We\nvalidate the efficacy of our proposed method through extensive experiments,\ndemonstrating its superiority over previous methods, particularly in the\ncontext of finetuning PLMs on low-resource datasets.\n","authors":["Sai Ashish Somayajula","Youwei Liang","Abhishek Singh","Li Zhang","Pengtao Xie"],"pdf_url":"https://arxiv.org/pdf/2403.12918v1.pdf","comment":"Accepted as a long paper to NAACL 2024 Main Conference; 18 pages, 11\n  tables, 3 figures"},{"id":"http://arxiv.org/abs/2403.11942v2","updated":"2024-03-19T17:20:59Z","published":"2024-03-18T16:36:54Z","title":"Exploring Facial Expression Recognition through Semi-Supervised\n  Pretraining and Temporal Modeling","summary":"  Facial Expression Recognition (FER) plays a crucial role in computer vision\nand finds extensive applications across various fields. This paper aims to\npresent our approach for the upcoming 6th Affective Behavior Analysis\nin-the-Wild (ABAW) competition, scheduled to be held at CVPR2024. In the facial\nexpression recognition task, The limited size of the FER dataset poses a\nchallenge to the expression recognition model's generalization ability,\nresulting in subpar recognition performance. To address this problem, we employ\na semi-supervised learning technique to generate expression category\npseudo-labels for unlabeled face data. At the same time, we uniformly sampled\nthe labeled facial expression samples and implemented a debiased feedback\nlearning strategy to address the problem of category imbalance in the dataset\nand the possible data bias in semi-supervised learning. Moreover, to further\ncompensate for the limitation and bias of features obtained only from static\nimages, we introduced a Temporal Encoder to learn and capture temporal\nrelationships between neighbouring expression image features. In the 6th ABAW\ncompetition, our method achieved outstanding results on the official validation\nset, a result that fully confirms the effectiveness and competitiveness of our\nproposed method.\n","authors":["Jun Yu","Zhihong Wei","Zhongpeng Cai","Gongpeng Zhao","Zerui Zhang","Yongqi Wang","Guochen Xie","Jichao Zhu","Wangyuan Zhu"],"pdf_url":"https://arxiv.org/pdf/2403.11942v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.15619v2","updated":"2024-03-19T17:17:50Z","published":"2023-11-27T08:32:28Z","title":"Align before Adapt: Leveraging Entity-to-Region Alignments for\n  Generalizable Video Action Recognition","summary":"  Large-scale visual-language pre-trained models have achieved significant\nsuccess in various video tasks. However, most existing methods follow an \"adapt\nthen align\" paradigm, which adapts pre-trained image encoders to model\nvideo-level representations and utilizes one-hot or text embedding of the\naction labels for supervision. This paradigm overlooks the challenge of mapping\nfrom static images to complicated activity concepts. In this paper, we propose\na novel \"Align before Adapt\" (ALT) paradigm. Prior to adapting to video\nrepresentation learning, we exploit the entity-to-region alignments for each\nframe. The alignments are fulfilled by matching the region-aware image\nembeddings to an offline-constructed text corpus. With the aligned entities, we\nfeed their text embeddings to a transformer-based video adapter as the queries,\nwhich can help extract the semantics of the most important entities from a\nvideo to a vector. This paradigm reuses the visual-language alignment of VLP\nduring adaptation and tries to explain an action by the underlying entities.\nThis helps understand actions by bridging the gap with complex activity\nsemantics, particularly when facing unfamiliar or unseen categories. ALT\ndemonstrates competitive performance while maintaining remarkably low\ncomputational costs. In fully supervised experiments, it achieves 88.1% top-1\naccuracy on Kinetics-400 with only 4947 GFLOPs. Moreover, ALT outperforms the\nprevious state-of-the-art methods in both zero-shot and few-shot experiments,\nemphasizing its superior generalizability across various learning scenarios.\n","authors":["Yifei Chen","Dapeng Chen","Ruijin Liu","Sai Zhou","Wenyuan Xue","Wei Peng"],"pdf_url":"https://arxiv.org/pdf/2311.15619v2.pdf","comment":"Accepted at CVPR 2024"},{"id":"http://arxiv.org/abs/2403.12910v1","updated":"2024-03-19T17:08:24Z","published":"2024-03-19T17:08:24Z","title":"Yell At Your Robot: Improving On-the-Fly from Language Corrections","summary":"  Hierarchical policies that combine language and low-level control have been\nshown to perform impressively long-horizon robotic tasks, by leveraging either\nzero-shot high-level planners like pretrained language and vision-language\nmodels (LLMs/VLMs) or models trained on annotated robotic demonstrations.\nHowever, for complex and dexterous skills, attaining high success rates on\nlong-horizon tasks still represents a major challenge -- the longer the task\nis, the more likely it is that some stage will fail. Can humans help the robot\nto continuously improve its long-horizon task performance through intuitive and\nnatural feedback? In this paper, we make the following observation: high-level\npolicies that index into sufficiently rich and expressive low-level\nlanguage-conditioned skills can be readily supervised with human feedback in\nthe form of language corrections. We show that even fine-grained corrections,\nsuch as small movements (\"move a bit to the left\"), can be effectively\nincorporated into high-level policies, and that such corrections can be readily\nobtained from humans observing the robot and making occasional suggestions.\nThis framework enables robots not only to rapidly adapt to real-time language\nfeedback, but also incorporate this feedback into an iterative training scheme\nthat improves the high-level policy's ability to correct errors in both\nlow-level execution and high-level decision-making purely from verbal feedback.\nOur evaluation on real hardware shows that this leads to significant\nperformance improvement in long-horizon, dexterous manipulation tasks without\nthe need for any additional teleoperation. Videos and code are available at\nhttps://yay-robot.github.io/.\n","authors":["Lucy Xiaoyang Shi","Zheyuan Hu","Tony Z. Zhao","Archit Sharma","Karl Pertsch","Jianlan Luo","Sergey Levine","Chelsea Finn"],"pdf_url":"https://arxiv.org/pdf/2403.12910v1.pdf","comment":"Project website: https://yay-robot.github.io/"},{"id":"http://arxiv.org/abs/2209.00626v6","updated":"2024-03-19T17:07:47Z","published":"2022-08-30T02:12:47Z","title":"The Alignment Problem from a Deep Learning Perspective","summary":"  In coming years or decades, artificial general intelligence (AGI) may surpass\nhuman capabilities at many critical tasks. We argue that, without substantial\neffort to prevent it, AGIs could learn to pursue goals that are in conflict\n(i.e. misaligned) with human interests. If trained like today's most capable\nmodels, AGIs could learn to act deceptively to receive higher reward, learn\nmisaligned internally-represented goals which generalize beyond their\nfine-tuning distributions, and pursue those goals using power-seeking\nstrategies. We review emerging evidence for these properties. AGIs with these\nproperties would be difficult to align and may appear aligned even when they\nare not. Finally, we briefly outline how the deployment of misaligned AGIs\nmight irreversibly undermine human control over the world, and we review\nresearch directions aimed at preventing this outcome.\n","authors":["Richard Ngo","Lawrence Chan","Sören Mindermann"],"pdf_url":"https://arxiv.org/pdf/2209.00626v6.pdf","comment":"Published in ICLR 2024"},{"id":"http://arxiv.org/abs/2401.07931v2","updated":"2024-03-19T17:07:40Z","published":"2024-01-15T19:47:14Z","title":"Vertical Federated Image Segmentation","summary":"  With the popularization of AI solutions for image based problems, there has\nbeen a growing concern for both data privacy and acquisition. In a large number\nof cases, information is located on separate data silos and it can be difficult\nfor a developer to consolidate all of it in a fashion that is appropriate for\nmachine learning model development. Alongside this, a portion of these\nlocalized data regions may not have access to a labelled ground truth. This\nindicates that they have the capacity to reach conclusions numerically, but are\nnot able to assign classifications amid a lack of pertinent information. Such a\ndetermination is often negligible, especially when attempting to develop image\nbased solutions that often necessitate this capability. With this being the\ncase, we propose an innovative vertical federated learning (VFL) model\narchitecture that can operate under this common set of conditions. This is the\nfirst (and currently the only) implementation of a system that can work under\nthe constraints of a VFL environment and perform image segmentation while\nmaintaining nominal accuracies. We achieved this by utilizing an FCN that\nboasts the ability to operate on federates that lack labelled data and\nprivately share the respective weights with a central server, that of which\nhosts the necessary features for classification. Tests were conducted on the\nCamVid dataset in order to determine the impact of heavy feature compression\nrequired for the transfer of information between federates, as well as to reach\nnominal conclusions about the overall performance metrics when working under\nsuch constraints.\n","authors":["Paul K. Mandal","Cole Leo"],"pdf_url":"https://arxiv.org/pdf/2401.07931v2.pdf","comment":"11 pages, 5 figures"},{"id":"http://arxiv.org/abs/2403.11492v2","updated":"2024-03-19T17:04:35Z","published":"2024-03-18T05:53:20Z","title":"SmartRefine: A Scenario-Adaptive Refinement Framework for Efficient\n  Motion Prediction","summary":"  Predicting the future motion of surrounding agents is essential for\nautonomous vehicles (AVs) to operate safely in dynamic, human-robot-mixed\nenvironments. Context information, such as road maps and surrounding agents'\nstates, provides crucial geometric and semantic information for motion behavior\nprediction. To this end, recent works explore two-stage prediction frameworks\nwhere coarse trajectories are first proposed, and then used to select critical\ncontext information for trajectory refinement. However, they either incur a\nlarge amount of computation or bring limited improvement, if not both. In this\npaper, we introduce a novel scenario-adaptive refinement strategy, named\nSmartRefine, to refine prediction with minimal additional computation.\nSpecifically, SmartRefine can comprehensively adapt refinement configurations\nbased on each scenario's properties, and smartly chooses the number of\nrefinement iterations by introducing a quality score to measure the prediction\nquality and remaining refinement potential of each scenario. SmartRefine is\ndesigned as a generic and flexible approach that can be seamlessly integrated\ninto most state-of-the-art motion prediction models. Experiments on Argoverse\n(1 & 2) show that our method consistently improves the prediction accuracy of\nmultiple state-of-the-art prediction models. Specifically, by adding\nSmartRefine to QCNet, we outperform all published ensemble-free works on the\nArgoverse 2 leaderboard (single agent track) at submission. Comprehensive\nstudies are also conducted to ablate design choices and explore the mechanism\nbehind multi-iteration refinement. Codes are available at\nhttps://github.com/opendilab/SmartRefine/\n","authors":["Yang Zhou","Hao Shao","Letian Wang","Steven L. Waslander","Hongsheng Li","Yu Liu"],"pdf_url":"https://arxiv.org/pdf/2403.11492v2.pdf","comment":"Camera-ready version for CVPR 2024"},{"id":"http://arxiv.org/abs/2306.08666v2","updated":"2024-03-19T17:01:03Z","published":"2023-06-14T17:57:24Z","title":"Radiology-GPT: A Large Language Model for Radiology","summary":"  We introduce Radiology-GPT, a large language model for radiology. Using an\ninstruction tuning approach on an extensive dataset of radiology domain\nknowledge, Radiology-GPT demonstrates superior performance compared to general\nlanguage models such as StableLM, Dolly and LLaMA. It exhibits significant\nversatility in radiological diagnosis, research, and communication. This work\nserves as a catalyst for future developments in clinical NLP. The successful\nimplementation of Radiology-GPT is indicative of the potential of localizing\ngenerative large language models, specifically tailored for distinctive medical\nspecialties, while ensuring adherence to privacy standards such as HIPAA. The\nprospect of developing individualized, large-scale language models that cater\nto specific needs of various hospitals presents a promising direction. The\nfusion of conversational competence and domain-specific knowledge in these\nmodels is set to foster future development in healthcare AI. A demo of\nRadiology-GPT is available at\nhttps://huggingface.co/spaces/allen-eric/radiology-gpt.\n","authors":["Zhengliang Liu","Aoxiao Zhong","Yiwei Li","Longtao Yang","Chao Ju","Zihao Wu","Chong Ma","Peng Shu","Cheng Chen","Sekeun Kim","Haixing Dai","Lin Zhao","Lichao Sun","Dajiang Zhu","Jun Liu","Wei Liu","Dinggang Shen","Xiang Li","Quanzheng Li","Tianming Liu"],"pdf_url":"https://arxiv.org/pdf/2306.08666v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00420v2","updated":"2024-03-19T16:56:53Z","published":"2023-12-31T08:06:53Z","title":"SynCDR : Training Cross Domain Retrieval Models with Synthetic Data","summary":"  In cross-domain retrieval, a model is required to identify images from the\nsame semantic category across two visual domains. For instance, given a sketch\nof an object, a model needs to retrieve a real image of it from an online\nstore's catalog. A standard approach for such a problem is learning a feature\nspace of images where Euclidean distances reflect similarity. Even without\nhuman annotations, which may be expensive to acquire, prior methods function\nreasonably well using unlabeled images for training. Our problem constraint\ntakes this further to scenarios where the two domains do not necessarily share\nany common categories in training data. This can occur when the two domains in\nquestion come from different versions of some biometric sensor recording\nidentities of different people. We posit a simple solution, which is to\ngenerate synthetic data to fill in these missing category examples across\ndomains. This, we do via category preserving translation of images from one\nvisual domain to another. We compare approaches specifically trained for this\ntranslation for a pair of domains, as well as those that can use large-scale\npre-trained text-to-image diffusion models via prompts, and find that the\nlatter can generate better replacement synthetic data, leading to more accurate\ncross-domain retrieval models. Our best SynCDR model can outperform prior art\nby up to 15\\%. Code for our work is available at\nhttps://github.com/samarth4149/SynCDR .\n","authors":["Samarth Mishra","Carlos D. Castillo","Hongcheng Wang","Kate Saenko","Venkatesh Saligrama"],"pdf_url":"https://arxiv.org/pdf/2401.00420v2.pdf","comment":"Pre-print"},{"id":"http://arxiv.org/abs/2403.12900v1","updated":"2024-03-19T16:53:53Z","published":"2024-03-19T16:53:53Z","title":"Toward Sustainable GenAI using Generation Directives for Carbon-Friendly\n  Large Language Model Inference","summary":"  The rapid advancement of Generative Artificial Intelligence (GenAI) across\ndiverse sectors raises significant environmental concerns, notably the carbon\nemissions from their cloud and high performance computing (HPC) infrastructure.\nThis paper presents Sprout, an innovative framework designed to address these\nconcerns by reducing the carbon footprint of generative Large Language Model\n(LLM) inference services. Sprout leverages the innovative concept of\n\"generation directives\" to guide the autoregressive generation process, thereby\nenhancing carbon efficiency. Our proposed method meticulously balances the need\nfor ecological sustainability with the demand for high-quality generation\noutcomes. Employing a directive optimizer for the strategic assignment of\ngeneration directives to user prompts and an original offline quality\nevaluator, Sprout demonstrates a significant reduction in carbon emissions by\nover 40% in real-world evaluations using the Llama2 LLM and global electricity\ngrid data. This research marks a critical step toward aligning AI technology\nwith sustainable practices, highlighting the potential for mitigating\nenvironmental impacts in the rapidly expanding domain of generative artificial\nintelligence.\n","authors":["Baolin Li","Yankai Jiang","Vijay Gadepally","Devesh Tiwari"],"pdf_url":"https://arxiv.org/pdf/2403.12900v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09481v2","updated":"2024-03-19T16:48:27Z","published":"2024-03-14T15:25:23Z","title":"Clinical Reasoning over Tabular Data and Text with Bayesian Networks","summary":"  Bayesian networks are well-suited for clinical reasoning on tabular data, but\nare less compatible with natural language data, for which neural networks\nprovide a successful framework. This paper compares and discusses strategies to\naugment Bayesian networks with neural text representations, both in a\ngenerative and discriminative manner. This is illustrated with simulation\nresults for a primary care use case (diagnosis of pneumonia) and discussed in a\nbroader clinical context.\n","authors":["Paloma Rabaey","Johannes Deleu","Stefan Heytens","Thomas Demeester"],"pdf_url":"https://arxiv.org/pdf/2403.09481v2.pdf","comment":"10 pages, 2 figures"},{"id":"http://arxiv.org/abs/2402.10109v2","updated":"2024-03-19T16:43:09Z","published":"2024-02-15T17:05:48Z","title":"Towards Reducing Diagnostic Errors with Interpretable Risk Prediction","summary":"  Many diagnostic errors occur because clinicians cannot easily access relevant\ninformation in patient Electronic Health Records (EHRs). In this work we\npropose a method to use LLMs to identify pieces of evidence in patient EHR data\nthat indicate increased or decreased risk of specific diagnoses; our ultimate\naim is to increase access to evidence and reduce diagnostic errors. In\nparticular, we propose a Neural Additive Model to make predictions backed by\nevidence with individualized risk estimates at time-points where clinicians are\nstill uncertain, aiming to specifically mitigate delays in diagnosis and errors\nstemming from an incomplete differential. To train such a model, it is\nnecessary to infer temporally fine-grained retrospective labels of eventual\n\"true\" diagnoses. We do so with LLMs, to ensure that the input text is from\nbefore a confident diagnosis can be made. We use an LLM to retrieve an initial\npool of evidence, but then refine this set of evidence according to\ncorrelations learned by the model. We conduct an in-depth evaluation of the\nusefulness of our approach by simulating how it might be used by a clinician to\ndecide between a pre-defined list of differential diagnoses.\n","authors":["Denis Jered McInerney","William Dickinson","Lucy C. Flynn","Andrea C. Young","Geoffrey S. Young","Jan-Willem van de Meent","Byron C. Wallace"],"pdf_url":"https://arxiv.org/pdf/2402.10109v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12891v1","updated":"2024-03-19T16:40:57Z","published":"2024-03-19T16:40:57Z","title":"Adaptive Visual Imitation Learning for Robotic Assisted Feeding Across\n  Varied Bowl Configurations and Food Types","summary":"  In this study, we introduce a novel visual imitation network with a spatial\nattention module for robotic assisted feeding (RAF). The goal is to acquire\n(i.e., scoop) food items from a bowl. However, achieving robust and adaptive\nfood manipulation is particularly challenging. To deal with this, we propose a\nframework that integrates visual perception with imitation learning to enable\nthe robot to handle diverse scenarios during scooping. Our approach, named AVIL\n(adaptive visual imitation learning), exhibits adaptability and robustness\nacross different bowl configurations in terms of material, size, and position,\nas well as diverse food types including granular, semi-solid, and liquid, even\nin the presence of distractors. We validate the effectiveness of our approach\nby conducting experiments on a real robot. We also compare its performance with\na baseline. The results demonstrate improvement over the baseline across all\nscenarios, with an enhancement of up to 2.5 times in terms of a success metric.\nNotably, our model, trained solely on data from a transparent glass bowl\ncontaining granular cereals, showcases generalization ability when tested\nzero-shot on other bowl configurations with different types of food.\n","authors":["Rui Liu","Amisha Bhaskar","Pratap Tokekar"],"pdf_url":"https://arxiv.org/pdf/2403.12891v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01838v2","updated":"2024-03-19T16:40:25Z","published":"2023-04-04T14:44:06Z","title":"BugNIST - a Large Volumetric Dataset for Object Detection under Domain\n  Shift","summary":"  Domain shift significantly influences the performance of deep learning\nalgorithms, particularly for object detection within volumetric 3D images.\nAnnotated training data is essential for deep learning-based object detection.\nHowever, annotating densely packed objects is time-consuming and costly.\nInstead, we suggest training models on individually scanned objects, causing a\ndomain shift between training and detection data. To address this challenge, we\nintroduce the BugNIST dataset, comprising 9154 micro-CT volumes of 12 bug types\nand 388 volumes of tightly packed bug mixtures. This dataset is characterized\nby having objects with the same appearance in the source and target domain,\nwhich is uncommon for other benchmark datasets for domain shift. During\ntraining, individual bug volumes labeled by class are utilized, while testing\nemploys mixtures with center point annotations and bug type labels. Together\nwith the dataset, we provide a baseline detection analysis, aiming at advancing\nthe field of 3D object detection methods.\n","authors":["Patrick Møller Jensen","Vedrana Andersen Dahl","Carsten Gundlach","Rebecca Engberg","Hans Martin Kjer","Anders Bjorholm Dahl"],"pdf_url":"https://arxiv.org/pdf/2304.01838v2.pdf","comment":"20 pages, 6 figures, 2 tables"},{"id":"http://arxiv.org/abs/2311.09684v2","updated":"2024-03-19T16:27:37Z","published":"2023-11-16T08:54:52Z","title":"Do Physicians Know How to Prompt? The Need for Automatic Prompt\n  Optimization Help in Clinical Note Generation","summary":"  This study examines the effect of prompt engineering on the performance of\nLarge Language Models (LLMs) in clinical note generation. We introduce an\nAutomatic Prompt Optimization (APO) framework to refine initial prompts and\ncompare the outputs of medical experts, non-medical experts, and APO-enhanced\nGPT3.5 and GPT4. Results highlight GPT4 APO's superior performance in\nstandardizing prompt quality across clinical note sections. A human-in-the-loop\napproach shows that experts maintain content quality post-APO, with a\npreference for their own modifications, suggesting the value of expert\ncustomization. We recommend a two-phase optimization process, leveraging\nAPO-GPT4 for consistency and expert input for personalization.\n","authors":["Zonghai Yao","Ahmed Jaafar","Beining Wang","Zhichao Yang","Hong Yu"],"pdf_url":"https://arxiv.org/pdf/2311.09684v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.09283v2","updated":"2024-03-19T16:23:20Z","published":"2024-02-14T16:14:03Z","title":"Attacks, Defenses and Evaluations for LLM Conversation Safety: A Survey","summary":"  Large Language Models (LLMs) are now commonplace in conversation\napplications. However, their risks of misuse for generating harmful responses\nhave raised serious societal concerns and spurred recent research on LLM\nconversation safety. Therefore, in this survey, we provide a comprehensive\noverview of recent studies, covering three critical aspects of LLM conversation\nsafety: attacks, defenses, and evaluations. Our goal is to provide a structured\nsummary that enhances understanding of LLM conversation safety and encourages\nfurther investigation into this important subject. For easy reference, we have\ncategorized all the studies mentioned in this survey according to our taxonomy,\navailable at: https://github.com/niconi19/LLM-conversation-safety.\n","authors":["Zhichen Dong","Zhanhui Zhou","Chao Yang","Jing Shao","Yu Qiao"],"pdf_url":"https://arxiv.org/pdf/2402.09283v2.pdf","comment":"Accepted to NAACL 2024"},{"id":"http://arxiv.org/abs/2402.09450v3","updated":"2024-03-19T16:17:00Z","published":"2024-02-02T10:04:13Z","title":"Guiding Masked Representation Learning to Capture Spatio-Temporal\n  Relationship of Electrocardiogram","summary":"  Electrocardiograms (ECG) are widely employed as a diagnostic tool for\nmonitoring electrical signals originating from a heart. Recent machine learning\nresearch efforts have focused on the application of screening various diseases\nusing ECG signals. However, adapting to the application of screening disease is\nchallenging in that labeled ECG data are limited. Achieving general\nrepresentation through self-supervised learning (SSL) is a well-known approach\nto overcome the scarcity of labeled data; however, a naive application of SSL\nto ECG data, without considering the spatial-temporal relationships inherent in\nECG signals, may yield suboptimal results. In this paper, we introduce ST-MEM\n(Spatio-Temporal Masked Electrocardiogram Modeling), designed to learn\nspatio-temporal features by reconstructing masked 12-lead ECG data. ST-MEM\noutperforms other SSL baseline methods in various experimental settings for\narrhythmia classification tasks. Moreover, we demonstrate that ST-MEM is\nadaptable to various lead combinations. Through quantitative and qualitative\nanalysis, we show a spatio-temporal relationship within ECG data. Our code is\navailable at https://github.com/bakqui/ST-MEM.\n","authors":["Yeongyeon Na","Minje Park","Yunwon Tae","Sunghoon Joo"],"pdf_url":"https://arxiv.org/pdf/2402.09450v3.pdf","comment":"ICLR 2024. The first three authors contribute equally"},{"id":"http://arxiv.org/abs/2306.06192v4","updated":"2024-03-19T16:16:16Z","published":"2023-06-09T18:45:15Z","title":"Ada-NAV: Adaptive Trajectory Length-Based Sample Efficient Policy\n  Learning for Robotic Navigation","summary":"  Trajectory length stands as a crucial hyperparameter within reinforcement\nlearning (RL) algorithms, significantly contributing to the sample inefficiency\nin robotics applications. Motivated by the pivotal role trajectory length plays\nin the training process, we introduce Ada-NAV, a novel adaptive trajectory\nlength scheme designed to enhance the training sample efficiency of RL\nalgorithms in robotic navigation tasks. Unlike traditional approaches that\ntreat trajectory length as a fixed hyperparameter, we propose to dynamically\nadjust it based on the entropy of the underlying navigation policy.\nInterestingly, Ada-NAV can be applied to both existing on-policy and off-policy\nRL methods, which we demonstrate by empirically validating its efficacy on\nthree popular RL methods: REINFORCE, Proximal Policy Optimization (PPO), and\nSoft Actor-Critic (SAC). We demonstrate through simulated and real-world\nrobotic experiments that Ada-NAV outperforms conventional methods that employ\nconstant or randomly sampled trajectory lengths. Specifically, for a fixed\nsample budget, Ada-NAV achieves an 18\\% increase in navigation success rate, a\n20-38\\% reduction in navigation path length, and a 9.32\\% decrease in elevation\ncosts. Furthermore, we showcase the versatility of Ada-NAV by integrating it\nwith the Clearpath Husky robot, illustrating its applicability in complex\noutdoor environments.\n","authors":["Bhrij Patel","Kasun Weerakoon","Wesley A. Suttle","Alec Koppel","Brian M. Sadler","Tianyi Zhou","Amrit Singh Bedi","Dinesh Manocha"],"pdf_url":"https://arxiv.org/pdf/2306.06192v4.pdf","comment":"11 pages, 9 figures, 2 tables"},{"id":"http://arxiv.org/abs/2305.16635v2","updated":"2024-03-19T16:14:04Z","published":"2023-05-26T05:19:24Z","title":"Impossible Distillation: from Low-Quality Model to High-Quality Dataset\n  & Model for Summarization and Paraphrasing","summary":"  We present Impossible Distillation, a novel framework for paraphrasing and\nsentence summarization, that distills a high-quality dataset and model from a\nlow-quality teacher that itself cannot perform these tasks. Unlike prior works\nthat rely on an extreme-scale teacher model (e.g., GPT3) or task-specific\narchitecture, we hypothesize and verify the paraphrastic proximity intrinsic to\npre-trained LMs (e.g., GPT2), where paraphrases occupy a proximal subspace in\nthe LM distribution. By identifying and distilling generations from these\nsubspaces, Impossible Distillation produces a high-quality dataset and model\neven from GPT2-scale LMs. We evaluate our method on multiple benchmarks\nspanning unconstrained / syntax-controlled paraphrase generation and sentence\nsummarization. Our model with 770M parameters consistently outperforms strong\nbaselines, including models distilled from ChatGPT, and sometimes, even ChatGPT\nitself. Also, we find that our distilled dataset from 1.5B LMs exhibits higher\ndiversity and fidelity than up to 13 times larger datasets.\n","authors":["Jaehun Jung","Peter West","Liwei Jiang","Faeze Brahman","Ximing Lu","Jillian Fisher","Taylor Sorensen","Yejin Choi"],"pdf_url":"https://arxiv.org/pdf/2305.16635v2.pdf","comment":"NAACL 2024"},{"id":"http://arxiv.org/abs/2403.12869v1","updated":"2024-03-19T16:12:25Z","published":"2024-03-19T16:12:25Z","title":"Regularization in Spider-Style Strategy Discovery and Schedule\n  Construction","summary":"  To achieve the best performance, automatic theorem provers often rely on\nschedules of diverse proving strategies to be tried out (either sequentially or\nin parallel) on a given problem. In this paper, we report on a large-scale\nexperiment with discovering strategies for the Vampire prover, targeting the\nFOF fragment of the TPTP library and constructing a schedule for it, based on\nthe ideas of Andrei Voronkov's system Spider. We examine the process from\nvarious angles, discuss the difficulty (or ease) of obtaining a strong Vampire\nschedule for the CASC competition, and establish how well a schedule can be\nexpected to generalize to unseen problems and what factors influence this\nproperty.\n","authors":["Filip Bártek","Karel Chvalovský","Martin Suda"],"pdf_url":"https://arxiv.org/pdf/2403.12869v1.pdf","comment":"25 pages, 8 figures, submitted to IJCAR 2024"},{"id":"http://arxiv.org/abs/2312.03799v2","updated":"2024-03-19T16:08:37Z","published":"2023-12-06T14:58:03Z","title":"Low-power, Continuous Remote Behavioral Localization with Event Cameras","summary":"  Researchers in natural science need reliable methods for quantifying animal\nbehavior. Recently, numerous computer vision methods emerged to automate the\nprocess. However, observing wild species at remote locations remains a\nchallenging task due to difficult lighting conditions and constraints on power\nsupply and data storage. Event cameras offer unique advantages for\nbattery-dependent remote monitoring due to their low power consumption and high\ndynamic range capabilities. We use this novel sensor to quantify a behavior in\nChinstrap penguins called ecstatic display. We formulate the problem as a\ntemporal action detection task, determining the start and end times of the\nbehavior. For this purpose, we recorded a colony of breeding penguins in\nAntarctica for several weeks and labeled event data on 16 nests. The developed\nmethod consists of a generator of candidate time intervals (proposals) and a\nclassifier of the actions within them. The experiments show that the event\ncameras' natural response to motion is effective for continuous behavior\nmonitoring and detection, reaching a mean average precision (mAP) of 58% (which\nincreases to 63% in good weather conditions). The results also demonstrate the\nrobustness against various lighting conditions contained in the challenging\ndataset. The low-power capabilities of the event camera allow it to record\nsignificantly longer than with a conventional camera. This work pioneers the\nuse of event cameras for remote wildlife observation, opening new\ninterdisciplinary opportunities. https://tub-rip.github.io/eventpenguins/\n","authors":["Friedhelm Hamann","Suman Ghosh","Ignacio Juarez Martinez","Tom Hart","Alex Kacelnik","Guillermo Gallego"],"pdf_url":"https://arxiv.org/pdf/2312.03799v2.pdf","comment":"13 pages, 8 figures, 12 tables, Project page:\n  https://tub-rip.github.io/eventpenguins/"},{"id":"http://arxiv.org/abs/2301.00693v2","updated":"2024-03-19T16:06:19Z","published":"2022-12-30T10:35:25Z","title":"Deep Recurrent Learning Through Long Short Term Memory and TOPSIS","summary":"  Enterprise resource planning (ERP) software brings resources, data together\nto keep software-flow within business processes in a company. However, cloud\ncomputing's cheap, easy and quick management promise pushes business-owners for\na transition from monolithic to a data-center/cloud based ERP. Since cloud-ERP\ndevelopment involves a cyclic process, namely planning, implementing, testing\nand upgrading, its adoption is realized as a deep recurrent neural network\nproblem. Eventually, a classification algorithm based on long short term memory\n(LSTM) and TOPSIS is proposed to identify and rank, respectively, adoption\nfeatures. Our theoretical model is validated over a reference model by\narticulating key players, services, architecture, functionalities. Qualitative\nsurvey is conducted among users by considering technology, innovation and\nresistance issues, to formulate hypotheses on key adoption factors.\n","authors":["Rossi Kamal","Zuzana Kubincova","Mosaddek Hossain Kamal","Upama Kabir"],"pdf_url":"https://arxiv.org/pdf/2301.00693v2.pdf","comment":"This submission has been withdrawn by arXiv administrators as the\n  second author was added without their knowledge or consent"},{"id":"http://arxiv.org/abs/2403.12853v1","updated":"2024-03-19T15:57:32Z","published":"2024-03-19T15:57:32Z","title":"RASP: A Drone-based Reconfigurable Actuation and Sensing Platform\n  Towards Ambient Intelligent Systems","summary":"  Realizing consumer-grade drones that are as useful as robot vacuums\nthroughout our homes or personal smartphones in our daily lives requires drones\nto sense, actuate, and respond to general scenarios that may arise. Towards\nthis vision, we propose RASP, a modular and reconfigurable sensing and\nactuation platform that allows drones to autonomously swap onboard sensors and\nactuators in only 25 seconds, allowing a single drone to quickly adapt to a\ndiverse range of tasks. RASP consists of a mechanical layer to physically swap\nsensor modules, an electrical layer to maintain power and communication lines\nto the sensor/actuator, and a software layer to maintain a common interface\nbetween the drone and any sensor module in our platform. Leveraging recent\nadvances in large language and visual language models, we further introduce the\narchitecture, implementation, and real-world deployments of a personal\nassistant system utilizing RASP. We demonstrate that RASP can enable a diverse\nrange of useful tasks in home, office, lab, and other indoor settings.\n","authors":["Minghui Zhao","Junxi Xia","Kaiyuan Hou","Yanchen Liu","Stephen Xia","Xiaofan Jiang"],"pdf_url":"https://arxiv.org/pdf/2403.12853v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.12518v2","updated":"2024-03-19T15:38:29Z","published":"2024-02-19T20:29:34Z","title":"Gaussian Process Neural Additive Models","summary":"  Deep neural networks have revolutionized many fields, but their black-box\nnature also occasionally prevents their wider adoption in fields such as\nhealthcare and finance, where interpretable and explainable models are\nrequired. The recent development of Neural Additive Models (NAMs) is a\nsignificant step in the direction of interpretable deep learning for tabular\ndatasets. In this paper, we propose a new subclass of NAMs that use a\nsingle-layer neural network construction of the Gaussian process via random\nFourier features, which we call Gaussian Process Neural Additive Models\n(GP-NAM). GP-NAMs have the advantage of a convex objective function and number\nof trainable parameters that grows linearly with feature dimensionality. It\nsuffers no loss in performance compared to deeper NAM approaches because GPs\nare well-suited for learning complex non-parametric univariate functions. We\ndemonstrate the performance of GP-NAM on several tabular datasets, showing that\nit achieves comparable or better performance in both classification and\nregression tasks with a large reduction in the number of parameters.\n","authors":["Wei Zhang","Brian Barr","John Paisley"],"pdf_url":"https://arxiv.org/pdf/2402.12518v2.pdf","comment":"Appears at AAAI 2024"},{"id":"http://arxiv.org/abs/2403.12823v1","updated":"2024-03-19T15:24:49Z","published":"2024-03-19T15:24:49Z","title":"Answer Set Programming for Flexible Payroll Management","summary":"  Payroll management is a critical business task that is subject to a large\nnumber of rules, which vary widely between companies, sectors, and countries.\nMoreover, the rules are often complex and change regularly. Therefore, payroll\nmanagement systems must be flexible in design. In this paper, we suggest an\napproach based on a flexible Answer Set Programming (ASP) model and an\neasy-to-read tabular representation based on the Decision Model and Notation\n(DMN) standard. It allows HR consultants to represent complex rules without the\nneed for a software engineer, and to ultimately design payroll systems for a\nvariety of different scenarios. We show how the multi-shot solving capabilities\nof the clingo ASP system can be used to reach the performance that is necessary\nto handle real-world instances.\n","authors":["Benjamin Callewaert","Joost Vennekens"],"pdf_url":"https://arxiv.org/pdf/2403.12823v1.pdf","comment":"Under consideration in Theory and Practice of Logic Programming\n  (TPLP)"},{"id":"http://arxiv.org/abs/2403.12821v1","updated":"2024-03-19T15:21:10Z","published":"2024-03-19T15:21:10Z","title":"FlowerFormer: Empowering Neural Architecture Encoding using a Flow-aware\n  Graph Transformer","summary":"  The success of a specific neural network architecture is closely tied to the\ndataset and task it tackles; there is no one-size-fits-all solution. Thus,\nconsiderable efforts have been made to quickly and accurately estimate the\nperformances of neural architectures, without full training or evaluation, for\ngiven tasks and datasets. Neural architecture encoding has played a crucial\nrole in the estimation, and graphbased methods, which treat an architecture as\na graph, have shown prominent performance. For enhanced representation learning\nof neural architectures, we introduce FlowerFormer, a powerful graph\ntransformer that incorporates the information flows within a neural\narchitecture. FlowerFormer consists of two key components: (a) bidirectional\nasynchronous message passing, inspired by the flows; (b) global attention built\non flow-based masking. Our extensive experiments demonstrate the superiority of\nFlowerFormer over existing neural encoding methods, and its effectiveness\nextends beyond computer vision models to include graph neural networks and auto\nspeech recognition models. Our code is available at\nhttp://github.com/y0ngjaenius/CVPR2024_FLOWERFormer.\n","authors":["Dongyeong Hwang","Hyunju Kim","Sunwoo Kim","Kijung Shin"],"pdf_url":"https://arxiv.org/pdf/2403.12821v1.pdf","comment":"CVPR 2024 Camera-Ready"},{"id":"http://arxiv.org/abs/2403.12816v1","updated":"2024-03-19T15:15:19Z","published":"2024-03-19T15:15:19Z","title":"Re-identification from histopathology images","summary":"  In numerous studies, deep learning algorithms have proven their potential for\nthe analysis of histopathology images, for example, for revealing the subtypes\nof tumors or the primary origin of metastases. These models require large\ndatasets for training, which must be anonymized to prevent possible patient\nidentity leaks. This study demonstrates that even relatively simple deep\nlearning algorithms can re-identify patients in large histopathology datasets\nwith substantial accuracy. We evaluated our algorithms on two TCIA datasets\nincluding lung squamous cell carcinoma (LSCC) and lung adenocarcinoma (LUAD).\nWe also demonstrate the algorithm's performance on an in-house dataset of\nmeningioma tissue. We predicted the source patient of a slide with F1 scores of\n50.16 % and 52.30 % on the LSCC and LUAD datasets, respectively, and with 62.31\n% on our meningioma dataset. Based on our findings, we formulated a risk\nassessment scheme to estimate the risk to the patient's privacy prior to\npublication.\n","authors":["Jonathan Ganz","Jonas Ammeling","Samir Jabari","Katharina Breininger","Marc Aubreville"],"pdf_url":"https://arxiv.org/pdf/2403.12816v1.pdf","comment":"20 pages, 7 figures, 2 tables"},{"id":"http://arxiv.org/abs/2403.12809v1","updated":"2024-03-19T15:07:22Z","published":"2024-03-19T15:07:22Z","title":"Comparing Explanation Faithfulness between Multilingual and Monolingual\n  Fine-tuned Language Models","summary":"  In many real natural language processing application scenarios, practitioners\nnot only aim to maximize predictive performance but also seek faithful\nexplanations for the model predictions. Rationales and importance distribution\ngiven by feature attribution methods (FAs) provide insights into how different\nparts of the input contribute to a prediction. Previous studies have explored\nhow different factors affect faithfulness, mainly in the context of monolingual\nEnglish models. On the other hand, the differences in FA faithfulness between\nmultilingual and monolingual models have yet to be explored. Our extensive\nexperiments, covering five languages and five popular FAs, show that FA\nfaithfulness varies between multilingual and monolingual models. We find that\nthe larger the multilingual model, the less faithful the FAs are compared to\nits counterpart monolingual models.Our further analysis shows that the\nfaithfulness disparity is potentially driven by the differences between model\ntokenizers. Our code is available:\nhttps://github.com/casszhao/multilingual-faith.\n","authors":["Zhixue Zhao","Nikolaos Aletras"],"pdf_url":"https://arxiv.org/pdf/2403.12809v1.pdf","comment":"Accepted at NAACL 2024 Main Conference"},{"id":"http://arxiv.org/abs/2403.12805v1","updated":"2024-03-19T15:06:53Z","published":"2024-03-19T15:06:53Z","title":"Contextual Moral Value Alignment Through Context-Based Aggregation","summary":"  Developing value-aligned AI agents is a complex undertaking and an ongoing\nchallenge in the field of AI. Specifically within the domain of Large Language\nModels (LLMs), the capability to consolidate multiple independently trained\ndialogue agents, each aligned with a distinct moral value, into a unified\nsystem that can adapt to and be aligned with multiple moral values is of\nparamount importance. In this paper, we propose a system that does contextual\nmoral value alignment based on contextual aggregation. Here, aggregation is\ndefined as the process of integrating a subset of LLM responses that are best\nsuited to respond to a user input, taking into account features extracted from\nthe user's input. The proposed system shows better results in term of alignment\nto human value compared to the state of the art.\n","authors":["Pierre Dognin","Jesus Rios","Ronny Luss","Inkit Padhi","Matthew D Riemer","Miao Liu","Prasanna Sattigeri","Manish Nagireddy","Kush R. Varshney","Djallel Bouneffouf"],"pdf_url":"https://arxiv.org/pdf/2403.12805v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12799v1","updated":"2024-03-19T15:01:14Z","published":"2024-03-19T15:01:14Z","title":"Investigating Text Shortening Strategy in BERT: Truncation vs\n  Summarization","summary":"  The parallelism of Transformer-based models comes at the cost of their input\nmax-length. Some studies proposed methods to overcome this limitation, but none\nof them reported the effectiveness of summarization as an alternative. In this\nstudy, we investigate the performance of document truncation and summarization\nin text classification tasks. Each of the two was investigated with several\nvariations. This study also investigated how close their performances are to\nthe performance of full-text. We used a dataset of summarization tasks based on\nIndonesian news articles (IndoSum) to do classification tests. This study shows\nhow the summaries outperform the majority of truncation method variations and\nlose to only one. The best strategy obtained in this study is taking the head\nof the document. The second is extractive summarization. This study explains\nwhat happened to the result, leading to further research in order to exploit\nthe potential of document summarization as a shortening alternative. The code\nand data used in this work are publicly available in\nhttps://github.com/mirzaalimm/TruncationVsSummarization.\n","authors":["Mirza Alim Mutasodirin","Radityo Eko Prasojo"],"pdf_url":"https://arxiv.org/pdf/2403.12799v1.pdf","comment":"The 13th International Conference on Advanced Computer Science and\n  Information Systems (ICACSIS 2021)"},{"id":"http://arxiv.org/abs/2402.04527v2","updated":"2024-03-19T14:56:54Z","published":"2024-02-07T02:14:58Z","title":"RA-Rec: An Efficient ID Representation Alignment Framework for LLM-based\n  Recommendation","summary":"  Large language models (LLM) have recently emerged as a powerful tool for a\nvariety of natural language processing tasks, bringing a new surge of combining\nLLM with recommendation systems, termed as LLM-based RS. Current approaches\ngenerally fall into two main paradigms, the ID direct usage paradigm and the ID\ntranslation paradigm, noting their core weakness stems from lacking\nrecommendation knowledge and uniqueness. To address this limitation, we propose\na new paradigm, ID representation, which incorporates pre-trained ID embeddings\ninto LLMs in a complementary manner. In this work, we present RA-Rec, an\nefficient ID representation alignment framework for LLM-based recommendation,\nwhich is compatible with multiple ID-based methods and LLM architectures.\nSpecifically, we treat ID embeddings as soft prompts and design an innovative\nalignment module and an efficient tuning method with tailored data construction\nfor alignment. Extensive experiments demonstrate RA-Rec substantially\noutperforms current state-of-the-art methods, achieving up to 3.0% absolute\nHitRate@100 improvements while utilizing less than 10x training data.\n","authors":["Xiaohan Yu","Li Zhang","Xin Zhao","Yue Wang","Zhongrui Ma"],"pdf_url":"https://arxiv.org/pdf/2402.04527v2.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2403.12777v1","updated":"2024-03-19T14:44:54Z","published":"2024-03-19T14:44:54Z","title":"Discover and Mitigate Multiple Biased Subgroups in Image Classifiers","summary":"  Machine learning models can perform well on in-distribution data but often\nfail on biased subgroups that are underrepresented in the training data,\nhindering the robustness of models for reliable applications. Such subgroups\nare typically unknown due to the absence of subgroup labels. Discovering biased\nsubgroups is the key to understanding models' failure modes and further\nimproving models' robustness. Most previous works of subgroup discovery make an\nimplicit assumption that models only underperform on a single biased subgroup,\nwhich does not hold on in-the-wild data where multiple biased subgroups exist.\n  In this work, we propose Decomposition, Interpretation, and Mitigation (DIM),\na novel method to address a more challenging but also more practical problem of\ndiscovering multiple biased subgroups in image classifiers. Our approach\ndecomposes the image features into multiple components that represent multiple\nsubgroups. This decomposition is achieved via a bilinear dimension reduction\nmethod, Partial Least Square (PLS), guided by useful supervision from the image\nclassifier. We further interpret the semantic meaning of each subgroup\ncomponent by generating natural language descriptions using vision-language\nfoundation models. Finally, DIM mitigates multiple biased subgroups\nsimultaneously via two strategies, including the data- and model-centric\nstrategies. Extensive experiments on CIFAR-100 and Breeds datasets demonstrate\nthe effectiveness of DIM in discovering and mitigating multiple biased\nsubgroups. Furthermore, DIM uncovers the failure modes of the classifier on\nHard ImageNet, showcasing its broader applicability to understanding model bias\nin image classifiers. The code is available at\nhttps://github.com/ZhangAIPI/DIM.\n","authors":["Zeliang Zhang","Mingqian Feng","Zhiheng Li","Chenliang Xu"],"pdf_url":"https://arxiv.org/pdf/2403.12777v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.11409v3","updated":"2024-03-19T14:23:07Z","published":"2023-10-17T17:15:41Z","title":"LLMs as Hackers: Autonomous Linux Privilege Escalation Attacks","summary":"  Penetration testing, an essential component of software security testing,\nallows organizations to proactively identify and remediate vulnerabilities in\ntheir systems, thus bolstering their defense mechanisms against potential\ncyberattacks. One recent advancement in the realm of penetration testing is the\nutilization of Language Models (LLMs).\n  We explore the intersection of LLMs and penetration testing to gain insight\ninto their capabilities and challenges in the context of privilege escalation.\nWe create an automated Linux privilege-escalation benchmark utilizing local\nvirtual machines. We introduce an LLM-guided privilege-escalation tool designed\nfor evaluating different LLMs and prompt strategies against our benchmark.\n  Our results show that GPT-4 is well suited for detecting file-based exploits\nas it can typically solve 75-100\\% of test-cases of that vulnerability class.\nGPT-3.5-turbo was only able to solve 25-50% of those, while local models, such\nas Llama2 were not able to detect any exploits. We analyze the impact of\ndifferent prompt designs, the benefits of in-context learning, and the\nadvantages of offering high-level guidance to LLMs. We discuss challenging\nareas for LLMs, including maintaining focus during testing, coping with errors,\nand finally comparing them with both stochastic parrots as well as with human\nhackers.\n","authors":["Andreas Happe","Aaron Kaplan","Jürgen Cito"],"pdf_url":"https://arxiv.org/pdf/2310.11409v3.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2402.14698v2","updated":"2024-03-19T14:21:32Z","published":"2024-02-22T16:50:32Z","title":"Using construction waste hauling trucks' GPS data to classify\n  earthwork-related locations: A Chengdu case study","summary":"  Earthwork-related locations (ERLs), such as construction sites, earth dumping\nground, and concrete mixing stations, are major sources of urban dust pollution\n(particulate matters). The effective management of ERLs is crucial and requires\ntimely and efficient tracking of these locations throughout the city. This work\naims to identify and classify urban ERLs using GPS trajectory data of over\n16,000 construction waste hauling trucks (CWHTs), as well as 58 urban features\nencompassing geographic, land cover, POI and transport dimensions. We compare\nseveral machine learning models and examine the impact of various\nspatial-temporal features on classification performance using real-world data\nin Chengdu, China. The results demonstrate that 77.8% classification accuracy\ncan be achieved with a limited number of features. This classification\nframework was implemented in the Alpha MAPS system in Chengdu, which has\nsuccessfully identified 724 construction cites/earth dumping ground, 48\nconcrete mixing stations, and 80 truck parking locations in the city during\nDecember 2023, which has enabled local authority to effectively manage urban\ndust pollution at low personnel costs.\n","authors":["Lei Yu","Ke Han"],"pdf_url":"https://arxiv.org/pdf/2402.14698v2.pdf","comment":"12 pages, 8 figures"},{"id":"http://arxiv.org/abs/2308.04792v2","updated":"2024-03-19T14:13:53Z","published":"2023-08-09T08:31:05Z","title":"A Fast and Optimal Learning-based Path Planning Method for Planetary\n  Rovers","summary":"  Intelligent autonomous path planning is crucial to improve the exploration\nefficiency of planetary rovers. In this paper, we propose a learning-based\nmethod to quickly search for optimal paths in an elevation map, which is called\nNNPP. The NNPP model learns semantic information about start and goal\nlocations, as well as map representations, from numerous pre-annotated optimal\npath demonstrations, and produces a probabilistic distribution over each pixel\nrepresenting the likelihood of it belonging to an optimal path on the map. More\nspecifically, the paper computes the traversal cost for each grid cell from the\nslope, roughness and elevation difference obtained from the DEM. Subsequently,\nthe start and goal locations are encoded using a Gaussian distribution and\ndifferent location encoding parameters are analyzed for their effect on model\nperformance. After training, the NNPP model is able to perform path planning on\nnovel maps. Experiments show that the guidance field generated by the NNPP\nmodel can significantly reduce the search time for optimal paths under the same\nhardware conditions, and the advantage of NNPP increases with the scale of the\nmap.\n","authors":["Yiming Ji","Yang Liu","Guanghu Xie","Boyu Ma","Zongwu Xie","Baoshi Cao"],"pdf_url":"https://arxiv.org/pdf/2308.04792v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10433v2","updated":"2024-03-19T14:12:24Z","published":"2024-03-15T16:11:15Z","title":"AI-enhanced Collective Intelligence: The State of the Art and Prospects","summary":"  The current societal challenges exceed the capacity of human individual or\ncollective effort alone. As AI evolves, its role within human collectives is\npoised to vary from an assistive tool to a participatory member. Humans and AI\npossess complementary capabilities that, when synergized, can achieve a level\nof collective intelligence that surpasses the collective capabilities of either\nhumans or AI in isolation. However, the interactions in human-AI systems are\ninherently complex, involving intricate processes and interdependencies. This\nreview incorporates perspectives from network science to conceptualize a\nmultilayer representation of human-AI collective intelligence, comprising a\ncognition layer, a physical layer, and an information layer. Within this\nmultilayer network, humans and AI agents exhibit varying characteristics;\nhumans differ in diversity from surface-level to deep-level attributes, while\nAI agents range in degrees of functionality and anthropomorphism. The interplay\namong these agents shapes the overall structure and dynamics of the system. We\nexplore how agents' diversity and interactions influence the system's\ncollective intelligence. Furthermore, we present an analysis of real-world\ninstances of AI-enhanced collective intelligence. We conclude by addressing the\npotential challenges in AI-enhanced collective intelligence and offer\nperspectives on future developments in this field.\n","authors":["Hao Cui","Taha Yasseri"],"pdf_url":"https://arxiv.org/pdf/2403.10433v2.pdf","comment":"27 pages, 2 figures"},{"id":"http://arxiv.org/abs/2403.12748v1","updated":"2024-03-19T14:11:26Z","published":"2024-03-19T14:11:26Z","title":"Building Brain Tumor Segmentation Networks with User-Assisted Filter\n  Estimation and Selection","summary":"  Brain tumor image segmentation is a challenging research topic in which\ndeep-learning models have presented the best results. However, the traditional\nway of training those models from many pre-annotated images leaves several\nunanswered questions. Hence methodologies, such as Feature Learning from Image\nMarkers (FLIM), have involved an expert in the learning loop to reduce human\neffort in data annotation and build models sufficiently deep for a given\nproblem. FLIM has been successfully used to create encoders, estimating the\nfilters of all convolutional layers from patches centered at marker voxels. In\nthis work, we present Multi-Step (MS) FLIM - a user-assisted approach to\nestimating and selecting the most relevant filters from multiple FLIM\nexecutions. MS-FLIM is used only for the first convolutional layer, and the\nresults already indicate improvement over FLIM. For evaluation, we build a\nsimple U-shaped encoder-decoder network, named sU-Net, for glioblastoma\nsegmentation using T1Gd and FLAIR MRI scans, varying the encoder's training\nmethod, using FLIM, MS-FLIM, and backpropagation algorithm. Also, we compared\nthese sU-Nets with two State-Of-The-Art (SOTA) deep-learning models using two\ndatasets. The results show that the sU-Net based on MS-FLIM outperforms the\nother training methods and achieves effectiveness within the standard\ndeviations of the SOTA models.\n","authors":["Matheus A. Cerqueira","Flávia Sprenger","Bernardo C. A. Teixeira","Alexandre X. Falcão"],"pdf_url":"https://arxiv.org/pdf/2403.12748v1.pdf","comment":"10 pages, 5 figures, 2 tables, 24 references, manuscript of\n  conference paper"},{"id":"http://arxiv.org/abs/2403.09499v2","updated":"2024-03-19T13:53:16Z","published":"2024-03-14T15:42:26Z","title":"A Reinforcement Learning Approach to Dairy Farm Battery Management using\n  Q Learning","summary":"  Dairy farming consumes a significant amount of energy, making it an\nenergy-intensive sector within agriculture. Integrating renewable energy\ngeneration into dairy farming could help address this challenge. Effective\nbattery management is important for integrating renewable energy generation.\nManaging battery charging and discharging poses significant challenges because\nof fluctuations in electrical consumption, the intermittent nature of renewable\nenergy generation, and fluctuations in energy prices. Artificial Intelligence\n(AI) has the potential to significantly improve the use of renewable energy in\ndairy farming, however, there is limited research conducted in this particular\ndomain. This research considers Ireland as a case study as it works towards\nattaining its 2030 energy strategy centered on the utilization of renewable\nsources. This study proposes a Q-learning-based algorithm for scheduling\nbattery charging and discharging in a dairy farm setting. This research also\nexplores the effect of the proposed algorithm by adding wind generation data\nand considering additional case studies. The proposed algorithm reduces the\ncost of imported electricity from the grid by 13.41\\%, peak demand by 2\\%, and\n24.49\\% when utilizing wind generation. These results underline how\nreinforcement learning is highly effective in managing batteries in the dairy\nfarming sector.\n","authors":["Nawazish Ali","Abdul Wahid","Rachael Shaw","Karl Mason"],"pdf_url":"https://arxiv.org/pdf/2403.09499v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12730v1","updated":"2024-03-19T13:45:34Z","published":"2024-03-19T13:45:34Z","title":"What Does Evaluation of Explainable Artificial Intelligence Actually\n  Tell Us? A Case for Compositional and Contextual Validation of XAI Building\n  Blocks","summary":"  Despite significant progress, evaluation of explainable artificial\nintelligence remains elusive and challenging. In this paper we propose a\nfine-grained validation framework that is not overly reliant on any one facet\nof these sociotechnical systems, and that recognises their inherent modular\nstructure: technical building blocks, user-facing explanatory artefacts and\nsocial communication protocols. While we concur that user studies are\ninvaluable in assessing the quality and effectiveness of explanation\npresentation and delivery strategies from the explainees' perspective in a\nparticular deployment context, the underlying explanation generation mechanisms\nrequire a separate, predominantly algorithmic validation strategy that accounts\nfor the technical and human-centred desiderata of their (numerical) outputs.\nSuch a comprehensive sociotechnical utility-based evaluation framework could\nallow to systematically reason about the properties and downstream influence of\ndifferent building blocks from which explainable artificial intelligence\nsystems are composed -- accounting for a diverse range of their engineering and\nsocial aspects -- in view of the anticipated use case.\n","authors":["Kacper Sokol","Julia E. Vogt"],"pdf_url":"https://arxiv.org/pdf/2403.12730v1.pdf","comment":"Published in Extended Abstracts of the 2024 CHI Conference on Human\n  Factors in Computing Systems (CHI EA '24)"},{"id":"http://arxiv.org/abs/2403.12723v1","updated":"2024-03-19T13:41:11Z","published":"2024-03-19T13:41:11Z","title":"Python Fuzzing for Trustworthy Machine Learning Frameworks","summary":"  Ensuring the security and reliability of machine learning frameworks is\ncrucial for building trustworthy AI-based systems. Fuzzing, a popular technique\nin secure software development lifecycle (SSDLC), can be used to develop secure\nand robust software. Popular machine learning frameworks such as PyTorch and\nTensorFlow are complex and written in multiple programming languages including\nC/C++ and Python. We propose a dynamic analysis pipeline for Python projects\nusing the Sydr-Fuzz toolset. Our pipeline includes fuzzing, corpus\nminimization, crash triaging, and coverage collection. Crash triaging and\nseverity estimation are important steps to ensure that the most critical\nvulnerabilities are addressed promptly. Furthermore, the proposed pipeline is\nintegrated in GitLab CI. To identify the most vulnerable parts of the machine\nlearning frameworks, we analyze their potential attack surfaces and develop\nfuzz targets for PyTorch, TensorFlow, and related projects such as h5py.\nApplying our dynamic analysis pipeline to these targets, we were able to\ndiscover 3 new bugs and propose fixes for them.\n","authors":["Ilya Yegorov","Eli Kobrin","Darya Parygina","Alexey Vishnyakov","Andrey Fedotov"],"pdf_url":"https://arxiv.org/pdf/2403.12723v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.08260v2","updated":"2024-03-19T13:30:26Z","published":"2023-04-17T13:20:04Z","title":"Cross or Wait? Predicting Pedestrian Interaction Outcomes at\n  Unsignalized Crossings","summary":"  Predicting pedestrian behavior when interacting with vehicles is one of the\nmost critical challenges in the field of automated driving. Pedestrian crossing\nbehavior is influenced by various interaction factors, including time to\narrival, pedestrian waiting time, the presence of zebra crossing, and the\nproperties and personality traits of both pedestrians and drivers. However,\nthese factors have not been fully explored for use in predicting interaction\noutcomes. In this paper, we use machine learning to predict pedestrian crossing\nbehavior including pedestrian crossing decision, crossing initiation time\n(CIT), and crossing duration (CD) when interacting with vehicles at\nunsignalized crossings. Distributed simulator data are utilized for predicting\nand analyzing the interaction factors. Compared with the logistic regression\nbaseline model, our proposed neural network model improves the prediction\naccuracy and F1 score by 4.46% and 3.23%, respectively. Our model also reduces\nthe root mean squared error (RMSE) for CIT and CD by 21.56% and 30.14% compared\nwith the linear regression model. Additionally, we have analyzed the importance\nof interaction factors, and present the results of models using fewer factors.\nThis provides information for model selection in different scenarios with\nlimited input features.\n","authors":["Chi Zhang","Amir Hossein Kalantari","Yue Yang","Zhongjun Ni","Gustav Markkula","Natasha Merat","Christian Berger"],"pdf_url":"https://arxiv.org/pdf/2304.08260v2.pdf","comment":"8 pages, 7 figures, 9 tables. Accepted in 2023 IEEE Intelligent\n  Vehicles Symposium (IV). DOI: 10.1109/IV55152.2023.10186616"},{"id":"http://arxiv.org/abs/2403.11755v2","updated":"2024-03-19T13:28:27Z","published":"2024-03-18T13:03:24Z","title":"Meta-Prompting for Automating Zero-shot Visual Recognition with LLMs","summary":"  Prompt ensembling of Large Language Model (LLM) generated category-specific\nprompts has emerged as an effective method to enhance zero-shot recognition\nability of Vision-Language Models (VLMs). To obtain these category-specific\nprompts, the present methods rely on hand-crafting the prompts to the LLMs for\ngenerating VLM prompts for the downstream tasks. However, this requires\nmanually composing these task-specific prompts and still, they might not cover\nthe diverse set of visual concepts and task-specific styles associated with the\ncategories of interest. To effectively take humans out of the loop and\ncompletely automate the prompt generation process for zero-shot recognition, we\npropose Meta-Prompting for Visual Recognition (MPVR). Taking as input only\nminimal information about the target task, in the form of its short natural\nlanguage description, and a list of associated class labels, MPVR automatically\nproduces a diverse set of category-specific prompts resulting in a strong\nzero-shot classifier. MPVR generalizes effectively across various popular\nzero-shot image recognition benchmarks belonging to widely different domains\nwhen tested with multiple LLMs and VLMs. For example, MPVR obtains a zero-shot\nrecognition improvement over CLIP by up to 19.8% and 18.2% (5.0% and 4.5% on\naverage over 20 datasets) leveraging GPT and Mixtral LLMs, respectively\n","authors":["M. Jehanzeb Mirza","Leonid Karlinsky","Wei Lin","Sivan Doveh","Jakub Micorek","Mateusz Kozinski","Hilde Kuhene","Horst Possegger"],"pdf_url":"https://arxiv.org/pdf/2403.11755v2.pdf","comment":"Project Page (Code and Data):\n  https://jmiemirza.github.io/Meta-Prompting/"},{"id":"http://arxiv.org/abs/2310.08559v3","updated":"2024-03-19T13:18:22Z","published":"2023-10-12T17:51:10Z","title":"Phenomenal Yet Puzzling: Testing Inductive Reasoning Capabilities of\n  Language Models with Hypothesis Refinement","summary":"  The ability to derive underlying principles from a handful of observations\nand then generalize to novel situations -- known as inductive reasoning -- is\ncentral to human intelligence. Prior work suggests that language models (LMs)\noften fall short on inductive reasoning, despite achieving impressive success\non research benchmarks. In this work, we conduct a systematic study of the\ninductive reasoning capabilities of LMs through iterative hypothesis\nrefinement, a technique that more closely mirrors the human inductive process\nthan standard input-output prompting. Iterative hypothesis refinement employs a\nthree-step process: proposing, selecting, and refining hypotheses in the form\nof textual rules. By examining the intermediate rules, we observe that LMs are\nphenomenal hypothesis proposers (i.e., generating candidate rules), and when\ncoupled with a (task-specific) symbolic interpreter that is able to\nsystematically filter the proposed set of rules, this hybrid approach achieves\nstrong results across inductive reasoning benchmarks that require inducing\ncausal relations, language-like instructions, and symbolic concepts. However,\nthey also behave as puzzling inductive reasoners, showing notable performance\ngaps between rule induction (i.e., identifying plausible rules) and rule\napplication (i.e., applying proposed rules to instances), suggesting that LMs\nare proposing hypotheses without being able to actually apply the rules.\nThrough empirical and human analyses, we further reveal several discrepancies\nbetween the inductive reasoning processes of LMs and humans, shedding light on\nboth the potentials and limitations of using LMs in inductive reasoning tasks.\n","authors":["Linlu Qiu","Liwei Jiang","Ximing Lu","Melanie Sclar","Valentina Pyatkin","Chandra Bhagavatula","Bailin Wang","Yoon Kim","Yejin Choi","Nouha Dziri","Xiang Ren"],"pdf_url":"https://arxiv.org/pdf/2310.08559v3.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2311.02971v2","updated":"2024-03-19T13:08:59Z","published":"2023-11-06T09:17:18Z","title":"TabRepo: A Large Scale Repository of Tabular Model Evaluations and its\n  AutoML Applications","summary":"  We introduce TabRepo, a new dataset of tabular model evaluations and\npredictions. TabRepo contains the predictions and metrics of 1310 models\nevaluated on 200 classification and regression datasets. We illustrate the\nbenefit of our dataset in multiple ways. First, we show that it allows to\nperform analysis such as comparing Hyperparameter Optimization against current\nAutoML systems while also considering ensembling at marginal cost by using\nprecomputed model predictions. Second, we show that our dataset can be readily\nleveraged to perform transfer-learning. In particular, we show that applying\nstandard transfer-learning techniques allows to outperform current\nstate-of-the-art tabular systems in accuracy, runtime and latency.\n","authors":["David Salinas","Nick Erickson"],"pdf_url":"https://arxiv.org/pdf/2311.02971v2.pdf","comment":null}],"Databases":[{"id":"http://arxiv.org/abs/2311.15380v2","updated":"2024-03-19T15:40:57Z","published":"2023-11-26T18:51:33Z","title":"Grafite: Taming Adversarial Queries with Optimal Range Filters","summary":"  Range filters allow checking whether a query range intersects a given set of\nkeys with a chance of returning a false positive answer, thus generalising the\nfunctionality of Bloom filters from point to range queries. Existing practical\nrange filters have addressed this problem heuristically, resulting in high\nfalse positive rates and query times when dealing with adversarial inputs, such\nas in the common scenario where queries are correlated with the keys.\n  We introduce Grafite, a novel range filter that solves these issues with a\nsimple design and clear theoretical guarantees that hold regardless of the\ninput data and query distribution: given a fixed space budget of $B$ bits per\nkey, the query time is $O(1)$, and the false positive probability is upper\nbounded by $\\ell/2^{B-2}$, where $\\ell$ is the query range size. Our\nexperimental evaluation shows that Grafite is the only range filter to date to\nachieve robust and predictable false positive rates across all combinations of\ndatasets, query workloads, and range sizes, while providing faster queries and\nconstruction times, and dominating all competitors in the case of correlated\nqueries.\n  As a further contribution, we introduce a very simple heuristic range filter\nwhose performance on uncorrelated queries is very close to or better than the\none achieved by the best heuristic range filters proposed in the literature so\nfar.\n","authors":["Marco Costa","Paolo Ferragina","Giorgio Vinciguerra"],"pdf_url":"https://arxiv.org/pdf/2311.15380v2.pdf","comment":"Accepted for publication in Proceedings of the ACM on Management of\n  Data (SIGMOD 2024)"},{"id":"http://arxiv.org/abs/2311.10634v2","updated":"2024-03-19T11:48:32Z","published":"2023-11-17T16:42:29Z","title":"Counting Answers to Unions of Conjunctive Queries: Natural Tractability\n  Criteria and Meta-Complexity","summary":"  We study the problem of counting answers to unions of conjunctive queries\n(UCQs) under structural restrictions on the input query. Concretely, given a\nclass C of UCQs, the problem #UCQ(C) provides as input a UCQ Q in C and a\ndatabase D and the problem is to compute the number of answers of Q in D.\n  Chen and Mengel [PODS'16] have shown that for any recursively enumerable\nclass C, the problem #UCQ(C) is either fixed-parameter tractable or hard for\none of the parameterised complexity classes W[1] or #W[1]. However, their\ntractability criterion is unwieldy in the sense that, given any concrete class\nC of UCQs, it is not easy to determine how hard it is to count answers to\nqueries in C. Moreover, given a single specific UCQ Q, it is not easy to\ndetermine how hard it is to count answers to Q.\n  In this work, we address the question of finding a natural tractability\ncriterion: The combined conjunctive query of a UCQ $\\varphi_1 \\vee \\dots \\vee\n\\varphi_\\ell$ is the conjunctive query $\\varphi_1 \\wedge \\dots \\wedge\n\\varphi_\\ell$. We show that under natural closure properties of C, the problem\n#UCQ(C) is fixed-parameter tractable if and only if the combined conjunctive\nqueries of UCQs in C, and their contracts, have bounded treewidth. A contract\nof a conjunctive query is an augmented structure, taking into account how the\nquantified variables are connected to the free variables. If all variables are\nfree, then a conjunctive query is equal to its contract; in this special case\nthe criterion for fixed-parameter tractability of #UCQ(C) thus simplifies to\nthe combined queries having bounded treewidth.\n  Finally, we give evidence that a closure property on C is necessary for\nobtaining a natural tractability criterion: We show that even for a single UCQ\nQ, the meta problem of deciding whether #UCQ({Q}) can be solved in time\n$O(|D|^d)$ is NP-hard for any fixed $d\\geq 1$.\n","authors":["Jacob Focke","Leslie Ann Goldberg","Marc Roth","Stanislav Živný"],"pdf_url":"https://arxiv.org/pdf/2311.10634v2.pdf","comment":"41 pages, 2 figures, abstract shortened due to ArXiv requirements"},{"id":"http://arxiv.org/abs/2403.12605v1","updated":"2024-03-19T10:14:48Z","published":"2024-03-19T10:14:48Z","title":"A Benchmark for Data Management Challenges in Microservices","summary":"  Microservice architectures emerged as a popular architecture for designing\nscalable distributed applications. Although microservices have been extensively\nemployed in industry settings for over a decade, there is little understanding\nof the data management challenges that arise in these applications. As a\nresult, it is difficult to advance data system technologies for supporting\nmicroservice applications. To fill this gap, we present Online Marketplace, a\nmicroservice benchmark that incorporates core data management challenges that\nexisting benchmarks have not sufficiently addressed. These challenges include\ntransaction processing, query processing, event processing, constraint\nenforcement, and data replication. We have defined criteria for various data\nmanagement issues to enable proper comparison across data systems and\nplatforms.\n  After specifying the benchmark, we present the challenges we faced in\ncreating workloads that accurately reflect the dynamic state of the\nmicroservices. We also discuss implementation issues that we encountered when\ndeveloping Online Marketplace in state-of-the-art data platforms, which\nprevented us from meeting the specified data management requirements and\ncriteria. Our evaluation demonstrates that the benchmark is a valuable tool for\ntesting important properties sought by microservice practitioners. As a result,\nour proposed benchmark will facilitate the design of future data systems to\nmeet the expectations of microservice practitioners.\n","authors":["Rodrigo Laigner","Zhexiang Zhang","Yijian Liu","Leonardo Freitas Gomes","Yongluan Zhou"],"pdf_url":"https://arxiv.org/pdf/2403.12605v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12583v1","updated":"2024-03-19T09:45:49Z","published":"2024-03-19T09:45:49Z","title":"Quantixar: High-performance Vector Data Management System","summary":"  Traditional database management systems need help efficiently represent and\nquerying the complex, high-dimensional data prevalent in modern applications.\nVector databases offer a solution by storing data as numerical vectors within a\nmulti-dimensional space. This enables similarity-based search and analysis,\nsuch as image retrieval, recommendation engine generation, and natural language\nprocessing. This paper introduces Quantixar, a vector database project designed\nfor efficiency in high-dimensional settings. Quantixar tackles the challenge of\nmanaging high-dimensional data by strategically combining advanced indexing and\nquantization techniques. It employs HNSW indexing for accelerated ANN search.\nAdditionally, Quantixar incorporates binary and product quantization to\ncompress high-dimensional vectors, reducing storage requirements and\ncomputational costs during search. The paper delves into Quantixar's\narchitecture, specific implementation, and experimental methodology.\n","authors":["Gulshan Yadav","RahulKumar Yadav","Mansi Viramgama","Mayank Viramgama","Apeksha Mohite"],"pdf_url":"https://arxiv.org/pdf/2403.12583v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11395v2","updated":"2024-03-19T09:36:27Z","published":"2024-03-18T01:07:48Z","title":"Automated data processing and feature engineering for deep learning and\n  big data applications: a survey","summary":"  Modern approach to artificial intelligence (AI) aims to design algorithms\nthat learn directly from data. This approach has achieved impressive results\nand has contributed significantly to the progress of AI, particularly in the\nsphere of supervised deep learning. It has also simplified the design of\nmachine learning systems as the learning process is highly automated. However,\nnot all data processing tasks in conventional deep learning pipelines have been\nautomated. In most cases data has to be manually collected, preprocessed and\nfurther extended through data augmentation before they can be effective for\ntraining. Recently, special techniques for automating these tasks have emerged.\nThe automation of data processing tasks is driven by the need to utilize large\nvolumes of complex, heterogeneous data for machine learning and big data\napplications. Today, end-to-end automated data processing systems based on\nautomated machine learning (AutoML) techniques are capable of taking raw data\nand transforming them into useful features for Big Data tasks by automating all\nintermediate processing stages. In this work, we present a thorough review of\napproaches for automating data processing tasks in deep learning pipelines,\nincluding automated data preprocessing--e.g., data cleaning, labeling, missing\ndata imputation, and categorical data encoding--as well as data augmentation\n(including synthetic data generation using generative AI methods) and feature\nengineering--specifically, automated feature extraction, feature construction\nand feature selection. In addition to automating specific data processing\ntasks, we discuss the use of AutoML methods and tools to simultaneously\noptimize all stages of the machine learning pipeline.\n","authors":["Alhassan Mumuni","Fuseini Mumuni"],"pdf_url":"https://arxiv.org/pdf/2403.11395v2.pdf","comment":"Journal of Information and Intelligence (2024)"},{"id":"http://arxiv.org/abs/2403.12436v1","updated":"2024-03-19T04:49:17Z","published":"2024-03-19T04:49:17Z","title":"Evaluating Datalog over Semirings: A Grounding-based Approach","summary":"  Datalog is a powerful yet elegant language that allows expressing recursive\ncomputation. Although Datalog evaluation has been extensively studied in the\nliterature, so far, only loose upper bounds are known on how fast a Datalog\nprogram can be evaluated. In this work, we ask the following question: given a\nDatalog program over a naturally-ordered semiring $\\sigma$, what is the\ntightest possible runtime? To this end, our main contribution is a general\ntwo-phase framework for analyzing the data complexity of Datalog over $\\sigma$:\nfirst ground the program into an equivalent system of polynomial equations\n(i.e. grounding) and then find the least fixpoint of the grounding over\n$\\sigma$. We present algorithms that use structure-aware query evaluation\ntechniques to obtain the smallest possible groundings. Next, efficient\nalgorithms for fixpoint evaluation are introduced over two classes of\nsemirings: (1) finite-rank semirings and (2) absorptive semirings of total\norder. Combining both phases, we obtain state-of-the-art and new algorithmic\nresults. Finally, we complement our results with a matching fine-grained lower\nbound.\n","authors":["Hangdong Zhao","Shaleen Deep","Paraschos Koutris","Sudeepa Roy","Val Tannen"],"pdf_url":"https://arxiv.org/pdf/2403.12436v1.pdf","comment":"To appear at PODS 2024"},{"id":"http://arxiv.org/abs/2403.12433v1","updated":"2024-03-19T04:46:56Z","published":"2024-03-19T04:46:56Z","title":"Algorithmic Complexity Attacks on Dynamic Learned Indexes","summary":"  Learned Index Structures (LIS) view a sorted index as a model that learns the\ndata distribution, takes a data element key as input, and outputs the predicted\nposition of the key. The original LIS can only handle lookup operations with no\nsupport for updates, rendering it impractical to use for typical workloads. To\naddress this limitation, recent studies have focused on designing efficient\ndynamic learned indexes. ALEX, as the pioneering dynamic learned index\nstructures, enables dynamism by incorporating a series of design choices,\nincluding adaptive key space partitioning, dynamic model retraining, and\nsophisticated engineering and policies that prioritize read/write performance.\nWhile these design choices offer improved average-case performance, the\nemphasis on flexibility and performance increases the attack surface by\nallowing adversarial behaviors that maximize ALEX's memory space and time\ncomplexity in worst-case scenarios. In this work, we present the first\nsystematic investigation of algorithmic complexity attacks (ACAs) targeting the\nworst-case scenarios of ALEX. We introduce new ACAs that fall into two\ncategories, space ACAs and time ACAs, which target the memory space and time\ncomplexity, respectively. First, our space ACA on data nodes exploits ALEX's\ngapped array layout and uses Multiple-Choice Knapsack (MCK) to generate an\noptimal adversarial insertion plan for maximizing the memory consumption at the\ndata node level. Second, our space ACA on internal nodes exploits ALEX's\ncatastrophic cost mitigation mechanism, causing an out-of-memory error with\nonly a few hundred adversarial insertions. Third, our time ACA generates\npathological insertions to increase the disparity between the actual key\ndistribution and the linear models of data nodes, deteriorating the runtime\nperformance by up to 1,641X compared to ALEX operating under legitimate\nworkloads.\n","authors":["Rui Yang","Evgenios M. Kornaropoulos","Yue Cheng"],"pdf_url":"https://arxiv.org/pdf/2403.12433v1.pdf","comment":"VLDB 2024"},{"id":"http://arxiv.org/abs/2403.11920v2","updated":"2024-03-19T04:40:43Z","published":"2024-03-18T16:19:30Z","title":"Bangladesh Agricultural Knowledge Graph: Enabling Semantic Integration\n  and Data-driven Analysis--Full Version","summary":"  In Bangladesh, agriculture is a crucial driver for addressing Sustainable\nDevelopment Goal 1 (No Poverty) and 2 (Zero Hunger), playing a fundamental role\nin the economy and people's livelihoods. To enhance the sustainability and\nresilience of the agriculture industry through data-driven insights, the\nBangladesh Bureau of Statistics and other organizations consistently collect\nand publish agricultural data on the Web. Nevertheless, the current datasets\nencounter various challenges: 1) they are presented in an unsustainable,\nstatic, read-only, and aggregated format, 2) they do not conform to the\nFindability, Accessibility, Interoperability, and Reusability (FAIR)\nprinciples, and 3) they do not facilitate interactive analysis and integration\nwith other data sources. In this paper, we present a thorough solution,\ndelineating a systematic procedure for developing BDAKG: a knowledge graph that\nsemantically and analytically integrates agriculture data in Bangladesh. BDAKG\nincorporates multidimensional semantics, is linked with external knowledge\ngraphs, is compatible with OLAP, and adheres to the FAIR principles. Our\nexperimental evaluation centers on evaluating the integration process and\nassessing the quality of the resultant knowledge graph in terms of\ncompleteness, timeliness, FAIRness, OLAP compatibility and data-driven\nanalysis. Our federated data analysis recommend a strategic approach focused on\ndecreasing CO$_2$ emissions, fostering economic growth, and promoting\nsustainable forestry.\n","authors":["Rudra Pratap Deb Nath","Tithi Rani Das","Tonmoy Chandro Das","S. M. Shafkat Raihan"],"pdf_url":"https://arxiv.org/pdf/2403.11920v2.pdf","comment":"40 pages, 15 figures"},{"id":"http://arxiv.org/abs/2403.13181v1","updated":"2024-03-19T22:30:56Z","published":"2024-03-19T22:30:56Z","title":"Efficient k-step Weighted Reachability Query Processing Algorithms","summary":"  Given a data graph G, a source vertex u and a target vertex v of a\nreachability query, the reachability query is used to answer whether there\nexists a path from u to v in G. Reachability query processing is one of the\nfundamental operations in graph data management, which is widely used in\nbiological networks, communication networks, and social networks to assist data\nanalysis. The data graphs in practical applications usually contain information\nsuch as quantization weights associated with the structural relationships, in\naddition to the structural relationships between vertices. Thus, in addition to\nthe traditional reachability relationships, users may want to further\nunderstand whether such reachability relationships satisfy specific\nconstraints. In this paper, we study the problem of efficiently processing k\n-step reachability queries with weighted constraints in weighted graphs. The k\n-step weighted reachability query questions are used to answer the question of\nwhether there exists a path from a source vertex u to a goal vertex v in a\ngiven weighted graph. If it exists, the path needs to satisfy 1) all edges in\nthe path satisfy the given weight constraints, and 2) the length of the path\ndoes not exceed the given distance threshold k. To address the problem,\nfirstly, WKRI index supporting k -step weighted reachability query processing\nand index construction methods based on efficient pruning strategies are\nproposed. Secondly, the idea of constructing index based on part of the vertexs\nis proposed to reduce the size of the index. We design and implement two\noptimized indexes GWKRI and LWKRI based on the vertex coverage set. Finally,\nexperiments are conducted on several real datasets. The experimental results\nverify the efficiency of the method proposed in this paper in answering k -step\nweighted reachability queries.\n","authors":["Lian Chen","Junfeng Zhou","Ming Du","Sheng Yu","Xian Tang","Ziyang Chen"],"pdf_url":"https://arxiv.org/pdf/2403.13181v1.pdf","comment":null}],"Machine Learning":[{"id":"http://arxiv.org/abs/2403.12968v1","updated":"2024-03-19T17:59:56Z","published":"2024-03-19T17:59:56Z","title":"LLMLingua-2: Data Distillation for Efficient and Faithful Task-Agnostic\n  Prompt Compression","summary":"  This paper focuses on task-agnostic prompt compression for better\ngeneralizability and efficiency. Considering the redundancy in natural\nlanguage, existing approaches compress prompts by removing tokens or lexical\nunits according to their information entropy obtained from a causal language\nmodel such as LLaMa-7B. The challenge is that information entropy may be a\nsuboptimal compression metric: (i) it only leverages unidirectional context and\nmay fail to capture all essential information needed for prompt compression;\n(ii) it is not aligned with the prompt compression objective.\n  To address these issues, we propose a data distillation procedure to derive\nknowledge from an LLM to compress prompts without losing crucial information,\nand meantime, introduce an extractive text compression dataset. We formulate\nprompt compression as a token classification problem to guarantee the\nfaithfulness of the compressed prompt to the original one, and use a\nTransformer encoder as the base architecture to capture all essential\ninformation for prompt compression from the full bidirectional context. Our\napproach leads to lower latency by explicitly learning the compression\nobjective with smaller models such as XLM-RoBERTa-large and mBERT.\n  We evaluate our method on both in-domain and out-of-domain datasets,\nincluding MeetingBank, LongBench, ZeroScrolls, GSM8K, and BBH. Despite its\nsmall size, our model shows significant performance gains over strong baselines\nand demonstrates robust generalization ability across different LLMs.\nAdditionally, our model is 3x-6x faster than existing prompt compression\nmethods, while accelerating the end-to-end latency by 1.6x-2.9x with\ncompression ratios of 2x-5x.\n","authors":["Zhuoshi Pan","Qianhui Wu","Huiqiang Jiang","Menglin Xia","Xufang Luo","Jue Zhang","Qingwei Lin","Victor Rühle","Yuqing Yang","Chin-Yew Lin","H. Vicky Zhao","Lili Qiu","Dongmei Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.12968v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12961v1","updated":"2024-03-19T17:59:09Z","published":"2024-03-19T17:59:09Z","title":"TexTile: A Differentiable Metric for Texture Tileability","summary":"  We introduce TexTile, a novel differentiable metric to quantify the degree\nupon which a texture image can be concatenated with itself without introducing\nrepeating artifacts (i.e., the tileability). Existing methods for tileable\ntexture synthesis focus on general texture quality, but lack explicit analysis\nof the intrinsic repeatability properties of a texture. In contrast, our\nTexTile metric effectively evaluates the tileable properties of a texture,\nopening the door to more informed synthesis and analysis of tileable textures.\nUnder the hood, TexTile is formulated as a binary classifier carefully built\nfrom a large dataset of textures of different styles, semantics, regularities,\nand human annotations.Key to our method is a set of architectural modifications\nto baseline pre-train image classifiers to overcome their shortcomings at\nmeasuring tileability, along with a custom data augmentation and training\nregime aimed at increasing robustness and accuracy. We demonstrate that TexTile\ncan be plugged into different state-of-the-art texture synthesis methods,\nincluding diffusion-based strategies, and generate tileable textures while\nkeeping or even improving the overall texture quality. Furthermore, we show\nthat TexTile can objectively evaluate any tileable texture synthesis method,\nwhereas the current mix of existing metrics produces uncorrelated scores which\nheavily hinders progress in the field.\n","authors":["Carlos Rodriguez-Pardo","Dan Casas","Elena Garces","Jorge Lopez-Moreno"],"pdf_url":"https://arxiv.org/pdf/2403.12961v1.pdf","comment":"CVPR 2024. Project page: https://mslab.es/projects/TexTile/"},{"id":"http://arxiv.org/abs/2403.12959v1","updated":"2024-03-19T17:58:02Z","published":"2024-03-19T17:58:02Z","title":"WHAC: World-grounded Humans and Cameras","summary":"  Estimating human and camera trajectories with accurate scale in the world\ncoordinate system from a monocular video is a highly desirable yet challenging\nand ill-posed problem. In this study, we aim to recover expressive parametric\nhuman models (i.e., SMPL-X) and corresponding camera poses jointly, by\nleveraging the synergy between three critical players: the world, the human,\nand the camera. Our approach is founded on two key observations. Firstly,\ncamera-frame SMPL-X estimation methods readily recover absolute human depth.\nSecondly, human motions inherently provide absolute spatial cues. By\nintegrating these insights, we introduce a novel framework, referred to as\nWHAC, to facilitate world-grounded expressive human pose and shape estimation\n(EHPS) alongside camera pose estimation, without relying on traditional\noptimization techniques. Additionally, we present a new synthetic dataset,\nWHAC-A-Mole, which includes accurately annotated humans and cameras, and\nfeatures diverse interactive human motions as well as realistic camera\ntrajectories. Extensive experiments on both standard and newly established\nbenchmarks highlight the superiority and efficacy of our framework. We will\nmake the code and dataset publicly available.\n","authors":["Wanqi Yin","Zhongang Cai","Ruisi Wang","Fanzhou Wang","Chen Wei","Haiyi Mei","Weiye Xiao","Zhitao Yang","Qingping Sun","Atsushi Yamashita","Ziwei Liu","Lei Yang"],"pdf_url":"https://arxiv.org/pdf/2403.12959v1.pdf","comment":"Homepage: https://wqyin.github.io/projects/WHAC/"},{"id":"http://arxiv.org/abs/2403.12952v1","updated":"2024-03-19T17:54:34Z","published":"2024-03-19T17:54:34Z","title":"Just Shift It: Test-Time Prototype Shifting for Zero-Shot Generalization\n  with Vision-Language Models","summary":"  Advancements in vision-language models (VLMs) have propelled the field of\ncomputer vision, particularly in the zero-shot learning setting. Despite their\npromise, the effectiveness of these models often diminishes due to domain\nshifts in test environments. To address this, we introduce the Test-Time\nPrototype Shifting (TPS) framework, a pioneering approach designed to adapt\nVLMs to test datasets using unlabeled test inputs. Our method is based on the\nnotion of modulating per-class prototypes in the shared embedding space. By\npre-computing and caching prototypes generated with the pre-trained text\nencoder, TPS not only facilitates optimization-free prototype reuse for\nsubsequent predictions but also enables seamless integration with current\nadvancements in prompt engineering. At test-time, TPS dynamically learns shift\nvectors for each prototype based solely on the given test sample, effectively\nbridging the domain gap and enhancing classification accuracy. A notable aspect\nof our framework is its significantly reduced memory and computational demands\nwhen compared to conventional text-prompt tuning methods. Extensive evaluations\nacross 15 datasets involving natural distribution shifts and cross-dataset\ngeneralization demonstrate TPS's superior performance, achieving\nstate-of-the-art results while reducing resource requirements.\n","authors":["Elaine Sui","Xiaohan Wang","Serena Yeung-Levy"],"pdf_url":"https://arxiv.org/pdf/2403.12952v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06420v2","updated":"2024-03-19T17:52:09Z","published":"2024-03-11T04:13:26Z","title":"RLingua: Improving Reinforcement Learning Sample Efficiency in Robotic\n  Manipulations With Large Language Models","summary":"  Reinforcement learning (RL) has demonstrated its capability in solving\nvarious tasks but is notorious for its low sample efficiency. In this paper, we\npropose RLingua, a framework that can leverage the internal knowledge of large\nlanguage models (LLMs) to reduce the sample complexity of RL in robotic\nmanipulations. To this end, we first present a method for extracting the prior\nknowledge of LLMs by prompt engineering so that a preliminary rule-based robot\ncontroller for a specific task can be generated in a user-friendly manner.\nDespite being imperfect, the LLM-generated robot controller is utilized to\nproduce action samples during rollouts with a decaying probability, thereby\nimproving RL's sample efficiency. We employ TD3, the widely-used RL baseline\nmethod, and modify the actor loss to regularize the policy learning towards the\nLLM-generated controller. RLingua also provides a novel method of improving the\nimperfect LLM-generated robot controllers by RL. We demonstrate that RLingua\ncan significantly reduce the sample complexity of TD3 in four robot tasks of\npanda_gym and achieve high success rates in 12 sampled sparsely rewarded robot\ntasks in RLBench, where the standard TD3 fails. Additionally, We validated\nRLingua's effectiveness in real-world robot experiments through Sim2Real,\ndemonstrating that the learned policies are effectively transferable to real\nrobot tasks. Further details about our work are available at our project\nwebsite https://rlingua.github.io.\n","authors":["Liangliang Chen","Yutian Lei","Shiyu Jin","Ying Zhang","Liangjun Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.06420v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10081v2","updated":"2024-03-19T17:51:45Z","published":"2023-11-16T18:37:29Z","title":"DRESS: Instructing Large Vision-Language Models to Align and Interact\n  with Humans via Natural Language Feedback","summary":"  We present DRESS, a large vision language model (LVLM) that innovatively\nexploits Natural Language feedback (NLF) from Large Language Models to enhance\nits alignment and interactions by addressing two key limitations in the\nstate-of-the-art LVLMs. First, prior LVLMs generally rely only on the\ninstruction finetuning stage to enhance alignment with human preferences.\nWithout incorporating extra feedback, they are still prone to generate\nunhelpful, hallucinated, or harmful responses. Second, while the visual\ninstruction tuning data is generally structured in a multi-turn dialogue\nformat, the connections and dependencies among consecutive conversational turns\nare weak. This reduces the capacity for effective multi-turn interactions. To\ntackle these, we propose a novel categorization of the NLF into two key types:\ncritique and refinement. The critique NLF identifies the strengths and\nweaknesses of the responses and is used to align the LVLMs with human\npreferences. The refinement NLF offers concrete suggestions for improvement and\nis adopted to improve the interaction ability of the LVLMs-- which focuses on\nLVLMs' ability to refine responses by incorporating feedback in multi-turn\ninteractions. To address the non-differentiable nature of NLF, we generalize\nconditional reinforcement learning for training. Our experimental results\ndemonstrate that DRESS can generate more helpful (9.76%), honest (11.52%), and\nharmless (21.03%) responses, and more effectively learn from feedback during\nmulti-turn interactions compared to SOTA LVMLs.\n","authors":["Yangyi Chen","Karan Sikka","Michael Cogswell","Heng Ji","Ajay Divakaran"],"pdf_url":"https://arxiv.org/pdf/2311.10081v2.pdf","comment":"CVPR 2024. The feedback datasets are released at:\n  https://huggingface.co/datasets/YangyiYY/LVLM_NLF"},{"id":"http://arxiv.org/abs/2305.01604v3","updated":"2024-03-19T17:51:12Z","published":"2023-05-02T17:09:07Z","title":"The Training Process of Many Deep Networks Explores the Same\n  Low-Dimensional Manifold","summary":"  We develop information-geometric techniques to analyze the trajectories of\nthe predictions of deep networks during training. By examining the underlying\nhigh-dimensional probabilistic models, we reveal that the training process\nexplores an effectively low-dimensional manifold. Networks with a wide range of\narchitectures, sizes, trained using different optimization methods,\nregularization techniques, data augmentation techniques, and weight\ninitializations lie on the same manifold in the prediction space. We study the\ndetails of this manifold to find that networks with different architectures\nfollow distinguishable trajectories but other factors have a minimal influence;\nlarger networks train along a similar manifold as that of smaller networks,\njust faster; and networks initialized at very different parts of the prediction\nspace converge to the solution along a similar manifold.\n","authors":["Jialin Mao","Itay Griniasty","Han Kheng Teoh","Rahul Ramesh","Rubing Yang","Mark K. Transtrum","James P. Sethna","Pratik Chaudhari"],"pdf_url":"https://arxiv.org/pdf/2305.01604v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12950v1","updated":"2024-03-19T17:50:55Z","published":"2024-03-19T17:50:55Z","title":"Optimal and Adaptive Non-Stationary Dueling Bandits Under a Generalized\n  Borda Criterion","summary":"  In dueling bandits, the learner receives preference feedback between arms,\nand the regret of an arm is defined in terms of its suboptimality to a winner\narm. The more challenging and practically motivated non-stationary variant of\ndueling bandits, where preferences change over time, has been the focus of\nseveral recent works (Saha and Gupta, 2022; Buening and Saha, 2023; Suk and\nAgarwal, 2023). The goal is to design algorithms without foreknowledge of the\namount of change.\n  The bulk of known results here studies the Condorcet winner setting, where an\narm preferred over any other exists at all times. Yet, such a winner may not\nexist and, to contrast, the Borda version of this problem (which is always\nwell-defined) has received little attention. In this work, we establish the\nfirst optimal and adaptive Borda dynamic regret upper bound, which highlights\nfundamental differences in the learnability of severe non-stationarity between\nCondorcet vs. Borda regret objectives in dueling bandits.\n  Surprisingly, our techniques for non-stationary Borda dueling bandits also\nyield improved rates within the Condorcet winner setting, and reveal new\npreference models where tighter notions of non-stationarity are adaptively\nlearnable. This is accomplished through a novel generalized Borda score\nframework which unites the Borda and Condorcet problems, thus allowing\nreduction of Condorcet regret to a Borda-like task. Such a generalization was\nnot previously known and is likely to be of independent interest.\n","authors":["Joe Suk","Arpit Agarwal"],"pdf_url":"https://arxiv.org/pdf/2403.12950v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12948v1","updated":"2024-03-19T17:50:32Z","published":"2024-03-19T17:50:32Z","title":"On Safety in Safe Bayesian Optimization","summary":"  Optimizing an unknown function under safety constraints is a central task in\nrobotics, biomedical engineering, and many other disciplines, and increasingly\nsafe Bayesian Optimization (BO) is used for this. Due to the safety critical\nnature of these applications, it is of utmost importance that theoretical\nsafety guarantees for these algorithms translate into the real world. In this\nwork, we investigate three safety-related issues of the popular class of\nSafeOpt-type algorithms. First, these algorithms critically rely on frequentist\nuncertainty bounds for Gaussian Process (GP) regression, but concrete\nimplementations typically utilize heuristics that invalidate all safety\nguarantees. We provide a detailed analysis of this problem and introduce\nReal-\\b{eta}-SafeOpt, a variant of the SafeOpt algorithm that leverages recent\nGP bounds and thus retains all theoretical guarantees. Second, we identify\nassuming an upper bound on the reproducing kernel Hilbert space (RKHS) norm of\nthe target function, a key technical assumption in SafeOpt-like algorithms, as\na central obstacle to real-world usage. To overcome this challenge, we\nintroduce the Lipschitz-only Safe Bayesian Optimization (LoSBO) algorithm,\nwhich guarantees safety without an assumption on the RKHS bound, and\nempirically show that this algorithm is not only safe, but also exhibits\nsuperior performance compared to the state-of-the-art on several function\nclasses. Third, SafeOpt and derived algorithms rely on a discrete search space,\nmaking them difficult to apply to higher-dimensional problems. To widen the\napplicability of these algorithms, we introduce Lipschitz-only GP-UCB\n(LoS-GP-UCB), a variant of LoSBO applicable to moderately high-dimensional\nproblems, while retaining safety.\n","authors":["Christian Fiedler","Johanna Menn","Lukas Kreisköther","Sebastian Trimpe"],"pdf_url":"https://arxiv.org/pdf/2403.12948v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12946v1","updated":"2024-03-19T17:48:42Z","published":"2024-03-19T17:48:42Z","title":"Sample Complexity of Offline Distributionally Robust Linear Markov\n  Decision Processes","summary":"  In offline reinforcement learning (RL), the absence of active exploration\ncalls for attention on the model robustness to tackle the sim-to-real gap,\nwhere the discrepancy between the simulated and deployed environments can\nsignificantly undermine the performance of the learned policy. To endow the\nlearned policy with robustness in a sample-efficient manner in the presence of\nhigh-dimensional state-action space, this paper considers the sample complexity\nof distributionally robust linear Markov decision processes (MDPs) with an\nuncertainty set characterized by the total variation distance using offline\ndata. We develop a pessimistic model-based algorithm and establish its sample\ncomplexity bound under minimal data coverage assumptions, which outperforms\nprior art by at least $\\tilde{O}(d)$, where $d$ is the feature dimension. We\nfurther improve the performance guarantee of the proposed algorithm by\nincorporating a carefully-designed variance estimator.\n","authors":["He Wang","Laixi Shi","Yuejie Chi"],"pdf_url":"https://arxiv.org/pdf/2403.12946v1.pdf","comment":"under review"},{"id":"http://arxiv.org/abs/2403.12938v1","updated":"2024-03-19T17:43:57Z","published":"2024-03-19T17:43:57Z","title":"Neural Differential Algebraic Equations","summary":"  Differential-Algebraic Equations (DAEs) describe the temporal evolution of\nsystems that obey both differential and algebraic constraints. Of particular\ninterest are systems that contain implicit relationships between their\ncomponents, such as conservation relationships. Here, we present Neural\nDifferential-Algebraic Equations (NDAEs) suitable for data-driven modeling of\nDAEs. This methodology is built upon the concept of the Universal Differential\nEquation; that is, a model constructed as a system of Neural Ordinary\nDifferential Equations informed by theory from particular science domains. In\nthis work, we show that the proposed NDAEs abstraction is suitable for relevant\nsystem-theoretic data-driven modeling tasks. Presented examples include (i) the\ninverse problem of tank-manifold dynamics and (ii) discrepancy modeling of a\nnetwork of pumps, tanks, and pipes. Our experiments demonstrate the proposed\nmethod's robustness to noise and extrapolation ability to (i) learn the\nbehaviors of the system components and their interaction physics and (ii)\ndisambiguate between data trends and mechanistic relationships contained in the\nsystem.\n","authors":["James Koch","Madelyn Shapiro","Himanshu Sharma","Draguna Vrabie","Jan Drgona"],"pdf_url":"https://arxiv.org/pdf/2403.12938v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10682v2","updated":"2024-03-19T17:37:39Z","published":"2024-03-15T21:03:34Z","title":"Evaluation of GlassNet for physics-informed machine learning of glass\n  stability and glass-forming ability","summary":"  Glasses form the basis of many modern applications and also hold great\npotential for future medical and environmental applications. However, their\nstructural complexity and large composition space make design and optimization\nchallenging for certain applications. Of particular importance for glass\nprocessing is an estimate of a given composition's glass-forming ability (GFA).\nHowever, there remain many open questions regarding the physical mechanisms of\nglass formation, especially in oxide glasses. It is apparent that a proxy for\nGFA would be highly useful in glass processing and design, but identifying such\na surrogate property has proven itself to be difficult. Here, we explore the\napplication of an open-source pre-trained NN model, GlassNet, that can predict\nthe characteristic temperatures necessary to compute glass stability (GS) and\nassess the feasibility of using these physics-informed ML (PIML)-predicted GS\nparameters to estimate GFA. In doing so, we track the uncertainties at each\nstep of the computation - from the original ML prediction errors, to the\ncompounding of errors during GS estimation, and finally to the final estimation\nof GFA. While GlassNet exhibits reasonable accuracy on all individual\nproperties, we observe a large compounding of error in the combination of these\nindividual predictions for the prediction of GS, finding that random forest\nmodels offer similar accuracy to GlassNet. We also breakdown the ML performance\non different glass families and find that the error in GS prediction is\ncorrelated with the error in crystallization peak temperature prediction.\nLastly, we utilize this finding to assess the relationship between\ntop-performing GS parameters and GFA for two ternary glass systems: sodium\nborosilicate and sodium iron phosphate glasses. We conclude that to obtain true\nML predictive capability of GFA, significantly more data needs to be collected.\n","authors":["Sarah I. Allec","Xiaonan Lu","Daniel R. Cassar","Xuan T. Nguyen","Vinay I. Hegde","Thiruvillamalai Mahadevan","Miroslava Peterson","Jincheng Du","Brian J. Riley","John D. Vienna","James E. Saal"],"pdf_url":"https://arxiv.org/pdf/2403.10682v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.02960v2","updated":"2024-03-19T17:35:51Z","published":"2023-06-05T15:26:02Z","title":"Best of Both Worlds: Hybrid SNN-ANN Architecture for Event-based Optical\n  Flow Estimation","summary":"  In the field of robotics, event-based cameras are emerging as a promising\nlow-power alternative to traditional frame-based cameras for capturing\nhigh-speed motion and high dynamic range scenes. This is due to their sparse\nand asynchronous event outputs. Spiking Neural Networks (SNNs) with their\nasynchronous event-driven compute, show great potential for extracting the\nspatio-temporal features from these event streams. In contrast, the standard\nAnalog Neural Networks (ANNs) fail to process event data effectively. However,\ntraining SNNs is difficult due to additional trainable parameters (thresholds\nand leaks), vanishing spikes at deeper layers, and a non-differentiable binary\nactivation function. Furthermore, an additional data structure, membrane\npotential, responsible for keeping track of temporal information, must be\nfetched and updated at every timestep in SNNs. To overcome these challenges, we\npropose a novel SNN-ANN hybrid architecture that combines the strengths of\nboth. Specifically, we leverage the asynchronous compute capabilities of SNN\nlayers to effectively extract the input temporal information. Concurrently, the\nANN layers facilitate training and efficient hardware deployment on traditional\nmachine learning hardware such as GPUs. We provide extensive experimental\nanalysis for assigning each layer to be spiking or analog, leading to a network\nconfiguration optimized for performance and ease of training. We evaluate our\nhybrid architecture for optical flow estimation on DSEC-flow and Multi-Vehicle\nStereo Event-Camera (MVSEC) datasets. On the DSEC-flow dataset, the hybrid\nSNN-ANN architecture achieves a 40% reduction in average endpoint error (AEE)\nwith 22% lower energy consumption compared to Full-SNN, and 48% lower AEE\ncompared to Full-ANN, while maintaining comparable energy usage.\n","authors":["Shubham Negi","Deepika Sharma","Adarsh Kumar Kosta","Kaushik Roy"],"pdf_url":"https://arxiv.org/pdf/2306.02960v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2205.07473v3","updated":"2024-03-19T17:25:14Z","published":"2022-05-16T06:53:14Z","title":"Towards Lossless ANN-SNN Conversion under Ultra-Low Latency with\n  Dual-Phase Optimization","summary":"  Spiking neural networks (SNNs) operating with asynchronous discrete events\nshow higher energy efficiency with sparse computation. A popular approach for\nimplementing deep SNNs is ANN-SNN conversion combining both efficient training\nof ANNs and efficient inference of SNNs. However, the accuracy loss is usually\nnon-negligible, especially under a few time steps, which restricts the\napplications of SNN on latency-sensitive edge devices greatly. In this paper,\nwe first identify that such performance degradation stems from the\nmisrepresentation of the negative or overflow residual membrane potential in\nSNNs. Inspired by this, we decompose the conversion error into three parts:\nquantization error, clipping error, and residual membrane potential\nrepresentation error. With such insights, we propose a two-stage conversion\nalgorithm to minimize those errors respectively. Besides, We show each stage\nachieves significant performance gains in a complementary manner. By evaluating\non challenging datasets including CIFAR-10, CIFAR- 100 and ImageNet, the\nproposed method demonstrates the state-of-the-art performance in terms of\naccuracy, latency and energy preservation. Furthermore, our method is evaluated\nusing a more challenging object detection task, revealing notable gains in\nregression performance under ultra-low latency when compared to existing\nspike-based detection algorithms. Codes are available at\nhttps://github.com/Windere/snn-cvt-dual-phase.\n","authors":["Ziming Wang","Shuang Lian","Yuhao Zhang","Xiaoxin Cui","Rui Yan","Huajin Tang"],"pdf_url":"https://arxiv.org/pdf/2205.07473v3.pdf","comment":"Accepted by IEEE Transactions on Neural Networks and Learning Systems\n  (TNNLS)"},{"id":"http://arxiv.org/abs/2403.12918v1","updated":"2024-03-19T17:21:29Z","published":"2024-03-19T17:21:29Z","title":"Generalizable and Stable Finetuning of Pretrained Language Models on\n  Low-Resource Texts","summary":"  Pretrained Language Models (PLMs) have advanced Natural Language Processing\n(NLP) tasks significantly, but finetuning PLMs on low-resource datasets poses\nsignificant challenges such as instability and overfitting. Previous methods\ntackle these issues by finetuning a strategically chosen subnetwork on a\ndownstream task, while keeping the remaining weights fixed to the pretrained\nweights. However, they rely on a suboptimal criteria for sub-network selection,\nleading to suboptimal solutions. To address these limitations, we propose a\nregularization method based on attention-guided weight mixup for finetuning\nPLMs. Our approach represents each network weight as a mixup of task-specific\nweight and pretrained weight, controlled by a learnable attention parameter,\nproviding finer control over sub-network selection. Furthermore, we employ a\nbi-level optimization (BLO) based framework on two separate splits of the\ntraining dataset, improving generalization and combating overfitting. We\nvalidate the efficacy of our proposed method through extensive experiments,\ndemonstrating its superiority over previous methods, particularly in the\ncontext of finetuning PLMs on low-resource datasets.\n","authors":["Sai Ashish Somayajula","Youwei Liang","Abhishek Singh","Li Zhang","Pengtao Xie"],"pdf_url":"https://arxiv.org/pdf/2403.12918v1.pdf","comment":"Accepted as a long paper to NAACL 2024 Main Conference; 18 pages, 11\n  tables, 3 figures"},{"id":"http://arxiv.org/abs/2106.16239v8","updated":"2024-03-19T17:11:45Z","published":"2021-06-30T17:49:55Z","title":"Fixed points of nonnegative neural networks","summary":"  We use fixed point theory to analyze nonnegative neural networks, which we\ndefine as neural networks that map nonnegative vectors to nonnegative vectors.\nWe first show that nonnegative neural networks with nonnegative weights and\nbiases can be recognized as monotonic and (weakly) scalable mappings within the\nframework of nonlinear Perron-Frobenius theory. This fact enables us to provide\nconditions for the existence of fixed points of nonnegative neural networks\nhaving inputs and outputs of the same dimension, and these conditions are\nweaker than those recently obtained using arguments in convex analysis.\nFurthermore, we prove that the shape of the fixed point set of nonnegative\nneural networks with nonnegative weights and biases is an interval, which under\nmild conditions degenerates to a point. These results are then used to obtain\nthe existence of fixed points of more general nonnegative neural networks. From\na practical perspective, our results contribute to the understanding of the\nbehavior of autoencoders, and we also offer valuable mathematical machinery for\nfuture developments in deep equilibrium models.\n","authors":["Tomasz J. Piotrowski","Renato L. G. Cavalcante","Mateusz Gabor"],"pdf_url":"https://arxiv.org/pdf/2106.16239v8.pdf","comment":"39 pages"},{"id":"http://arxiv.org/abs/2403.12910v1","updated":"2024-03-19T17:08:24Z","published":"2024-03-19T17:08:24Z","title":"Yell At Your Robot: Improving On-the-Fly from Language Corrections","summary":"  Hierarchical policies that combine language and low-level control have been\nshown to perform impressively long-horizon robotic tasks, by leveraging either\nzero-shot high-level planners like pretrained language and vision-language\nmodels (LLMs/VLMs) or models trained on annotated robotic demonstrations.\nHowever, for complex and dexterous skills, attaining high success rates on\nlong-horizon tasks still represents a major challenge -- the longer the task\nis, the more likely it is that some stage will fail. Can humans help the robot\nto continuously improve its long-horizon task performance through intuitive and\nnatural feedback? In this paper, we make the following observation: high-level\npolicies that index into sufficiently rich and expressive low-level\nlanguage-conditioned skills can be readily supervised with human feedback in\nthe form of language corrections. We show that even fine-grained corrections,\nsuch as small movements (\"move a bit to the left\"), can be effectively\nincorporated into high-level policies, and that such corrections can be readily\nobtained from humans observing the robot and making occasional suggestions.\nThis framework enables robots not only to rapidly adapt to real-time language\nfeedback, but also incorporate this feedback into an iterative training scheme\nthat improves the high-level policy's ability to correct errors in both\nlow-level execution and high-level decision-making purely from verbal feedback.\nOur evaluation on real hardware shows that this leads to significant\nperformance improvement in long-horizon, dexterous manipulation tasks without\nthe need for any additional teleoperation. Videos and code are available at\nhttps://yay-robot.github.io/.\n","authors":["Lucy Xiaoyang Shi","Zheyuan Hu","Tony Z. Zhao","Archit Sharma","Karl Pertsch","Jianlan Luo","Sergey Levine","Chelsea Finn"],"pdf_url":"https://arxiv.org/pdf/2403.12910v1.pdf","comment":"Project website: https://yay-robot.github.io/"},{"id":"http://arxiv.org/abs/2209.00626v6","updated":"2024-03-19T17:07:47Z","published":"2022-08-30T02:12:47Z","title":"The Alignment Problem from a Deep Learning Perspective","summary":"  In coming years or decades, artificial general intelligence (AGI) may surpass\nhuman capabilities at many critical tasks. We argue that, without substantial\neffort to prevent it, AGIs could learn to pursue goals that are in conflict\n(i.e. misaligned) with human interests. If trained like today's most capable\nmodels, AGIs could learn to act deceptively to receive higher reward, learn\nmisaligned internally-represented goals which generalize beyond their\nfine-tuning distributions, and pursue those goals using power-seeking\nstrategies. We review emerging evidence for these properties. AGIs with these\nproperties would be difficult to align and may appear aligned even when they\nare not. Finally, we briefly outline how the deployment of misaligned AGIs\nmight irreversibly undermine human control over the world, and we review\nresearch directions aimed at preventing this outcome.\n","authors":["Richard Ngo","Lawrence Chan","Sören Mindermann"],"pdf_url":"https://arxiv.org/pdf/2209.00626v6.pdf","comment":"Published in ICLR 2024"},{"id":"http://arxiv.org/abs/2401.07931v2","updated":"2024-03-19T17:07:40Z","published":"2024-01-15T19:47:14Z","title":"Vertical Federated Image Segmentation","summary":"  With the popularization of AI solutions for image based problems, there has\nbeen a growing concern for both data privacy and acquisition. In a large number\nof cases, information is located on separate data silos and it can be difficult\nfor a developer to consolidate all of it in a fashion that is appropriate for\nmachine learning model development. Alongside this, a portion of these\nlocalized data regions may not have access to a labelled ground truth. This\nindicates that they have the capacity to reach conclusions numerically, but are\nnot able to assign classifications amid a lack of pertinent information. Such a\ndetermination is often negligible, especially when attempting to develop image\nbased solutions that often necessitate this capability. With this being the\ncase, we propose an innovative vertical federated learning (VFL) model\narchitecture that can operate under this common set of conditions. This is the\nfirst (and currently the only) implementation of a system that can work under\nthe constraints of a VFL environment and perform image segmentation while\nmaintaining nominal accuracies. We achieved this by utilizing an FCN that\nboasts the ability to operate on federates that lack labelled data and\nprivately share the respective weights with a central server, that of which\nhosts the necessary features for classification. Tests were conducted on the\nCamVid dataset in order to determine the impact of heavy feature compression\nrequired for the transfer of information between federates, as well as to reach\nnominal conclusions about the overall performance metrics when working under\nsuch constraints.\n","authors":["Paul K. Mandal","Cole Leo"],"pdf_url":"https://arxiv.org/pdf/2401.07931v2.pdf","comment":"11 pages, 5 figures"},{"id":"http://arxiv.org/abs/2403.12007v2","updated":"2024-03-19T16:55:05Z","published":"2024-03-18T17:43:40Z","title":"Defining Effective Engagement For Enhancing Cancer Patients' Well-being\n  with Mobile Digital Behavior Change Interventions","summary":"  Digital Behavior Change Interventions (DBCIs) are supporting development of\nnew health behaviors. Evaluating their effectiveness is crucial for their\nimprovement and understanding of success factors. However, comprehensive\nguidance for developers, particularly in small-scale studies with ethical\nconstraints, is limited. Building on the CAPABLE project, this study aims to\ndefine effective engagement with DBCIs for supporting cancer patients in\nenhancing their quality of life. We identify metrics for measuring engagement,\nexplore the interest of both patients and clinicians in DBCIs, and propose\nhypotheses for assessing the impact of DBCIs in such contexts. Our findings\nsuggest that clinician prescriptions significantly increase sustained\nengagement with mobile DBCIs. In addition, while one weekly engagement with a\nDBCI is sufficient to maintain well-being, transitioning from extrinsic to\nintrinsic motivation may require a higher level of engagement.\n","authors":["Aneta Lisowska","Szymon Wilk","Laura Locati","Mimma Rizzo","Lucia Sacchi","Silvana Quaglini","Matteo Terzaghi","Valentina Tibollo","Mor Peleg"],"pdf_url":"https://arxiv.org/pdf/2403.12007v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12900v1","updated":"2024-03-19T16:53:53Z","published":"2024-03-19T16:53:53Z","title":"Toward Sustainable GenAI using Generation Directives for Carbon-Friendly\n  Large Language Model Inference","summary":"  The rapid advancement of Generative Artificial Intelligence (GenAI) across\ndiverse sectors raises significant environmental concerns, notably the carbon\nemissions from their cloud and high performance computing (HPC) infrastructure.\nThis paper presents Sprout, an innovative framework designed to address these\nconcerns by reducing the carbon footprint of generative Large Language Model\n(LLM) inference services. Sprout leverages the innovative concept of\n\"generation directives\" to guide the autoregressive generation process, thereby\nenhancing carbon efficiency. Our proposed method meticulously balances the need\nfor ecological sustainability with the demand for high-quality generation\noutcomes. Employing a directive optimizer for the strategic assignment of\ngeneration directives to user prompts and an original offline quality\nevaluator, Sprout demonstrates a significant reduction in carbon emissions by\nover 40% in real-world evaluations using the Llama2 LLM and global electricity\ngrid data. This research marks a critical step toward aligning AI technology\nwith sustainable practices, highlighting the potential for mitigating\nenvironmental impacts in the rapidly expanding domain of generative artificial\nintelligence.\n","authors":["Baolin Li","Yankai Jiang","Vijay Gadepally","Devesh Tiwari"],"pdf_url":"https://arxiv.org/pdf/2403.12900v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.04720v2","updated":"2024-03-19T16:47:44Z","published":"2024-03-07T18:16:29Z","title":"Rethinking of Encoder-based Warm-start Methods in Hyperparameter\n  Optimization","summary":"  Effectively representing heterogeneous tabular datasets for meta-learning\nremains an open problem. Previous approaches rely on predefined meta-features,\nfor example, statistical measures or landmarkers. Encoder-based models, such as\nDataset2Vec, allow us to extract significant meta-features automatically\nwithout human intervention. This research introduces a novel encoder-based\nrepresentation of tabular datasets implemented within the liltab package\navailable on GitHub https://github.com/azoz01/liltab. Our package is based on\nan established model for heterogeneous tabular data proposed in [Tomoharu Iwata\nand Atsutoshi Kumagai. Meta-learning from Tasks with Heterogeneous Attribute\nSpaces. In Advances in Neural Information Processing Systems, 2020]. The\nproposed approach employs a different model for encoding feature relationships,\ngenerating alternative representations compared to existing methods like\nDataset2Vec. Both of them leverage the fundamental assumption of dataset\nsimilarity learning. In this work, we evaluate Dataset2Vec and liltab on two\ncommon meta-tasks - representing entire datasets and hyperparameter\noptimization warm-start. However, validation on an independent metaMIMIC\ndataset highlights the nuanced challenges in representation learning. We show\nthat general representations may not suffice for some meta-tasks where\nrequirements are not explicitly considered during extraction.\n","authors":["Dawid Płudowski","Antoni Zajko","Anna Kozak","Katarzyna Woźnica"],"pdf_url":"https://arxiv.org/pdf/2403.04720v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.15771v3","updated":"2024-03-19T16:46:42Z","published":"2024-01-28T21:19:15Z","title":"Bayesian Nonparametrics Meets Data-Driven Robust Optimization","summary":"  Training machine learning and statistical models often involves optimizing a\ndata-driven risk criterion. The risk is usually computed with respect to the\nempirical data distribution, but this may result in poor and unstable\nout-of-sample performance due to distributional uncertainty. In the spirit of\ndistributionally robust optimization, we propose a novel robust criterion by\ncombining insights from Bayesian nonparametric (i.e., Dirichlet Process) theory\nand recent decision-theoretic models of smooth ambiguity-averse preferences.\nFirst, we highlight novel connections with standard regularized empirical risk\nminimization techniques, among which Ridge and LASSO regressions. Then, we\ntheoretically demonstrate the existence of favorable finite-sample and\nasymptotic statistical guarantees on the performance of the robust optimization\nprocedure. For practical implementation, we propose and study tractable\napproximations of the criterion based on well-known Dirichlet Process\nrepresentations. We also show that the smoothness of the criterion naturally\nleads to standard gradient-based numerical optimization. Finally, we provide\ninsights into the workings of our method by applying it to high-dimensional\nsparse linear regression, binary classification, and robust location parameter\nestimation tasks.\n","authors":["Nicola Bariletto","Nhat Ho"],"pdf_url":"https://arxiv.org/pdf/2401.15771v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.10109v2","updated":"2024-03-19T16:43:09Z","published":"2024-02-15T17:05:48Z","title":"Towards Reducing Diagnostic Errors with Interpretable Risk Prediction","summary":"  Many diagnostic errors occur because clinicians cannot easily access relevant\ninformation in patient Electronic Health Records (EHRs). In this work we\npropose a method to use LLMs to identify pieces of evidence in patient EHR data\nthat indicate increased or decreased risk of specific diagnoses; our ultimate\naim is to increase access to evidence and reduce diagnostic errors. In\nparticular, we propose a Neural Additive Model to make predictions backed by\nevidence with individualized risk estimates at time-points where clinicians are\nstill uncertain, aiming to specifically mitigate delays in diagnosis and errors\nstemming from an incomplete differential. To train such a model, it is\nnecessary to infer temporally fine-grained retrospective labels of eventual\n\"true\" diagnoses. We do so with LLMs, to ensure that the input text is from\nbefore a confident diagnosis can be made. We use an LLM to retrieve an initial\npool of evidence, but then refine this set of evidence according to\ncorrelations learned by the model. We conduct an in-depth evaluation of the\nusefulness of our approach by simulating how it might be used by a clinician to\ndecide between a pre-defined list of differential diagnoses.\n","authors":["Denis Jered McInerney","William Dickinson","Lucy C. Flynn","Andrea C. Young","Geoffrey S. Young","Jan-Willem van de Meent","Byron C. Wallace"],"pdf_url":"https://arxiv.org/pdf/2402.10109v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.04906v2","updated":"2024-03-19T16:42:48Z","published":"2024-02-07T14:35:25Z","title":"Conformal Monte Carlo Meta-learners for Predictive Inference of\n  Individual Treatment Effects","summary":"  Knowledge of the effect of interventions, called the treatment effect, is\nparamount for decision-making. Approaches to estimating this treatment effect,\ne.g. by using Conditional Average Treatment Effect (CATE) estimators, often\nonly provide a point estimate of this treatment effect, while additional\nuncertainty quantification is frequently desired instead. Therefore, we present\na novel method, the Conformal Monte Carlo (CMC) meta-learners, leveraging\nconformal predictive systems, Monte Carlo sampling, and CATE meta-learners, to\ninstead produce a predictive distribution usable in individualized\ndecision-making. Furthermore, we show how specific assumptions on the noise\ndistribution of the outcome heavily affect these uncertainty predictions.\nNonetheless, the CMC framework shows strong experimental coverage while\nretaining small interval widths to provide estimates of the true individual\ntreatment effect.\n","authors":["Jef Jonkers","Jarne Verhaeghe","Glenn Van Wallendael","Luc Duchateau","Sofie Van Hoecke"],"pdf_url":"https://arxiv.org/pdf/2402.04906v2.pdf","comment":"21 pages, 8 figures"},{"id":"http://arxiv.org/abs/2306.05109v4","updated":"2024-03-19T16:39:03Z","published":"2023-06-08T11:16:20Z","title":"Yet Another ICU Benchmark: A Flexible Multi-Center Framework for\n  Clinical ML","summary":"  Medical applications of machine learning (ML) have experienced a surge in\npopularity in recent years. The intensive care unit (ICU) is a natural habitat\nfor ML given the abundance of available data from electronic health records.\nModels have been proposed to address numerous ICU prediction tasks like the\nearly detection of complications. While authors frequently report\nstate-of-the-art performance, it is challenging to verify claims of\nsuperiority. Datasets and code are not always published, and cohort\ndefinitions, preprocessing pipelines, and training setups are difficult to\nreproduce. This work introduces Yet Another ICU Benchmark (YAIB), a modular\nframework that allows researchers to define reproducible and comparable\nclinical ML experiments; we offer an end-to-end solution from cohort definition\nto model evaluation. The framework natively supports most open-access ICU\ndatasets (MIMIC III/IV, eICU, HiRID, AUMCdb) and is easily adaptable to future\nICU datasets. Combined with a transparent preprocessing pipeline and extensible\ntraining code for multiple ML and deep learning models, YAIB enables unified\nmodel development. Our benchmark comes with five predefined established\nprediction tasks (mortality, acute kidney injury, sepsis, kidney function, and\nlength of stay) developed in collaboration with clinicians. Adding further\ntasks is straightforward by design. Using YAIB, we demonstrate that the choice\nof dataset, cohort definition, and preprocessing have a major impact on the\nprediction performance - often more so than model class - indicating an urgent\nneed for YAIB as a holistic benchmarking tool. We provide our work to the\nclinical ML community to accelerate method development and enable real-world\nclinical implementations. Software Repository:\nhttps://github.com/rvandewater/YAIB.\n","authors":["Robin van de Water","Hendrik Schmidt","Paul Elbers","Patrick Thoral","Bert Arnrich","Patrick Rockenschaub"],"pdf_url":"https://arxiv.org/pdf/2306.05109v4.pdf","comment":"Main benchmark: https://github.com/rvandewater/YAIB, Cohort\n  generation: https://github.com/rvandewater/YAIB-cohorts, Models:\n  https://github.com/rvandewater/YAIB-models. To be published in ICLR 2024\n  proceedings"},{"id":"http://arxiv.org/abs/2403.09611v2","updated":"2024-03-19T16:37:13Z","published":"2024-03-14T17:51:32Z","title":"MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training","summary":"  In this work, we discuss building performant Multimodal Large Language Models\n(MLLMs). In particular, we study the importance of various architecture\ncomponents and data choices. Through careful and comprehensive ablations of the\nimage encoder, the vision language connector, and various pre-training data\nchoices, we identified several crucial design lessons. For example, we\ndemonstrate that for large-scale multimodal pre-training using a careful mix of\nimage-caption, interleaved image-text, and text-only data is crucial for\nachieving state-of-the-art (SOTA) few-shot results across multiple benchmarks,\ncompared to other published pre-training results. Further, we show that the\nimage encoder together with image resolution and the image token count has\nsubstantial impact, while the vision-language connector design is of\ncomparatively negligible importance. By scaling up the presented recipe, we\nbuild MM1, a family of multimodal models up to 30B parameters, including both\ndense models and mixture-of-experts (MoE) variants, that are SOTA in\npre-training metrics and achieve competitive performance after supervised\nfine-tuning on a range of established multimodal benchmarks. Thanks to\nlarge-scale pre-training, MM1 enjoys appealing properties such as enhanced\nin-context learning, and multi-image reasoning, enabling few-shot\nchain-of-thought prompting.\n","authors":["Brandon McKinzie","Zhe Gan","Jean-Philippe Fauconnier","Sam Dodge","Bowen Zhang","Philipp Dufter","Dhruti Shah","Xianzhi Du","Futang Peng","Floris Weers","Anton Belyi","Haotian Zhang","Karanjeet Singh","Doug Kang","Ankur Jain","Hongyu Hè","Max Schwarzer","Tom Gunter","Xiang Kong","Aonan Zhang","Jianyu Wang","Chong Wang","Nan Du","Tao Lei","Sam Wiseman","Mark Lee","Zirui Wang","Ruoming Pang","Peter Grasch","Alexander Toshev","Yinfei Yang"],"pdf_url":"https://arxiv.org/pdf/2403.09611v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12887v1","updated":"2024-03-19T16:34:31Z","published":"2024-03-19T16:34:31Z","title":"Understanding the training of infinitely deep and wide ResNets with\n  Conditional Optimal Transport","summary":"  We study the convergence of gradient flow for the training of deep neural\nnetworks. If Residual Neural Networks are a popular example of very deep\narchitectures, their training constitutes a challenging optimization problem\ndue notably to the non-convexity and the non-coercivity of the objective. Yet,\nin applications, those tasks are successfully solved by simple optimization\nalgorithms such as gradient descent. To better understand this phenomenon, we\nfocus here on a ``mean-field'' model of infinitely deep and arbitrarily wide\nResNet, parameterized by probability measures over the product set of layers\nand parameters and with constant marginal on the set of layers. Indeed, in the\ncase of shallow neural networks, mean field models have proven to benefit from\nsimplified loss-landscapes and good theoretical guarantees when trained with\ngradient flow for the Wasserstein metric on the set of probability measures.\nMotivated by this approach, we propose to train our model with gradient flow\nw.r.t. the conditional Optimal Transport distance: a restriction of the\nclassical Wasserstein distance which enforces our marginal condition. Relying\non the theory of gradient flows in metric spaces we first show the\nwell-posedness of the gradient flow equation and its consistency with the\ntraining of ResNets at finite width. Performing a local Polyak-\\L{}ojasiewicz\nanalysis, we then show convergence of the gradient flow for well-chosen\ninitializations: if the number of features is finite but sufficiently large and\nthe risk is sufficiently small at initialization, the gradient flow converges\ntowards a global minimizer. This is the first result of this type for\ninfinitely deep and arbitrarily wide ResNets.\n","authors":["Raphaël Barboni","Gabriel Peyré","François-Xavier Vialard"],"pdf_url":"https://arxiv.org/pdf/2403.12887v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.03094v2","updated":"2024-03-19T16:34:28Z","published":"2024-02-05T15:25:32Z","title":"Cross-Domain Few-Shot Object Detection via Enhanced Open-Set Object\n  Detector","summary":"  This paper studies the challenging cross-domain few-shot object detection\n(CD-FSOD), aiming to develop an accurate object detector for novel domains with\nminimal labeled examples. While transformer-based open-set detectors, such as\nDE-ViT, show promise in traditional few-shot object detection, their\ngeneralization to CD-FSOD remains unclear: 1) can such open-set detection\nmethods easily generalize to CD-FSOD? 2) If not, how can models be enhanced\nwhen facing huge domain gaps? To answer the first question, we employ measures\nincluding style, inter-class variance (ICV), and indefinable boundaries (IB) to\nunderstand the domain gap. Based on these measures, we establish a new\nbenchmark named CD-FSOD to evaluate object detection methods, revealing that\nmost of the current approaches fail to generalize across domains. Technically,\nwe observe that the performance decline is associated with our proposed\nmeasures: style, ICV, and IB. Consequently, we propose several novel modules to\naddress these issues. First, the learnable instance features align initial\nfixed instances with target categories, enhancing feature distinctiveness.\nSecond, the instance reweighting module assigns higher importance to\nhigh-quality instances with slight IB. Third, the domain prompter encourages\nfeatures resilient to different styles by synthesizing imaginary domains\nwithout altering semantic contents. These techniques collectively contribute to\nthe development of the Cross-Domain Vision Transformer for CD-FSOD (CD-ViTO),\nsignificantly improving upon the base DE-ViT. Experimental results validate the\nefficacy of our model. All datasets, codes, and models will be released to the\ncommunity.\n","authors":["Yuqian Fu","Yu Wang","Yixuan Pan","Lian Huai","Xingyu Qiu","Zeyu Shangguan","Tong Liu","Yanwei Fu","Luc Van Gool","Xingqun Jiang"],"pdf_url":"https://arxiv.org/pdf/2402.03094v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.09283v2","updated":"2024-03-19T16:23:20Z","published":"2024-02-14T16:14:03Z","title":"Attacks, Defenses and Evaluations for LLM Conversation Safety: A Survey","summary":"  Large Language Models (LLMs) are now commonplace in conversation\napplications. However, their risks of misuse for generating harmful responses\nhave raised serious societal concerns and spurred recent research on LLM\nconversation safety. Therefore, in this survey, we provide a comprehensive\noverview of recent studies, covering three critical aspects of LLM conversation\nsafety: attacks, defenses, and evaluations. Our goal is to provide a structured\nsummary that enhances understanding of LLM conversation safety and encourages\nfurther investigation into this important subject. For easy reference, we have\ncategorized all the studies mentioned in this survey according to our taxonomy,\navailable at: https://github.com/niconi19/LLM-conversation-safety.\n","authors":["Zhichen Dong","Zhanhui Zhou","Chao Yang","Jing Shao","Yu Qiao"],"pdf_url":"https://arxiv.org/pdf/2402.09283v2.pdf","comment":"Accepted to NAACL 2024"},{"id":"http://arxiv.org/abs/2403.11366v2","updated":"2024-03-19T16:19:49Z","published":"2024-03-17T23:02:04Z","title":"JORA: JAX Tensor-Parallel LoRA Library for Retrieval Augmented\n  Fine-Tuning","summary":"  The scaling of Large Language Models (LLMs) for retrieval-based tasks,\nparticularly in Retrieval Augmented Generation (RAG), faces significant memory\nconstraints, especially when fine-tuning extensive prompt sequences. Current\nopen-source libraries support full-model inference and fine-tuning across\nmultiple GPUs but fall short of accommodating the efficient parameter\ndistribution required for retrieved context. Addressing this gap, we introduce\na novel framework for PEFT-compatible fine-tuning of Llama-2 models, leveraging\ndistributed training. Our framework uniquely utilizes JAX's just-in-time (JIT)\ncompilation and tensor-sharding for efficient resource management, thereby\nenabling accelerated fine-tuning with reduced memory requirements. This\nadvancement significantly improves the scalability and feasibility of\nfine-tuning LLMs for complex RAG applications, even on systems with limited GPU\nresources. Our experiments show more than 12x improvement in runtime compared\nto Hugging Face/DeepSpeed implementation with four GPUs while consuming less\nthan half the VRAM per GPU.\n","authors":["Anique Tahir","Lu Cheng","Huan Liu"],"pdf_url":"https://arxiv.org/pdf/2403.11366v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12873v1","updated":"2024-03-19T16:17:21Z","published":"2024-03-19T16:17:21Z","title":"Short-Term Solar Irradiance Forecasting Under Data Transmission\n  Constraints","summary":"  We report a data-parsimonious machine learning model for short-term\nforecasting of solar irradiance. The model inputs include sky camera images\nthat are reduced to scalar features to meet data transmission constraints. The\noutput irradiance values are transformed to focus on unknown short-term\ndynamics. Inspired by control theory, a noise input is used to reflect\nunmeasured variables and is shown to improve model predictions, often\nconsiderably. Five years of data from the NREL Solar Radiation Research\nLaboratory were used to create three rolling train-validate sets and determine\nthe best representations for time, the optimal span of input measurements, and\nthe most impactful model input data (features). For the chosen test data, the\nmodel achieves a mean absolute error of 74.34 $W/m^2$ compared to a baseline\n134.35 $W/m^2$ using the persistence of cloudiness model.\n","authors":["Joshua Edward Hammond","Ricardo A. Lara Orozco","Michael Baldea","Brian A. Korgel"],"pdf_url":"https://arxiv.org/pdf/2403.12873v1.pdf","comment":"21 pages, 12 figures"},{"id":"http://arxiv.org/abs/2402.09450v3","updated":"2024-03-19T16:17:00Z","published":"2024-02-02T10:04:13Z","title":"Guiding Masked Representation Learning to Capture Spatio-Temporal\n  Relationship of Electrocardiogram","summary":"  Electrocardiograms (ECG) are widely employed as a diagnostic tool for\nmonitoring electrical signals originating from a heart. Recent machine learning\nresearch efforts have focused on the application of screening various diseases\nusing ECG signals. However, adapting to the application of screening disease is\nchallenging in that labeled ECG data are limited. Achieving general\nrepresentation through self-supervised learning (SSL) is a well-known approach\nto overcome the scarcity of labeled data; however, a naive application of SSL\nto ECG data, without considering the spatial-temporal relationships inherent in\nECG signals, may yield suboptimal results. In this paper, we introduce ST-MEM\n(Spatio-Temporal Masked Electrocardiogram Modeling), designed to learn\nspatio-temporal features by reconstructing masked 12-lead ECG data. ST-MEM\noutperforms other SSL baseline methods in various experimental settings for\narrhythmia classification tasks. Moreover, we demonstrate that ST-MEM is\nadaptable to various lead combinations. Through quantitative and qualitative\nanalysis, we show a spatio-temporal relationship within ECG data. Our code is\navailable at https://github.com/bakqui/ST-MEM.\n","authors":["Yeongyeon Na","Minje Park","Yunwon Tae","Sunghoon Joo"],"pdf_url":"https://arxiv.org/pdf/2402.09450v3.pdf","comment":"ICLR 2024. The first three authors contribute equally"},{"id":"http://arxiv.org/abs/2306.06192v4","updated":"2024-03-19T16:16:16Z","published":"2023-06-09T18:45:15Z","title":"Ada-NAV: Adaptive Trajectory Length-Based Sample Efficient Policy\n  Learning for Robotic Navigation","summary":"  Trajectory length stands as a crucial hyperparameter within reinforcement\nlearning (RL) algorithms, significantly contributing to the sample inefficiency\nin robotics applications. Motivated by the pivotal role trajectory length plays\nin the training process, we introduce Ada-NAV, a novel adaptive trajectory\nlength scheme designed to enhance the training sample efficiency of RL\nalgorithms in robotic navigation tasks. Unlike traditional approaches that\ntreat trajectory length as a fixed hyperparameter, we propose to dynamically\nadjust it based on the entropy of the underlying navigation policy.\nInterestingly, Ada-NAV can be applied to both existing on-policy and off-policy\nRL methods, which we demonstrate by empirically validating its efficacy on\nthree popular RL methods: REINFORCE, Proximal Policy Optimization (PPO), and\nSoft Actor-Critic (SAC). We demonstrate through simulated and real-world\nrobotic experiments that Ada-NAV outperforms conventional methods that employ\nconstant or randomly sampled trajectory lengths. Specifically, for a fixed\nsample budget, Ada-NAV achieves an 18\\% increase in navigation success rate, a\n20-38\\% reduction in navigation path length, and a 9.32\\% decrease in elevation\ncosts. Furthermore, we showcase the versatility of Ada-NAV by integrating it\nwith the Clearpath Husky robot, illustrating its applicability in complex\noutdoor environments.\n","authors":["Bhrij Patel","Kasun Weerakoon","Wesley A. Suttle","Alec Koppel","Brian M. Sadler","Tianyi Zhou","Amrit Singh Bedi","Dinesh Manocha"],"pdf_url":"https://arxiv.org/pdf/2306.06192v4.pdf","comment":"11 pages, 9 figures, 2 tables"},{"id":"http://arxiv.org/abs/2403.12871v1","updated":"2024-03-19T16:15:44Z","published":"2024-03-19T16:15:44Z","title":"Wildfire danger prediction optimization with transfer learning","summary":"  Convolutional Neural Networks (CNNs) have proven instrumental across various\ncomputer science domains, enabling advancements in object detection,\nclassification, and anomaly detection. This paper explores the application of\nCNNs to analyze geospatial data specifically for identifying wildfire-affected\nareas. Leveraging transfer learning techniques, we fine-tuned CNN\nhyperparameters and integrated the Canadian Fire Weather Index (FWI) to assess\nmoisture conditions. The study establishes a methodology for computing wildfire\nrisk levels on a scale of 0 to 5, dynamically linked to weather patterns.\nNotably, through the integration of transfer learning, the CNN model achieved\nan impressive accuracy of 95\\% in identifying burnt areas. This research sheds\nlight on the inner workings of CNNs and their practical, real-time utility in\npredicting and mitigating wildfires. By combining transfer learning and CNNs,\nthis study contributes a robust approach to assess burnt areas, facilitating\ntimely interventions and preventative measures against conflagrations.\n","authors":["Spiros Maggioros","Nikos Tsalkitzis"],"pdf_url":"https://arxiv.org/pdf/2403.12871v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.16635v2","updated":"2024-03-19T16:14:04Z","published":"2023-05-26T05:19:24Z","title":"Impossible Distillation: from Low-Quality Model to High-Quality Dataset\n  & Model for Summarization and Paraphrasing","summary":"  We present Impossible Distillation, a novel framework for paraphrasing and\nsentence summarization, that distills a high-quality dataset and model from a\nlow-quality teacher that itself cannot perform these tasks. Unlike prior works\nthat rely on an extreme-scale teacher model (e.g., GPT3) or task-specific\narchitecture, we hypothesize and verify the paraphrastic proximity intrinsic to\npre-trained LMs (e.g., GPT2), where paraphrases occupy a proximal subspace in\nthe LM distribution. By identifying and distilling generations from these\nsubspaces, Impossible Distillation produces a high-quality dataset and model\neven from GPT2-scale LMs. We evaluate our method on multiple benchmarks\nspanning unconstrained / syntax-controlled paraphrase generation and sentence\nsummarization. Our model with 770M parameters consistently outperforms strong\nbaselines, including models distilled from ChatGPT, and sometimes, even ChatGPT\nitself. Also, we find that our distilled dataset from 1.5B LMs exhibits higher\ndiversity and fidelity than up to 13 times larger datasets.\n","authors":["Jaehun Jung","Peter West","Liwei Jiang","Faeze Brahman","Ximing Lu","Jillian Fisher","Taylor Sorensen","Yejin Choi"],"pdf_url":"https://arxiv.org/pdf/2305.16635v2.pdf","comment":"NAACL 2024"},{"id":"http://arxiv.org/abs/2312.09778v2","updated":"2024-03-19T16:09:23Z","published":"2023-12-15T13:30:04Z","title":"Hypergraph-MLP: Learning on Hypergraphs without Message Passing","summary":"  Hypergraphs are vital in modelling data with higher-order relations\ncontaining more than two entities, gaining prominence in machine learning and\nsignal processing. Many hypergraph neural networks leverage message passing\nover hypergraph structures to enhance node representation learning, yielding\nimpressive performances in tasks like hypergraph node classification. However,\nthese message-passing-based models face several challenges, including\noversmoothing as well as high latency and sensitivity to structural\nperturbations at inference time. To tackle those challenges, we propose an\nalternative approach where we integrate the information about hypergraph\nstructures into training supervision without explicit message passing, thus\nalso removing the reliance on it at inference. Specifically, we introduce\nHypergraph-MLP, a novel learning framework for hypergraph-structured data,\nwhere the learning model is a straightforward multilayer perceptron (MLP)\nsupervised by a loss function based on a notion of signal smoothness on\nhypergraphs. Experiments on hypergraph node classification tasks demonstrate\nthat Hypergraph-MLP achieves competitive performance compared to existing\nbaselines, and is considerably faster and more robust against structural\nperturbations at inference.\n","authors":["Bohan Tang","Siheng Chen","Xiaowen Dong"],"pdf_url":"https://arxiv.org/pdf/2312.09778v2.pdf","comment":"Accepted by ICASSP 2024"},{"id":"http://arxiv.org/abs/2403.12864v1","updated":"2024-03-19T16:08:27Z","published":"2024-03-19T16:08:27Z","title":"A Comparison of Deep Learning Architectures for Spacecraft Anomaly\n  Detection","summary":"  Spacecraft operations are highly critical, demanding impeccable reliability\nand safety. Ensuring the optimal performance of a spacecraft requires the early\ndetection and mitigation of anomalies, which could otherwise result in unit or\nmission failures. With the advent of deep learning, a surge of interest has\nbeen seen in leveraging these sophisticated algorithms for anomaly detection in\nspace operations. This study aims to compare the efficacy of various deep\nlearning architectures in detecting anomalies in spacecraft data. The deep\nlearning models under investigation include Convolutional Neural Networks\n(CNNs), Recurrent Neural Networks (RNNs), Long Short-Term Memory (LSTM)\nnetworks, and Transformer-based architectures. Each of these models was trained\nand validated using a comprehensive dataset sourced from multiple spacecraft\nmissions, encompassing diverse operational scenarios and anomaly types. Initial\nresults indicate that while CNNs excel in identifying spatial patterns and may\nbe effective for some classes of spacecraft data, LSTMs and RNNs show a marked\nproficiency in capturing temporal anomalies seen in time-series spacecraft\ntelemetry. The Transformer-based architectures, given their ability to focus on\nboth local and global contexts, have showcased promising results, especially in\nscenarios where anomalies are subtle and span over longer durations.\nAdditionally, considerations such as computational efficiency, ease of\ndeployment, and real-time processing capabilities were evaluated. While CNNs\nand LSTMs demonstrated a balance between accuracy and computational demands,\nTransformer architectures, though highly accurate, require significant\ncomputational resources. In conclusion, the choice of deep learning\narchitecture for spacecraft anomaly detection is highly contingent on the\nnature of the data, the type of anomalies, and operational constraints.\n","authors":["Daniel Lakey","Tim Schlippe"],"pdf_url":"https://arxiv.org/pdf/2403.12864v1.pdf","comment":"accepted for IEEE Aeroconf 2024"},{"id":"http://arxiv.org/abs/2301.00016v2","updated":"2024-03-19T16:07:14Z","published":"2022-12-30T18:08:48Z","title":"Behave-XAI: Deep Explainable Learning of Behavioral Representational\n  Data","summary":"  According to the latest trend of artificial intelligence, AI-systems needs to\nclarify regarding general,specific decisions,services provided by it. Only\nconsumer is satisfied, with explanation , for example, why any classification\nresult is the outcome of any given time. This actually motivates us using\nexplainable or human understandable AI for a behavioral mining scenario, where\nusers engagement on digital platform is determined from context, such as\nemotion, activity, weather, etc. However, the output of AI-system is not always\nsystematically correct, and often systematically correct, but apparently\nnot-perfect and thereby creating confusions, such as, why the decision is\ngiven? What is the reason underneath? In this context, we first formulate the\nbehavioral mining problem in deep convolutional neural network architecture.\nEventually, we apply a recursive neural network due to the presence of\ntime-series data from users physiological and environmental sensor-readings.\nOnce the model is developed, explanations are presented with the advent of XAI\nmodels in front of users. This critical step involves extensive trial with\nusers preference on explanations over conventional AI, judgement of credibility\nof explanation.\n","authors":["Rossi Kamal","Zuzana Kubincova"],"pdf_url":"https://arxiv.org/pdf/2301.00016v2.pdf","comment":"This submission has been withdrawn by arXiv administrators as the\n  second author was added without their knowledge or consent"},{"id":"http://arxiv.org/abs/2301.00693v2","updated":"2024-03-19T16:06:19Z","published":"2022-12-30T10:35:25Z","title":"Deep Recurrent Learning Through Long Short Term Memory and TOPSIS","summary":"  Enterprise resource planning (ERP) software brings resources, data together\nto keep software-flow within business processes in a company. However, cloud\ncomputing's cheap, easy and quick management promise pushes business-owners for\na transition from monolithic to a data-center/cloud based ERP. Since cloud-ERP\ndevelopment involves a cyclic process, namely planning, implementing, testing\nand upgrading, its adoption is realized as a deep recurrent neural network\nproblem. Eventually, a classification algorithm based on long short term memory\n(LSTM) and TOPSIS is proposed to identify and rank, respectively, adoption\nfeatures. Our theoretical model is validated over a reference model by\narticulating key players, services, architecture, functionalities. Qualitative\nsurvey is conducted among users by considering technology, innovation and\nresistance issues, to formulate hypotheses on key adoption factors.\n","authors":["Rossi Kamal","Zuzana Kubincova","Mosaddek Hossain Kamal","Upama Kabir"],"pdf_url":"https://arxiv.org/pdf/2301.00693v2.pdf","comment":"This submission has been withdrawn by arXiv administrators as the\n  second author was added without their knowledge or consent"},{"id":"http://arxiv.org/abs/2403.12861v1","updated":"2024-03-19T16:05:51Z","published":"2024-03-19T16:05:51Z","title":"D-Cubed: Latent Diffusion Trajectory Optimisation for Dexterous\n  Deformable Manipulation","summary":"  Mastering dexterous robotic manipulation of deformable objects is vital for\novercoming the limitations of parallel grippers in real-world applications.\nCurrent trajectory optimisation approaches often struggle to solve such tasks\ndue to the large search space and the limited task information available from a\ncost function. In this work, we propose D-Cubed, a novel trajectory\noptimisation method using a latent diffusion model (LDM) trained from a\ntask-agnostic play dataset to solve dexterous deformable object manipulation\ntasks. D-Cubed learns a skill-latent space that encodes short-horizon actions\nin the play dataset using a VAE and trains a LDM to compose the skill latents\ninto a skill trajectory, representing a long-horizon action trajectory in the\ndataset. To optimise a trajectory for a target task, we introduce a novel\ngradient-free guided sampling method that employs the Cross-Entropy method\nwithin the reverse diffusion process. In particular, D-Cubed samples a small\nnumber of noisy skill trajectories using the LDM for exploration and evaluates\nthe trajectories in simulation. Then, D-Cubed selects the trajectory with the\nlowest cost for the subsequent reverse process. This effectively explores\npromising solution areas and optimises the sampled trajectories towards a\ntarget task throughout the reverse diffusion process. Through empirical\nevaluation on a public benchmark of dexterous deformable object manipulation\ntasks, we demonstrate that D-Cubed outperforms traditional trajectory\noptimisation and competitive baseline approaches by a significant margin. We\nfurther demonstrate that trajectories found by D-Cubed readily transfer to a\nreal-world LEAP hand on a folding task.\n","authors":["Jun Yamada","Shaohong Zhong","Jack Collins","Ingmar Posner"],"pdf_url":"https://arxiv.org/pdf/2403.12861v1.pdf","comment":"https://applied-ai-lab.github.io/D-cubed/"},{"id":"http://arxiv.org/abs/2403.12859v1","updated":"2024-03-19T16:03:03Z","published":"2024-03-19T16:03:03Z","title":"Primal Methods for Variational Inequality Problems with Functional\n  Constraints","summary":"  Constrained variational inequality problems are recognized for their broad\napplications across various fields including machine learning and operations\nresearch. First-order methods have emerged as the standard approach for solving\nthese problems due to their simplicity and scalability. However, they typically\nrely on projection or linear minimization oracles to navigate the feasible set,\nwhich becomes computationally expensive in practical scenarios featuring\nmultiple functional constraints. Existing efforts to tackle such functional\nconstrained variational inequality problems have centered on primal-dual\nalgorithms grounded in the Lagrangian function. These algorithms along with\ntheir theoretical analysis often require the existence and prior knowledge of\nthe optimal Lagrange multipliers. In this work, we propose a simple primal\nmethod, termed Constrained Gradient Method (CGM), for addressing functional\nconstrained variational inequality problems, without necessitating any\ninformation on the optimal Lagrange multipliers. We establish a non-asymptotic\nconvergence analysis of the algorithm for variational inequality problems with\nmonotone operators under smooth constraints. Remarkably, our algorithms match\nthe complexity of projection-based methods in terms of operator queries for\nboth monotone and strongly monotone settings, while utilizing significantly\ncheaper oracles based on quadratic programming. Furthermore, we provide several\nnumerical examples to evaluate the efficacy of our algorithms.\n","authors":["Liang Zhang","Niao He","Michael Muehlebach"],"pdf_url":"https://arxiv.org/pdf/2403.12859v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12856v1","updated":"2024-03-19T16:01:25Z","published":"2024-03-19T16:01:25Z","title":"Equivariant Ensembles and Regularization for Reinforcement Learning in\n  Map-based Path Planning","summary":"  In reinforcement learning (RL), exploiting environmental symmetries can\nsignificantly enhance efficiency, robustness, and performance. However,\nensuring that the deep RL policy and value networks are respectively\nequivariant and invariant to exploit these symmetries is a substantial\nchallenge. Related works try to design networks that are equivariant and\ninvariant by construction, limiting them to a very restricted library of\ncomponents, which in turn hampers the expressiveness of the networks. This\npaper proposes a method to construct equivariant policies and invariant value\nfunctions without specialized neural network components, which we term\nequivariant ensembles. We further add a regularization term for adding\ninductive bias during training. In a map-based path planning case study, we\nshow how equivariant ensembles and regularization benefit sample efficiency and\nperformance.\n","authors":["Mirco Theile","Hongpeng Cao","Marco Caccamo","Alberto L. Sangiovanni-Vincentelli"],"pdf_url":"https://arxiv.org/pdf/2403.12856v1.pdf","comment":"submitted for possible publication. A video can be found here:\n  https://youtu.be/L6NOdvU7n7s"},{"id":"http://arxiv.org/abs/2210.12624v2","updated":"2024-03-19T15:47:43Z","published":"2022-10-23T05:54:26Z","title":"Mitigating Gradient Bias in Multi-objective Learning: A Provably\n  Convergent Stochastic Approach","summary":"  Machine learning problems with multiple objective functions appear either in\nlearning with multiple criteria where learning has to make a trade-off between\nmultiple performance metrics such as fairness, safety and accuracy; or, in\nmulti-task learning where multiple tasks are optimized jointly, sharing\ninductive bias between them. This problems are often tackled by the\nmulti-objective optimization framework. However, existing stochastic\nmulti-objective gradient methods and its variants (e.g., MGDA, PCGrad, CAGrad,\netc.) all adopt a biased noisy gradient direction, which leads to degraded\nempirical performance. To this end, we develop a stochastic Multi-objective\ngradient Correction (MoCo) method for multi-objective optimization. The unique\nfeature of our method is that it can guarantee convergence without increasing\nthe batch size even in the non-convex setting. Simulations on multi-task\nsupervised and reinforcement learning demonstrate the effectiveness of our\nmethod relative to state-of-the-art methods.\n","authors":["Heshan Fernando","Han Shen","Miao Liu","Subhajit Chaudhury","Keerthiram Murugesan","Tianyi Chen"],"pdf_url":"https://arxiv.org/pdf/2210.12624v2.pdf","comment":"Changed hyper-parameter choice which affects some of the convergence\n  rate results in the paper"},{"id":"http://arxiv.org/abs/2312.03675v2","updated":"2024-03-19T15:41:44Z","published":"2023-12-06T18:39:29Z","title":"GeoShapley: A Game Theory Approach to Measuring Spatial Effects in\n  Machine Learning Models","summary":"  This paper introduces GeoShapley, a game theory approach to measuring spatial\neffects in machine learning models. GeoShapley extends the Nobel Prize-winning\nShapley value framework in game theory by conceptualizing location as a player\nin a model prediction game, which enables the quantification of the importance\nof location and the synergies between location and other features in a model.\nGeoShapley is a model-agnostic approach and can be applied to statistical or\nblack-box machine learning models in various structures. The interpretation of\nGeoShapley is directly linked with spatially varying coefficient models for\nexplaining spatial effects and additive models for explaining non-spatial\neffects. Using simulated data, GeoShapley values are validated against known\ndata-generating processes and are used for cross-comparison of seven\nstatistical and machine learning models. An empirical example of house price\nmodeling is used to illustrate GeoShapley's utility and interpretation with\nreal world data. The method is available as an open-source Python package named\ngeoshapley.\n","authors":["Ziqi Li"],"pdf_url":"https://arxiv.org/pdf/2312.03675v2.pdf","comment":"30 pages, 10 figures, 6 tables"},{"id":"http://arxiv.org/abs/2402.12518v2","updated":"2024-03-19T15:38:29Z","published":"2024-02-19T20:29:34Z","title":"Gaussian Process Neural Additive Models","summary":"  Deep neural networks have revolutionized many fields, but their black-box\nnature also occasionally prevents their wider adoption in fields such as\nhealthcare and finance, where interpretable and explainable models are\nrequired. The recent development of Neural Additive Models (NAMs) is a\nsignificant step in the direction of interpretable deep learning for tabular\ndatasets. In this paper, we propose a new subclass of NAMs that use a\nsingle-layer neural network construction of the Gaussian process via random\nFourier features, which we call Gaussian Process Neural Additive Models\n(GP-NAM). GP-NAMs have the advantage of a convex objective function and number\nof trainable parameters that grows linearly with feature dimensionality. It\nsuffers no loss in performance compared to deeper NAM approaches because GPs\nare well-suited for learning complex non-parametric univariate functions. We\ndemonstrate the performance of GP-NAM on several tabular datasets, showing that\nit achieves comparable or better performance in both classification and\nregression tasks with a large reduction in the number of parameters.\n","authors":["Wei Zhang","Brian Barr","John Paisley"],"pdf_url":"https://arxiv.org/pdf/2402.12518v2.pdf","comment":"Appears at AAAI 2024"},{"id":"http://arxiv.org/abs/2403.12830v1","updated":"2024-03-19T15:37:27Z","published":"2024-03-19T15:37:27Z","title":"Has Approximate Machine Unlearning been evaluated properly? From\n  Auditing to Side Effects","summary":"  The growing concerns surrounding data privacy and security have underscored\nthe critical necessity for machine unlearning--aimed at fully removing data\nlineage from machine learning models. MLaaS providers expect this to be their\nultimate safeguard for regulatory compliance. Despite its critical importance,\nthe pace at which privacy communities have been developing and implementing\nstrong methods to verify the effectiveness of machine unlearning has been\ndisappointingly slow, with this vital area often receiving insufficient focus.\nThis paper seeks to address this shortfall by introducing well-defined and\neffective metrics for black-box unlearning auditing tasks. We transform the\nauditing challenge into a question of non-membership inference and develop\nefficient metrics for auditing. By relying exclusively on the original and\nunlearned models--eliminating the need to train additional shadow models--our\napproach simplifies the evaluation of unlearning at the individual data point\nlevel. Utilizing these metrics, we conduct an in-depth analysis of current\napproximate machine unlearning algorithms, identifying three key directions\nwhere these approaches fall short: utility, resilience, and equity. Our aim is\nthat this work will greatly improve our understanding of approximate machine\nunlearning methods, taking a significant stride towards converting the\ntheoretical right to data erasure into a auditable reality.\n","authors":["Cheng-Long Wang","Qi Li","Zihang Xiang","Di Wang"],"pdf_url":"https://arxiv.org/pdf/2403.12830v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.05430v2","updated":"2024-03-19T15:22:18Z","published":"2023-02-10T18:45:52Z","title":"Oracle-Efficient Smoothed Online Learning for Piecewise Continuous\n  Decision Making","summary":"  Smoothed online learning has emerged as a popular framework to mitigate the\nsubstantial loss in statistical and computational complexity that arises when\none moves from classical to adversarial learning. Unfortunately, for some\nspaces, it has been shown that efficient algorithms suffer an exponentially\nworse regret than that which is minimax optimal, even when the learner has\naccess to an optimization oracle over the space. To mitigate that exponential\ndependence, this work introduces a new notion of complexity, the generalized\nbracketing numbers, which marries constraints on the adversary to the size of\nthe space, and shows that an instantiation of Follow-the-Perturbed-Leader can\nattain low regret with the number of calls to the optimization oracle scaling\noptimally with respect to average regret. We then instantiate our bounds in\nseveral problems of interest, including online prediction and planning of\npiecewise continuous functions, which has many applications in fields as\ndiverse as econometrics and robotics.\n","authors":["Adam Block","Alexander Rakhlin","Max Simchowitz"],"pdf_url":"https://arxiv.org/pdf/2302.05430v2.pdf","comment":null}],"Computer Science and Game Theory":[{"id":"http://arxiv.org/abs/2403.12807v1","updated":"2024-03-19T15:07:12Z","published":"2024-03-19T15:07:12Z","title":"Freshness-aware Block Propagation Optimization in 6G-based Web 3.0: An\n  Evolutionary Game Approach","summary":"  Driven by the aspiration to establish a decentralized digital economy, Web\n3.0 is emerging as the fundamental technology for digital transformation.\nIncorporating the promising sixth-generation (6G) technology with large\nbandwidth and space-air-ground integrated coverage, 6G-based Web 3.0 holds\ngreat potential in empowering users with enhanced data control and facilitating\nsecure peer-to-peer transactions, especially in consumer electronics, through\nthe utilization of blockchain technologies. However, 6G-based Web 3.0 is still\nin its infancy, such as ensuring block freshness and optimizing block\npropagation to improve blockchain performance. In this paper, we develop a\nfreshness-aware block propagation optimization framework for 6G-based Web 3.0.\nWe first propose a novel metric called Age of Block Information (AoBI) based on\nthe concept of age of information to quantify block freshness. To make block\npropagation optimization tractable, we classify miners into five different\nstates and propose a block propagation model for public blockchains inspired by\nepidemic models. Moreover, considering that the miners are bounded rational, we\npropose an incentive mechanism based on the evolutionary game for block\npropagation to improve block propagation efficiency. Numerical results\ndemonstrate that compared with other block propagation mechanisms, the proposed\nscheme has a higher block forwarding probability, which improves block\npropagation efficiency.\n","authors":["Jinbo Wen","Jiawen Kang","Zehui Xiong","Hongyang Du","Zhaohui Yang","Dusit Niyato","Meng Shen","Yutao Jiao","Yang Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.12807v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.02761v2","updated":"2024-03-19T14:41:35Z","published":"2023-11-05T20:43:08Z","title":"One-Shot Strategic Classification Under Unknown Costs","summary":"  The goal of strategic classification is to learn decision rules which are\nrobust to strategic input manipulation. Earlier works assume that these\nresponses are known; while some recent works handle unknown responses, they\nexclusively study online settings with repeated model deployments. But there\nare many domains$\\unicode{x2014}$particularly in public policy, a common\nmotivating use case$\\unicode{x2014}$where multiple deployments are infeasible,\nor where even one bad round is unacceptable. To address this gap, we initiate\nthe formal study of one-shot strategic classification under unknown responses,\nwhich requires committing to a single classifier once. Focusing on uncertainty\nin the users' cost function, we begin by proving that for a broad class of\ncosts, even a small mis-estimation of the true cost can entail trivial accuracy\nin the worst case. In light of this, we frame the task as a minimax problem,\nwith the goal of identifying the classifier with the smallest worst-case risk\nover an uncertainty set of possible costs. We design efficient algorithms for\nboth the full-batch and stochastic settings, which we prove converge (offline)\nto the minimax solution at the dimension-independent rate of\n$\\tilde{\\mathcal{O}}(T^{-\\frac{1}{2}})$. Our theoretical analysis reveals\nimportant structure stemming from strategic responses, particularly the value\nof dual norm regularization with respect to the cost function.\n","authors":["Elan Rosenfeld","Nir Rosenfeld"],"pdf_url":"https://arxiv.org/pdf/2311.02761v2.pdf","comment":"Fixed a bug in Algorithm 1, significantly strengthened Theorem 4.2,\n  and added Figure 1 to help visualize the lower bound in Theorem 3.2"},{"id":"http://arxiv.org/abs/2402.19292v2","updated":"2024-03-19T07:05:45Z","published":"2024-02-29T15:55:10Z","title":"Fundamental Limits of Throughput and Availability: Applications to\n  prophet inequalities & transaction fee mechanism design","summary":"  This paper studies the fundamental limits of availability and throughput for\nindependent and heterogeneous demands of a limited resource. Availability is\nthe probability that the demands are below the capacity of the resource.\nThroughput is the expected fraction of the resource that is utilized by the\ndemands. We offer a concentration inequality generator that gives lower bounds\non feasible availability and throughput pairs with a given capacity and\nindependent but not necessarily identical distributions of up-to-unit demands.\nWe show that availability and throughput cannot both be poor. These bounds are\nanalogous to tail inequalities on sums of independent random variables, but\nhold throughout the support of the demand distribution. This analysis gives\nanalytically tractable bounds supporting the unit-demand characterization of\nChawla, Devanur, and Lykouris (2023) and generalizes to up-to-unit demands. Our\nbounds also provide an approach towards improved multi-unit prophet\ninequalities (Hajiaghayi, Kleinberg, and Sandholm, 2007). They have\napplications to transaction fee mechanism design (for blockchains) where high\navailability limits the probability of profitable user-miner coalitions (Chung\nand Shi, 2023).\n","authors":["Aadityan Ganesh","Jason Hartline","Atanu R Sinha","Matthew vonAllmen"],"pdf_url":"https://arxiv.org/pdf/2402.19292v2.pdf","comment":"34 pages, 7 figures; updated author information to include\n  institutions and email addresses"},{"id":"http://arxiv.org/abs/2403.13083v1","updated":"2024-03-19T18:27:38Z","published":"2024-03-19T18:27:38Z","title":"Uber Stable: Formulating the Rideshare System as a Stable Matching\n  Problem","summary":"  Peer-to-peer ride-sharing platforms like Uber, Lyft, and DiDi have\nrevolutionized the transportation industry and labor market. At its essence,\nthese systems tackle the bipartite matching problem between two populations:\nriders and drivers. This research paper comprises two main components: an\ninitial literature review of existing ride-sharing platforms and efforts to\nenhance driver satisfaction, and the development of a novel algorithm\nimplemented through simulation testing to allow us to make our own\nobservations. The core algorithm utilized is the Gale-Shapley deferred\nacceptance algorithm, applied to a static matching problem over multiple time\nperiods. In this simulation, we construct a preference-aware task assignment\nmodel, considering both overall revenue maximization and individual preference\nsatisfaction. Specifically, the algorithm design incorporates factors such as\npassenger willingness-to-pay, driver preferences, and location attractiveness,\nwith an overarching goal of achieving equitable income distribution for drivers\nwhile maintaining overall system efficiency.\n  Through simulation, the paper compares the performance of the proposed\nalgorithm with random matching and closest neighbor algorithms, looking at\nmetrics such as total revenue, revenue per ride, and standard deviation to\nidentify trends and impacts of shifting priorities. Additionally, the DA\nalgorithm is compared to the Boston algorithm, and the paper explores the\neffect of prioritizing proximity to passengers versus distance from city\ncenter. Ultimately, the research underscores the importance of continued\nexploration in areas such as dynamic pricing models and additional modeling for\nunconventional driving times to further enhance the findings on the\neffectiveness and fairness of ride-sharing platforms.\n","authors":["Rhea Acharya","Jessica Chen","Helen Xiao"],"pdf_url":"https://arxiv.org/pdf/2403.13083v1.pdf","comment":"6 pages, 10 figures"},{"id":"http://arxiv.org/abs/2305.11261v2","updated":"2024-03-19T18:25:15Z","published":"2023-05-18T18:55:54Z","title":"Game Theory with Simulation of Other Players","summary":"  Game-theoretic interactions with AI agents could differ from traditional\nhuman-human interactions in various ways. One such difference is that it may be\npossible to simulate an AI agent (for example because its source code is\nknown), which allows others to accurately predict the agent's actions. This\ncould lower the bar for trust and cooperation. In this paper, we formalize\ngames in which one player can simulate another at a cost. We first derive some\nbasic properties of such games and then prove a number of results for them,\nincluding: (1) introducing simulation into generic-payoff normal-form games\nmakes them easier to solve; (2) if the only obstacle to cooperation is a lack\nof trust in the possibly-simulated agent, simulation enables equilibria that\nimprove the outcome for both agents; and however (3) there are settings where\nintroducing simulation results in strictly worse outcomes for both players.\n","authors":["Vojtech Kovarik","Caspar Oesterheld","Vincent Conitzer"],"pdf_url":"https://arxiv.org/pdf/2305.11261v2.pdf","comment":"The latest version fixes some typos in the proof of Theorem 5"}],"Information Theory":[{"id":"http://arxiv.org/abs/2403.12893v1","updated":"2024-03-19T16:45:45Z","published":"2024-03-19T16:45:45Z","title":"Wideband Modeling and Beamforming for Beyond Diagonal Reconfigurable\n  Intelligent Surfaces","summary":"  This work studies the wideband modeling and beamforming design of beyond\ndiagonal reconfigurable intelligent surface (BD-RIS), which generalizes and\ngoes beyond conventional RIS with diagonal phase shift matrices to achieve\nenhanced channel gain. Specifically, we investigate the response of BD-RIS in\nwideband systems by going back to its hardware circuit realizations. We propose\na novel wideband model which has simple expressions while capturing the\nresponse variations of BD-RIS for signals with different frequencies. With this\nwideband model, we propose a BD-RIS design algorithm for an orthogonal\nfrequency division multiplexing system to maximize the average rate over all\nsubcarriers. Finally, we provide simulation results to evaluate the performance\nof the proposed design and show the importance of wideband modeling for BD-RIS.\n","authors":["Hongyu Li","Matteo Nerini","Shanpu Shen","Bruno Clerckx"],"pdf_url":"https://arxiv.org/pdf/2403.12893v1.pdf","comment":"Submitted to IEEE for publication"},{"id":"http://arxiv.org/abs/2310.18138v2","updated":"2024-03-19T15:53:56Z","published":"2023-10-27T13:35:49Z","title":"Optimal Single-Shot Decoding of Quantum Codes","summary":"  We discuss single-shot decoding of quantum Calderbank-Shor-Steane codes with\nfaulty syndrome measurements. We state the problem as a joint source-channel\ncoding problem. By adding redundant rows to the code's parity-check matrix we\nobtain an additional syndrome error correcting code which addresses faulty\nsyndrome measurements. Thereby, the redundant rows are chosen to obtain good\nsyndrome error correcting capabilities while keeping the stabilizer weights\nlow. Optimal joint decoding rules are derived which, though too complex for\ngeneral codes, can be evaluated for short quantum codes.\n","authors":["Aldo Cumitini","Stefano Tinelli","Balázs Matuz","Francisco Lázaro","Luca Barletta"],"pdf_url":"https://arxiv.org/pdf/2310.18138v2.pdf","comment":"6 pages, 4 figures"},{"id":"http://arxiv.org/abs/2403.12813v1","updated":"2024-03-19T15:12:56Z","published":"2024-03-19T15:12:56Z","title":"Knowledge and Data Dual-Driven Channel Estimation and Feedback for\n  Ultra-Massive MIMO Systems under Hybrid Field Beam Squint Effect","summary":"  Acquiring accurate channel state information (CSI) at an access point (AP) is\nchallenging for wideband millimeter wave (mmWave) ultra-massive multiple-input\nand multiple-output (UMMIMO) systems, due to the high-dimensional channel\nmatrices, hybrid near- and far- field channel feature, beam squint effects, and\nimperfect hardware constraints, such as low-resolution analog-to-digital\nconverters, and in-phase and quadrature imbalance. To overcome these\nchallenges, this paper proposes an efficient downlink channel estimation (CE)\nand CSI feedback approach based on knowledge and data dual-driven deep learning\n(DL) networks. Specifically, we first propose a data-driven residual neural\nnetwork de-quantizer (ResNet-DQ) to pre-process the received pilot signals at\nuser equipment (UEs), where the noise and distortion brought by imperfect\nhardware can be mitigated. A knowledge-driven generalized multiple measurement\nvector learned approximate message passing (GMMV-LAMP) network is then\ndeveloped to jointly estimate the channels by exploiting the approximately same\nphysical angle shared by different subcarriers. In particular, two wideband\nredundant dictionaries (WRDs) are proposed such that the measurement matrices\nof the GMMV-LAMP network can accommodate the far-field and near-field beam\nsquint effect, respectively. Finally, we propose an encoder at the UEs and a\ndecoder at the AP by a data-driven CSI residual network (CSI-ResNet) to\ncompress the CSI matrix into a low-dimensional quantized bit vector for\nfeedback, thereby reducing the feedback overhead substantially. Simulation\nresults show that the proposed knowledge and data dual-driven approach\noutperforms conventional downlink CE and CSI feedback methods, especially in\nthe case of low signal-to-noise ratios.\n","authors":["Kuiyu Wang","Zhen Gao","Sheng Chen","Boyu Ning","Gaojie Chen","Yu Su","Zhaocheng Wang","H. Vincent Poor"],"pdf_url":"https://arxiv.org/pdf/2403.12813v1.pdf","comment":"17 pages, 22 figures, 3 tables"},{"id":"http://arxiv.org/abs/2403.00665v2","updated":"2024-03-19T14:31:54Z","published":"2024-03-01T16:51:48Z","title":"Complex-Valued Neural Network based Federated Learning for Multi-user\n  Indoor Positioning Performance Optimization","summary":"  In this article, the use of channel state information (CSI) for indoor\npositioning is studied. In the considered model, a server equipped with several\nantennas sends pilot signals to users, while each user uses the received pilot\nsignals to estimate channel states for user positioning. To this end, we\nformulate the positioning problem as an optimization problem aiming to minimize\nthe gap between the estimated positions and the ground truth positions of\nusers. To solve this problem, we design a complex-valued neural network (CVNN)\nmodel based federated learning (FL) algorithm. Compared to standard real-valued\ncentralized machine learning (ML) methods, our proposed algorithm has two main\nadvantages. First, our proposed algorithm can directly process complex-valued\nCSI data without data transformation. Second, our proposed algorithm is a\ndistributed ML method that does not require users to send their CSI data to the\nserver. Since the output of our proposed algorithm is complex-valued which\nconsists of the real and imaginary parts, we study the use of the CVNN to\nimplement two learning tasks. First, the proposed algorithm directly outputs\nthe estimated positions of a user. Here, the real and imaginary parts of an\noutput neuron represent the 2D coordinates of the user. Second, the proposed\nmethod can output two CSI features (i.e., line-of-sight/non-line-of-sight\ntransmission link classification and time of arrival (TOA) prediction) which\ncan be used in traditional positioning algorithms. Simulation results\ndemonstrate that our designed CVNN based FL can reduce the mean positioning\nerror between the estimated position and the actual position by up to 36%,\ncompared to a RVNN based FL which requires to transform CSI data into\nreal-valued data.\n","authors":["Hanzhi Yu","Yuchen Liu","Mingzhe Chen"],"pdf_url":"https://arxiv.org/pdf/2403.00665v2.pdf","comment":"13 pages, 10 figures"},{"id":"http://arxiv.org/abs/2309.02046v2","updated":"2024-03-19T13:59:56Z","published":"2023-09-05T08:47:14Z","title":"A Fast and Provable Algorithm for Sparse Phase Retrieval","summary":"  We study the sparse phase retrieval problem, which seeks to recover a sparse\nsignal from a limited set of magnitude-only measurements. In contrast to\nprevalent sparse phase retrieval algorithms that primarily use first-order\nmethods, we propose an innovative second-order algorithm that employs a\nNewton-type method with hard thresholding. This algorithm overcomes the linear\nconvergence limitations of first-order methods while preserving their hallmark\nper-iteration computational efficiency. We provide theoretical guarantees that\nour algorithm converges to the $s$-sparse ground truth signal\n$\\mathbf{x}^{\\natural} \\in \\mathbb{R}^n$ (up to a global sign) at a quadratic\nconvergence rate after at most $O(\\log (\\Vert\\mathbf{x}^{\\natural} \\Vert\n/x_{\\min}^{\\natural}))$ iterations, using $\\Omega(s^2\\log n)$ Gaussian random\nsamples. Numerical experiments show that our algorithm achieves a significantly\nfaster convergence rate than state-of-the-art methods.\n","authors":["Jian-Feng Cai","Yu Long","Ruixue Wen","Jiaxi Ying"],"pdf_url":"https://arxiv.org/pdf/2309.02046v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.02405v2","updated":"2024-03-19T12:46:02Z","published":"2023-11-04T13:59:26Z","title":"SplitMAC: Wireless Split Learning over Multiple Access Channels","summary":"  This paper presents a novel split learning (SL) framework, referred to as\nSplitMAC, which reduces the latency of SL by leveraging simultaneous uplink\ntransmission over multiple access channels. The key strategy is to divide\ndevices into multiple groups and allow the devices within the same group to\nsimultaneously transmit their smashed data and device-side models over the\nmultiple access channels. The optimization problem of device grouping to\nminimize SL latency is formulated, and the benefit of device grouping in\nreducing the uplink latency of SL is theoretically derived. By examining a\ntwo-device grouping case, two asymptotically-optimal algorithms are devised for\ndevice grouping in low and high signal-to-noise ratio (SNR) scenarios,\nrespectively, while providing proofs of their optimality. By merging these\nalgorithms, a near-optimal device grouping algorithm is proposed to cover a\nwide range of SNR. Our SL framework is also extended to consider practical\nfading channels and to support a general group size. Simulation results\ndemonstrate that our SL framework with the proposed device grouping algorithm\nis superior to existing SL frameworks in reducing SL latency.\n","authors":["Seonjung Kim","Yongjeong Oh","Yo-Seb Jeon"],"pdf_url":"https://arxiv.org/pdf/2311.02405v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12588v1","updated":"2024-03-19T09:47:54Z","published":"2024-03-19T09:47:54Z","title":"Machine Learning of the Prime Distribution","summary":"  In the present work we use maximum entropy methods to derive several theorems\nin probabilistic number theory, including a version of the Hardy-Ramanujan\nTheorem. We also provide a theoretical argument explaining the experimental\nobservations of Y.-H. He about the learnability of primes, and posit that the\nErd\\H{o}s-Kac law would very unlikely be discovered by current machine learning\ntechniques. Numerical experiments that we perform corroborate our theoretical\nfindings.\n","authors":["Alexander Kolpakov","Aidan Rocke"],"pdf_url":"https://arxiv.org/pdf/2403.12588v1.pdf","comment":"10 pages; parts of arXiv:2308.10817 reworked and amended; author's\n  draft; accepted in PLOS ONE"},{"id":"http://arxiv.org/abs/2403.12578v1","updated":"2024-03-19T09:38:39Z","published":"2024-03-19T09:38:39Z","title":"Self-Orthogonal Codes from Vectorial Dual-Bent Functions","summary":"  Self-orthogonal codes are a significant class of linear codes in coding\ntheory and have attracted a lot of attention. In \\cite{HLL2023Te,LH2023Se},\n$p$-ary self-orthogonal codes were constructed by using $p$-ary weakly regular\nbent functions, where $p$ is an odd prime. In \\cite{WH2023Se}, two classes of\nnon-degenerate quadratic forms were used to construct $q$-ary self-orthogonal\ncodes, where $q$ is a power of a prime. In this paper, we construct new\nfamilies of $q$-ary self-orthogonal codes using vectorial dual-bent functions.\nSome classes of at least almost optimal linear codes are obtained from the dual\ncodes of the constructed self-orthogonal codes. In some cases, we completely\ndetermine the weight distributions of the constructed self-orthogonal codes.\nFrom the view of vectorial dual-bent functions, we illustrate that the works on\nconstructing self-orthogonal codes from $p$-ary weakly regular bent functions\n\\cite{HLL2023Te,LH2023Se} and non-degenerate quadratic forms with $q$ being odd\n\\cite{WH2023Se} can be obtained by our results. We partially answer an open\nproblem on determining the weight distribution of a class of self-orthogonal\ncodes given in \\cite{LH2023Se}. As applications, we construct new infinite\nfamilies of at least almost optimal $q$-ary linear complementary dual codes\n(for short, LCD codes) and quantum codes.\n","authors":["Jiaxin Wang","Yadi Wei","Fang-Wei Fu","Juan Li"],"pdf_url":"https://arxiv.org/pdf/2403.12578v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12571v1","updated":"2024-03-19T09:30:40Z","published":"2024-03-19T09:30:40Z","title":"Optimizing Reconfigurable Antenna MIMO Systems with Coherent Ising\n  Machines","summary":"  Reconfigurable antenna multiple-input multiple-output (MIMO) is a promising\ntechnology for upcoming 6G communication systems. In this paper, we deal with\nthe problem of configuration selection for reconfigurable antenna MIMO by\nleveraging Coherent Ising Machines (CIMs). By adopting the CIM as a heuristic\nsolver for the Ising problem, the optimal antenna configuration that maximizes\nthe received signal-to-noise ratio is investigated. A mathematical framework\nthat converts the selection problem into a CIM-compatible unconstrained\nquadratic formulation is presented. Numerical studies show that the proposed\nCIM-based design outperforms classical counterparts and achieves near-optimal\nperformance (similar to exponentially complex exhaustive searching) while\nensuring polynomial complexity.\n","authors":["Ioannis Krikidis","Abhishek Kumar Singh","Kyle Jamieson"],"pdf_url":"https://arxiv.org/pdf/2403.12571v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10089v2","updated":"2024-03-19T07:58:17Z","published":"2024-03-15T08:05:16Z","title":"Approximation and bounding techniques for the Fisher-Rao distances","summary":"  The Fisher-Rao distance between two probability distributions of a\nstatistical model is defined as the Riemannian geodesic distance induced by the\nFisher information metric. In order to calculate the Fisher-Rao distance in\nclosed-form, we need (1) to elicit a formula for the Fisher-Rao geodesics, and\n(2) to integrate the Fisher length element along those geodesics. We consider\nseveral numerically robust approximation and bounding techniques for the\nFisher-Rao distances: First, we report generic upper bounds on Fisher-Rao\ndistances based on closed-form 1D Fisher-Rao distances of submodels. Second, we\ndescribe several generic approximation schemes depending on whether the\nFisher-Rao geodesics or pregeodesics are available in closed-form or not. In\nparticular, we obtain a generic method to guarantee an arbitrarily small\nadditive error on the approximation provided that Fisher-Rao pregeodesics and\ntight lower and upper bounds are available. Third, we consider the case of\nFisher metrics being Hessian metrics, and report generic tight upper bounds on\nthe Fisher-Rao distances using techniques of information geometry.\nUniparametric and biparametric statistical models always have Fisher Hessian\nmetrics, and in general a simple test allows to check whether the Fisher\ninformation matrix yields a Hessian metric or not. Fourth, we consider\nelliptical distribution families and show how to apply the above techniques to\nthese models. We also propose two new distances based either on the Fisher-Rao\nlengths of curves serving as proxies of Fisher-Rao geodesics, or based on the\nBirkhoff/Hilbert projective cone distance. Last, we consider an alternative\ngroup-theoretic approach for statistical transformation models based on the\nnotion of maximal invariant which yields insights on the structures of the\nFisher-Rao distance formula which may be used fruitfully in applications.\n","authors":["Frank Nielsen"],"pdf_url":"https://arxiv.org/pdf/2403.10089v2.pdf","comment":"43 pages"}]},"2024-03-20T00:00:00Z":{"Machine Learning":[{"id":"http://arxiv.org/abs/2403.12847v2","updated":"2024-03-20T03:13:47Z","published":"2024-03-19T15:54:38Z","title":"Policy Bifurcation in Safe Reinforcement Learning","summary":"  Safe reinforcement learning (RL) offers advanced solutions to constrained\noptimal control problems. Existing studies in safe RL implicitly assume\ncontinuity in policy functions, where policies map states to actions in a\nsmooth, uninterrupted manner; however, our research finds that in some\nscenarios, the feasible policy should be discontinuous or multi-valued,\ninterpolating between discontinuous local optima can inevitably lead to\nconstraint violations. We are the first to identify the generating mechanism of\nsuch a phenomenon, and employ topological analysis to rigorously prove the\nexistence of policy bifurcation in safe RL, which corresponds to the\ncontractibility of the reachable tuple. Our theorem reveals that in scenarios\nwhere the obstacle-free state space is non-simply connected, a feasible policy\nis required to be bifurcated, meaning its output action needs to change\nabruptly in response to the varying state. To train such a bifurcated policy,\nwe propose a safe RL algorithm called multimodal policy optimization (MUPO),\nwhich utilizes a Gaussian mixture distribution as the policy output. The\nbifurcated behavior can be achieved by selecting the Gaussian component with\nthe highest mixing coefficient. Besides, MUPO also integrates spectral\nnormalization and forward KL divergence to enhance the policy's capability of\nexploring different modes. Experiments with vehicle control tasks show that our\nalgorithm successfully learns the bifurcated policy and ensures satisfying\nsafety, while a continuous policy suffers from inevitable constraint\nviolations.\n","authors":["Wenjun Zou","Yao Lv","Jie Li","Yujie Yang","Shengbo Eben Li","Jingliang Duan","Xianyuan Zhan","Jingjing Liu","Yaqin Zhang","Keqiang Li"],"pdf_url":"https://arxiv.org/pdf/2403.12847v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12844v2","updated":"2024-03-20T09:06:08Z","published":"2024-03-19T15:51:21Z","title":"MELTing point: Mobile Evaluation of Language Transformers","summary":"  Transformers have revolutionized the machine learning landscape, gradually\nmaking their way into everyday tasks and equipping our computers with ``sparks\nof intelligence''. However, their runtime requirements have prevented them from\nbeing broadly deployed on mobile. As personal devices become increasingly\npowerful and prompt privacy becomes an ever more pressing issue, we explore the\ncurrent state of mobile execution of Large Language Models (LLMs). To achieve\nthis, we have created our own automation infrastructure, MELT, which supports\nthe headless execution and benchmarking of LLMs on device, supporting different\nmodels, devices and frameworks, including Android, iOS and Nvidia Jetson\ndevices. We evaluate popular instruction fine-tuned LLMs and leverage different\nframeworks to measure their end-to-end and granular performance, tracing their\nmemory and energy requirements along the way.\n  Our analysis is the first systematic study of on-device LLM execution,\nquantifying performance, energy efficiency and accuracy across various\nstate-of-the-art models and showcases the state of on-device intelligence in\nthe era of hyperscale models. Results highlight the performance heterogeneity\nacross targets and corroborates that LLM inference is largely memory-bound.\nQuantization drastically reduces memory requirements and renders execution\nviable, but at a non-negligible accuracy cost. Drawing from its energy\nfootprint and thermal behavior, the continuous execution of LLMs remains\nelusive, as both factors negatively affect user experience. Last, our\nexperience shows that the ecosystem is still in its infancy, and algorithmic as\nwell as hardware breakthroughs can significantly shift the execution cost. We\nexpect NPU acceleration, and framework-hardware co-design to be the biggest bet\ntowards efficient standalone execution, with the alternative of offloading\ntailored towards edge deployments.\n","authors":["Stefanos Laskaridis","Kleomenis Katevas","Lorenzo Minto","Hamed Haddadi"],"pdf_url":"https://arxiv.org/pdf/2403.12844v2.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2403.13808v1","updated":"2024-03-20T17:59:58Z","published":"2024-03-20T17:59:58Z","title":"On Pretraining Data Diversity for Self-Supervised Learning","summary":"  We explore the impact of training with more diverse datasets, characterized\nby the number of unique samples, on the performance of self-supervised learning\n(SSL) under a fixed computational budget. Our findings consistently demonstrate\nthat increasing pretraining data diversity enhances SSL performance, albeit\nonly when the distribution distance to the downstream data is minimal. Notably,\neven with an exceptionally large pretraining data diversity achieved through\nmethods like web crawling or diffusion-generated data, among other ways, the\ndistribution shift remains a challenge. Our experiments are comprehensive with\nseven SSL methods using large-scale datasets such as ImageNet and YFCC100M\namounting to over 200 GPU days. Code and trained models will be available at\nhttps://github.com/hammoudhasan/DiversitySSL .\n","authors":["Hasan Abed Al Kader Hammoud","Tuhin Das","Fabio Pizzati","Philip Torr","Adel Bibi","Bernard Ghanem"],"pdf_url":"https://arxiv.org/pdf/2403.13808v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2403.13807v1","updated":"2024-03-20T17:59:57Z","published":"2024-03-20T17:59:57Z","title":"Editing Massive Concepts in Text-to-Image Diffusion Models","summary":"  Text-to-image diffusion models suffer from the risk of generating outdated,\ncopyrighted, incorrect, and biased content. While previous methods have\nmitigated the issues on a small scale, it is essential to handle them\nsimultaneously in larger-scale real-world scenarios. We propose a two-stage\nmethod, Editing Massive Concepts In Diffusion Models (EMCID). The first stage\nperforms memory optimization for each individual concept with dual\nself-distillation from text alignment loss and diffusion noise prediction loss.\nThe second stage conducts massive concept editing with multi-layer, closed form\nmodel editing. We further propose a comprehensive benchmark, named ImageNet\nConcept Editing Benchmark (ICEB), for evaluating massive concept editing for\nT2I models with two subtasks, free-form prompts, massive concept categories,\nand extensive evaluation metrics. Extensive experiments conducted on our\nproposed benchmark and previous benchmarks demonstrate the superior scalability\nof EMCID for editing up to 1,000 concepts, providing a practical approach for\nfast adjustment and re-deployment of T2I diffusion models in real-world\napplications.\n","authors":["Tianwei Xiong","Yue Wu","Enze Xie","Yue Wu","Zhenguo Li","Xihui Liu"],"pdf_url":"https://arxiv.org/pdf/2403.13807v1.pdf","comment":"Project page: https://silentview.github.io/EMCID/ . Code:\n  https://github.com/SilentView/EMCID"},{"id":"http://arxiv.org/abs/2403.13805v1","updated":"2024-03-20T17:59:55Z","published":"2024-03-20T17:59:55Z","title":"RAR: Retrieving And Ranking Augmented MLLMs for Visual Recognition","summary":"  CLIP (Contrastive Language-Image Pre-training) uses contrastive learning from\nnoise image-text pairs to excel at recognizing a wide array of candidates, yet\nits focus on broad associations hinders the precision in distinguishing subtle\ndifferences among fine-grained items. Conversely, Multimodal Large Language\nModels (MLLMs) excel at classifying fine-grained categories, thanks to their\nsubstantial knowledge from pre-training on web-level corpora. However, the\nperformance of MLLMs declines with an increase in category numbers, primarily\ndue to growing complexity and constraints of limited context window size. To\nsynergize the strengths of both approaches and enhance the few-shot/zero-shot\nrecognition abilities for datasets characterized by extensive and fine-grained\nvocabularies, this paper introduces RAR, a Retrieving And Ranking augmented\nmethod for MLLMs. We initially establish a multi-modal retriever based on CLIP\nto create and store explicit memory for different categories beyond the\nimmediate context window. During inference, RAR retrieves the top-k similar\nresults from the memory and uses MLLMs to rank and make the final predictions.\nOur proposed approach not only addresses the inherent limitations in\nfine-grained recognition but also preserves the model's comprehensive knowledge\nbase, significantly boosting accuracy across a range of vision-language\nrecognition tasks. Notably, our approach demonstrates a significant improvement\nin performance on 5 fine-grained visual recognition benchmarks, 11 few-shot\nimage recognition datasets, and the 2 object detection datasets under the\nzero-shot recognition setting.\n","authors":["Ziyu Liu","Zeyi Sun","Yuhang Zang","Wei Li","Pan Zhang","Xiaoyi Dong","Yuanjun Xiong","Dahua Lin","Jiaqi Wang"],"pdf_url":"https://arxiv.org/pdf/2403.13805v1.pdf","comment":"Project: https://github.com/Liuziyu77/RAR"},{"id":"http://arxiv.org/abs/2403.13804v1","updated":"2024-03-20T17:59:43Z","published":"2024-03-20T17:59:43Z","title":"Learning from Models and Data for Visual Grounding","summary":"  We introduce SynGround, a novel framework that combines data-driven learning\nand knowledge transfer from various large-scale pretrained models to enhance\nthe visual grounding capabilities of a pretrained vision-and-language model.\nThe knowledge transfer from the models initiates the generation of image\ndescriptions through an image description generator. These descriptions serve\ndual purposes: they act as prompts for synthesizing images through a\ntext-to-image generator, and as queries for synthesizing text, from which\nphrases are extracted using a large language model. Finally, we leverage an\nopen-vocabulary object detector to generate synthetic bounding boxes for the\nsynthetic images and texts. We finetune a pretrained vision-and-language model\non this dataset by optimizing a mask-attention consistency objective that\naligns region annotations with gradient-based model explanations. The resulting\nmodel improves the grounding capabilities of an off-the-shelf\nvision-and-language model. Particularly, SynGround improves the pointing game\naccuracy of ALBEF on the Flickr30k dataset from 79.38% to 87.26%, and on\nRefCOCO+ Test A from 69.35% to 79.06% and on RefCOCO+ Test B from 53.77% to\n63.67%.\n","authors":["Ruozhen He","Paola Cascante-Bonilla","Ziyan Yang","Alexander C. Berg","Vicente Ordonez"],"pdf_url":"https://arxiv.org/pdf/2403.13804v1.pdf","comment":"Project Page: https://catherine-r-he.github.io/SynGround/"},{"id":"http://arxiv.org/abs/2403.13802v1","updated":"2024-03-20T17:59:14Z","published":"2024-03-20T17:59:14Z","title":"ZigMa: Zigzag Mamba Diffusion Model","summary":"  The diffusion model has long been plagued by scalability and quadratic\ncomplexity issues, especially within transformer-based structures. In this\nstudy, we aim to leverage the long sequence modeling capability of a\nState-Space Model called Mamba to extend its applicability to visual data\ngeneration. Firstly, we identify a critical oversight in most current\nMamba-based vision methods, namely the lack of consideration for spatial\ncontinuity in the scan scheme of Mamba. Secondly, building upon this insight,\nwe introduce a simple, plug-and-play, zero-parameter method named Zigzag Mamba,\nwhich outperforms Mamba-based baselines and demonstrates improved speed and\nmemory utilization compared to transformer-based baselines. Lastly, we\nintegrate Zigzag Mamba with the Stochastic Interpolant framework to investigate\nthe scalability of the model on large-resolution visual datasets, such as\nFacesHQ $1024\\times 1024$ and UCF101, MultiModal-CelebA-HQ, and MS COCO\n$256\\times 256$. Code will be released at https://taohu.me/zigma/\n","authors":["Vincent Tao Hu","Stefan Andreas Baumann","Ming Gui","Olga Grebenkova","Pingchuan Ma","Johannes Fischer","Bjorn Ommer"],"pdf_url":"https://arxiv.org/pdf/2403.13802v1.pdf","comment":"Project Page: https://taohu.me/zigma/"},{"id":"http://arxiv.org/abs/2310.07923v4","updated":"2024-03-20T17:55:48Z","published":"2023-10-11T22:35:18Z","title":"The Expressive Power of Transformers with Chain of Thought","summary":"  Recent theoretical work has identified surprisingly simple reasoning\nproblems, such as checking if two nodes in a graph are connected or simulating\nfinite-state machines, that are provably unsolvable by standard transformers\nthat answer immediately after reading their input. However, in practice,\ntransformers' reasoning can be improved by allowing them to use a \"chain of\nthought\" or \"scratchpad\", i.e., generate and condition on a sequence of\nintermediate tokens before answering. Motivated by this, we ask: Does such\nintermediate generation fundamentally extend the computational power of a\ndecoder-only transformer? We show that the answer is yes, but the amount of\nincrease depends crucially on the amount of intermediate generation. For\ninstance, we find that transformer decoders with a logarithmic number of\ndecoding steps (w.r.t. the input length) push the limits of standard\ntransformers only slightly, while a linear number of decoding steps, assuming a\nslight generalization to standard pre-norm, adds a clear new ability (under\nstandard complexity conjectures): recognizing all regular languages. Our\nresults also imply that linear steps keep transformer decoders within\ncontext-sensitive languages, and polynomial steps with generalized pre-norm\nmake them recognize exactly the class of polynomial-time solvable problems --\nthe first exact characterization of a type of transformers in terms of standard\ncomplexity classes. Together, our results provide a nuanced framework for\nunderstanding how the length of a transformer's chain of thought or scratchpad\nimpacts its reasoning power.\n","authors":["William Merrill","Ashish Sabharwal"],"pdf_url":"https://arxiv.org/pdf/2310.07923v4.pdf","comment":"9-page preprint. Updated March 20 after ICLR acceptance"},{"id":"http://arxiv.org/abs/2403.13798v1","updated":"2024-03-20T17:55:21Z","published":"2024-03-20T17:55:21Z","title":"Hierarchical NeuroSymbolic Approach for Action Quality Assessment","summary":"  Action quality assessment (AQA) applies computer vision to quantitatively\nassess the performance or execution of a human action. Current AQA approaches\nare end-to-end neural models, which lack transparency and tend to be biased\nbecause they are trained on subjective human judgements as ground-truth. To\naddress these issues, we introduce a neuro-symbolic paradigm for AQA, which\nuses neural networks to abstract interpretable symbols from video data and\nmakes quality assessments by applying rules to those symbols. We take diving as\nthe case study. We found that domain experts prefer our system and find it more\ninformative than purely neural approaches to AQA in diving. Our system also\nachieves state-of-the-art action recognition and temporal segmentation, and\nautomatically generates a detailed report that breaks the dive down into its\nelements and provides objective scoring with visual evidence. As verified by a\ngroup of domain experts, this report may be used to assist judges in scoring,\nhelp train judges, and provide feedback to divers. We will open-source all of\nour annotated training data and code for ease of reproducibility.\n","authors":["Lauren Okamoto","Paritosh Parmar"],"pdf_url":"https://arxiv.org/pdf/2403.13798v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13797v1","updated":"2024-03-20T17:54:58Z","published":"2024-03-20T17:54:58Z","title":"Bridge the Modality and Capacity Gaps in Vision-Language Model Selection","summary":"  Vision Language Models (VLMs) excel in zero-shot image classification by\npairing images with textual category names. The expanding variety of\nPre-Trained VLMs enhances the likelihood of identifying a suitable VLM for\nspecific tasks. Thus, a promising zero-shot image classification strategy is\nselecting the most appropriate Pre-Trained VLM from the VLM Zoo, relying solely\non the text data of the target dataset without access to the dataset's images.\nIn this paper, we analyze two inherent challenges in assessing the ability of a\nVLM in this Language-Only VLM selection: the \"Modality Gap\" -- the disparity in\nVLM's embeddings across two different modalities, making text a less reliable\nsubstitute for images; and the \"Capability Gap\" -- the discrepancy between the\nVLM's overall ranking and its ranking for target dataset, hindering direct\nprediction of a model's dataset-specific performance from its general\nperformance. We propose VLM Selection With gAp Bridging (SWAB) to mitigate the\nnegative impact of these two gaps. SWAB first adopts optimal transport to\ncapture the relevance between open-source datasets and target dataset with a\ntransportation matrix. It then uses this matrix to transfer useful statistics\nof VLMs from open-source datasets to the target dataset for bridging those two\ngaps and enhancing the VLM's capacity estimation for VLM selection. Experiments\nacross various VLMs and image classification datasets validate SWAB's\neffectiveness.\n","authors":["Chao Yi","De-Chuan Zhan","Han-Jia Ye"],"pdf_url":"https://arxiv.org/pdf/2403.13797v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13793v1","updated":"2024-03-20T17:54:26Z","published":"2024-03-20T17:54:26Z","title":"Evaluating Frontier Models for Dangerous Capabilities","summary":"  To understand the risks posed by a new AI system, we must understand what it\ncan and cannot do. Building on prior work, we introduce a programme of new\n\"dangerous capability\" evaluations and pilot them on Gemini 1.0 models. Our\nevaluations cover four areas: (1) persuasion and deception; (2) cyber-security;\n(3) self-proliferation; and (4) self-reasoning. We do not find evidence of\nstrong dangerous capabilities in the models we evaluated, but we flag early\nwarning signs. Our goal is to help advance a rigorous science of dangerous\ncapability evaluation, in preparation for future models.\n","authors":["Mary Phuong","Matthew Aitchison","Elliot Catt","Sarah Cogan","Alexandre Kaskasoli","Victoria Krakovna","David Lindner","Matthew Rahtz","Yannis Assael","Sarah Hodkinson","Heidi Howard","Tom Lieberum","Ramana Kumar","Maria Abi Raad","Albert Webson","Lewis Ho","Sharon Lin","Sebastian Farquhar","Marcus Hutter","Gregoire Deletang","Anian Ruoss","Seliem El-Sayed","Sasha Brown","Anca Dragan","Rohin Shah","Allan Dafoe","Toby Shevlane"],"pdf_url":"https://arxiv.org/pdf/2403.13793v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13787v1","updated":"2024-03-20T17:49:54Z","published":"2024-03-20T17:49:54Z","title":"RewardBench: Evaluating Reward Models for Language Modeling","summary":"  Reward models (RMs) are at the crux of successful RLHF to align pretrained\nmodels to human preferences, yet there has been relatively little study that\nfocuses on evaluation of those reward models. Evaluating reward models presents\nan opportunity to understand the opaque technologies used for alignment of\nlanguage models and which values are embedded in them. To date, very few\ndescriptors of capabilities, training methods, or open-source reward models\nexist. In this paper, we present RewardBench, a benchmark dataset and code-base\nfor evaluation, to enhance scientific understanding of reward models. The\nRewardBench dataset is a collection of prompt-win-lose trios spanning chat,\nreasoning, and safety, to benchmark how reward models perform on challenging,\nstructured and out-of-distribution queries. We created specific comparison\ndatasets for RMs that have subtle, but verifiable reasons (e.g. bugs, incorrect\nfacts) why one answer should be preferred to another. On the RewardBench\nleaderboard, we evaluate reward models trained with a variety of methods, such\nas the direct MLE training of classifiers and the implicit reward modeling of\nDirect Preference Optimization (DPO), and on a spectrum of datasets. We present\nmany findings on propensity for refusals, reasoning limitations, and\ninstruction following shortcomings of various reward models towards a better\nunderstanding of the RLHF process.\n","authors":["Nathan Lambert","Valentina Pyatkin","Jacob Morrison","LJ Miranda","Bill Yuchen Lin","Khyathi Chandu","Nouha Dziri","Sachin Kumar","Tom Zick","Yejin Choi","Noah A. Smith","Hannaneh Hajishirzi"],"pdf_url":"https://arxiv.org/pdf/2403.13787v1.pdf","comment":"40 pages, 19 figures, 12 tables"},{"id":"http://arxiv.org/abs/2403.13785v1","updated":"2024-03-20T17:47:25Z","published":"2024-03-20T17:47:25Z","title":"Towards an extension of Fault Trees in the Predictive Maintenance\n  Scenario","summary":"  One of the most appreciated features of Fault Trees (FTs) is their\nsimplicity, making them fit into industrial processes. As such processes evolve\nin time, considering new aspects of large modern systems, modelling techniques\nbased on FTs have adapted to these needs. This paper proposes an extension of\nFTs to take into account the problem of Predictive Maintenance, one of the\nchallenges of the modern dependability field of study. The paper sketches the\nPredictive Fault Tree language and proposes some use cases to support their\nmodelling and analysis in concrete industrial settings.\n","authors":["Roberta De Fazio","Stefano Marrone","Laura Verde","Vincenzo Reccia","Paolo Valletta"],"pdf_url":"https://arxiv.org/pdf/2403.13785v1.pdf","comment":"S. Bernardi, T. Zoppi (Editors), Fast Abstracts and Student Forum\n  Proceedings - EDCC 2024 - 19th European Dependable Computing Conference,\n  Leuven, Belgium, 8-11 April 2024"},{"id":"http://arxiv.org/abs/2403.13784v1","updated":"2024-03-20T17:47:08Z","published":"2024-03-20T17:47:08Z","title":"The Model Openness Framework: Promoting Completeness and Openness for\n  Reproducibility, Transparency and Usability in AI","summary":"  Generative AI (GAI) offers unprecedented possibilities but its\ncommercialization has raised concerns about transparency, reproducibility,\nbias, and safety. Many \"open-source\" GAI models lack the necessary components\nfor full understanding and reproduction, and some use restrictive licenses, a\npractice known as \"openwashing.\" We propose the Model Openness Framework (MOF),\na ranked classification system that rates machine learning models based on\ntheir completeness and openness, following principles of open science, open\nsource, open data, and open access. The MOF requires specific components of the\nmodel development lifecycle to be included and released under appropriate open\nlicenses. This framework aims to prevent misrepresentation of models claiming\nto be open, guide researchers and developers in providing all model components\nunder permissive licenses, and help companies, academia, and hobbyists identify\nmodels that can be safely adopted without restrictions. Wide adoption of the\nMOF will foster a more open AI ecosystem, accelerating research, innovation,\nand adoption.\n","authors":["Matt White","Ibrahim Haddad","Cailean Osborne","Xiao-Yang Liu","Ahmed Abdelmonsef","Sachin Varghese"],"pdf_url":"https://arxiv.org/pdf/2403.13784v1.pdf","comment":"45 pages"},{"id":"http://arxiv.org/abs/2403.13781v1","updated":"2024-03-20T17:43:58Z","published":"2024-03-20T17:43:58Z","title":"Sparse Implementation of Versatile Graph-Informed Layers","summary":"  Graph Neural Networks (GNNs) have emerged as effective tools for learning\ntasks on graph-structured data. Recently, Graph-Informed (GI) layers were\nintroduced to address regression tasks on graph nodes, extending their\napplicability beyond classic GNNs. However, existing implementations of GI\nlayers lack efficiency due to dense memory allocation. This paper presents a\nsparse implementation of GI layers, leveraging the sparsity of adjacency\nmatrices to reduce memory usage significantly. Additionally, a versatile\ngeneral form of GI layers is introduced, enabling their application to subsets\nof graph nodes. The proposed sparse implementation improves the concrete\ncomputational efficiency and scalability of the GI layers, permitting to build\ndeeper Graph-Informed Neural Networks (GINNs) and facilitating their\nscalability to larger graphs.\n","authors":["Francesco Della Santa"],"pdf_url":"https://arxiv.org/pdf/2403.13781v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.06192v5","updated":"2024-03-20T17:36:07Z","published":"2023-06-09T18:45:15Z","title":"Ada-NAV: Adaptive Trajectory Length-Based Sample Efficient Policy\n  Learning for Robotic Navigation","summary":"  Trajectory length stands as a crucial hyperparameter within reinforcement\nlearning (RL) algorithms, significantly contributing to the sample inefficiency\nin robotics applications. Motivated by the pivotal role trajectory length plays\nin the training process, we introduce Ada-NAV, a novel adaptive trajectory\nlength scheme designed to enhance the training sample efficiency of RL\nalgorithms in robotic navigation tasks. Unlike traditional approaches that\ntreat trajectory length as a fixed hyperparameter, we propose to dynamically\nadjust it based on the entropy of the underlying navigation policy.\nInterestingly, Ada-NAV can be applied to both existing on-policy and off-policy\nRL methods, which we demonstrate by empirically validating its efficacy on\nthree popular RL methods: REINFORCE, Proximal Policy Optimization (PPO), and\nSoft Actor-Critic (SAC). We demonstrate through simulated and real-world\nrobotic experiments that Ada-NAV outperforms conventional methods that employ\nconstant or randomly sampled trajectory lengths. Specifically, for a fixed\nsample budget, Ada-NAV achieves an 18\\% increase in navigation success rate, a\n20-38\\% reduction in navigation path length, and a 9.32\\% decrease in elevation\ncosts. Furthermore, we showcase the versatility of Ada-NAV by integrating it\nwith the Clearpath Husky robot, illustrating its applicability in complex\noutdoor environments.\n","authors":["Bhrij Patel","Kasun Weerakoon","Wesley A. Suttle","Alec Koppel","Brian M. Sadler","Tianyi Zhou","Amrit Singh Bedi","Dinesh Manocha"],"pdf_url":"https://arxiv.org/pdf/2306.06192v5.pdf","comment":"11 pages, 9 figures, 2 tables"},{"id":"http://arxiv.org/abs/2403.13771v1","updated":"2024-03-20T17:33:02Z","published":"2024-03-20T17:33:02Z","title":"Describe-and-Dissect: Interpreting Neurons in Vision Networks with\n  Language Models","summary":"  In this paper, we propose Describe-and-Dissect (DnD), a novel method to\ndescribe the roles of hidden neurons in vision networks. DnD utilizes recent\nadvancements in multimodal deep learning to produce complex natural language\ndescriptions, without the need for labeled training data or a predefined set of\nconcepts to choose from. Additionally, DnD is training-free, meaning we don't\ntrain any new models and can easily leverage more capable general purpose\nmodels in the future. We have conducted extensive qualitative and quantitative\nanalysis to show that DnD outperforms prior work by providing higher quality\nneuron descriptions. Specifically, our method on average provides the highest\nquality labels and is more than 2 times as likely to be selected as the best\nexplanation for a neuron than the best baseline.\n","authors":["Nicholas Bai","Rahul A. Iyer","Tuomas Oikarinen","Tsui-Wei Weng"],"pdf_url":"https://arxiv.org/pdf/2403.13771v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13765v1","updated":"2024-03-20T17:28:17Z","published":"2024-03-20T17:28:17Z","title":"Towards Principled Representation Learning from Videos for Reinforcement\n  Learning","summary":"  We study pre-training representations for decision-making using video data,\nwhich is abundantly available for tasks such as game agents and software\ntesting. Even though significant empirical advances have been made on this\nproblem, a theoretical understanding remains absent. We initiate the\ntheoretical investigation into principled approaches for representation\nlearning and focus on learning the latent state representations of the\nunderlying MDP using video data. We study two types of settings: one where\nthere is iid noise in the observation, and a more challenging setting where\nthere is also the presence of exogenous noise, which is non-iid noise that is\ntemporally correlated, such as the motion of people or cars in the background.\nWe study three commonly used approaches: autoencoding, temporal contrastive\nlearning, and forward modeling. We prove upper bounds for temporal contrastive\nlearning and forward modeling in the presence of only iid noise. We show that\nthese approaches can learn the latent state and use it to do efficient\ndownstream RL with polynomial sample complexity. When exogenous noise is also\npresent, we establish a lower bound result showing that the sample complexity\nof learning from video data can be exponentially worse than learning from\naction-labeled trajectory data. This partially explains why reinforcement\nlearning with video pre-training is hard. We evaluate these representational\nlearning methods in two visual domains, yielding results that are consistent\nwith our theoretical findings.\n","authors":["Dipendra Misra","Akanksha Saran","Tengyang Xie","Alex Lamb","John Langford"],"pdf_url":"https://arxiv.org/pdf/2403.13765v1.pdf","comment":"ICLR 2024 Spotlight Conference Paper"},{"id":"http://arxiv.org/abs/2305.17282v5","updated":"2024-03-20T17:25:52Z","published":"2023-05-26T22:01:47Z","title":"Universal consistency of the $k$-NN rule in metric spaces and Nagata\n  dimension. II","summary":"  We continue to investigate the $k$ nearest neighbour ($k$-NN) learning rule\nin complete separable metric spaces. Thanks to the results of C\\'erou and\nGuyader (2006) and Preiss (1983), this rule is known to be universally\nconsistent in every such metric space that is sigma-finite dimensional in the\nsense of Nagata. Here we show that the rule is strongly universally consistent\nin such spaces in the absence of ties. Under the tie-breaking strategy applied\nby Devroye, Gy\\\"{o}rfi, Krzy\\.{z}ak, and Lugosi (1994) in the Euclidean\nsetting, we manage to show the strong universal consistency in non-Archimedian\nmetric spaces (that is, those of Nagata dimension zero). Combining the theorem\nof C\\'erou and Guyader with results of Assouad and Quentin de Gromard (2006),\none deduces that the $k$-NN rule is universally consistent in metric spaces\nhaving finite dimension in the sense of de Groot. In particular, the $k$-NN\nrule is universally consistent in the Heisenberg group which is not\nsigma-finite dimensional in the sense of Nagata as follows from an example\nindependently constructed by Kor\\'anyi and Reimann (1995) and Sawyer and\nWheeden (1992).\n","authors":["Sushma Kumari","Vladimir G. Pestov"],"pdf_url":"https://arxiv.org/pdf/2305.17282v5.pdf","comment":"Latex 2e, 27 pages, 1 figure. Minor revisions to conform with the\n  last set of journal page proofs: two typos corrected, the bibliography\n  rearranged in the order of citations (the ESAIM:PS home style), and two\n  articles that were no longer cited removed"},{"id":"http://arxiv.org/abs/2305.14456v4","updated":"2024-03-20T17:16:37Z","published":"2023-05-23T18:27:51Z","title":"Having Beer after Prayer? Measuring Cultural Bias in Large Language\n  Models","summary":"  As the reach of large language models (LMs) expands globally, their ability\nto cater to diverse cultural contexts becomes crucial. Despite advancements in\nmultilingual capabilities, models are not designed with appropriate cultural\nnuances. In this paper, we show that multilingual and Arabic monolingual LMs\nexhibit bias towards entities associated with Western culture. We introduce\nCAMeL, a novel resource of 628 naturally-occurring prompts and 20,368 entities\nspanning eight types that contrast Arab and Western cultures. CAMeL provides a\nfoundation for measuring cultural biases in LMs through both extrinsic and\nintrinsic evaluations. Using CAMeL, we examine the cross-cultural performance\nin Arabic of 16 different LMs on tasks such as story generation, NER, and\nsentiment analysis, where we find concerning cases of stereotyping and cultural\nunfairness. We further test their text-infilling performance, revealing the\nincapability of appropriate adaptation to Arab cultural contexts. Finally, we\nanalyze 6 Arabic pre-training corpora and find that commonly used sources such\nas Wikipedia may not be best suited to build culturally aware LMs, if used as\nthey are without adjustment. We will make CAMeL publicly available at:\nhttps://github.com/tareknaous/camel\n","authors":["Tarek Naous","Michael J. Ryan","Alan Ritter","Wei Xu"],"pdf_url":"https://arxiv.org/pdf/2305.14456v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2002.10121v4","updated":"2024-03-20T17:15:32Z","published":"2020-02-24T08:59:34Z","title":"The Unreasonable Effectiveness of Greedy Algorithms in Multi-Armed\n  Bandit with Many Arms","summary":"  We investigate a Bayesian $k$-armed bandit problem in the \\emph{many-armed}\nregime, where $k \\geq \\sqrt{T}$ and $T$ represents the time horizon. Initially,\nand aligned with recent literature on many-armed bandit problems, we observe\nthat subsampling plays a key role in designing optimal algorithms; the\nconventional UCB algorithm is sub-optimal, whereas a subsampled UCB (SS-UCB),\nwhich selects $\\Theta(\\sqrt{T})$ arms for execution under the UCB framework,\nachieves rate-optimality. However, despite SS-UCB's theoretical promise of\noptimal regret, it empirically underperforms compared to a greedy algorithm\nthat consistently chooses the empirically best arm. This observation extends to\ncontextual settings through simulations with real-world data. Our findings\nsuggest a new form of \\emph{free exploration} beneficial to greedy algorithms\nin the many-armed context, fundamentally linked to a tail event concerning the\nprior distribution of arm rewards. This finding diverges from the notion of\nfree exploration, which relates to covariate variation, as recently discussed\nin contextual bandit literature. Expanding upon these insights, we establish\nthat the subsampled greedy approach not only achieves rate-optimality for\nBernoulli bandits within the many-armed regime but also attains sublinear\nregret across broader distributions. Collectively, our research indicates that\nin the many-armed regime, practitioners might find greater value in adopting\ngreedy algorithms.\n","authors":["Mohsen Bayati","Nima Hamidi","Ramesh Johari","Khashayar Khosravi"],"pdf_url":"https://arxiv.org/pdf/2002.10121v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13749v1","updated":"2024-03-20T16:58:28Z","published":"2024-03-20T16:58:28Z","title":"Weisfeiler and Leman Go Loopy: A New Hierarchy for Graph\n  Representational Learning","summary":"  We introduce $r$-loopy Weisfeiler-Leman ($r$-$\\ell{}$WL), a novel hierarchy\nof graph isomorphism tests and a corresponding GNN framework, $r$-$\\ell{}$MPNN,\nthat can count cycles up to length $r + 2$. Most notably, we show that\n$r$-$\\ell{}$WL can count homomorphisms of cactus graphs. This strictly extends\nclassical 1-WL, which can only count homomorphisms of trees and, in fact, is\nincomparable to $k$-WL for any fixed $k$. We empirically validate the\nexpressive and counting power of the proposed $r$-$\\ell{}$MPNN on several\nsynthetic datasets and present state-of-the-art predictive performance on\nvarious real-world datasets. The code is available at\nhttps://github.com/RPaolino/loopy\n","authors":["Raffaele Paolino","Sohir Maskey","Pascal Welke","Gitta Kutyniok"],"pdf_url":"https://arxiv.org/pdf/2403.13749v1.pdf","comment":"Accepted at ICLR 2024 Workshop on Bridging the Gap Between Practice\n  and Theory in Deep Learning"},{"id":"http://arxiv.org/abs/2403.13748v1","updated":"2024-03-20T16:56:08Z","published":"2024-03-20T16:56:08Z","title":"An Ordering of Divergences for Variational Inference with Factorized\n  Gaussian Approximations","summary":"  Given an intractable distribution $p$, the problem of variational inference\n(VI) is to compute the best approximation $q$ from some more tractable family\n$\\mathcal{Q}$. Most commonly the approximation is found by minimizing a\nKullback-Leibler (KL) divergence. However, there exist other valid choices of\ndivergences, and when $\\mathcal{Q}$ does not contain~$p$, each divergence\nchampions a different solution. We analyze how the choice of divergence affects\nthe outcome of VI when a Gaussian with a dense covariance matrix is\napproximated by a Gaussian with a diagonal covariance matrix. In this setting\nwe show that different divergences can be \\textit{ordered} by the amount that\ntheir variational approximations misestimate various measures of uncertainty,\nsuch as the variance, precision, and entropy. We also derive an impossibility\ntheorem showing that no two of these measures can be simultaneously matched by\na factorized approximation; hence, the choice of divergence informs which\nmeasure, if any, is correctly estimated. Our analysis covers the KL divergence,\nthe R\\'enyi divergences, and a score-based divergence that compares $\\nabla\\log\np$ and $\\nabla\\log q$. We empirically evaluate whether these orderings hold\nwhen VI is used to approximate non-Gaussian distributions.\n","authors":["Charles C. Margossian","Loucas Pillaud-Vivien","Lawrence K. Saul"],"pdf_url":"https://arxiv.org/pdf/2403.13748v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.05666v5","updated":"2024-03-20T16:50:25Z","published":"2023-02-11T11:56:06Z","title":"Jaccard Metric Losses: Optimizing the Jaccard Index with Soft Labels","summary":"  Intersection over Union (IoU) losses are surrogates that directly optimize\nthe Jaccard index. Leveraging IoU losses as part of the loss function have\ndemonstrated superior performance in semantic segmentation tasks compared to\noptimizing pixel-wise losses such as the cross-entropy loss alone. However, we\nidentify a lack of flexibility in these losses to support vital training\ntechniques like label smoothing, knowledge distillation, and semi-supervised\nlearning, mainly due to their inability to process soft labels. To address\nthis, we introduce Jaccard Metric Losses (JMLs), which are identical to the\nsoft Jaccard loss in standard settings with hard labels but are fully\ncompatible with soft labels. We apply JMLs to three prominent use cases of soft\nlabels: label smoothing, knowledge distillation and semi-supervised learning,\nand demonstrate their potential to enhance model accuracy and calibration. Our\nexperiments show consistent improvements over the cross-entropy loss across 4\nsemantic segmentation datasets (Cityscapes, PASCAL VOC, ADE20K, DeepGlobe Land)\nand 13 architectures, including classic CNNs and recent vision transformers.\nRemarkably, our straightforward approach significantly outperforms\nstate-of-the-art knowledge distillation and semi-supervised learning methods.\nThe code is available at\n\\href{https://github.com/zifuwanggg/JDTLosses}{https://github.com/zifuwanggg/JDTLosses}.\n","authors":["Zifu Wang","Xuefei Ning","Matthew B. Blaschko"],"pdf_url":"https://arxiv.org/pdf/2302.05666v5.pdf","comment":"NeurIPS 2023"},{"id":"http://arxiv.org/abs/2403.13740v1","updated":"2024-03-20T16:47:28Z","published":"2024-03-20T16:47:28Z","title":"Uncertainty-Aware Explanations Through Probabilistic Self-Explainable\n  Neural Networks","summary":"  The lack of transparency of Deep Neural Networks continues to be a limitation\nthat severely undermines their reliability and usage in high-stakes\napplications. Promising approaches to overcome such limitations are\nPrototype-Based Self-Explainable Neural Networks (PSENNs), whose predictions\nrely on the similarity between the input at hand and a set of prototypical\nrepresentations of the output classes, offering therefore a deep, yet\ntransparent-by-design, architecture. So far, such models have been designed by\nconsidering pointwise estimates for the prototypes, which remain fixed after\nthe learning phase of the model. In this paper, we introduce a probabilistic\nreformulation of PSENNs, called Prob-PSENN, which replaces point estimates for\nthe prototypes with probability distributions over their values. This provides\nnot only a more flexible framework for an end-to-end learning of prototypes,\nbut can also capture the explanatory uncertainty of the model, which is a\nmissing feature in previous approaches. In addition, since the prototypes\ndetermine both the explanation and the prediction, Prob-PSENNs allow us to\ndetect when the model is making uninformed or uncertain predictions, and to\nobtain valid explanations for them. Our experiments demonstrate that\nProb-PSENNs provide more meaningful and robust explanations than their\nnon-probabilistic counterparts, thus enhancing the explainability and\nreliability of the models.\n","authors":["Jon Vadillo","Roberto Santana","Jose A. Lozano","Marta Kwiatkowska"],"pdf_url":"https://arxiv.org/pdf/2403.13740v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2110.14961v3","updated":"2024-03-20T16:45:00Z","published":"2021-10-28T09:03:37Z","title":"Roto-translated Local Coordinate Frames For Interacting Dynamical\n  Systems","summary":"  Modelling interactions is critical in learning complex dynamical systems,\nnamely systems of interacting objects with highly non-linear and time-dependent\nbehaviour. A large class of such systems can be formalized as\n$\\textit{geometric graphs}$, $\\textit{i.e.}$, graphs with nodes positioned in\nthe Euclidean space given an $\\textit{arbitrarily}$ chosen global coordinate\nsystem, for instance vehicles in a traffic scene. Notwithstanding the arbitrary\nglobal coordinate system, the governing dynamics of the respective dynamical\nsystems are invariant to rotations and translations, also known as\n$\\textit{Galilean invariance}$. As ignoring these invariances leads to worse\ngeneralization, in this work we propose local coordinate frames per node-object\nto induce roto-translation invariance to the geometric graph of the interacting\ndynamical system. Further, the local coordinate frames allow for a natural\ndefinition of anisotropic filtering in graph neural networks. Experiments in\ntraffic scenes, 3D motion capture, and colliding particles demonstrate that the\nproposed approach comfortably outperforms the recent state-of-the-art.\n","authors":["Miltiadis Kofinas","Naveen Shankar Nagaraja","Efstratios Gavves"],"pdf_url":"https://arxiv.org/pdf/2110.14961v3.pdf","comment":"In NeurIPS 2021. Source code: https://github.com/mkofinas/locs"},{"id":"http://arxiv.org/abs/2403.13729v1","updated":"2024-03-20T16:39:17Z","published":"2024-03-20T16:39:17Z","title":"Reinforcement Learning for Online Testing of Autonomous Driving Systems:\n  a Replication and Extension Study","summary":"  In a recent study, Reinforcement Learning (RL) used in combination with\nmany-objective search, has been shown to outperform alternative techniques\n(random search and many-objective search) for online testing of Deep Neural\nNetwork-enabled systems. The empirical evaluation of these techniques was\nconducted on a state-of-the-art Autonomous Driving System (ADS). This work is a\nreplication and extension of that empirical study. Our replication shows that\nRL does not outperform pure random test generation in a comparison conducted\nunder the same settings of the original study, but with no confounding factor\ncoming from the way collisions are measured. Our extension aims at eliminating\nsome of the possible reasons for the poor performance of RL observed in our\nreplication: (1) the presence of reward components providing contrasting or\nuseless feedback to the RL agent; (2) the usage of an RL algorithm (Q-learning)\nwhich requires discretization of an intrinsically continuous state space.\nResults show that our new RL agent is able to converge to an effective policy\nthat outperforms random testing. Results also highlight other possible\nimprovements, which open to further investigations on how to best leverage RL\nfor online ADS testing.\n","authors":["Luca Giamattei","Matteo Biagiola","Roberto Pietrantuono","Stefano Russo","Paolo Tonella"],"pdf_url":"https://arxiv.org/pdf/2403.13729v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13728v1","updated":"2024-03-20T16:38:26Z","published":"2024-03-20T16:38:26Z","title":"M-HOF-Opt: Multi-Objective Hierarchical Output Feedback Optimization via\n  Multiplier Induced Loss Landscape Scheduling","summary":"  When a neural network parameterized loss function consists of many terms, the\ncombinatorial choice of weight multipliers during the optimization process\nforms a challenging problem. To address this, we proposed a probabilistic\ngraphical model (PGM) for the joint model parameter and multiplier evolution\nprocess, with a hypervolume based likelihood that promotes multi-objective\ndescent of each loss term. The corresponding parameter and multiplier\nestimation as a sequential decision process is then cast into an optimal\ncontrol problem, where the multi-objective descent goal is dispatched\nhierarchically into a series of constraint optimization sub-problems. The\nsub-problem constraint automatically adapts itself according to Pareto\ndominance and serves as the setpoint for the low level multiplier controller to\nschedule loss landscapes via output feedback of each loss term. Our method is\nmultiplier-free and operates at the timescale of epochs, thus saves tremendous\ncomputational resources compared to full training cycle multiplier tuning. We\napplied it to domain invariant variational auto-encoding with 6 loss terms on\nthe PACS domain generalization task, and observed robust performance across a\nrange of controller hyperparameters, as well as different multiplier initial\nconditions, outperforming other multiplier scheduling methods. We offered\nmodular implementation of our method, admitting custom definition of many loss\nterms for applying our multi-objective hierarchical output feedback training\nscheme to other deep learning fields.\n","authors":["Xudong Sun","Nutan Chen","Alexej Gossmann","Yu Xing","Carla Feistner","Emilio Dorigatt","Felix Drost","Daniele Scarcella","Lisa Beer","Carsten Marr"],"pdf_url":"https://arxiv.org/pdf/2403.13728v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.19212v3","updated":"2024-03-20T16:34:27Z","published":"2024-02-29T14:41:31Z","title":"Deep Reinforcement Learning: A Convex Optimization Approach","summary":"  In this paper, we consider reinforcement learning of nonlinear systems with\ncontinuous state and action spaces. We present an episodic learning algorithm,\nwhere we for each episode use convex optimization to find a two-layer neural\nnetwork approximation of the optimal $Q$-function. The convex optimization\napproach guarantees that the weights calculated at each episode are optimal,\nwith respect to the given sampled states and actions of the current episode.\nFor stable nonlinear systems, we show that the algorithm converges and that the\nconverging parameters of the trained neural network can be made arbitrarily\nclose to the optimal neural network parameters. In particular, if the\nregularization parameter is $\\rho$ and the time horizon is $T$, then the\nparameters of the trained neural network converge to $w$, where the distance\nbetween $w$ from the optimal parameters $w^\\star$ is bounded by\n$\\mathcal{O}(\\rho T^{-1})$. That is, when the number of episodes goes to\ninfinity, there exists a constant $C$ such that \\[\\|w-w^\\star\\| \\le\nC\\cdot\\frac{\\rho}{T}.\\] In particular, our algorithm converges arbitrarily\nclose to the optimal neural network parameters as the time horizon increases or\nas the regularization parameter decreases.\n","authors":["Ather Gattami"],"pdf_url":"https://arxiv.org/pdf/2402.19212v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13724v1","updated":"2024-03-20T16:33:06Z","published":"2024-03-20T16:33:06Z","title":"Probabilistic Forecasting with Stochastic Interpolants and Föllmer\n  Processes","summary":"  We propose a framework for probabilistic forecasting of dynamical systems\nbased on generative modeling. Given observations of the system state over time,\nwe formulate the forecasting problem as sampling from the conditional\ndistribution of the future system state given its current state. To this end,\nwe leverage the framework of stochastic interpolants, which facilitates the\nconstruction of a generative model between an arbitrary base distribution and\nthe target. We design a fictitious, non-physical stochastic dynamics that takes\nas initial condition the current system state and produces as output a sample\nfrom the target conditional distribution in finite time and without bias. This\nprocess therefore maps a point mass centered at the current state onto a\nprobabilistic ensemble of forecasts. We prove that the drift coefficient\nentering the stochastic differential equation (SDE) achieving this task is\nnon-singular, and that it can be learned efficiently by square loss regression\nover the time-series data. We show that the drift and the diffusion\ncoefficients of this SDE can be adjusted after training, and that a specific\nchoice that minimizes the impact of the estimation error gives a F\\\"ollmer\nprocess. We highlight the utility of our approach on several complex,\nhigh-dimensional forecasting problems, including stochastically forced\nNavier-Stokes and video prediction on the KTH and CLEVRER datasets.\n","authors":["Yifan Chen","Mark Goldstein","Mengjian Hua","Michael S. Albergo","Nicholas M. Boffi","Eric Vanden-Eijnden"],"pdf_url":"https://arxiv.org/pdf/2403.13724v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.18430v3","updated":"2024-03-20T16:32:42Z","published":"2023-10-27T19:02:22Z","title":"MCRAGE: Synthetic Healthcare Data for Fairness","summary":"  In the field of healthcare, electronic health records (EHR) serve as crucial\ntraining data for developing machine learning models for diagnosis, treatment,\nand the management of healthcare resources. However, medical datasets are often\nimbalanced in terms of sensitive attributes such as race/ethnicity, gender, and\nage. Machine learning models trained on class-imbalanced EHR datasets perform\nsignificantly worse in deployment for individuals of the minority classes\ncompared to those from majority classes, which may lead to inequitable\nhealthcare outcomes for minority groups. To address this challenge, we propose\nMinority Class Rebalancing through Augmentation by Generative modeling\n(MCRAGE), a novel approach to augment imbalanced datasets using samples\ngenerated by a deep generative model. The MCRAGE process involves training a\nConditional Denoising Diffusion Probabilistic Model (CDDPM) capable of\ngenerating high-quality synthetic EHR samples from underrepresented classes. We\nuse this synthetic data to augment the existing imbalanced dataset, resulting\nin a more balanced distribution across all classes, which can be used to train\nless biased downstream models. We measure the performance of MCRAGE versus\nalternative approaches using Accuracy, F1 score and AUROC of these downstream\nmodels. We provide theoretical justification for our method in terms of recent\nconvergence results for DDPMs.\n","authors":["Keira Behal","Jiayi Chen","Caleb Fikes","Sophia Xiao"],"pdf_url":"https://arxiv.org/pdf/2310.18430v3.pdf","comment":"Keywords: synthetic electronic health records, conditional denoising\n  diffusion probabilistic model, healthcare AI, tabular data, fairness,\n  synthetic data. This paper is the result of work completed at the 2023 Emory\n  University Department of Mathematics REU/RET program under the direction of\n  Project Advisor Dr. Xi Yuanzhe. This work is sponsored by NSF DMS 2051019"},{"id":"http://arxiv.org/abs/2310.13805v2","updated":"2024-03-20T16:23:20Z","published":"2023-10-20T20:32:43Z","title":"Normalizing flow-based deep variational Bayesian network for seismic\n  multi-hazards and impacts estimation from InSAR imagery","summary":"  Onsite disasters like earthquakes can trigger cascading hazards and impacts,\nsuch as landslides and infrastructure damage, leading to catastrophic losses;\nthus, rapid and accurate estimates are crucial for timely and effective\npost-disaster responses. Interferometric Synthetic aperture radar (InSAR) data\nis important in providing high-resolution onsite information for rapid hazard\nestimation. Most recent methods using InSAR imagery signals predict a single\ntype of hazard and thus often suffer low accuracy due to noisy and complex\nsignals induced by co-located hazards, impacts, and irrelevant environmental\nchanges (e.g., vegetation changes, human activities). We introduce a novel\nstochastic variational inference with normalizing flows derived to jointly\napproximate posteriors of multiple unobserved hazards and impacts from noisy\nInSAR imagery.\n","authors":["Xuechun Li","Paula M. Burgi","Wei Ma","Hae Young Noh","David J. Wald","Susu Xu"],"pdf_url":"https://arxiv.org/pdf/2310.13805v2.pdf","comment":"This paper needs to be reviewed by the USGS"},{"id":"http://arxiv.org/abs/2403.12143v2","updated":"2024-03-20T16:12:12Z","published":"2024-03-18T18:01:01Z","title":"Graph Neural Networks for Learning Equivariant Representations of Neural\n  Networks","summary":"  Neural networks that process the parameters of other neural networks find\napplications in domains as diverse as classifying implicit neural\nrepresentations, generating neural network weights, and predicting\ngeneralization errors. However, existing approaches either overlook the\ninherent permutation symmetry in the neural network or rely on intricate\nweight-sharing patterns to achieve equivariance, while ignoring the impact of\nthe network architecture itself. In this work, we propose to represent neural\nnetworks as computational graphs of parameters, which allows us to harness\npowerful graph neural networks and transformers that preserve permutation\nsymmetry. Consequently, our approach enables a single model to encode neural\ncomputational graphs with diverse architectures. We showcase the effectiveness\nof our method on a wide range of tasks, including classification and editing of\nimplicit neural representations, predicting generalization performance, and\nlearning to optimize, while consistently outperforming state-of-the-art\nmethods. The source code is open-sourced at\nhttps://github.com/mkofinas/neural-graphs.\n","authors":["Miltiadis Kofinas","Boris Knyazev","Yan Zhang","Yunlu Chen","Gertjan J. Burghouts","Efstratios Gavves","Cees G. M. Snoek","David W. Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.12143v2.pdf","comment":"In ICLR 2024. Source code: https://github.com/mkofinas/neural-graphs"},{"id":"http://arxiv.org/abs/2403.13704v1","updated":"2024-03-20T16:08:27Z","published":"2024-03-20T16:08:27Z","title":"Improving the Adaptive Moment Estimation (ADAM) stochastic optimizer\n  through an Implicit-Explicit (IMEX) time-stepping approach","summary":"  The Adam optimizer, often used in Machine Learning for neural network\ntraining, corresponds to an underlying ordinary differential equation (ODE) in\nthe limit of very small learning rates. This work shows that the classical Adam\nalgorithm is a first order implicit-explicit (IMEX) Euler discretization of the\nunderlying ODE. Employing the time discretization point of view, we propose new\nextensions of the Adam scheme obtained by using higher order IMEX methods to\nsolve the ODE. Based on this approach, we derive a new optimization algorithm\nfor neural network training that performs better than classical Adam on several\nregression and classification problems.\n","authors":["Abhinab Bhattacharjee","Andrey A. Popov","Arash Sarshar","Adrian Sandu"],"pdf_url":"https://arxiv.org/pdf/2403.13704v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13701v1","updated":"2024-03-20T16:06:01Z","published":"2024-03-20T16:06:01Z","title":"What Matters for Active Texture Recognition With Vision-Based Tactile\n  Sensors","summary":"  This paper explores active sensing strategies that employ vision-based\ntactile sensors for robotic perception and classification of fabric textures.\nWe formalize the active sampling problem in the context of tactile fabric\nrecognition and provide an implementation of information-theoretic exploration\nstrategies based on minimizing predictive entropy and variance of probabilistic\nmodels. Through ablation studies and human experiments, we investigate which\ncomponents are crucial for quick and reliable texture recognition. Along with\nthe active sampling strategies, we evaluate neural network architectures,\nrepresentations of uncertainty, influence of data augmentation, and dataset\nvariability. By evaluating our method on a previously published Active Clothing\nPerception Dataset and on a real robotic system, we establish that the choice\nof the active exploration strategy has only a minor influence on the\nrecognition accuracy, whereas data augmentation and dropout rate play a\nsignificantly larger role. In a comparison study, while humans achieve 66.9%\nrecognition accuracy, our best approach reaches 90.0% in under 5 touches,\nhighlighting that vision-based tactile sensors are highly effective for fabric\ntexture recognition.\n","authors":["Alina Böhm","Tim Schneider","Boris Belousov","Alap Kshirsagar","Lisa Lin","Katja Doerschner","Knut Drewing","Constantin A. Rothkopf","Jan Peters"],"pdf_url":"https://arxiv.org/pdf/2403.13701v1.pdf","comment":"7 pages, 9 figures, accepted at 2024 IEEE International Conference on\n  Robotics and Automation (ICRA)"},{"id":"http://arxiv.org/abs/2310.20679v2","updated":"2024-03-20T16:05:03Z","published":"2023-10-31T17:45:39Z","title":"Latent Field Discovery In Interacting Dynamical Systems With Neural\n  Fields","summary":"  Systems of interacting objects often evolve under the influence of field\neffects that govern their dynamics, yet previous works have abstracted away\nfrom such effects, and assume that systems evolve in a vacuum. In this work, we\nfocus on discovering these fields, and infer them from the observed dynamics\nalone, without directly observing them. We theorize the presence of latent\nforce fields, and propose neural fields to learn them. Since the observed\ndynamics constitute the net effect of local object interactions and global\nfield effects, recently popularized equivariant networks are inapplicable, as\nthey fail to capture global information. To address this, we propose to\ndisentangle local object interactions -- which are $\\mathrm{SE}(n)$ equivariant\nand depend on relative states -- from external global field effects -- which\ndepend on absolute states. We model interactions with equivariant graph\nnetworks, and combine them with neural fields in a novel graph network that\nintegrates field forces. Our experiments show that we can accurately discover\nthe underlying fields in charged particles settings, traffic scenes, and\ngravitational n-body problems, and effectively use them to learn the system and\nforecast future trajectories.\n","authors":["Miltiadis Kofinas","Erik J. Bekkers","Naveen Shankar Nagaraja","Efstratios Gavves"],"pdf_url":"https://arxiv.org/pdf/2310.20679v2.pdf","comment":"In NeurIPS 2023. Source code: https://github.com/mkofinas/aether"},{"id":"http://arxiv.org/abs/2403.13695v1","updated":"2024-03-20T15:57:44Z","published":"2024-03-20T15:57:44Z","title":"Loss Regularizing Robotic Terrain Classification","summary":"  Locomotion mechanics of legged robots are suitable when pacing through\ndifficult terrains. Recognising terrains for such robots are important to fully\nyoke the versatility of their movements. Consequently, robotic terrain\nclassification becomes significant to classify terrains in real time with high\naccuracy. The conventional classifiers suffer from overfitting problem, low\naccuracy problem, high variance problem, and not suitable for live dataset. On\nthe other hand, classifying a growing dataset is difficult for convolution\nbased terrain classification. Supervised recurrent models are also not\npractical for this classification. Further, the existing recurrent\narchitectures are still evolving to improve accuracy of terrain classification\nbased on live variable-length sensory data collected from legged robots. This\npaper proposes a new semi-supervised method for terrain classification of\nlegged robots, avoiding preprocessing of long variable-length dataset. The\nproposed method has a stacked Long Short-Term Memory architecture, including a\nnew loss regularization. The proposed method solves the existing problems and\nimproves accuracy. Comparison with the existing architectures show the\nimprovements.\n","authors":["Shakti Deo Kumar","Sudhanshu Tripathi","Krishna Ujjwal","Sarvada Sakshi Jha","Suddhasil De"],"pdf_url":"https://arxiv.org/pdf/2403.13695v1.pdf","comment":"Preliminary draft of the work published in IEEE conference 2023"},{"id":"http://arxiv.org/abs/2303.16296v4","updated":"2024-03-20T15:52:49Z","published":"2023-03-28T20:35:38Z","title":"Dice Semimetric Losses: Optimizing the Dice Score with Soft Labels","summary":"  The soft Dice loss (SDL) has taken a pivotal role in numerous automated\nsegmentation pipelines in the medical imaging community. Over the last years,\nsome reasons behind its superior functioning have been uncovered and further\noptimizations have been explored. However, there is currently no implementation\nthat supports its direct utilization in scenarios involving soft labels. Hence,\na synergy between the use of SDL and research leveraging the use of soft\nlabels, also in the context of model calibration, is still missing. In this\nwork, we introduce Dice semimetric losses (DMLs), which (i) are by design\nidentical to SDL in a standard setting with hard labels, but (ii) can be\nemployed in settings with soft labels. Our experiments on the public QUBIQ,\nLiTS and KiTS benchmarks confirm the potential synergy of DMLs with soft labels\n(e.g. averaging, label smoothing, and knowledge distillation) over hard labels\n(e.g. majority voting and random selection). As a result, we obtain superior\nDice scores and model calibration, which supports the wider adoption of DMLs in\npractice. The code is available at https://github.com/zifuwanggg/JDTLosses\n","authors":["Zifu Wang","Teodora Popordanoska","Jeroen Bertels","Robin Lemmens","Matthew B. Blaschko"],"pdf_url":"https://arxiv.org/pdf/2303.16296v4.pdf","comment":"MICCAI 2023"},{"id":"http://arxiv.org/abs/2305.09098v2","updated":"2024-03-20T15:41:07Z","published":"2023-05-16T01:51:22Z","title":"Weight-Inherited Distillation for Task-Agnostic BERT Compression","summary":"  Knowledge Distillation (KD) is a predominant approach for BERT compression.\nPrevious KD-based methods focus on designing extra alignment losses for the\nstudent model to mimic the behavior of the teacher model. These methods\ntransfer the knowledge in an indirect way. In this paper, we propose a novel\nWeight-Inherited Distillation (WID), which directly transfers knowledge from\nthe teacher. WID does not require any additional alignment loss and trains a\ncompact student by inheriting the weights, showing a new perspective of\nknowledge distillation. Specifically, we design the row compactors and column\ncompactors as mappings and then compress the weights via structural\nre-parameterization. Experimental results on the GLUE and SQuAD benchmarks show\nthat WID outperforms previous state-of-the-art KD-based baselines. Further\nanalysis indicates that WID can also learn the attention patterns from the\nteacher model without any alignment loss on attention distributions. The code\nis available at https://github.com/wutaiqiang/WID-NAACL2024.\n","authors":["Taiqiang Wu","Cheng Hou","Shanshan Lao","Jiayi Li","Ngai Wong","Zhe Zhao","Yujiu Yang"],"pdf_url":"https://arxiv.org/pdf/2305.09098v2.pdf","comment":"9 pages, 4 figures, NAACL2024 findings"},{"id":"http://arxiv.org/abs/2403.13681v1","updated":"2024-03-20T15:39:54Z","published":"2024-03-20T15:39:54Z","title":"PARAMANU-AYN: An Efficient Novel Generative and Instruction-tuned\n  Language Model for Indian Legal Case Documents","summary":"  In this paper, we present PARAMANU-AYN, a language model based exclusively on\ncase documents of the Supreme Court of India, the Constitution of India, and\nthe Indian Penal Code. The novel Auto Regressive (AR) decoder based model is\npretrained from scratch at a context size of 8192. We evaluated our pretrained\nlegal model on perplexity metrics. We also instruction-tuned our pretrained\nmodel on a set of 10,763 instructions covering various legal tasks such as\nlegal reasoning, judgement explanation, legal clause generation, legal\ndrafting, legal contract drafting, case summarization, constitutional\nquestion-answering, etc. We also evaluated the responses of prompts for\ninstruction-tuned models by GPT-3.5-Turbo on clarity, relevance, completeness,\nand legal reasoning metrics in a scale of 10. Our model can be run on CPU and\nachieved 42.46 tokens/sec CPU inference speed. We found that our models,\ndespite not being pretrained on legal books, various legal contracts, and legal\ndocuments, were able to learn the domain knowledge required for drafting\nvarious legal contracts and legal clauses, and generalize to draft legal\ncontracts and legal clauses with limited instruction tuning. Hence, we conclude\nthat for a strong domain-specialized generative language model (such as legal),\nvery large amounts of data are not required to develop models from scratch. We\nbelieve that this work is the first attempt to make a dedicated generative\nlegal language model from scratch for Indian Supreme Court jurisdiction or in\nlegal NLP overall. We plan to release our Paramanu-Ayn model at\nhttps://www.bharatgpts.com.\n","authors":["Mitodru Niyogi","Arnab Bhattacharya"],"pdf_url":"https://arxiv.org/pdf/2403.13681v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.05667v2","updated":"2024-03-20T15:33:23Z","published":"2024-02-08T13:38:23Z","title":"S$Ω$I: Score-based O-INFORMATION Estimation","summary":"  The analysis of scientific data and complex multivariate systems requires\ninformation quantities that capture relationships among multiple random\nvariables. Recently, new information-theoretic measures have been developed to\novercome the shortcomings of classical ones, such as mutual information, that\nare restricted to considering pairwise interactions. Among them, the concept of\ninformation synergy and redundancy is crucial for understanding the high-order\ndependencies between variables. One of the most prominent and versatile\nmeasures based on this concept is O-information, which provides a clear and\nscalable way to quantify the synergy-redundancy balance in multivariate\nsystems. However, its practical application is limited to simplified cases. In\nthis work, we introduce S$\\Omega$I, which allows for the first time to compute\nO-information without restrictive assumptions about the system. Our experiments\nvalidate our approach on synthetic data, and demonstrate the effectiveness of\nS$\\Omega$I in the context of a real-world use case.\n","authors":["Mustapha Bounoua","Giulio Franzese","Pietro Michiardi"],"pdf_url":"https://arxiv.org/pdf/2402.05667v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10026v2","updated":"2024-03-20T15:30:31Z","published":"2023-11-16T17:14:26Z","title":"Guaranteeing Control Requirements via Reward Shaping in Reinforcement\n  Learning","summary":"  In addressing control problems such as regulation and tracking through\nreinforcement learning, it is often required to guarantee that the acquired\npolicy meets essential performance and stability criteria such as a desired\nsettling time and steady-state error prior to deployment. Motivated by this\nnecessity, we present a set of results and a systematic reward shaping\nprocedure that (i) ensures the optimal policy generates trajectories that align\nwith specified control requirements and (ii) allows to assess whether any given\npolicy satisfies them. We validate our approach through comprehensive numerical\nexperiments conducted in two representative environments from OpenAI Gym: the\nInverted Pendulum swing-up problem and the Lunar Lander. Utilizing both tabular\nand deep reinforcement learning methods, our experiments consistently affirm\nthe efficacy of our proposed framework, highlighting its effectiveness in\nensuring policy adherence to the prescribed control requirements.\n","authors":["Francesco De Lellis","Marco Coraggio","Giovanni Russo","Mirco Musolesi","Mario di Bernardo"],"pdf_url":"https://arxiv.org/pdf/2311.10026v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13672v1","updated":"2024-03-20T15:29:59Z","published":"2024-03-20T15:29:59Z","title":"Machine Learning Optimized Approach for Parameter Selection in MESHFREE\n  Simulations","summary":"  Meshfree simulation methods are emerging as compelling alternatives to\nconventional mesh-based approaches, particularly in the fields of Computational\nFluid Dynamics (CFD) and continuum mechanics. In this publication, we provide a\ncomprehensive overview of our research combining Machine Learning (ML) and\nFraunhofer's MESHFREE software (www.meshfree.eu), a powerful tool utilizing a\nnumerical point cloud in a Generalized Finite Difference Method (GFDM). This\ntool enables the effective handling of complex flow domains, moving geometries,\nand free surfaces, while allowing users to finely tune local refinement and\nquality parameters for an optimal balance between computation time and results\naccuracy. However, manually determining the optimal parameter combination poses\nchallenges, especially for less experienced users. We introduce a novel\nML-optimized approach, using active learning, regression trees, and\nvisualization on MESHFREE simulation data, demonstrating the impact of input\ncombinations on results quality and computation time. This research contributes\nvaluable insights into parameter optimization in meshfree simulations,\nenhancing accessibility and usability for a broader user base in scientific and\nengineering applications.\n","authors":["Paulami Banerjee","Mohan Padmanabha","Chaitanya Sanghavi","Isabel Michel","Simone Gramsch"],"pdf_url":"https://arxiv.org/pdf/2403.13672v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12166v2","updated":"2024-03-20T15:27:44Z","published":"2024-03-18T18:30:22Z","title":"The Power of Few: Accelerating and Enhancing Data Reweighting with\n  Coreset Selection","summary":"  As machine learning tasks continue to evolve, the trend has been to gather\nlarger datasets and train increasingly larger models. While this has led to\nadvancements in accuracy, it has also escalated computational costs to\nunsustainable levels. Addressing this, our work aims to strike a delicate\nbalance between computational efficiency and model accuracy, a persisting\nchallenge in the field. We introduce a novel method that employs core subset\nselection for reweighting, effectively optimizing both computational time and\nmodel performance. By focusing on a strategically selected coreset, our\napproach offers a robust representation, as it efficiently minimizes the\ninfluence of outliers. The re-calibrated weights are then mapped back to and\npropagated across the entire dataset. Our experimental results substantiate the\neffectiveness of this approach, underscoring its potential as a scalable and\nprecise solution for model training.\n","authors":["Mohammad Jafari","Yimeng Zhang","Yihua Zhang","Sijia Liu"],"pdf_url":"https://arxiv.org/pdf/2403.12166v2.pdf","comment":"Accepted to ICASSP 2024"},{"id":"http://arxiv.org/abs/2310.10705v4","updated":"2024-03-20T15:26:55Z","published":"2023-10-16T14:46:45Z","title":"Observational and Experimental Insights into Machine Learning-Based\n  Defect Classification in Wafers","summary":"  This survey paper offers a comprehensive review of methodologies utilizing\nmachine learning (ML) classification techniques for identifying wafer defects\nin semiconductor manufacturing. Despite the growing body of research\ndemonstrating the effectiveness of ML in wafer defect identification, there is\na noticeable absence of comprehensive reviews on this subject. This survey\nattempts to fill this void by amalgamating available literature and providing\nan in-depth analysis of the advantages, limitations, and potential applications\nof various ML classification algorithms in the realm of wafer defect detection.\nAn innovative taxonomy of methodologies that we present provides a detailed\nclassification of algorithms into more refined categories and techniques. This\ntaxonomy follows a three-tier structure, starting from broad methodology\ncategories and ending with specific techniques. It aids researchers in\ncomprehending the complex relationships between different algorithms and their\ntechniques. We employ a rigorous Observational and experimental evaluation to\nrank these varying techniques. For the Observational evaluation, we assess\ntechniques based on a set of four criteria. The experimental evaluation ranks\nthe algorithms employing the same techniques, sub-categories, and categories.\nAlso the paper illuminates the future prospects of ML classification techniques\nfor wafer defect identification, underscoring potential advancements and\nopportunities for further research in this field\n","authors":["Kamal Taha"],"pdf_url":"https://arxiv.org/pdf/2310.10705v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.00477v2","updated":"2024-03-20T15:25:02Z","published":"2023-12-01T10:18:50Z","title":"Interpretable Meta-Learning of Physical Systems","summary":"  Machine learning methods can be a valuable aid in the scientific process, but\nthey need to face challenging settings where data come from inhomogeneous\nexperimental conditions. Recent meta-learning methods have made significant\nprogress in multi-task learning, but they rely on black-box neural networks,\nresulting in high computational costs and limited interpretability. Leveraging\nthe structure of the learning problem, we argue that multi-environment\ngeneralization can be achieved using a simpler learning model, with an affine\nstructure with respect to the learning task. Crucially, we prove that this\narchitecture can identify the physical parameters of the system, enabling\ninterpreable learning. We demonstrate the competitive generalization\nperformance and the low computational cost of our method by comparing it to\nstate-of-the-art algorithms on physical systems, ranging from toy models to\ncomplex, non-analytical systems. The interpretability of our method is\nillustrated with original applications to physical-parameter-induced adaptation\nand to adaptive control.\n","authors":["Matthieu Blanke","Marc Lelarge"],"pdf_url":"https://arxiv.org/pdf/2312.00477v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.00618v2","updated":"2024-03-20T15:17:43Z","published":"2023-07-02T17:18:17Z","title":"Bounce: Reliable High-Dimensional Bayesian Optimization for\n  Combinatorial and Mixed Spaces","summary":"  Impactful applications such as materials discovery, hardware design, neural\narchitecture search, or portfolio optimization require optimizing\nhigh-dimensional black-box functions with mixed and combinatorial input spaces.\nWhile Bayesian optimization has recently made significant progress in solving\nsuch problems, an in-depth analysis reveals that the current state-of-the-art\nmethods are not reliable. Their performances degrade substantially when the\nunknown optima of the function do not have a certain structure. To fill the\nneed for a reliable algorithm for combinatorial and mixed spaces, this paper\nproposes Bounce that relies on a novel map of various variable types into\nnested embeddings of increasing dimensionality. Comprehensive experiments show\nthat Bounce reliably achieves and often even improves upon state-of-the-art\nperformance on a variety of high-dimensional problems.\n","authors":["Leonard Papenmeier","Luigi Nardi","Matthias Poloczek"],"pdf_url":"https://arxiv.org/pdf/2307.00618v2.pdf","comment":"30 pages, 22 figures"},{"id":"http://arxiv.org/abs/2305.11288v2","updated":"2024-03-20T15:10:09Z","published":"2023-05-18T20:12:22Z","title":"Riemannian Multinomial Logistics Regression for SPD Neural Networks","summary":"  Deep neural networks for learning Symmetric Positive Definite (SPD) matrices\nare gaining increasing attention in machine learning. Despite the significant\nprogress, most existing SPD networks use traditional Euclidean classifiers on\nan approximated space rather than intrinsic classifiers that accurately capture\nthe geometry of SPD manifolds. Inspired by Hyperbolic Neural Networks (HNNs),\nwe propose Riemannian Multinomial Logistics Regression (RMLR) for the\nclassification layers in SPD networks. We introduce a unified framework for\nbuilding Riemannian classifiers under the metrics pulled back from the\nEuclidean space, and showcase our framework under the parameterized\nLog-Euclidean Metric (LEM) and Log-Cholesky Metric (LCM). Besides, our\nframework offers a novel intrinsic explanation for the most popular LogEig\nclassifier in existing SPD networks. The effectiveness of our method is\ndemonstrated in three applications: radar recognition, human action\nrecognition, and electroencephalography (EEG) classification. The code is\navailable at https://github.com/GitZH-Chen/SPDMLR.git.\n","authors":["Ziheng Chen","Yue Song","Gaowen Liu","Ramana Rao Kompella","Xiaojun Wu","Nicu Sebe"],"pdf_url":"https://arxiv.org/pdf/2305.11288v2.pdf","comment":"Accepted to CVPR 2024"},{"id":"http://arxiv.org/abs/2402.03818v2","updated":"2024-03-20T15:08:27Z","published":"2024-02-06T09:07:26Z","title":"Asymptotic generalization error of a single-layer graph convolutional\n  network","summary":"  While graph convolutional networks show great practical promises, the\ntheoretical understanding of their generalization properties as a function of\nthe number of samples is still in its infancy compared to the more broadly\nstudied case of supervised fully connected neural networks. In this article, we\npredict the performances of a single-layer graph convolutional network (GCN)\ntrained on data produced by attributed stochastic block models (SBMs) in the\nhigh-dimensional limit. Previously, only ridge regression on contextual-SBM\n(CSBM) has been considered in Shi et al. 2022; we generalize the analysis to\narbitrary convex loss and regularization for the CSBM and add the analysis for\nanother data model, the neural-prior SBM. We also study the high\nsignal-to-noise ratio limit, detail the convergence rates of the GCN and show\nthat, while consistent, it does not reach the Bayes-optimal rate for any of the\nconsidered cases.\n","authors":["O. Duranthon","L. Zdeborová"],"pdf_url":"https://arxiv.org/pdf/2402.03818v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13658v1","updated":"2024-03-20T15:06:49Z","published":"2024-03-20T15:06:49Z","title":"Multimodal Variational Autoencoder for Low-cost Cardiac Hemodynamics\n  Instability Detection","summary":"  Recent advancements in non-invasive detection of cardiac hemodynamic\ninstability (CHDI) primarily focus on applying machine learning techniques to a\nsingle data modality, e.g. cardiac magnetic resonance imaging (MRI). Despite\ntheir potential, these approaches often fall short especially when the size of\nlabeled patient data is limited, a common challenge in the medical domain.\nFurthermore, only a few studies have explored multimodal methods to study CHDI,\nwhich mostly rely on costly modalities such as cardiac MRI and echocardiogram.\nIn response to these limitations, we propose a novel multimodal variational\nautoencoder ($\\text{CardioVAE}_\\text{X,G}$) to integrate low-cost chest X-ray\n(CXR) and electrocardiogram (ECG) modalities with pre-training on a large\nunlabeled dataset. Specifically, $\\text{CardioVAE}_\\text{X,G}$ introduces a\nnovel tri-stream pre-training strategy to learn both shared and\nmodality-specific features, thus enabling fine-tuning with both unimodal and\nmultimodal datasets. We pre-train $\\text{CardioVAE}_\\text{X,G}$ on a large,\nunlabeled dataset of $50,982$ subjects from a subset of MIMIC database and then\nfine-tune the pre-trained model on a labeled dataset of $795$ subjects from the\nASPIRE registry. Comprehensive evaluations against existing methods show that\n$\\text{CardioVAE}_\\text{X,G}$ offers promising performance (AUROC $=0.79$ and\nAccuracy $=0.77$), representing a significant step forward in non-invasive\nprediction of CHDI. Our model also excels in producing fine interpretations of\npredictions directly associated with clinical features, thereby supporting\nclinical decision-making.\n","authors":["Mohammod N. I. Suvon","Prasun C. Tripathi","Wenrui Fan","Shuo Zhou","Xianyuan Liu","Samer Alabed","Venet Osmani","Andrew J. Swift","Chen Chen","Haiping Lu"],"pdf_url":"https://arxiv.org/pdf/2403.13658v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.07999v3","updated":"2024-03-20T14:30:41Z","published":"2024-02-12T19:04:32Z","title":"NetInfoF Framework: Measuring and Exploiting Network Usable Information","summary":"  Given a node-attributed graph, and a graph task (link prediction or node\nclassification), can we tell if a graph neural network (GNN) will perform well?\nMore specifically, do the graph structure and the node features carry enough\nusable information for the task? Our goals are (1) to develop a fast tool to\nmeasure how much information is in the graph structure and in the node\nfeatures, and (2) to exploit the information to solve the task, if there is\nenough. We propose NetInfoF, a framework including NetInfoF_Probe and\nNetInfoF_Act, for the measurement and the exploitation of network usable\ninformation (NUI), respectively. Given a graph data, NetInfoF_Probe measures\nNUI without any model training, and NetInfoF_Act solves link prediction and\nnode classification, while two modules share the same backbone. In summary,\nNetInfoF has following notable advantages: (a) General, handling both link\nprediction and node classification; (b) Principled, with theoretical guarantee\nand closed-form solution; (c) Effective, thanks to the proposed adjustment to\nnode similarity; (d) Scalable, scaling linearly with the input size. In our\ncarefully designed synthetic datasets, NetInfoF correctly identifies the ground\ntruth of NUI and is the only method being robust to all graph scenarios.\nApplied on real-world datasets, NetInfoF wins in 11 out of 12 times on link\nprediction compared to general GNN baselines.\n","authors":["Meng-Chieh Lee","Haiyang Yu","Jian Zhang","Vassilis N. Ioannidis","Xiang Song","Soji Adeshina","Da Zheng","Christos Faloutsos"],"pdf_url":"https://arxiv.org/pdf/2402.07999v3.pdf","comment":"Accepted to ICLR 2024 (Spotlight)"},{"id":"http://arxiv.org/abs/2310.00117v3","updated":"2024-03-20T14:26:12Z","published":"2023-09-29T20:11:15Z","title":"ABScribe: Rapid Exploration & Organization of Multiple Writing\n  Variations in Human-AI Co-Writing Tasks using Large Language Models","summary":"  Exploring alternative ideas by rewriting text is integral to the writing\nprocess. State-of-the-art Large Language Models (LLMs) can simplify writing\nvariation generation. However, current interfaces pose challenges for\nsimultaneous consideration of multiple variations: creating new variations\nwithout overwriting text can be difficult, and pasting them sequentially can\nclutter documents, increasing workload and disrupting writers' flow. To tackle\nthis, we present ABScribe, an interface that supports rapid, yet visually\nstructured, exploration and organization of writing variations in human-AI\nco-writing tasks. With ABScribe, users can swiftly modify variations using LLM\nprompts, which are auto-converted into reusable buttons. Variations are stored\nadjacently within text fields for rapid in-place comparisons using mouse-over\ninteractions on a popup toolbar. Our user study with 12 writers shows that\nABScribe significantly reduces task workload (d = 1.20, p < 0.001), enhances\nuser perceptions of the revision process (d = 2.41, p < 0.001) compared to a\npopular baseline workflow, and provides insights into how writers explore\nvariations using LLMs.\n","authors":["Mohi Reza","Nathan Laundry","Ilya Musabirov","Peter Dushniku","Zhi Yuan \"Michael\" Yu","Kashish Mittal","Tovi Grossman","Michael Liut","Anastasia Kuzminykh","Joseph Jay Williams"],"pdf_url":"https://arxiv.org/pdf/2310.00117v3.pdf","comment":"CHI 2024"}],"Distributed, Parallel, and Cluster Computing":[{"id":"http://arxiv.org/abs/2403.13716v1","updated":"2024-03-20T16:22:53Z","published":"2024-03-20T16:22:53Z","title":"Agent-based MST Construction","summary":"  {\\em Minimum-weight spanning tree} (MST) is one of the fundamental and\nwell-studied problems in distributed computing. In this paper, we initiate the\nstudy of constructing MST using mobile agents (aka robots). Suppose $n$ agents\nare positioned initially arbitrarily on the nodes of a connected, undirected,\narbitrary, anonymous, port-labeled, weighted $n$-node, $m$-edge graph $G$ of\ndiameter $D$ and maximum degree $\\Delta$. The agents relocate themselves\nautonomously and compute an MST of $G$ such that exactly one agent positions on\na node and tracks in its memory which of its adjacent edges belong to the MST.\nThe objective is to minimize time and memory requirements. Following the\nliterature, we consider the synchronous setting in which each agent performs\nits operations synchronously with others and hence time can be measured in\nrounds. We first establish a generic result: if $n$ and $\\Delta$ are known a\npriori and memory per agent is as much as node memory in the message-passing\nmodel (of distributed computing), agents can simulate any $O(T)$-round\ndeterministic algorithm for any problem in the message-passing model to the\nagent model in $O(\\Delta T \\log n+n\\log^2n)$ rounds. As a corollary, MST can be\nconstructed in the agent model in $O(\\max\\{\\Delta \\sqrt{n} \\log n \\log^*n,\n\\Delta D \\log n,n\\log^2n\\})$ rounds simulating the celebrated $O(\\sqrt{n}\n\\log^*n +D)$-round GKP algorithm for MST in the message-passing model. We then\nestablish that, without knowing any graph parameter a priori, there exists a\ndeterministic algorithm to construct MST in the agent model in $O(m+n\\log n)$\nrounds with $O(n \\log n)$ bits memory at each agent. The presented algorithm\nneeds to overcome highly non-trivial challenges on how to synchronize agents in\ncomputing MST as they may initially be positioned arbitrarily on the graph\nnodes.\n","authors":["Ajay D. Kshemkalyani","Manish Kumar","Anisur Rahaman Molla","Gokarna Sharma"],"pdf_url":"https://arxiv.org/pdf/2403.13716v1.pdf","comment":"26 pages"},{"id":"http://arxiv.org/abs/2403.13644v1","updated":"2024-03-20T14:50:54Z","published":"2024-03-20T14:50:54Z","title":"How to Relax Instantly: Elastic Relaxation of Concurrent Data Structures","summary":"  The sequential semantics of many concurrent data structures, such as stacks\nand queues, inevitably lead to memory contention in parallel environments, thus\nlimiting scalability. Semantic relaxation has the potential to address this\nissue, increasing the parallelism at the expense of weakened semantics.\nAlthough prior research has shown that improved performance can be attained by\nrelaxing concurrent data structure semantics, there is no one-size-fits-all\nrelaxation that adequately addresses the varying needs of dynamic executions.\n  In this paper, we first introduce the concept of elastic relaxation and\nconsequently present the Lateral structure, which is an algorithmic component\ncapable of supporting the design of elastically relaxed concurrent data\nstructures. Using the Lateral , we design novel elastically relaxed, lock-free\nqueues and stacks capable of reconfiguring relaxation during run time. We\nestablish linearizability and define upper bounds for relaxation errors in our\ndesigns. Experimental evaluations show that our elastic designs hold up against\nstate-of-the-art statically relaxed designs, while also swiftly managing\ntrade-offs between relaxation and operational latency. We also outline how to\nuse the Lateral to design elastically relaxed lock-free counters and deques.\n","authors":["Kåre von Geijer","Philippas Tsigas"],"pdf_url":"https://arxiv.org/pdf/2403.13644v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13629v1","updated":"2024-03-20T14:27:49Z","published":"2024-03-20T14:27:49Z","title":"CheckMate: Evaluating Checkpointing Protocols for Streaming Dataflows","summary":"  Stream processing in the last decade has seen broad adoption in both\ncommercial and research settings. One key element for this success is the\nability of modern stream processors to handle failures while ensuring\nexactly-once processing guarantees. At the moment of writing, virtually all\nstream processors that guarantee exactly-once processing implement a variant of\nApache Flink's coordinated checkpoints - an extension of the original\nChandy-Lamport checkpoints from 1985. However, the reasons behind this\nprevalence of the coordinated approach remain anecdotal, as reported by\npractitioners of the stream processing community. At the same time, common\ncheckpointing approaches, such as the uncoordinated and the\ncommunication-induced ones, remain largely unexplored.\n  This paper is the first to address this gap by i) shedding light on why\npractitioners have favored the coordinated approach and ii) by investigating\nwhether there are viable alternatives. To this end, we implement three\ncheckpointing approaches that we surveyed and adapted for the distinct needs of\nstreaming dataflows. Our analysis shows that the coordinated approach\noutperforms the uncoordinated and communication-induced protocols under\nuniformly distributed workloads. To our surprise, however, the uncoordinated\napproach is not only competitive to the coordinated one in uniformly\ndistributed workloads, but it also outperforms the coordinated approach in\nskewed workloads. We conclude that rather than blindly employing coordinated\ncheckpointing, research should focus on optimizing the very promising\nuncoordinated approach, as it can address issues with skew and support\nprevalent cyclic queries. We believe that our findings can trigger further\nresearch into checkpointing mechanisms.\n","authors":["George Siachamis","Kyriakos Psarakis","Marios Fragkoulis","Arie van Deursen","Paris Carbone","Asterios Katsifodimos"],"pdf_url":"https://arxiv.org/pdf/2403.13629v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13619v1","updated":"2024-03-20T14:13:44Z","published":"2024-03-20T14:13:44Z","title":"Dynamic Resource Allocation for Virtual Machine Migration Optimization\n  using Machine Learning","summary":"  The paragraph is grammatically correct and logically coherent. It discusses\nthe importance of mobile terminal cloud computing migration technology in\nmeeting the demands of evolving computer and cloud computing technologies. It\nemphasizes the need for efficient data access and storage, as well as the\nutilization of cloud computing migration technology to prevent additional time\ndelays. The paragraph also highlights the contributions of cloud computing\nmigration technology to expanding cloud computing services. Additionally, it\nacknowledges the role of virtualization as a fundamental capability of cloud\ncomputing while emphasizing that cloud computing and virtualization are not\ninherently interconnected. Finally, it introduces machine learning-based\nvirtual machine migration optimization and dynamic resource allocation as a\ncritical research direction in cloud computing, citing the limitations of\nstatic rules or manual settings in traditional cloud computing environments.\nOverall, the paragraph effectively communicates the importance of machine\nlearning technology in addressing resource allocation and virtual machine\nmigration challenges in cloud computing.\n","authors":["Yulu Gong","Jiaxin Huang","Bo Liu","Jingyu Xu","Binbin Wu","Yifan Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.13619v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07858v2","updated":"2024-03-20T13:08:25Z","published":"2024-03-12T17:48:11Z","title":"Accelerating Biclique Counting on GPU","summary":"  Counting (p,q)-bicliques in bipartite graphs poses a foundational challenge\nwith broad applications, from densest subgraph discovery in algorithmic\nresearch to personalized content recommendation in practical scenarios. Despite\nits significance, current leading (p,q)-biclique counting algorithms fall\nshort, particularly when faced with larger graph sizes and clique scales.\nFortunately, the problem's inherent structure, allowing for the independent\ncounting of each biclique starting from every vertex, combined with a\nsubstantial set intersections, makes it highly amenable to parallelization.\nRecent successes in GPU-accelerated algorithms across various domains motivate\nour exploration into harnessing the parallelism power of GPUs to efficiently\naddress the (p,q)-biclique counting challenge. We introduce GBC (GPU-based\nBiclique Counting), a novel approach designed to enable efficient and scalable\n(p,q)-biclique counting on GPUs. To address major bottleneck arising from\nredundant comparisons in set intersections (occupying an average of 90% of the\nruntime), we introduce a novel data structure that hashes adjacency lists into\ntruncated bitmaps to enable efficient set intersection on GPUs via bit-wise AND\noperations. Our innovative hybrid DFS-BFS exploration strategy further enhances\nthread utilization and effectively manages memory constraints. A composite load\nbalancing strategy, integrating pre-runtime and runtime workload allocation,\nensures equitable distribution among threads. Additionally, we employ vertex\nreordering and graph partitioning strategies for improved compactness and\nscalability. Experimental evaluations on eight real-life and two synthetic\ndatasets demonstrate that GBC outperforms state-of-the-art algorithms by a\nsubstantial margin. In particular, GBC achieves an average speedup of 497.8x,\nwith the largest instance achieving a remarkable 1217.7x speedup when p = q =\n8.\n","authors":["Linshan Qiu","Zhonggen Li","Xiangyu Ke","Lu Chen","Yunjun Gao"],"pdf_url":"https://arxiv.org/pdf/2403.07858v2.pdf","comment":"This paper has been accepted by ICDE24"},{"id":"http://arxiv.org/abs/2003.03255v2","updated":"2024-03-20T11:00:58Z","published":"2020-03-06T14:53:44Z","title":"The Topology of Local Computing in Networks","summary":"  Modeling distributed computing in a way enabling the use of formal methods is\na challenge that has been approached from different angles, among which two\ntechniques emerged at the turn of the century: protocol complexes, and directed\nalgebraic topology. In both cases, the considered computational model generally\nassumes communication via shared objects, typically a shared memory consisting\nof a collection of read-write registers. Our paper is concerned with network\ncomputing, where the processes are located at the nodes of a network, and\ncommunicate by exchanging messages along the edges of that network. Applying\nthe topological approach for verification in network computing is a\nconsiderable challenge, mainly because the presence of identifiers assigned to\nthe nodes yields protocol complexes whose size grows exponentially with the\nsize of the underlying network. However, many of the problems studied in this\ncontext are of local nature, and their definitions do not depend on the\nidentifiers or on the size of the network. We leverage this independence in\norder to meet the above challenge, and present $\\textit{local}$ protocol\ncomplexes, whose sizes do not depend on the size of the network. As an\napplication of the design of \"compact\" protocol complexes, we reformulate the\ncelebrated lower bound of $\\Omega(\\log^*n)$ rounds for 3-coloring the $n$-node\nring, in the algebraic topology framework.\n","authors":["Pierre Fraigniaud","Ami Paz"],"pdf_url":"https://arxiv.org/pdf/2003.03255v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.19364v2","updated":"2024-03-20T10:13:01Z","published":"2024-02-29T17:13:25Z","title":"Arrow Matrix Decomposition: A Novel Approach for Communication-Efficient\n  Sparse Matrix Multiplication","summary":"  We propose a novel approach to iterated sparse matrix dense matrix\nmultiplication, a fundamental computational kernel in scientific computing and\ngraph neural network training. In cases where matrix sizes exceed the memory of\na single compute node, data transfer becomes a bottleneck. An approach based on\ndense matrix multiplication algorithms leads to suboptimal scalability and\nfails to exploit the sparsity in the problem. To address these challenges, we\npropose decomposing the sparse matrix into a small number of highly structured\nmatrices called arrow matrices, which are connected by permutations. Our\napproach enables communication-avoiding multiplications, achieving a polynomial\nreduction in communication volume per iteration for matrices corresponding to\nplanar graphs and other minor-excluded families of graphs. Our evaluation\ndemonstrates that our approach outperforms a state-of-the-art method for sparse\nmatrix multiplication on matrices with hundreds of millions of rows, offering\nnear-linear strong and weak scaling.\n","authors":["Lukas Gianinazzi","Alexandros Nikolaos Ziogas","Langwen Huang","Piotr Luczynski","Saleh Ashkboos","Florian Scheidl","Armon Carigiet","Chio Ge","Nabil Abubaker","Maciej Besta","Tal Ben-Nun","Torsten Hoefler"],"pdf_url":"https://arxiv.org/pdf/2402.19364v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13454v1","updated":"2024-03-20T09:59:58Z","published":"2024-03-20T09:59:58Z","title":"Adaptive time step selection for Spectral Deferred Corrections","summary":"  Spectral Deferred Corrections (SDC) is an iterative method for the numerical\nsolution of ordinary differential equations. It works by refining the numerical\nsolution for an initial value problem by approximately solving differential\nequations for the error, and can be interpreted as a preconditioned fixed-point\niteration for solving the fully implicit collocation problem. We adopt\ntechniques from embedded Runge-Kutta Methods (RKM) to SDC in order to provide a\nmechanism for adaptive time step size selection and thus increase computational\nefficiency of SDC. We propose two SDC-specific estimates of the local error\nthat are generic and require only minimal problem specific tuning. We\ndemonstrate a gain in efficiency over standard SDC with fixed step size,\ncompare efficiency favorably against state-of-the-art adaptive RKM and show\nthat due to its iterative nature, adaptive SDC can cope efficiently with silent\ndata corruption.\n","authors":["Thomas Baumann","Sebastian Götschel","Thibaut Lunet","Daniel Ruprecht","Robert Speck"],"pdf_url":"https://arxiv.org/pdf/2403.13454v1.pdf","comment":"34 pages including references, 12 figures. Submitted to Springer\n  Numerical Algorithms"},{"id":"http://arxiv.org/abs/2403.13411v1","updated":"2024-03-20T08:53:35Z","published":"2024-03-20T08:53:35Z","title":"Optimal Fixed Priority Scheduling in Multi-Stage Multi-Resource\n  Distributed Real-Time Systems","summary":"  This work studies fixed priority (FP) scheduling of real-time jobs with\nend-to-end deadlines in a distributed system. Specifically, given a multi-stage\npipeline with multiple heterogeneous resources of the same type at each stage,\nthe problem is to assign priorities to a set of real-time jobs with different\nrelease times to access a resource at each stage of the pipeline subject to the\nend-to-end deadline constraints. Note, in such a system, jobs may compete with\ndifferent sets of jobs at different stages of the pipeline depending on the\njob-to-resource mapping. To this end, following are the two major contributions\nof this work. We show that an OPA-compatible schedulability test based on the\ndelay composition algebra can be constructed, which we then use with an optimal\npriority assignment algorithm to compute a priority ordering. Further, we\nestablish the versatility of pairwise priority assignment in such a multi-stage\nmulti-resource system, compared to a total priority ordering. In particular, we\nshow that a pairwise priority assignment may be feasible even if a priority\nordering does not exist. We propose an integer linear programming formulation\nand a scalable heuristic to compute a pairwise priority assignment. We also\nshow through simulation experiments that the proposed approaches can be used\nfor the holistic scheduling of real-time jobs in edge computing systems.\n","authors":["Niraj Kumar","Chuanchao Gao","Arvind Easwaran"],"pdf_url":"https://arxiv.org/pdf/2403.13411v1.pdf","comment":"Accepted in DATE (Design, Automation and Test in Europe Conference)\n  2024"},{"id":"http://arxiv.org/abs/2403.13393v1","updated":"2024-03-20T08:35:14Z","published":"2024-03-20T08:35:14Z","title":"Causal Graph Dynamics and Kan Extensions","summary":"  On the one side, the formalism of Global Transformations comes with the claim\nof capturing any transformation of space that is local, synchronous and\ndeterministic.The claim has been proven for different classes of models such as\nmesh refinements from computer graphics, Lindenmayer systems from morphogenesis\nmodeling and cellular automata from biological, physical and parallel\ncomputation modeling.The Global Transformation formalism achieves this by using\ncategory theory for its genericity, and more precisely the notion of Kan\nextension to determine the global behaviors based on the local ones.On the\nother side, Causal Graph Dynamics describe the transformation of port graphs in\na synchronous and deterministic way and has not yet being tackled.In this\npaper, we show the precise sense in which the claim of Global Transformations\nholds for them as well.This is done by showing different ways in which they can\nbe expressed as Kan extensions, each of them highlighting different features of\nCausal Graph Dynamics.Along the way, this work uncovers the interesting class\nof Monotonic Causal Graph Dynamics and their universality among General Causal\nGraph Dynamics.\n","authors":["Luidnel Maignan","Antoine Spicher"],"pdf_url":"https://arxiv.org/pdf/2403.13393v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13287v1","updated":"2024-03-20T03:59:10Z","published":"2024-03-20T03:59:10Z","title":"Regent based parallel meshfree LSKUM solver for heterogenous HPC\n  platforms","summary":"  Regent is an implicitly parallel programming language that allows the\ndevelopment of a single codebase for heterogeneous platforms targeting CPUs and\nGPUs. This paper presents the development of a parallel meshfree solver in\nRegent for two-dimensional inviscid compressible flows. The meshfree solver is\nbased on the least squares kinetic upwind method. Example codes are presented\nto show the difference between the Regent and CUDA-C implementations of the\nmeshfree solver on a GPU node. For CPU parallel computations, details are\npresented on how the data communication and synchronisation are handled by\nRegent and Fortran+MPI codes. The Regent solver is verified by applying it to\nthe standard test cases for inviscid flows. Benchmark simulations are performed\non coarse to very fine point distributions to assess the solver's performance.\nThe computational efficiency of the Regent solver on an A100 GPU is compared\nwith an equivalent meshfree solver written in CUDA-C. The codes are then\nprofiled to investigate the differences in their performance. The performance\nof the Regent solver on CPU cores is compared with an equivalent explicitly\nparallel Fortran meshfree solver based on MPI. Scalability results are shown to\noffer insights into performance.\n","authors":["Sanath Salil","Nischay Ram Mamidi","Anil Nemili","Elliott Slaughter"],"pdf_url":"https://arxiv.org/pdf/2403.13287v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.13279v2","updated":"2024-03-20T02:25:36Z","published":"2023-11-22T09:55:20Z","title":"Comprehensive Evaluation of GNN Training Systems: A Data Management\n  Perspective","summary":"  Many Graph Neural Network (GNN) training systems have emerged recently to\nsupport efficient GNN training. Since GNNs embody complex data dependencies\nbetween training samples, the training of GNNs should address distinct\nchallenges different from DNN training in data management, such as data\npartitioning, batch preparation for mini-batch training, and data transferring\nbetween CPUs and GPUs. These factors, which take up a large proportion of\ntraining time, make data management in GNN training more significant. This\npaper reviews GNN training from a data management perspective and provides a\ncomprehensive analysis and evaluation of the representative approaches. We\nconduct extensive experiments on various benchmark datasets and show many\ninteresting and valuable results. We also provide some practical tips learned\nfrom these experiments, which are helpful for designing GNN training systems in\nthe future.\n","authors":["Hao Yuan","Yajiong Liu","Yanfeng Zhang","Xin Ai","Qiange Wang","Chaoyi Chen","Yu Gu","Ge Yu"],"pdf_url":"https://arxiv.org/pdf/2311.13279v2.pdf","comment":"12 pages, 18 figures. (Accepted by VLDB 2024)"},{"id":"http://arxiv.org/abs/2403.13247v1","updated":"2024-03-20T02:17:47Z","published":"2024-03-20T02:17:47Z","title":"Decentralized Federated Learning: Model Update Tracking Under Imperfect\n  Information Sharing","summary":"  A novel Decentralized Noisy Model Update Tracking Federated Learning\nalgorithm (FedNMUT) is proposed, which is tailored to function efficiently in\nthe presence of noisy communication channels that reflect imperfect information\nexchange. This algorithm uses gradient tracking to minimize the impact of data\nheterogeneity while minimizing communication overhead. The proposed algorithm\nincorporates noise into its parameters to mimic the conditions of noisy\ncommunication channels, thereby enabling consensus among clients through a\ncommunication graph topology in such challenging environments. FedNMUT\nprioritizes parameter sharing and noise incorporation to increase the\nresilience of decentralized learning systems against noisy communications.\nThrough theoretical and empirical validation, it is demonstrated that the\nperformance of FedNMUT is superior compared to the existing state-of-the-art\nmethods and conventional parameter-mixing approaches in dealing with imperfect\ninformation sharing. This proves the capability of the proposed algorithm to\ncounteract the negative effects of communication noise in a decentralized\nlearning framework.\n","authors":["Vishnu Pandi Chellapandi","Antesh Upadhyay","Abolfazl Hashemi","Stanislaw H. Żak"],"pdf_url":"https://arxiv.org/pdf/2403.13247v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2303.10695"},{"id":"http://arxiv.org/abs/2403.13245v1","updated":"2024-03-20T02:16:54Z","published":"2024-03-20T02:16:54Z","title":"Federated reinforcement learning for robot motion planning with\n  zero-shot generalization","summary":"  This paper considers the problem of learning a control policy for robot\nmotion planning with zero-shot generalization, i.e., no data collection and\npolicy adaptation is needed when the learned policy is deployed in new\nenvironments. We develop a federated reinforcement learning framework that\nenables collaborative learning of multiple learners and a central server, i.e.,\nthe Cloud, without sharing their raw data. In each iteration, each learner\nuploads its local control policy and the corresponding estimated normalized\narrival time to the Cloud, which then computes the global optimum among the\nlearners and broadcasts the optimal policy to the learners. Each learner then\nselects between its local control policy and that from the Cloud for next\niteration. The proposed framework leverages on the derived zero-shot\ngeneralization guarantees on arrival time and safety. Theoretical guarantees on\nalmost-sure convergence, almost consensus, Pareto improvement and optimality\ngap are also provided. Monte Carlo simulation is conducted to evaluate the\nproposed framework.\n","authors":["Zhenyuan Yuan","Siyuan Xu","Minghui Zhu"],"pdf_url":"https://arxiv.org/pdf/2403.13245v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13918v1","updated":"2024-03-20T18:39:47Z","published":"2024-03-20T18:39:47Z","title":"Automated Calibration of Parallel and Distributed Computing Simulators:\n  A Case Study","summary":"  Many parallel and distributed computing research results are obtained in\nsimulation, using simulators that mimic real-world executions on some target\nsystem. Each such simulator is configured by picking values for parameters that\ndefine the behavior of the underlying simulation models it implements. The main\nconcern for a simulator is accuracy: simulated behaviors should be as close as\npossible to those observed in the real-world target system. This requires that\nvalues for each of the simulator's parameters be carefully picked, or\n\"calibrated,\" based on ground-truth real-world executions. Examining the\ncurrent state of the art shows that simulator calibration, at least in the\nfield of parallel and distributed computing, is often undocumented (and thus\nperhaps often not performed) and, when documented, is described as a\nlabor-intensive, manual process. In this work we evaluate the benefit of\nautomating simulation calibration using simple algorithms. Specifically, we use\na real-world case study from the field of High Energy Physics and compare\nautomated calibration to calibration performed by a domain scientist. Our main\nfinding is that automated calibration is on par with or significantly\noutperforms the calibration performed by the domain scientist. Furthermore,\nautomated calibration makes it straightforward to operate desirable trade-offs\nbetween simulation accuracy and simulation speed.\n","authors":["Jesse McDonald","Maximilian Horzela","Frédéric Suter","Henri Casanova"],"pdf_url":"https://arxiv.org/pdf/2403.13918v1.pdf","comment":"To appear in Proc. of the 25th IEEE International Workshop on\n  Parallel and Distributed Scientific and Engineering Computing (PDSEC 2024)"}],"Performance":[{"id":"http://arxiv.org/abs/2403.13656v1","updated":"2024-03-20T15:01:40Z","published":"2024-03-20T15:01:40Z","title":"Network Calculus Bounds for Time-Sensitive Networks: A Revisit","summary":"  Network calculus (NC), particularly its min-plus branch, has been extensively\nutilized to construct service models and compute delay bounds for\ntime-sensitive networks (TSNs). This paper provides a revisit to the\nfundamental results. In particular, counterexamples to the most basic min-plus\nservice models, which have been proposed for TSNs and used for computing delay\nbounds, indicate that the packetization effect has often been overlooked. To\naddress, the max-plus branch of NC is also considered in this paper, whose\nmodels handle packetized traffic more explicitly. It is found that mapping the\nmin-plus models to the max-plus models may bring in an immediate improvement\nover delay bounds derived from the min-plus analysis. In addition, an\nintegrated analytical approach that combines models from both the min-plus and\nthe max-plus NC branches is introduced. In this approach, the max-plus\n$g$-server model is extended and the extended model, called $g^{x}$-server, is\nused together with the min-plus arrival curve traffic model. By applying the\nintegrated NC approach, service and delay bounds are derived for several\nsettings that are fundamental in TSNs.\n","authors":["Yuming Jiang"],"pdf_url":"https://arxiv.org/pdf/2403.13656v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13497v1","updated":"2024-03-20T10:56:58Z","published":"2024-03-20T10:56:58Z","title":"Starlink on the Road: A First Look at Mobile Starlink Performance in\n  Central Europe","summary":"  Low Earth Orbit Satellite Networks such as Starlink promise to provide\nworld-wide Internet access. While traditionally designed for stationary use, a\nnew dish, released in April 2023 in Europe, provides mobile Internet access\nincluding in-motion usage, e.g., while mounted on a car. In this paper, we\ndesign and build a mobile measurement setup. Our goal is to fully autonomously\nconduct continuous Starlink measurements while the car is in motion. We share\nour practical experiences, including challenges regarding the permanent power\nsupply. We measure the Starlink performance over the span of two months from\nmid-January to mid-March 2024 when the car is in motion. The measurements\nconsist of all relevant network parameters, such as the download and upload\nthroughput, the RTT, and packet loss, as well as detailed power consumption\ndata. We analyze our dataset to assess Starlink's mobile performance in Central\nEurope, Germany, and compare it to stationary measurements in proximity. We\nfind that the mobile performance is significantly worse than stationary\nperformance. The power consumption of the new dish is higher, but seems to be\nmore correlated to the heating function of the dish than to the speed of the\nvehicle.\n","authors":["Dominic Laniewski","Eric Lanfer","Simon Beginn","Jan Dunker","Michael Dückers","Nils Aschenbruck"],"pdf_url":"https://arxiv.org/pdf/2403.13497v1.pdf","comment":"This work has been submitted to the 2024 Network Traffic Measurement\n  and Analysis Conference (TMA) for possible publication. Copyright may be\n  transferred without notice, after which this version may no longer be\n  accessible"},{"id":"http://arxiv.org/abs/2403.13918v1","updated":"2024-03-20T18:39:47Z","published":"2024-03-20T18:39:47Z","title":"Automated Calibration of Parallel and Distributed Computing Simulators:\n  A Case Study","summary":"  Many parallel and distributed computing research results are obtained in\nsimulation, using simulators that mimic real-world executions on some target\nsystem. Each such simulator is configured by picking values for parameters that\ndefine the behavior of the underlying simulation models it implements. The main\nconcern for a simulator is accuracy: simulated behaviors should be as close as\npossible to those observed in the real-world target system. This requires that\nvalues for each of the simulator's parameters be carefully picked, or\n\"calibrated,\" based on ground-truth real-world executions. Examining the\ncurrent state of the art shows that simulator calibration, at least in the\nfield of parallel and distributed computing, is often undocumented (and thus\nperhaps often not performed) and, when documented, is described as a\nlabor-intensive, manual process. In this work we evaluate the benefit of\nautomating simulation calibration using simple algorithms. Specifically, we use\na real-world case study from the field of High Energy Physics and compare\nautomated calibration to calibration performed by a domain scientist. Our main\nfinding is that automated calibration is on par with or significantly\noutperforms the calibration performed by the domain scientist. Furthermore,\nautomated calibration makes it straightforward to operate desirable trade-offs\nbetween simulation accuracy and simulation speed.\n","authors":["Jesse McDonald","Maximilian Horzela","Frédéric Suter","Henri Casanova"],"pdf_url":"https://arxiv.org/pdf/2403.13918v1.pdf","comment":"To appear in Proc. of the 25th IEEE International Workshop on\n  Parallel and Distributed Scientific and Engineering Computing (PDSEC 2024)"}],"Artificial Intelligence":[{"id":"http://arxiv.org/abs/2403.13808v1","updated":"2024-03-20T17:59:58Z","published":"2024-03-20T17:59:58Z","title":"On Pretraining Data Diversity for Self-Supervised Learning","summary":"  We explore the impact of training with more diverse datasets, characterized\nby the number of unique samples, on the performance of self-supervised learning\n(SSL) under a fixed computational budget. Our findings consistently demonstrate\nthat increasing pretraining data diversity enhances SSL performance, albeit\nonly when the distribution distance to the downstream data is minimal. Notably,\neven with an exceptionally large pretraining data diversity achieved through\nmethods like web crawling or diffusion-generated data, among other ways, the\ndistribution shift remains a challenge. Our experiments are comprehensive with\nseven SSL methods using large-scale datasets such as ImageNet and YFCC100M\namounting to over 200 GPU days. Code and trained models will be available at\nhttps://github.com/hammoudhasan/DiversitySSL .\n","authors":["Hasan Abed Al Kader Hammoud","Tuhin Das","Fabio Pizzati","Philip Torr","Adel Bibi","Bernard Ghanem"],"pdf_url":"https://arxiv.org/pdf/2403.13808v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2403.13805v1","updated":"2024-03-20T17:59:55Z","published":"2024-03-20T17:59:55Z","title":"RAR: Retrieving And Ranking Augmented MLLMs for Visual Recognition","summary":"  CLIP (Contrastive Language-Image Pre-training) uses contrastive learning from\nnoise image-text pairs to excel at recognizing a wide array of candidates, yet\nits focus on broad associations hinders the precision in distinguishing subtle\ndifferences among fine-grained items. Conversely, Multimodal Large Language\nModels (MLLMs) excel at classifying fine-grained categories, thanks to their\nsubstantial knowledge from pre-training on web-level corpora. However, the\nperformance of MLLMs declines with an increase in category numbers, primarily\ndue to growing complexity and constraints of limited context window size. To\nsynergize the strengths of both approaches and enhance the few-shot/zero-shot\nrecognition abilities for datasets characterized by extensive and fine-grained\nvocabularies, this paper introduces RAR, a Retrieving And Ranking augmented\nmethod for MLLMs. We initially establish a multi-modal retriever based on CLIP\nto create and store explicit memory for different categories beyond the\nimmediate context window. During inference, RAR retrieves the top-k similar\nresults from the memory and uses MLLMs to rank and make the final predictions.\nOur proposed approach not only addresses the inherent limitations in\nfine-grained recognition but also preserves the model's comprehensive knowledge\nbase, significantly boosting accuracy across a range of vision-language\nrecognition tasks. Notably, our approach demonstrates a significant improvement\nin performance on 5 fine-grained visual recognition benchmarks, 11 few-shot\nimage recognition datasets, and the 2 object detection datasets under the\nzero-shot recognition setting.\n","authors":["Ziyu Liu","Zeyi Sun","Yuhang Zang","Wei Li","Pan Zhang","Xiaoyi Dong","Yuanjun Xiong","Dahua Lin","Jiaqi Wang"],"pdf_url":"https://arxiv.org/pdf/2403.13805v1.pdf","comment":"Project: https://github.com/Liuziyu77/RAR"},{"id":"http://arxiv.org/abs/2403.13802v1","updated":"2024-03-20T17:59:14Z","published":"2024-03-20T17:59:14Z","title":"ZigMa: Zigzag Mamba Diffusion Model","summary":"  The diffusion model has long been plagued by scalability and quadratic\ncomplexity issues, especially within transformer-based structures. In this\nstudy, we aim to leverage the long sequence modeling capability of a\nState-Space Model called Mamba to extend its applicability to visual data\ngeneration. Firstly, we identify a critical oversight in most current\nMamba-based vision methods, namely the lack of consideration for spatial\ncontinuity in the scan scheme of Mamba. Secondly, building upon this insight,\nwe introduce a simple, plug-and-play, zero-parameter method named Zigzag Mamba,\nwhich outperforms Mamba-based baselines and demonstrates improved speed and\nmemory utilization compared to transformer-based baselines. Lastly, we\nintegrate Zigzag Mamba with the Stochastic Interpolant framework to investigate\nthe scalability of the model on large-resolution visual datasets, such as\nFacesHQ $1024\\times 1024$ and UCF101, MultiModal-CelebA-HQ, and MS COCO\n$256\\times 256$. Code will be released at https://taohu.me/zigma/\n","authors":["Vincent Tao Hu","Stefan Andreas Baumann","Ming Gui","Olga Grebenkova","Pingchuan Ma","Johannes Fischer","Bjorn Ommer"],"pdf_url":"https://arxiv.org/pdf/2403.13802v1.pdf","comment":"Project Page: https://taohu.me/zigma/"},{"id":"http://arxiv.org/abs/2403.13801v1","updated":"2024-03-20T17:58:12Z","published":"2024-03-20T17:58:12Z","title":"Natural Language as Polices: Reasoning for Coordinate-Level Embodied\n  Control with LLMs","summary":"  We demonstrate experimental results with LLMs that address robotics action\nplanning problems. Recently, LLMs have been applied in robotics action\nplanning, particularly using a code generation approach that converts complex\nhigh-level instructions into mid-level policy codes. In contrast, our approach\nacquires text descriptions of the task and scene objects, then formulates\naction planning through natural language reasoning, and outputs coordinate\nlevel control commands, thus reducing the necessity for intermediate\nrepresentation code as policies. Our approach is evaluated on a multi-modal\nprompt simulation benchmark, demonstrating that our prompt engineering\nexperiments with natural language reasoning significantly enhance success rates\ncompared to its absence. Furthermore, our approach illustrates the potential\nfor natural language descriptions to transfer robotics skills from known tasks\nto previously unseen tasks.\n","authors":["Yusuke Mikami","Andrew Melnik","Jun Miura","Ville Hautamäki"],"pdf_url":"https://arxiv.org/pdf/2403.13801v1.pdf","comment":"8 pages, 2 figures"},{"id":"http://arxiv.org/abs/2312.06644v2","updated":"2024-03-20T17:58:05Z","published":"2023-12-11T18:56:37Z","title":"AnyHome: Open-Vocabulary Generation of Structured and Textured 3D Homes","summary":"  Inspired by cognitive theories, we introduce AnyHome, a framework that\ntranslates any text into well-structured and textured indoor scenes at a\nhouse-scale. By prompting Large Language Models (LLMs) with designed templates,\nour approach converts provided textual narratives into amodal structured\nrepresentations. These representations guarantee consistent and realistic\nspatial layouts by directing the synthesis of a geometry mesh within defined\nconstraints. A Score Distillation Sampling process is then employed to refine\nthe geometry, followed by an egocentric inpainting process that adds lifelike\ntextures to it. AnyHome stands out with its editability, customizability,\ndiversity, and realism. The structured representations for scenes allow for\nextensive editing at varying levels of granularity. Capable of interpreting\ntexts ranging from simple labels to detailed narratives, AnyHome generates\ndetailed geometries and textures that outperform existing methods in both\nquantitative and qualitative measures.\n","authors":["Rao Fu","Zehao Wen","Zichen Liu","Srinath Sridhar"],"pdf_url":"https://arxiv.org/pdf/2312.06644v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13799v1","updated":"2024-03-20T17:55:35Z","published":"2024-03-20T17:55:35Z","title":"Reverse Training to Nurse the Reversal Curse","summary":"  Large language models (LLMs) have a surprising failure: when trained on \"A\nhas a feature B\", they do not generalize to \"B is a feature of A\", which is\ntermed the Reversal Curse. Even when training with trillions of tokens this\nissue still appears due to Zipf's law - hence even if we train on the entire\ninternet. This work proposes an alternative training scheme, called reverse\ntraining, whereby all words are used twice, doubling the amount of available\ntokens. The LLM is trained in both forward and reverse directions by reversing\nthe training strings while preserving (i.e., not reversing) chosen substrings,\nsuch as entities. We show that data-matched reverse-trained models provide\nsuperior performance to standard models on standard tasks, and compute-matched\nreverse-trained models provide far superior performance on reversal tasks,\nhelping resolve the reversal curse issue.\n","authors":["Olga Golovneva","Zeyuan Allen-Zhu","Jason Weston","Sainbayar Sukhbaatar"],"pdf_url":"https://arxiv.org/pdf/2403.13799v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13798v1","updated":"2024-03-20T17:55:21Z","published":"2024-03-20T17:55:21Z","title":"Hierarchical NeuroSymbolic Approach for Action Quality Assessment","summary":"  Action quality assessment (AQA) applies computer vision to quantitatively\nassess the performance or execution of a human action. Current AQA approaches\nare end-to-end neural models, which lack transparency and tend to be biased\nbecause they are trained on subjective human judgements as ground-truth. To\naddress these issues, we introduce a neuro-symbolic paradigm for AQA, which\nuses neural networks to abstract interpretable symbols from video data and\nmakes quality assessments by applying rules to those symbols. We take diving as\nthe case study. We found that domain experts prefer our system and find it more\ninformative than purely neural approaches to AQA in diving. Our system also\nachieves state-of-the-art action recognition and temporal segmentation, and\nautomatically generates a detailed report that breaks the dive down into its\nelements and provides objective scoring with visual evidence. As verified by a\ngroup of domain experts, this report may be used to assist judges in scoring,\nhelp train judges, and provide feedback to divers. We will open-source all of\nour annotated training data and code for ease of reproducibility.\n","authors":["Lauren Okamoto","Paritosh Parmar"],"pdf_url":"https://arxiv.org/pdf/2403.13798v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13784v1","updated":"2024-03-20T17:47:08Z","published":"2024-03-20T17:47:08Z","title":"The Model Openness Framework: Promoting Completeness and Openness for\n  Reproducibility, Transparency and Usability in AI","summary":"  Generative AI (GAI) offers unprecedented possibilities but its\ncommercialization has raised concerns about transparency, reproducibility,\nbias, and safety. Many \"open-source\" GAI models lack the necessary components\nfor full understanding and reproduction, and some use restrictive licenses, a\npractice known as \"openwashing.\" We propose the Model Openness Framework (MOF),\na ranked classification system that rates machine learning models based on\ntheir completeness and openness, following principles of open science, open\nsource, open data, and open access. The MOF requires specific components of the\nmodel development lifecycle to be included and released under appropriate open\nlicenses. This framework aims to prevent misrepresentation of models claiming\nto be open, guide researchers and developers in providing all model components\nunder permissive licenses, and help companies, academia, and hobbyists identify\nmodels that can be safely adopted without restrictions. Wide adoption of the\nMOF will foster a more open AI ecosystem, accelerating research, innovation,\nand adoption.\n","authors":["Matt White","Ibrahim Haddad","Cailean Osborne","Xiao-Yang Liu","Ahmed Abdelmonsef","Sachin Varghese"],"pdf_url":"https://arxiv.org/pdf/2403.13784v1.pdf","comment":"45 pages"},{"id":"http://arxiv.org/abs/2403.13780v1","updated":"2024-03-20T17:42:08Z","published":"2024-03-20T17:42:08Z","title":"Information-Theoretic Distillation for Reference-less Summarization","summary":"  The current winning recipe for automatic summarization is using proprietary\nlarge-scale language models (LLMs) such as ChatGPT as is, or imitation learning\nfrom them as teacher models. While increasingly ubiquitous dependence on such\nlarge-scale language models is convenient, there remains an important question\nof whether small-scale models could have achieved competitive results, if we\nwere to seek an alternative learning method -- that allows for a more\ncost-efficient, controllable, yet powerful summarizer. We present InfoSumm, a\nnovel framework to distill a powerful summarizer based on the\ninformation-theoretic objective for summarization, without relying on either\nthe LLM's capability or human-written references. To achieve this, we first\npropose a novel formulation of the desiderata of summarization (saliency,\nfaithfulness and brevity) through the lens of mutual information between the\noriginal document and the summary. Based on this formulation, we start off from\nPythia-2.8B as the teacher model, which is not yet capable of summarization,\nthen self-train the model to optimize for the information-centric measures of\nideal summaries. Distilling from the improved teacher, we arrive at a compact\nbut powerful summarizer with only 568M parameters that performs competitively\nagainst ChatGPT, without ever relying on ChatGPT's capabilities. Extensive\nanalysis demonstrates that our approach outperforms in-domain supervised models\nin human evaluation, let alone state-of-the-art unsupervised methods, and wins\nover ChatGPT in controllable summarization.\n","authors":["Jaehun Jung","Ximing Lu","Liwei Jiang","Faeze Brahman","Peter West","Pang Wei Koh","Yejin Choi"],"pdf_url":"https://arxiv.org/pdf/2403.13780v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.09368v2","updated":"2024-03-20T17:36:35Z","published":"2024-02-14T18:13:51Z","title":"Magic-Me: Identity-Specific Video Customized Diffusion","summary":"  Creating content with specified identities (ID) has attracted significant\ninterest in the field of generative models. In the field of text-to-image\ngeneration (T2I), subject-driven creation has achieved great progress with the\nidentity controlled via reference images. However, its extension to video\ngeneration is not well explored. In this work, we propose a simple yet\neffective subject identity controllable video generation framework, termed\nVideo Custom Diffusion (VCD). With a specified identity defined by a few\nimages, VCD reinforces the identity characteristics and injects frame-wise\ncorrelation at the initialization stage for stable video outputs. To achieve\nthis, we propose three novel components that are essential for high-quality\nidentity preservation and stable video generation: 1) a noise initialization\nmethod with 3D Gaussian Noise Prior for better inter-frame stability; 2) an ID\nmodule based on extended Textual Inversion trained with the cropped identity to\ndisentangle the ID information from the background 3) Face VCD and Tiled VCD\nmodules to reinforce faces and upscale the video to higher resolution while\npreserving the identity's features. We conducted extensive experiments to\nverify that VCD is able to generate stable videos with better ID over the\nbaselines. Besides, with the transferability of the encoded identity in the ID\nmodule, VCD is also working well with personalized text-to-image models\navailable publicly. The codes are available at\nhttps://github.com/Zhen-Dong/Magic-Me.\n","authors":["Ze Ma","Daquan Zhou","Chun-Hsiao Yeh","Xue-She Wang","Xiuyu Li","Huanrui Yang","Zhen Dong","Kurt Keutzer","Jiashi Feng"],"pdf_url":"https://arxiv.org/pdf/2402.09368v2.pdf","comment":"Project Page at https://magic-me-webpage.github.io"},{"id":"http://arxiv.org/abs/2306.06192v5","updated":"2024-03-20T17:36:07Z","published":"2023-06-09T18:45:15Z","title":"Ada-NAV: Adaptive Trajectory Length-Based Sample Efficient Policy\n  Learning for Robotic Navigation","summary":"  Trajectory length stands as a crucial hyperparameter within reinforcement\nlearning (RL) algorithms, significantly contributing to the sample inefficiency\nin robotics applications. Motivated by the pivotal role trajectory length plays\nin the training process, we introduce Ada-NAV, a novel adaptive trajectory\nlength scheme designed to enhance the training sample efficiency of RL\nalgorithms in robotic navigation tasks. Unlike traditional approaches that\ntreat trajectory length as a fixed hyperparameter, we propose to dynamically\nadjust it based on the entropy of the underlying navigation policy.\nInterestingly, Ada-NAV can be applied to both existing on-policy and off-policy\nRL methods, which we demonstrate by empirically validating its efficacy on\nthree popular RL methods: REINFORCE, Proximal Policy Optimization (PPO), and\nSoft Actor-Critic (SAC). We demonstrate through simulated and real-world\nrobotic experiments that Ada-NAV outperforms conventional methods that employ\nconstant or randomly sampled trajectory lengths. Specifically, for a fixed\nsample budget, Ada-NAV achieves an 18\\% increase in navigation success rate, a\n20-38\\% reduction in navigation path length, and a 9.32\\% decrease in elevation\ncosts. Furthermore, we showcase the versatility of Ada-NAV by integrating it\nwith the Clearpath Husky robot, illustrating its applicability in complex\noutdoor environments.\n","authors":["Bhrij Patel","Kasun Weerakoon","Wesley A. Suttle","Alec Koppel","Brian M. Sadler","Tianyi Zhou","Amrit Singh Bedi","Dinesh Manocha"],"pdf_url":"https://arxiv.org/pdf/2306.06192v5.pdf","comment":"11 pages, 9 figures, 2 tables"},{"id":"http://arxiv.org/abs/2403.13765v1","updated":"2024-03-20T17:28:17Z","published":"2024-03-20T17:28:17Z","title":"Towards Principled Representation Learning from Videos for Reinforcement\n  Learning","summary":"  We study pre-training representations for decision-making using video data,\nwhich is abundantly available for tasks such as game agents and software\ntesting. Even though significant empirical advances have been made on this\nproblem, a theoretical understanding remains absent. We initiate the\ntheoretical investigation into principled approaches for representation\nlearning and focus on learning the latent state representations of the\nunderlying MDP using video data. We study two types of settings: one where\nthere is iid noise in the observation, and a more challenging setting where\nthere is also the presence of exogenous noise, which is non-iid noise that is\ntemporally correlated, such as the motion of people or cars in the background.\nWe study three commonly used approaches: autoencoding, temporal contrastive\nlearning, and forward modeling. We prove upper bounds for temporal contrastive\nlearning and forward modeling in the presence of only iid noise. We show that\nthese approaches can learn the latent state and use it to do efficient\ndownstream RL with polynomial sample complexity. When exogenous noise is also\npresent, we establish a lower bound result showing that the sample complexity\nof learning from video data can be exponentially worse than learning from\naction-labeled trajectory data. This partially explains why reinforcement\nlearning with video pre-training is hard. We evaluate these representational\nlearning methods in two visual domains, yielding results that are consistent\nwith our theoretical findings.\n","authors":["Dipendra Misra","Akanksha Saran","Tengyang Xie","Alex Lamb","John Langford"],"pdf_url":"https://arxiv.org/pdf/2403.13765v1.pdf","comment":"ICLR 2024 Spotlight Conference Paper"},{"id":"http://arxiv.org/abs/2312.00651v2","updated":"2024-03-20T17:28:02Z","published":"2023-12-01T15:24:38Z","title":"TrackDiffusion: Tracklet-Conditioned Video Generation via Diffusion\n  Models","summary":"  Despite remarkable achievements in video synthesis, achieving granular\ncontrol over complex dynamics, such as nuanced movement among multiple\ninteracting objects, still presents a significant hurdle for dynamic world\nmodeling, compounded by the necessity to manage appearance and disappearance,\ndrastic scale changes, and ensure consistency for instances across frames.\nThese challenges hinder the development of video generation that can faithfully\nmimic real-world complexity, limiting utility for applications requiring\nhigh-level realism and controllability, including advanced scene simulation and\ntraining of perception systems. To address that, we propose TrackDiffusion, a\nnovel video generation framework affording fine-grained trajectory-conditioned\nmotion control via diffusion models, which facilitates the precise manipulation\nof the object trajectories and interactions, overcoming the prevalent\nlimitation of scale and continuity disruptions. A pivotal component of\nTrackDiffusion is the instance enhancer, which explicitly ensures inter-frame\nconsistency of multiple objects, a critical factor overlooked in the current\nliterature. Moreover, we demonstrate that generated video sequences by our\nTrackDiffusion can be used as training data for visual perception models. To\nthe best of our knowledge, this is the first work to apply video diffusion\nmodels with tracklet conditions and demonstrate that generated frames can be\nbeneficial for improving the performance of object trackers.\n","authors":["Pengxiang Li","Kai Chen","Zhili Liu","Ruiyuan Gao","Lanqing Hong","Guo Zhou","Hua Yao","Dit-Yan Yeung","Huchuan Lu","Xu Jia"],"pdf_url":"https://arxiv.org/pdf/2312.00651v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.14456v4","updated":"2024-03-20T17:16:37Z","published":"2023-05-23T18:27:51Z","title":"Having Beer after Prayer? Measuring Cultural Bias in Large Language\n  Models","summary":"  As the reach of large language models (LMs) expands globally, their ability\nto cater to diverse cultural contexts becomes crucial. Despite advancements in\nmultilingual capabilities, models are not designed with appropriate cultural\nnuances. In this paper, we show that multilingual and Arabic monolingual LMs\nexhibit bias towards entities associated with Western culture. We introduce\nCAMeL, a novel resource of 628 naturally-occurring prompts and 20,368 entities\nspanning eight types that contrast Arab and Western cultures. CAMeL provides a\nfoundation for measuring cultural biases in LMs through both extrinsic and\nintrinsic evaluations. Using CAMeL, we examine the cross-cultural performance\nin Arabic of 16 different LMs on tasks such as story generation, NER, and\nsentiment analysis, where we find concerning cases of stereotyping and cultural\nunfairness. We further test their text-infilling performance, revealing the\nincapability of appropriate adaptation to Arab cultural contexts. Finally, we\nanalyze 6 Arabic pre-training corpora and find that commonly used sources such\nas Wikipedia may not be best suited to build culturally aware LMs, if used as\nthey are without adjustment. We will make CAMeL publicly available at:\nhttps://github.com/tareknaous/camel\n","authors":["Tarek Naous","Michael J. Ryan","Alan Ritter","Wei Xu"],"pdf_url":"https://arxiv.org/pdf/2305.14456v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.05666v5","updated":"2024-03-20T16:50:25Z","published":"2023-02-11T11:56:06Z","title":"Jaccard Metric Losses: Optimizing the Jaccard Index with Soft Labels","summary":"  Intersection over Union (IoU) losses are surrogates that directly optimize\nthe Jaccard index. Leveraging IoU losses as part of the loss function have\ndemonstrated superior performance in semantic segmentation tasks compared to\noptimizing pixel-wise losses such as the cross-entropy loss alone. However, we\nidentify a lack of flexibility in these losses to support vital training\ntechniques like label smoothing, knowledge distillation, and semi-supervised\nlearning, mainly due to their inability to process soft labels. To address\nthis, we introduce Jaccard Metric Losses (JMLs), which are identical to the\nsoft Jaccard loss in standard settings with hard labels but are fully\ncompatible with soft labels. We apply JMLs to three prominent use cases of soft\nlabels: label smoothing, knowledge distillation and semi-supervised learning,\nand demonstrate their potential to enhance model accuracy and calibration. Our\nexperiments show consistent improvements over the cross-entropy loss across 4\nsemantic segmentation datasets (Cityscapes, PASCAL VOC, ADE20K, DeepGlobe Land)\nand 13 architectures, including classic CNNs and recent vision transformers.\nRemarkably, our straightforward approach significantly outperforms\nstate-of-the-art knowledge distillation and semi-supervised learning methods.\nThe code is available at\n\\href{https://github.com/zifuwanggg/JDTLosses}{https://github.com/zifuwanggg/JDTLosses}.\n","authors":["Zifu Wang","Xuefei Ning","Matthew B. Blaschko"],"pdf_url":"https://arxiv.org/pdf/2302.05666v5.pdf","comment":"NeurIPS 2023"},{"id":"http://arxiv.org/abs/2403.13741v1","updated":"2024-03-20T16:47:53Z","published":"2024-03-20T16:47:53Z","title":"Hyper Strategy Logic","summary":"  Strategy logic (SL) is a powerful temporal logic that enables strategic\nreasoning in multi-agent systems. SL supports explicit (first-order)\nquantification over strategies and provides a logical framework to express many\nimportant properties such as Nash equilibria, dominant strategies, etc. While\nin SL the same strategy can be used in multiple strategy profiles, each such\nprofile is evaluated w.r.t. a path-property, i.e., a property that considers\nthe single path resulting from a particular strategic interaction. In this\npaper, we present Hyper Strategy Logic (HyperSL), a strategy logic where the\noutcome of multiple strategy profiles can be compared w.r.t. a hyperproperty,\ni.e., a property that relates multiple paths. We show that HyperSL can capture\nimportant properties that cannot be expressed in SL, including\nnon-interference, quantitative Nash equilibria, optimal adversarial planning,\nand reasoning under imperfect information. On the algorithmic side, we identify\nan expressive fragment of HyperSL with decidable model checking and present a\nmodel-checking algorithm. We contribute a prototype implementation of our\nalgorithm and report on encouraging experimental results.\n","authors":["Raven Beutner","Bernd Finkbeiner"],"pdf_url":"https://arxiv.org/pdf/2403.13741v1.pdf","comment":"AAMAS 2024"},{"id":"http://arxiv.org/abs/2403.13729v1","updated":"2024-03-20T16:39:17Z","published":"2024-03-20T16:39:17Z","title":"Reinforcement Learning for Online Testing of Autonomous Driving Systems:\n  a Replication and Extension Study","summary":"  In a recent study, Reinforcement Learning (RL) used in combination with\nmany-objective search, has been shown to outperform alternative techniques\n(random search and many-objective search) for online testing of Deep Neural\nNetwork-enabled systems. The empirical evaluation of these techniques was\nconducted on a state-of-the-art Autonomous Driving System (ADS). This work is a\nreplication and extension of that empirical study. Our replication shows that\nRL does not outperform pure random test generation in a comparison conducted\nunder the same settings of the original study, but with no confounding factor\ncoming from the way collisions are measured. Our extension aims at eliminating\nsome of the possible reasons for the poor performance of RL observed in our\nreplication: (1) the presence of reward components providing contrasting or\nuseless feedback to the RL agent; (2) the usage of an RL algorithm (Q-learning)\nwhich requires discretization of an intrinsically continuous state space.\nResults show that our new RL agent is able to converge to an effective policy\nthat outperforms random testing. Results also highlight other possible\nimprovements, which open to further investigations on how to best leverage RL\nfor online ADS testing.\n","authors":["Luca Giamattei","Matteo Biagiola","Roberto Pietrantuono","Stefano Russo","Paolo Tonella"],"pdf_url":"https://arxiv.org/pdf/2403.13729v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13728v1","updated":"2024-03-20T16:38:26Z","published":"2024-03-20T16:38:26Z","title":"M-HOF-Opt: Multi-Objective Hierarchical Output Feedback Optimization via\n  Multiplier Induced Loss Landscape Scheduling","summary":"  When a neural network parameterized loss function consists of many terms, the\ncombinatorial choice of weight multipliers during the optimization process\nforms a challenging problem. To address this, we proposed a probabilistic\ngraphical model (PGM) for the joint model parameter and multiplier evolution\nprocess, with a hypervolume based likelihood that promotes multi-objective\ndescent of each loss term. The corresponding parameter and multiplier\nestimation as a sequential decision process is then cast into an optimal\ncontrol problem, where the multi-objective descent goal is dispatched\nhierarchically into a series of constraint optimization sub-problems. The\nsub-problem constraint automatically adapts itself according to Pareto\ndominance and serves as the setpoint for the low level multiplier controller to\nschedule loss landscapes via output feedback of each loss term. Our method is\nmultiplier-free and operates at the timescale of epochs, thus saves tremendous\ncomputational resources compared to full training cycle multiplier tuning. We\napplied it to domain invariant variational auto-encoding with 6 loss terms on\nthe PACS domain generalization task, and observed robust performance across a\nrange of controller hyperparameters, as well as different multiplier initial\nconditions, outperforming other multiplier scheduling methods. We offered\nmodular implementation of our method, admitting custom definition of many loss\nterms for applying our multi-objective hierarchical output feedback training\nscheme to other deep learning fields.\n","authors":["Xudong Sun","Nutan Chen","Alexej Gossmann","Yu Xing","Carla Feistner","Emilio Dorigatt","Felix Drost","Daniele Scarcella","Lisa Beer","Carsten Marr"],"pdf_url":"https://arxiv.org/pdf/2403.13728v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.12963v3","updated":"2024-03-20T16:36:06Z","published":"2023-10-19T17:57:39Z","title":"AutoMix: Automatically Mixing Language Models","summary":"  Large language models (LLMs) are now available from cloud API providers in\nvarious sizes and configurations. While this diversity offers a broad spectrum\nof choices, effectively leveraging the options to optimize computational cost\nand performance remains challenging. In this work, we present AutoMix, an\napproach that strategically routes queries to larger LMs, based on the\napproximate correctness of outputs from a smaller LM. Central to AutoMix is a\nfew-shot self-verification mechanism, which estimates the reliability of its\nown outputs without requiring training. Given that verifications can be noisy,\nwe employ a meta-verifier in AutoMix to refine the accuracy of these\nassessments. Our experiments using LLAMA2-13B and GPT-4, on five\ncontext-grounded reasoning datasets demonstrate that AutoMix surpasses\nestablished baselines, improving the incremental benefit per cost by up to 86%.\nOur code and data are available at https://github.com/automix-llm/automix.\n","authors":["Aman Madaan","Pranjal Aggarwal","Ankit Anand","Srividya Pranavi Potharaju","Swaroop Mishra","Pei Zhou","Aditya Gupta","Dheeraj Rajagopal","Karthik Kappaganthu","Yiming Yang","Shyam Upadhyay"," Mausam","Manaal Faruqui"],"pdf_url":"https://arxiv.org/pdf/2310.12963v3.pdf","comment":"The first two authors contributed equally. Work started and partly\n  done during Aman's internship at Google. This version adds results on\n  additional models and datasets"},{"id":"http://arxiv.org/abs/2403.09184v2","updated":"2024-03-20T16:34:37Z","published":"2024-03-14T08:54:19Z","title":"Learning Algorithms for Verification of Markov Decision Processes","summary":"  We present a general framework for applying learning algorithms and\nheuristical guidance to the verification of Markov decision processes (MDPs).\nThe primary goal of our techniques is to improve performance by avoiding an\nexhaustive exploration of the state space, instead focussing on particularly\nrelevant areas of the system, guided by heuristics. Our work builds on the\nprevious results of Br{\\'{a}}zdil et al., significantly extending it as well as\nrefining several details and fixing errors.\n  The presented framework focuses on probabilistic reachability, which is a\ncore problem in verification, and is instantiated in two distinct scenarios.\nThe first assumes that full knowledge of the MDP is available, in particular\nprecise transition probabilities. It performs a heuristic-driven partial\nexploration of the model, yielding precise lower and upper bounds on the\nrequired probability. The second tackles the case where we may only sample the\nMDP without knowing the exact transition dynamics. Here, we obtain\nprobabilistic guarantees, again in terms of both the lower and upper bounds,\nwhich provides efficient stopping criteria for the approximation. In\nparticular, the latter is an extension of statistical model-checking (SMC) for\nunbounded properties in MDPs. In contrast to other related approaches, we do\nnot restrict our attention to time-bounded (finite-horizon) or discounted\nproperties, nor assume any particular structural properties of the MDP.\n","authors":["Tomáš Brázdil","Krishnendu Chatterjee","Martin Chmelik","Vojtěch Forejt","Jan Křetínský","Marta Kwiatkowska","Tobias Meggendorfer","David Parker","Mateusz Ujma"],"pdf_url":"https://arxiv.org/pdf/2403.09184v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13721v1","updated":"2024-03-20T16:29:52Z","published":"2024-03-20T16:29:52Z","title":"Large Language Models meet Network Slicing Management and Orchestration","summary":"  Network slicing, a cornerstone technology for future networks, enables the\ncreation of customized virtual networks on a shared physical infrastructure.\nThis fosters innovation and agility by providing dedicated resources tailored\nto specific applications. However, current orchestration and management\napproaches face limitations in handling the complexity of new service demands\nwithin multi-administrative domain environments. This paper proposes a future\nvision for network slicing powered by Large Language Models (LLMs) and\nmulti-agent systems, offering a framework that can be integrated with existing\nManagement and Orchestration (MANO) frameworks. This framework leverages LLMs\nto translate user intent into technical requirements, map network functions to\ninfrastructure, and manage the entire slice lifecycle, while multi-agent\nsystems facilitate collaboration across different administrative domains. We\nalso discuss the challenges associated with implementing this framework and\npotential solutions to mitigate them.\n","authors":["Abdulhalim Dandoush","Viswanath Kumarskandpriya","Mueen Uddin","Usman Khalil"],"pdf_url":"https://arxiv.org/pdf/2403.13721v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.07915v3","updated":"2024-03-20T16:17:02Z","published":"2023-09-14T17:59:17Z","title":"MMICL: Empowering Vision-language Model with Multi-Modal In-Context\n  Learning","summary":"  Since the resurgence of deep learning, vision-language models (VLMs) enhanced\nby large language models (LLMs) have grown exponentially in popularity.\nHowever, while LLMs can utilize extensive background knowledge and task\ninformation with in-context learning, most VLMs still struggle with\nunderstanding complex multi-modal prompts with multiple images, making VLMs\nless effective in downstream vision-language tasks. In this paper, we address\nthe limitation above by 1) introducing vision-language Model with Multi-Modal\nIn-Context Learning(MMICL), a new approach to allow the VLM to deal with\nmulti-modal inputs efficiently; 2) proposing a novel context scheme to augment\nthe in-context learning ability of the VLM; 3) constructing the Multi-modal\nIn-Context Learning (MIC) dataset, designed to enhance the VLM's ability to\nunderstand complex multi-modal prompts. Our experiments confirm that MMICL\nachieves new state-of-the-art zero-shot performance on a wide range of general\nvision-language tasks, especially for complex benchmarks, including MME and\nMMBench. Our analysis demonstrates that MMICL effectively tackles the challenge\nof complex multi-modal prompt understanding and emerges the impressive ICL\nability. Furthermore, we observe that MMICL successfully alleviates language\nbias in VLMs, a common issue for VLMs that often leads to hallucination when\nfaced with extensive textual context. Our code, dataset, dataset tool, and\nmodel are available at https://github.com/PKUnlp-icler/MIC\n","authors":["Haozhe Zhao","Zefan Cai","Shuzheng Si","Xiaojian Ma","Kaikai An","Liang Chen","Zixuan Liu","Sheng Wang","Wenjuan Han","Baobao Chang"],"pdf_url":"https://arxiv.org/pdf/2309.07915v3.pdf","comment":"Accepted by ICLR2024"},{"id":"http://arxiv.org/abs/2403.12143v2","updated":"2024-03-20T16:12:12Z","published":"2024-03-18T18:01:01Z","title":"Graph Neural Networks for Learning Equivariant Representations of Neural\n  Networks","summary":"  Neural networks that process the parameters of other neural networks find\napplications in domains as diverse as classifying implicit neural\nrepresentations, generating neural network weights, and predicting\ngeneralization errors. However, existing approaches either overlook the\ninherent permutation symmetry in the neural network or rely on intricate\nweight-sharing patterns to achieve equivariance, while ignoring the impact of\nthe network architecture itself. In this work, we propose to represent neural\nnetworks as computational graphs of parameters, which allows us to harness\npowerful graph neural networks and transformers that preserve permutation\nsymmetry. Consequently, our approach enables a single model to encode neural\ncomputational graphs with diverse architectures. We showcase the effectiveness\nof our method on a wide range of tasks, including classification and editing of\nimplicit neural representations, predicting generalization performance, and\nlearning to optimize, while consistently outperforming state-of-the-art\nmethods. The source code is open-sourced at\nhttps://github.com/mkofinas/neural-graphs.\n","authors":["Miltiadis Kofinas","Boris Knyazev","Yan Zhang","Yunlu Chen","Gertjan J. Burghouts","Efstratios Gavves","Cees G. M. Snoek","David W. Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.12143v2.pdf","comment":"In ICLR 2024. Source code: https://github.com/mkofinas/neural-graphs"},{"id":"http://arxiv.org/abs/2403.13705v1","updated":"2024-03-20T16:08:57Z","published":"2024-03-20T16:08:57Z","title":"Research Re: search & Re-search","summary":"  Search algorithms are often categorized by their node expansion strategy. One\noption is the depth-first strategy, a simple backtracking strategy that\ntraverses the search space in the order in which successor nodes are generated.\nAn alternative is the best-first strategy, which was designed to make it\npossible to use domain-specific heuristic information. By exploring promising\nparts of the search space first, best-first algorithms are usually more\nefficient than depth-first algorithms.\n  In programs that play minimax games such as chess and checkers, the\nefficiency of the search is of crucial importance. Given the success of\nbest-first algorithms in other domains, one would expect them to be used for\nminimax games too. However, all high-performance game-playing programs are\nbased on a depth-first algorithm.\n  This study takes a closer look at a depth-first algorithm, AB, and a\nbest-first algorithm, SSS. The prevailing opinion on these algorithms is that\nSSS offers the potential for a more efficient search, but that its complicated\nformulation and exponential memory requirements render it impractical. The\ntheoretical part of this work shows that there is a surprisingly\nstraightforward link between the two algorithms -- for all practical purposes,\nSSS is a special case of AB. Subsequent empirical evidence proves the\nprevailing opinion on SSS to be wrong: it is not a complicated algorithm, it\ndoes not need too much memory, and it is also not more efficient than\ndepth-first search.\n","authors":["Aske Plaat"],"pdf_url":"https://arxiv.org/pdf/2403.13705v1.pdf","comment":"PhD thesis Aske Plaat 20 June 1996. AlphaBeta, SSS*, MTD(f)"},{"id":"http://arxiv.org/abs/2403.13703v1","updated":"2024-03-20T16:07:04Z","published":"2024-03-20T16:07:04Z","title":"Fostc3net:A Lightweight YOLOv5 Based On the Network Structure\n  Optimization","summary":"  Transmission line detection technology is crucial for automatic monitoring\nand ensuring the safety of electrical facilities. The YOLOv5 series is\ncurrently one of the most advanced and widely used methods for object\ndetection. However, it faces inherent challenges, such as high computational\nload on devices and insufficient detection accuracy. To address these concerns,\nthis paper presents an enhanced lightweight YOLOv5 technique customized for\nmobile devices, specifically intended for identifying objects associated with\ntransmission lines. The C3Ghost module is integrated into the convolutional\nnetwork of YOLOv5 to reduce floating point operations per second (FLOPs) in the\nfeature channel fusion process and improve feature expression performance. In\naddition, a FasterNet module is introduced to replace the c3 module in the\nYOLOv5 Backbone. The FasterNet module uses Partial Convolutions to process only\na portion of the input channels, improving feature extraction efficiency and\nreducing computational overhead. To address the imbalance between simple and\nchallenging samples in the dataset and the diversity of aspect ratios of\nbounding boxes, the wIoU v3 LOSS is adopted as the loss function. To validate\nthe performance of the proposed approach, Experiments are conducted on a custom\ndataset of transmission line poles. The results show that the proposed model\nachieves a 1% increase in detection accuracy, a 13% reduction in FLOPs, and a\n26% decrease in model parameters compared to the existing YOLOv5.In the\nablation experiment, it was also discovered that while the Fastnet module and\nthe CSghost module improved the precision of the original YOLOv5 baseline\nmodel, they caused a decrease in the mAP@.5-.95 metric. However, the\nimprovement of the wIoUv3 loss function significantly mitigated the decline of\nthe mAP@.5-.95 metric.\n","authors":["Danqing Ma","Shaojie Li","Bo Dang","Hengyi Zang","Xinqi Dong"],"pdf_url":"https://arxiv.org/pdf/2403.13703v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16296v4","updated":"2024-03-20T15:52:49Z","published":"2023-03-28T20:35:38Z","title":"Dice Semimetric Losses: Optimizing the Dice Score with Soft Labels","summary":"  The soft Dice loss (SDL) has taken a pivotal role in numerous automated\nsegmentation pipelines in the medical imaging community. Over the last years,\nsome reasons behind its superior functioning have been uncovered and further\noptimizations have been explored. However, there is currently no implementation\nthat supports its direct utilization in scenarios involving soft labels. Hence,\na synergy between the use of SDL and research leveraging the use of soft\nlabels, also in the context of model calibration, is still missing. In this\nwork, we introduce Dice semimetric losses (DMLs), which (i) are by design\nidentical to SDL in a standard setting with hard labels, but (ii) can be\nemployed in settings with soft labels. Our experiments on the public QUBIQ,\nLiTS and KiTS benchmarks confirm the potential synergy of DMLs with soft labels\n(e.g. averaging, label smoothing, and knowledge distillation) over hard labels\n(e.g. majority voting and random selection). As a result, we obtain superior\nDice scores and model calibration, which supports the wider adoption of DMLs in\npractice. The code is available at https://github.com/zifuwanggg/JDTLosses\n","authors":["Zifu Wang","Teodora Popordanoska","Jeroen Bertels","Robin Lemmens","Matthew B. Blaschko"],"pdf_url":"https://arxiv.org/pdf/2303.16296v4.pdf","comment":"MICCAI 2023"},{"id":"http://arxiv.org/abs/2403.13684v1","updated":"2024-03-20T15:41:39Z","published":"2024-03-20T15:41:39Z","title":"SPTNet: An Efficient Alternative Framework for Generalized Category\n  Discovery with Spatial Prompt Tuning","summary":"  Generalized Category Discovery (GCD) aims to classify unlabelled images from\nboth `seen' and `unseen' classes by transferring knowledge from a set of\nlabelled `seen' class images. A key theme in existing GCD approaches is\nadapting large-scale pre-trained models for the GCD task. An alternate\nperspective, however, is to adapt the data representation itself for better\nalignment with the pre-trained model. As such, in this paper, we introduce a\ntwo-stage adaptation approach termed SPTNet, which iteratively optimizes model\nparameters (i.e., model-finetuning) and data parameters (i.e., prompt\nlearning). Furthermore, we propose a novel spatial prompt tuning method (SPT)\nwhich considers the spatial property of image data, enabling the method to\nbetter focus on object parts, which can transfer between seen and unseen\nclasses. We thoroughly evaluate our SPTNet on standard benchmarks and\ndemonstrate that our method outperforms existing GCD methods. Notably, we find\nour method achieves an average accuracy of 61.4% on the SSB, surpassing prior\nstate-of-the-art methods by approximately 10%. The improvement is particularly\nremarkable as our method yields extra parameters amounting to only 0.117% of\nthose in the backbone architecture. Project page:\nhttps://visual-ai.github.io/sptnet.\n","authors":["Hongjun Wang","Sagar Vaze","Kai Han"],"pdf_url":"https://arxiv.org/pdf/2403.13684v1.pdf","comment":"Accepted as a conference paper at ICLR 2024; Project page:\n  https://visual-ai.github.io/sptnet"},{"id":"http://arxiv.org/abs/2403.13682v1","updated":"2024-03-20T15:40:18Z","published":"2024-03-20T15:40:18Z","title":"Threats, Attacks, and Defenses in Machine Unlearning: A Survey","summary":"  Recently, Machine Unlearning (MU) has gained considerable attention for its\npotential to improve AI safety by removing the influence of specific data from\ntrained Machine Learning (ML) models. This process, known as knowledge removal,\naddresses concerns about data such as sensitivity, copyright restrictions,\nobsolescence, or low quality. This capability is also crucial for ensuring\ncompliance with privacy regulations such as the Right To Be Forgotten (RTBF).\nTherefore, strategic knowledge removal mitigates the risk of harmful outcomes,\nsafeguarding against biases, misinformation, and unauthorized data\nexploitation, thereby enhancing the ethical use and reliability of AI systems.\nEfforts have been made to design efficient unlearning approaches, with MU\nservices being examined for integration with existing machine learning as a\nservice (MLaaS), allowing users to submit requests to erase data. However,\nrecent research highlights vulnerabilities in machine unlearning systems, such\nas information leakage and malicious unlearning requests, that can lead to\nsignificant security and privacy concerns. Moreover, extensive research\nindicates that unlearning methods and prevalent attacks fulfill diverse roles\nwithin MU systems. For instance, unlearning can act as a mechanism to recover\nmodels from backdoor attacks, while backdoor attacks themselves can serve as an\nevaluation metric for unlearning effectiveness. This underscores the intricate\nrelationship and complex interplay between these elements in maintaining system\nfunctionality and safety. Therefore, this survey seeks to bridge the gap\nbetween the extensive number of studies on threats, attacks, and defenses in\nmachine unlearning and the absence of a comprehensive review that categorizes\ntheir taxonomy, methods, and solutions, thus offering valuable insights for\nfuture research directions and practical implementations.\n","authors":["Ziyao Liu","Huanyi Ye","Chen Chen","Kwok-Yan Lam"],"pdf_url":"https://arxiv.org/pdf/2403.13682v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13681v1","updated":"2024-03-20T15:39:54Z","published":"2024-03-20T15:39:54Z","title":"PARAMANU-AYN: An Efficient Novel Generative and Instruction-tuned\n  Language Model for Indian Legal Case Documents","summary":"  In this paper, we present PARAMANU-AYN, a language model based exclusively on\ncase documents of the Supreme Court of India, the Constitution of India, and\nthe Indian Penal Code. The novel Auto Regressive (AR) decoder based model is\npretrained from scratch at a context size of 8192. We evaluated our pretrained\nlegal model on perplexity metrics. We also instruction-tuned our pretrained\nmodel on a set of 10,763 instructions covering various legal tasks such as\nlegal reasoning, judgement explanation, legal clause generation, legal\ndrafting, legal contract drafting, case summarization, constitutional\nquestion-answering, etc. We also evaluated the responses of prompts for\ninstruction-tuned models by GPT-3.5-Turbo on clarity, relevance, completeness,\nand legal reasoning metrics in a scale of 10. Our model can be run on CPU and\nachieved 42.46 tokens/sec CPU inference speed. We found that our models,\ndespite not being pretrained on legal books, various legal contracts, and legal\ndocuments, were able to learn the domain knowledge required for drafting\nvarious legal contracts and legal clauses, and generalize to draft legal\ncontracts and legal clauses with limited instruction tuning. Hence, we conclude\nthat for a strong domain-specialized generative language model (such as legal),\nvery large amounts of data are not required to develop models from scratch. We\nbelieve that this work is the first attempt to make a dedicated generative\nlegal language model from scratch for Indian Supreme Court jurisdiction or in\nlegal NLP overall. We plan to release our Paramanu-Ayn model at\nhttps://www.bharatgpts.com.\n","authors":["Mitodru Niyogi","Arnab Bhattacharya"],"pdf_url":"https://arxiv.org/pdf/2403.13681v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.10705v4","updated":"2024-03-20T15:26:55Z","published":"2023-10-16T14:46:45Z","title":"Observational and Experimental Insights into Machine Learning-Based\n  Defect Classification in Wafers","summary":"  This survey paper offers a comprehensive review of methodologies utilizing\nmachine learning (ML) classification techniques for identifying wafer defects\nin semiconductor manufacturing. Despite the growing body of research\ndemonstrating the effectiveness of ML in wafer defect identification, there is\na noticeable absence of comprehensive reviews on this subject. This survey\nattempts to fill this void by amalgamating available literature and providing\nan in-depth analysis of the advantages, limitations, and potential applications\nof various ML classification algorithms in the realm of wafer defect detection.\nAn innovative taxonomy of methodologies that we present provides a detailed\nclassification of algorithms into more refined categories and techniques. This\ntaxonomy follows a three-tier structure, starting from broad methodology\ncategories and ending with specific techniques. It aids researchers in\ncomprehending the complex relationships between different algorithms and their\ntechniques. We employ a rigorous Observational and experimental evaluation to\nrank these varying techniques. For the Observational evaluation, we assess\ntechniques based on a set of four criteria. The experimental evaluation ranks\nthe algorithms employing the same techniques, sub-categories, and categories.\nAlso the paper illuminates the future prospects of ML classification techniques\nfor wafer defect identification, underscoring potential advancements and\nopportunities for further research in this field\n","authors":["Kamal Taha"],"pdf_url":"https://arxiv.org/pdf/2310.10705v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13653v1","updated":"2024-03-20T14:58:40Z","published":"2024-03-20T14:58:40Z","title":"Learning User Embeddings from Human Gaze for Personalised Saliency\n  Prediction","summary":"  Reusable embeddings of user behaviour have shown significant performance\nimprovements for the personalised saliency prediction task. However, prior\nworks require explicit user characteristics and preferences as input, which are\noften difficult to obtain. We present a novel method to extract user embeddings\nfrom pairs of natural images and corresponding saliency maps generated from a\nsmall amount of user-specific eye tracking data. At the core of our method is a\nSiamese convolutional neural encoder that learns the user embeddings by\ncontrasting the image and personal saliency map pairs of different users.\nEvaluations on two public saliency datasets show that the generated embeddings\nhave high discriminative power, are effective at refining universal saliency\nmaps to the individual users, and generalise well across users and images.\nFinally, based on our model's ability to encode individual user\ncharacteristics, our work points towards other applications that can benefit\nfrom reusable embeddings of gaze behaviour.\n","authors":["Florian Strohm","Mihai Bâce","Andreas Bulling"],"pdf_url":"https://arxiv.org/pdf/2403.13653v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.00117v3","updated":"2024-03-20T14:26:12Z","published":"2023-09-29T20:11:15Z","title":"ABScribe: Rapid Exploration & Organization of Multiple Writing\n  Variations in Human-AI Co-Writing Tasks using Large Language Models","summary":"  Exploring alternative ideas by rewriting text is integral to the writing\nprocess. State-of-the-art Large Language Models (LLMs) can simplify writing\nvariation generation. However, current interfaces pose challenges for\nsimultaneous consideration of multiple variations: creating new variations\nwithout overwriting text can be difficult, and pasting them sequentially can\nclutter documents, increasing workload and disrupting writers' flow. To tackle\nthis, we present ABScribe, an interface that supports rapid, yet visually\nstructured, exploration and organization of writing variations in human-AI\nco-writing tasks. With ABScribe, users can swiftly modify variations using LLM\nprompts, which are auto-converted into reusable buttons. Variations are stored\nadjacently within text fields for rapid in-place comparisons using mouse-over\ninteractions on a popup toolbar. Our user study with 12 writers shows that\nABScribe significantly reduces task workload (d = 1.20, p < 0.001), enhances\nuser perceptions of the revision process (d = 2.41, p < 0.001) compared to a\npopular baseline workflow, and provides insights into how writers explore\nvariations using LLMs.\n","authors":["Mohi Reza","Nathan Laundry","Ilya Musabirov","Peter Dushniku","Zhi Yuan \"Michael\" Yu","Kashish Mittal","Tovi Grossman","Michael Liut","Anastasia Kuzminykh","Joseph Jay Williams"],"pdf_url":"https://arxiv.org/pdf/2310.00117v3.pdf","comment":"CHI 2024"},{"id":"http://arxiv.org/abs/2403.13619v1","updated":"2024-03-20T14:13:44Z","published":"2024-03-20T14:13:44Z","title":"Dynamic Resource Allocation for Virtual Machine Migration Optimization\n  using Machine Learning","summary":"  The paragraph is grammatically correct and logically coherent. It discusses\nthe importance of mobile terminal cloud computing migration technology in\nmeeting the demands of evolving computer and cloud computing technologies. It\nemphasizes the need for efficient data access and storage, as well as the\nutilization of cloud computing migration technology to prevent additional time\ndelays. The paragraph also highlights the contributions of cloud computing\nmigration technology to expanding cloud computing services. Additionally, it\nacknowledges the role of virtualization as a fundamental capability of cloud\ncomputing while emphasizing that cloud computing and virtualization are not\ninherently interconnected. Finally, it introduces machine learning-based\nvirtual machine migration optimization and dynamic resource allocation as a\ncritical research direction in cloud computing, citing the limitations of\nstatic rules or manual settings in traditional cloud computing environments.\nOverall, the paragraph effectively communicates the importance of machine\nlearning technology in addressing resource allocation and virtual machine\nmigration challenges in cloud computing.\n","authors":["Yulu Gong","Jiaxin Huang","Bo Liu","Jingyu Xu","Binbin Wu","Yifan Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.13619v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13597v1","updated":"2024-03-20T13:44:30Z","published":"2024-03-20T13:44:30Z","title":"No more optimization rules: LLM-enabled policy-based multi-modal query\n  optimizer (version 1)","summary":"  Large language model (LLM) has marked a pivotal moment in the field of\nmachine learning and deep learning. Recently its capability for query planning\nhas been investigated, including both single-modal and multi-modal queries.\nHowever, there is no work on the query optimization capability of LLM. As a\ncritical (or could even be the most important) step that significantly impacts\nthe execution performance of the query plan, such analysis and attempts should\nnot be missed. From another aspect, existing query optimizers are usually\nrule-based or rule-based + cost-based, i.e., they are dependent on manually\ncreated rules to complete the query plan rewrite/transformation. Given the fact\nthat modern optimizers include hundreds to thousands of rules, designing a\nmulti-modal query optimizer following a similar way is significantly\ntime-consuming since we will have to enumerate as many multi-modal optimization\nrules as possible, which has not been well addressed today. In this paper, we\ninvestigate the query optimization ability of LLM and use LLM to design LaPuda,\na novel LLM and Policy based multi-modal query optimizer. Instead of\nenumerating specific and detailed rules, LaPuda only needs a few abstract\npolicies to guide LLM in the optimization, by which much time and human effort\nare saved. Furthermore, to prevent LLM from making mistakes or negative\noptimization, we borrow the idea of gradient descent and propose a guided cost\ndescent (GCD) algorithm to perform the optimization, such that the optimization\ncan be kept in the correct direction. In our evaluation, our methods\nconsistently outperform the baselines in most cases. For example, the optimized\nplans generated by our methods result in 1~3x higher execution speed than those\nby the baselines.\n","authors":["Yifan Wang","Haodi Ma","Daisy Zhe Wang"],"pdf_url":"https://arxiv.org/pdf/2403.13597v1.pdf","comment":"Yifan and Haodi contribute equally to the work"},{"id":"http://arxiv.org/abs/2306.11335v4","updated":"2024-03-20T13:18:18Z","published":"2023-06-20T07:06:04Z","title":"Surfer: Progressive Reasoning with World Models for Robotic Manipulation","summary":"  Considering how to make the model accurately understand and follow natural\nlanguage instructions and perform actions consistent with world knowledge is a\nkey challenge in robot manipulation. This mainly includes human fuzzy\ninstruction reasoning and the following of physical knowledge. Therefore, the\nembodied intelligence agent must have the ability to model world knowledge from\ntraining data. However, most existing vision and language robot manipulation\nmethods mainly operate in less realistic simulator and language settings and\nlack explicit modeling of world knowledge. To bridge this gap, we introduce a\nnovel and simple robot manipulation framework, called Surfer. It is based on\nthe world model, treats robot manipulation as a state transfer of the visual\nscene, and decouples it into two parts: action and scene. Then, the\ngeneralization ability of the model on new instructions and new scenes is\nenhanced by explicit modeling of the action and scene prediction in multi-modal\ninformation. In addition to the framework, we also built a robot manipulation\nsimulator that supports full physics execution based on the MuJoCo physics\nengine. It can automatically generate demonstration training data and test\ndata, effectively reducing labor costs. To conduct a comprehensive and\nsystematic evaluation of the robot manipulation model in terms of language\nunderstanding and physical execution, we also created a robotic manipulation\nbenchmark with progressive reasoning tasks, called SeaWave. It contains 4\nlevels of progressive reasoning tasks and can provide a standardized testing\nplatform for embedded AI agents in multi-modal environments. On average, Surfer\nachieved a success rate of 54.74% on the defined four levels of manipulation\ntasks, exceeding the best baseline performance of 47.64%.\n","authors":["Pengzhen Ren","Kaidong Zhang","Hetao Zheng","Zixuan Li","Yuhang Wen","Fengda Zhu","Mas Ma","Xiaodan Liang"],"pdf_url":"https://arxiv.org/pdf/2306.11335v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11552v2","updated":"2024-03-20T13:15:39Z","published":"2024-03-18T08:03:47Z","title":"LLM3:Large Language Model-based Task and Motion Planning with Motion\n  Failure Reasoning","summary":"  Conventional Task and Motion Planning (TAMP) approaches rely on manually\ncrafted interfaces connecting symbolic task planning with continuous motion\ngeneration. These domain-specific and labor-intensive modules are limited in\naddressing emerging tasks in real-world settings. Here, we present LLM^3, a\nnovel Large Language Model (LLM)-based TAMP framework featuring a\ndomain-independent interface. Specifically, we leverage the powerful reasoning\nand planning capabilities of pre-trained LLMs to propose symbolic action\nsequences and select continuous action parameters for motion planning.\nCrucially, LLM^3 incorporates motion planning feedback through prompting,\nallowing the LLM to iteratively refine its proposals by reasoning about motion\nfailure. Consequently, LLM^3 interfaces between task planning and motion\nplanning, alleviating the intricate design process of handling domain-specific\nmessages between them. Through a series of simulations in a box-packing domain,\nwe quantitatively demonstrate the effectiveness of LLM^3 in solving TAMP\nproblems and the efficiency in selecting action parameters. Ablation studies\nunderscore the significant contribution of motion failure reasoning to the\nsuccess of LLM^3. Furthermore, we conduct qualitative experiments on a physical\nmanipulator, demonstrating the practical applicability of our approach in\nreal-world settings.\n","authors":["Shu Wang","Muzhi Han","Ziyuan Jiao","Zeyu Zhang","Ying Nian Wu","Song-Chun Zhu","Hangxin Liu"],"pdf_url":"https://arxiv.org/pdf/2403.11552v2.pdf","comment":"Submitted to IROS 2024. Codes available:\n  https://github.com/AssassinWS/LLM-TAMP"},{"id":"http://arxiv.org/abs/2403.13574v1","updated":"2024-03-20T13:14:29Z","published":"2024-03-20T13:14:29Z","title":"A Large Language Model Enhanced Sequential Recommender for Joint Video\n  and Comment Recommendation","summary":"  In online video platforms, reading or writing comments on interesting videos\nhas become an essential part of the video watching experience. However,\nexisting video recommender systems mainly model users' interaction behaviors\nwith videos, lacking consideration of comments in user behavior modeling. In\nthis paper, we propose a novel recommendation approach called LSVCR by\nleveraging user interaction histories with both videos and comments, so as to\njointly conduct personalized video and comment recommendation. Specifically,\nour approach consists of two key components, namely sequential recommendation\n(SR) model and supplemental large language model (LLM) recommender. The SR\nmodel serves as the primary recommendation backbone (retained in deployment) of\nour approach, allowing for efficient user preference modeling. Meanwhile, we\nleverage the LLM recommender as a supplemental component (discarded in\ndeployment) to better capture underlying user preferences from heterogeneous\ninteraction behaviors. In order to integrate the merits of the SR model and the\nsupplemental LLM recommender, we design a twostage training paradigm. The first\nstage is personalized preference alignment, which aims to align the preference\nrepresentations from both components, thereby enhancing the semantics of the SR\nmodel. The second stage is recommendation-oriented fine-tuning, in which the\nalignment-enhanced SR model is fine-tuned according to specific objectives.\nExtensive experiments in both video and comment recommendation tasks\ndemonstrate the effectiveness of LSVCR. Additionally, online A/B testing on the\nKuaiShou platform verifies the actual benefits brought by our approach. In\nparticular, we achieve a significant overall gain of 4.13% in comment watch\ntime.\n","authors":["Bowen Zheng","Zihan Lin","Enze Liu","Chen Yang","Enyang Bai","Cheng Ling","Wayne Xin Zhao","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2403.13574v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.18248v3","updated":"2024-03-20T13:12:48Z","published":"2023-05-29T17:12:03Z","title":"Do Language Models Know When They're Hallucinating References?","summary":"  State-of-the-art language models (LMs) are notoriously susceptible to\ngenerating hallucinated information. Such inaccurate outputs not only undermine\nthe reliability of these models but also limit their use and raise serious\nconcerns about misinformation and propaganda. In this work, we focus on\nhallucinated book and article references and present them as the \"model\norganism\" of language model hallucination research, due to their frequent and\neasy-to-discern nature. We posit that if a language model cites a particular\nreference in its output, then it should ideally possess sufficient information\nabout its authors and content, among other relevant details. Using this basic\ninsight, we illustrate that one can identify hallucinated references without\never consulting any external resources, by asking a set of direct or indirect\nqueries to the language model about the references. These queries can be\nconsidered as \"consistency checks.\" Our findings highlight that while LMs,\nincluding GPT-4, often produce inconsistent author lists for hallucinated\nreferences, they also often accurately recall the authors of real references.\nIn this sense, the LM can be said to \"know\" when it is hallucinating\nreferences. Furthermore, these findings show how hallucinated references can be\ndissected to shed light on their nature. Replication code and results can be\nfound at https://github.com/microsoft/hallucinated-references.\n","authors":["Ayush Agrawal","Mirac Suzgun","Lester Mackey","Adam Tauman Kalai"],"pdf_url":"https://arxiv.org/pdf/2305.18248v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.09389v2","updated":"2024-03-20T13:11:19Z","published":"2023-02-18T17:45:11Z","title":"Vulnerability analysis of captcha using Deep learning","summary":"  Several websites improve their security and avoid dangerous Internet attacks\nby implementing CAPTCHAs (Completely Automated Public Turing test to tell\nComputers and Humans Apart), a type of verification to identify whether the\nend-user is human or a robot. The most prevalent type of CAPTCHA is text-based,\ndesigned to be easily recognized by humans while being unsolvable towards\nmachines or robots. However, as deep learning technology progresses,\ndevelopment of convolutional neural network (CNN) models that predict\ntext-based CAPTCHAs becomes easier. The purpose of this research is to\ninvestigate the flaws and vulnerabilities in the CAPTCHA generating systems in\norder to design more resilient CAPTCHAs. To achieve this, we created CapNet, a\nConvolutional Neural Network. The proposed platform can evaluate both numerical\nand alphanumerical CAPTCHAs\n","authors":["Jaskaran Singh Walia","Aryan Odugoudar"],"pdf_url":"https://arxiv.org/pdf/2302.09389v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.02696v2","updated":"2024-03-20T12:58:14Z","published":"2023-12-05T11:55:47Z","title":"Analyzing and Improving the Training Dynamics of Diffusion Models","summary":"  Diffusion models currently dominate the field of data-driven image synthesis\nwith their unparalleled scaling to large datasets. In this paper, we identify\nand rectify several causes for uneven and ineffective training in the popular\nADM diffusion model architecture, without altering its high-level structure.\nObserving uncontrolled magnitude changes and imbalances in both the network\nactivations and weights over the course of training, we redesign the network\nlayers to preserve activation, weight, and update magnitudes on expectation. We\nfind that systematic application of this philosophy eliminates the observed\ndrifts and imbalances, resulting in considerably better networks at equal\ncomputational complexity. Our modifications improve the previous record FID of\n2.41 in ImageNet-512 synthesis to 1.81, achieved using fast deterministic\nsampling.\n  As an independent contribution, we present a method for setting the\nexponential moving average (EMA) parameters post-hoc, i.e., after completing\nthe training run. This allows precise tuning of EMA length without the cost of\nperforming several training runs, and reveals its surprising interactions with\nnetwork architecture, training time, and guidance.\n","authors":["Tero Karras","Miika Aittala","Jaakko Lehtinen","Janne Hellsten","Timo Aila","Samuli Laine"],"pdf_url":"https://arxiv.org/pdf/2312.02696v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.10402v2","updated":"2024-03-20T12:52:10Z","published":"2023-10-16T13:45:26Z","title":"Real-Fake: Effective Training Data Synthesis Through Distribution\n  Matching","summary":"  Synthetic training data has gained prominence in numerous learning tasks and\nscenarios, offering advantages such as dataset augmentation, generalization\nevaluation, and privacy preservation. Despite these benefits, the efficiency of\nsynthetic data generated by current methodologies remains inferior when\ntraining advanced deep models exclusively, limiting its practical utility. To\naddress this challenge, we analyze the principles underlying training data\nsynthesis for supervised learning and elucidate a principled theoretical\nframework from the distribution-matching perspective that explicates the\nmechanisms governing synthesis efficacy. Through extensive experiments, we\ndemonstrate the effectiveness of our synthetic data across diverse image\nclassification tasks, both as a replacement for and augmentation to real\ndatasets, while also benefits such as out-of-distribution generalization,\nprivacy preservation, and scalability. Specifically, we achieve 70.9% top1\nclassification accuracy on ImageNet1K when training solely with synthetic data\nequivalent to 1 X the original real data size, which increases to 76.0% when\nscaling up to 10 X synthetic data.\n","authors":["Jianhao Yuan","Jie Zhang","Shuyang Sun","Philip Torr","Bo Zhao"],"pdf_url":"https://arxiv.org/pdf/2310.10402v2.pdf","comment":"Code released at\n  (https://github.com/BAAI-DCAI/Training-Data-Synthesis)"},{"id":"http://arxiv.org/abs/2403.13556v1","updated":"2024-03-20T12:51:30Z","published":"2024-03-20T12:51:30Z","title":"Find n' Propagate: Open-Vocabulary 3D Object Detection in Urban\n  Environments","summary":"  In this work, we tackle the limitations of current LiDAR-based 3D object\ndetection systems, which are hindered by a restricted class vocabulary and the\nhigh costs associated with annotating new object classes. Our exploration of\nopen-vocabulary (OV) learning in urban environments aims to capture novel\ninstances using pre-trained vision-language models (VLMs) with multi-sensor\ndata. We design and benchmark a set of four potential solutions as baselines,\ncategorizing them into either top-down or bottom-up approaches based on their\ninput data strategies. While effective, these methods exhibit certain\nlimitations, such as missing novel objects in 3D box estimation or applying\nrigorous priors, leading to biases towards objects near the camera or of\nrectangular geometries. To overcome these limitations, we introduce a universal\n\\textsc{Find n' Propagate} approach for 3D OV tasks, aimed at maximizing the\nrecall of novel objects and propagating this detection capability to more\ndistant areas thereby progressively capturing more. In particular, we utilize a\ngreedy box seeker to search against 3D novel boxes of varying orientations and\ndepth in each generated frustum and ensure the reliability of newly identified\nboxes by cross alignment and density ranker. Additionally, the inherent bias\ntowards camera-proximal objects is alleviated by the proposed remote simulator,\nwhich randomly diversifies pseudo-labeled novel instances in the self-training\nprocess, combined with the fusion of base samples in the memory bank. Extensive\nexperiments demonstrate a 53% improvement in novel recall across diverse OV\nsettings, VLMs, and 3D detectors. Notably, we achieve up to a 3.97-fold\nincrease in Average Precision (AP) for novel object classes. The source code is\nmade available in the supplementary material.\n","authors":["Djamahl Etchegaray","Zi Huang","Tatsuya Harada","Yadan Luo"],"pdf_url":"https://arxiv.org/pdf/2403.13556v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13553v1","updated":"2024-03-20T12:46:02Z","published":"2024-03-20T12:46:02Z","title":"VCounselor: A Psychological Intervention Chat Agent Based on a\n  Knowledge-Enhanced Large Language Model","summary":"  Conversational artificial intelligence can already independently engage in\nbrief conversations with clients with psychological problems and provide\nevidence-based psychological interventions. The main objective of this study is\nto improve the effectiveness and credibility of the large language model in\npsychological intervention by creating a specialized agent, the VCounselor, to\naddress the limitations observed in popular large language models such as\nChatGPT in domain applications. We achieved this goal by proposing a new\naffective interaction structure and knowledge-enhancement structure. In order\nto evaluate VCounselor, this study compared the general large language model,\nthe fine-tuned large language model, and VCounselor's knowledge-enhanced large\nlanguage model. At the same time, the general large language model and the\nfine-tuned large language model will also be provided with an avatar to compare\nthem as an agent with VCounselor. The comparison results indicated that the\naffective interaction structure and knowledge-enhancement structure of\nVCounselor significantly improved the effectiveness and credibility of the\npsychological intervention, and VCounselor significantly provided positive\ntendencies for clients' emotions. The conclusion of this study strongly\nsupports that VConselor has a significant advantage in providing psychological\nsupport to clients by being able to analyze the patient's problems with\nrelative accuracy and provide professional-level advice that enhances support\nfor clients.\n","authors":["H. Zhang","Z. Qiao","H. Wang","B. Duan","J. Yin"],"pdf_url":"https://arxiv.org/pdf/2403.13553v1.pdf","comment":"24 pages, 6 figures"},{"id":"http://arxiv.org/abs/2403.13537v1","updated":"2024-03-20T12:14:54Z","published":"2024-03-20T12:14:54Z","title":"What explains the success of cross-modal fine-tuning with ORCA?","summary":"  ORCA (Shen et al., 2023) is a recent technique for cross-modal fine-tuning,\ni.e., applying pre-trained transformer models to modalities beyond their\ntraining data. The technique consists primarily of training an embedder and\nfine-tuning the embedder and model. Despite its high performance on a variety\nof downstream tasks, we do not understand precisely how each of these\ncomponents contribute to ORCA's success. Therefore, we run a series of\nablations and find that embedder training does not help 2D tasks at all,\ncontrary to what the original paper posits. In 1D tasks, some amount of\nembedder training is necessary but more is not better. In 4 out of 6 datasets\nwe experiment with, it is model fine-tuning that makes the biggest difference.\nThrough our ablations and baselines, we contribute a better understanding of\nthe individual components of ORCA.\n","authors":["Paloma García-de-Herreros","Vagrant Gautam","Philipp Slusallek","Dietrich Klakow","Marius Mosbach"],"pdf_url":"https://arxiv.org/pdf/2403.13537v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.08309v2","updated":"2024-03-20T12:12:59Z","published":"2023-07-17T08:09:40Z","title":"LogPrécis: Unleashing Language Models for Automated Shell Log Analysis","summary":"  The collection of security-related logs holds the key to understanding attack\nbehaviors and diagnosing vulnerabilities. Still, their analysis remains a\ndaunting challenge. Recently, Language Models (LMs) have demonstrated unmatched\npotential in understanding natural and programming languages. The question\narises whether and how LMs could be also useful for security experts since\ntheir logs contain intrinsically confused and obfuscated information. In this\npaper, we systematically study how to benefit from the state-of-the-art in LM\nto automatically analyze text-like Unix shell attack logs. We present a\nthorough design methodology that leads to LogPr\\'ecis. It receives as input raw\nshell sessions and automatically identifies and assigns the attacker tactic to\neach portion of the session, i.e., unveiling the sequence of the attacker's\ngoals. We demonstrate LogPr\\'ecis capability to support the analysis of two\nlarge datasets containing about 400,000 unique Unix shell attacks. LogPr\\'ecis\nreduces them into about 3,000 fingerprints, each grouping sessions with the\nsame sequence of tactics. The abstraction it provides lets the analyst better\nunderstand attacks, identify fingerprints, detect novelty, link similar\nattacks, and track families and mutations. Overall, LogPr\\'ecis, released as\nopen source, paves the way for better and more responsive defense against\ncyberattacks.\n","authors":["Matteo Boffa","Rodolfo Vieira Valentim","Luca Vassio","Danilo Giordano","Idilio Drago","Marco Mellia","Zied Ben Houidi"],"pdf_url":"https://arxiv.org/pdf/2307.08309v2.pdf","comment":"18 pages, Computer&Security\n  (https://www.sciencedirect.com/science/article/pii/S0167404824001068), code\n  available at https://github.com/SmartData-Polito/logprecis, models available\n  at https://huggingface.co/SmartDataPolito"},{"id":"http://arxiv.org/abs/2403.11959v2","updated":"2024-03-20T11:58:23Z","published":"2024-03-18T16:56:47Z","title":"IVAC-P2L: Leveraging Irregular Repetition Priors for Improving Video\n  Action Counting","summary":"  Video Action Counting (VAC) is crucial in analyzing sports, fitness, and\neveryday activities by quantifying repetitive actions in videos. However,\ntraditional VAC methods have overlooked the complexity of action repetitions,\nsuch as interruptions and the variability in cycle duration. Our research\naddresses the shortfall by introducing a novel approach to VAC, called\nIrregular Video Action Counting (IVAC). IVAC prioritizes modeling irregular\nrepetition patterns in videos, which we define through two primary aspects:\nInter-cycle Consistency and Cycle-interval Inconsistency. Inter-cycle\nConsistency ensures homogeneity in the spatial-temporal representations of\ncycle segments, signifying action uniformity within cycles. Cycle-interval\ninconsistency highlights the importance of distinguishing between cycle\nsegments and intervals based on their inherent content differences. To\nencapsulate these principles, we propose a new methodology that includes\nconsistency and inconsistency modules, supported by a unique pull-push loss\n(P2L) mechanism. The IVAC-P2L model applies a pull loss to promote coherence\namong cycle segment features and a push loss to clearly distinguish features of\ncycle segments from interval segments. Empirical evaluations conducted on the\nRepCount dataset demonstrate that the IVAC-P2L model sets a new benchmark in\nVAC task performance. Furthermore, the model demonstrates exceptional\nadaptability and generalization across various video contents, outperforming\nexisting models on two additional datasets, UCFRep and Countix, without the\nneed for dataset-specific optimization. These results confirm the efficacy of\nour approach in addressing irregular repetitions in videos and pave the way for\nfurther advancements in video analysis and understanding.\n","authors":["Hang Wang","Zhi-Qi Cheng","Youtian Du","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.11959v2.pdf","comment":"Source code: https://github.com/hwang-cs-ime/IVAC-P2L"},{"id":"http://arxiv.org/abs/2403.13524v1","updated":"2024-03-20T11:51:04Z","published":"2024-03-20T11:51:04Z","title":"Compress3D: a Compressed Latent Space for 3D Generation from a Single\n  Image","summary":"  3D generation has witnessed significant advancements, yet efficiently\nproducing high-quality 3D assets from a single image remains challenging. In\nthis paper, we present a triplane autoencoder, which encodes 3D models into a\ncompact triplane latent space to effectively compress both the 3D geometry and\ntexture information. Within the autoencoder framework, we introduce a 3D-aware\ncross-attention mechanism, which utilizes low-resolution latent representations\nto query features from a high-resolution 3D feature volume, thereby enhancing\nthe representation capacity of the latent space. Subsequently, we train a\ndiffusion model on this refined latent space. In contrast to solely relying on\nimage embedding for 3D generation, our proposed method advocates for the\nsimultaneous utilization of both image embedding and shape embedding as\nconditions. Specifically, the shape embedding is estimated via a diffusion\nprior model conditioned on the image embedding. Through comprehensive\nexperiments, we demonstrate that our method outperforms state-of-the-art\nalgorithms, achieving superior performance while requiring less training data\nand time. Our approach enables the generation of high-quality 3D assets in\nmerely 7 seconds on a single A100 GPU.\n","authors":["Bowen Zhang","Tianyu Yang","Yu Li","Lei Zhang","Xi Zhao"],"pdf_url":"https://arxiv.org/pdf/2403.13524v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13523v1","updated":"2024-03-20T11:50:16Z","published":"2024-03-20T11:50:16Z","title":"Have You Poisoned My Data? Defending Neural Networks against Data\n  Poisoning","summary":"  The unprecedented availability of training data fueled the rapid development\nof powerful neural networks in recent years. However, the need for such large\namounts of data leads to potential threats such as poisoning attacks:\nadversarial manipulations of the training data aimed at compromising the\nlearned model to achieve a given adversarial goal.\n  This paper investigates defenses against clean-label poisoning attacks and\nproposes a novel approach to detect and filter poisoned datapoints in the\ntransfer learning setting. We define a new characteristic vector representation\nof datapoints and show that it effectively captures the intrinsic properties of\nthe data distribution. Through experimental analysis, we demonstrate that\neffective poisons can be successfully differentiated from clean points in the\ncharacteristic vector space. We thoroughly evaluate our proposed approach and\ncompare it to existing state-of-the-art defenses using multiple architectures,\ndatasets, and poison budgets. Our evaluation shows that our proposal\noutperforms existing approaches in defense rate and final trained model\nperformance across all experimental settings.\n","authors":["Fabio De Gaspari","Dorjan Hitaj","Luigi V. Mancini"],"pdf_url":"https://arxiv.org/pdf/2403.13523v1.pdf","comment":"Paper accepted for publication at European Symposium on Research in\n  Computer Security (ESORICS) 2024"},{"id":"http://arxiv.org/abs/2403.13518v1","updated":"2024-03-20T11:38:30Z","published":"2024-03-20T11:38:30Z","title":"Motion Generation from Fine-grained Textual Descriptions","summary":"  The task of text2motion is to generate motion sequences from given textual\ndescriptions, where a model should explore the interactions between natural\nlanguage instructions and human body movements. While most existing works are\nconfined to coarse-grained motion descriptions (e.g., \"A man squats.\"),\nfine-grained ones specifying movements of relevant body parts are barely\nexplored. Models trained with coarse texts may not be able to learn mappings\nfrom fine-grained motion-related words to motion primitives, resulting in the\nfailure in generating motions from unseen descriptions. In this paper, we build\na large-scale language-motion dataset with fine-grained textual descriptions,\nFineHumanML3D, by feeding GPT-3.5-turbo with delicate prompts. Accordingly, we\ndesign a new text2motion model, FineMotionDiffuse, which makes full use of\nfine-grained textual information. Our experiments show that FineMotionDiffuse\ntrained on FineHumanML3D acquires good results in quantitative evaluation. We\nalso find this model can better generate spatially/chronologically composite\nmotions by learning the implicit mappings from simple descriptions to the\ncorresponding basic motions.\n","authors":["Kunhang Li","Yansong Feng"],"pdf_url":"https://arxiv.org/pdf/2403.13518v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13513v1","updated":"2024-03-20T11:27:20Z","published":"2024-03-20T11:27:20Z","title":"What if...?: Counterfactual Inception to Mitigate Hallucination Effects\n  in Large Multimodal Models","summary":"  This paper presents a way of enhancing the reliability of Large Multimodal\nModels (LMMs) in addressing hallucination effects, where models generate\nincorrect or unrelated responses. Without additional instruction tuning\nparadigm, we introduce Counterfactual Inception, a novel method that implants\ncounterfactual thoughts into LMMs using carefully chosen, misaligned\ncounterfactual keywords. This method is grounded in the concept of\ncounterfactual thinking, a cognitive process where humans consider alternative\nrealities and outcomes. By applying this human-like reasoning mechanism to\nLMMs, we aim to reduce hallucination effects and improve the models'\ntrustworthiness. We also propose Dual-modality Verification Process (DVP), a\nrigorous framework for selecting optimal counterfactual keywords to trigger\ncounterfactual thinking into LMMs, concurrently considering visual and\nlinguistic context. Our extensive experiments across various LMMs, including\nboth open-source and proprietary models, corroborate that our method\nsignificantly mitigates hallucination phenomena across different datasets.\n","authors":["Junho Kim","Yeon Ju Kim","Yong Man Ro"],"pdf_url":"https://arxiv.org/pdf/2403.13513v1.pdf","comment":"under review, code available:\n  https://github.com/IVY-LVLM/Counterfactual-Inception"}],"Databases":[{"id":"http://arxiv.org/abs/2312.06502v5","updated":"2024-03-20T17:38:28Z","published":"2023-12-11T16:27:04Z","title":"On enforcing dyadic-type homogeneous binary function product constraints\n  in MatBase","summary":"  Homogeneous binary function products are often encountered in the\nsub-universes modeled by databases, from genealogical trees to sports, from\neducation to healthcare, etc. Their properties must be discovered and enforced\nby the software applications managing such data to guarantee plausibility. The\n(Elementary) Mathematical Data Model provides 18 dyadic-type homogeneous binary\nfunction product constraint types. MatBase, an intelligent data and knowledge\nbase management system prototype, allows database designers to simply declare\nthem by only clicking corresponding checkboxes and automatically generates code\nfor enforcing them. This paper describes the algorithms that MatBase uses for\nenforcing all these 18 homogeneous binary function product constraint types,\nwhich may also be used by developers not having access to MatBase.\n","authors":["Christian Mancas"],"pdf_url":"https://arxiv.org/pdf/2312.06502v5.pdf","comment":"submitted on Dec. 7, 2023, to the Journal of Data Science and\n  Intelligent Systems (JDSIS), on Dec. 20, 2023, to the Journal of\n  Computational and Cognitive Engineering, both of Bon View Publishing,\n  Singapore, on Dec. 30 to the Journal of Current Research and Studies, and on\n  Jan. 25, 2024, to the Journal of Computer Science Research, Bilingual\n  Publishing Group, Singapore"},{"id":"http://arxiv.org/abs/2403.13694v1","updated":"2024-03-20T15:56:31Z","published":"2024-03-20T15:56:31Z","title":"Overview of Publicly Available Degradation Data Sets for Tasks within\n  Prognostics and Health Management","summary":"  Central to the efficacy of prognostics and health management methods is the\nacquisition and analysis of degradation data, which encapsulates the evolving\nhealth condition of engineering systems over time. Degradation data serves as a\nrich source of information, offering invaluable insights into the underlying\ndegradation processes, failure modes, and performance trends of engineering\nsystems. This paper provides an overview of publicly available degradation data\nsets.\n","authors":["Fabian Mauthe","Christopher Braun","Julian Raible","Peter Zeiler","Marco F. Huber"],"pdf_url":"https://arxiv.org/pdf/2403.13694v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13629v1","updated":"2024-03-20T14:27:49Z","published":"2024-03-20T14:27:49Z","title":"CheckMate: Evaluating Checkpointing Protocols for Streaming Dataflows","summary":"  Stream processing in the last decade has seen broad adoption in both\ncommercial and research settings. One key element for this success is the\nability of modern stream processors to handle failures while ensuring\nexactly-once processing guarantees. At the moment of writing, virtually all\nstream processors that guarantee exactly-once processing implement a variant of\nApache Flink's coordinated checkpoints - an extension of the original\nChandy-Lamport checkpoints from 1985. However, the reasons behind this\nprevalence of the coordinated approach remain anecdotal, as reported by\npractitioners of the stream processing community. At the same time, common\ncheckpointing approaches, such as the uncoordinated and the\ncommunication-induced ones, remain largely unexplored.\n  This paper is the first to address this gap by i) shedding light on why\npractitioners have favored the coordinated approach and ii) by investigating\nwhether there are viable alternatives. To this end, we implement three\ncheckpointing approaches that we surveyed and adapted for the distinct needs of\nstreaming dataflows. Our analysis shows that the coordinated approach\noutperforms the uncoordinated and communication-induced protocols under\nuniformly distributed workloads. To our surprise, however, the uncoordinated\napproach is not only competitive to the coordinated one in uniformly\ndistributed workloads, but it also outperforms the coordinated approach in\nskewed workloads. We conclude that rather than blindly employing coordinated\ncheckpointing, research should focus on optimizing the very promising\nuncoordinated approach, as it can address issues with skew and support\nprevalent cyclic queries. We believe that our findings can trigger further\nresearch into checkpointing mechanisms.\n","authors":["George Siachamis","Kyriakos Psarakis","Marios Fragkoulis","Arie van Deursen","Paris Carbone","Asterios Katsifodimos"],"pdf_url":"https://arxiv.org/pdf/2403.13629v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13597v1","updated":"2024-03-20T13:44:30Z","published":"2024-03-20T13:44:30Z","title":"No more optimization rules: LLM-enabled policy-based multi-modal query\n  optimizer (version 1)","summary":"  Large language model (LLM) has marked a pivotal moment in the field of\nmachine learning and deep learning. Recently its capability for query planning\nhas been investigated, including both single-modal and multi-modal queries.\nHowever, there is no work on the query optimization capability of LLM. As a\ncritical (or could even be the most important) step that significantly impacts\nthe execution performance of the query plan, such analysis and attempts should\nnot be missed. From another aspect, existing query optimizers are usually\nrule-based or rule-based + cost-based, i.e., they are dependent on manually\ncreated rules to complete the query plan rewrite/transformation. Given the fact\nthat modern optimizers include hundreds to thousands of rules, designing a\nmulti-modal query optimizer following a similar way is significantly\ntime-consuming since we will have to enumerate as many multi-modal optimization\nrules as possible, which has not been well addressed today. In this paper, we\ninvestigate the query optimization ability of LLM and use LLM to design LaPuda,\na novel LLM and Policy based multi-modal query optimizer. Instead of\nenumerating specific and detailed rules, LaPuda only needs a few abstract\npolicies to guide LLM in the optimization, by which much time and human effort\nare saved. Furthermore, to prevent LLM from making mistakes or negative\noptimization, we borrow the idea of gradient descent and propose a guided cost\ndescent (GCD) algorithm to perform the optimization, such that the optimization\ncan be kept in the correct direction. In our evaluation, our methods\nconsistently outperform the baselines in most cases. For example, the optimized\nplans generated by our methods result in 1~3x higher execution speed than those\nby the baselines.\n","authors":["Yifan Wang","Haodi Ma","Daisy Zhe Wang"],"pdf_url":"https://arxiv.org/pdf/2403.13597v1.pdf","comment":"Yifan and Haodi contribute equally to the work"},{"id":"http://arxiv.org/abs/2310.04268v3","updated":"2024-03-20T11:46:41Z","published":"2023-10-06T14:11:58Z","title":"WaZI: A Learned and Workload-aware Z-Index","summary":"  Learned indexes fit machine learning (ML) models to the data and use them to\nmake query operations more time and space-efficient. Recent works propose using\nlearned spatial indexes to improve spatial query performance by optimizing the\nstorage layout or internal search structures according to the data\ndistribution. However, only a few learned indexes exploit the query workload\ndistribution to enhance their performance. In addition, building and updating\nlearned spatial indexes are often costly on large datasets due to the\ninefficiency of (re)training ML models. In this paper, we present WaZI, a\nlearned and workload-aware variant of the Z-index, which jointly optimizes the\nstorage layout and search structures, as a viable solution for the above\nchallenges of spatial indexing. Specifically, we first formulate a cost\nfunction to measure the performance of a Z-index on a dataset for a range-query\nworkload. Then, we optimize the Z-index structure by minimizing the cost\nfunction through adaptive partitioning and ordering for index construction.\nMoreover, we design a novel page-skipping mechanism to improve the query\nperformance of WaZI by reducing access to irrelevant data pages. Our extensive\nexperiments show that the WaZI index improves range query time by 40% on\naverage over the baselines while always performing better or comparably to\nstate-of-the-art spatial indexes. Additionally, it also maintains good point\nquery performance. Generally, WaZI provides favorable tradeoffs among query\nlatency, construction time, and index size.\n","authors":["Sachith Pai","Michael Mathioudakis","Yanhao Wang"],"pdf_url":"https://arxiv.org/pdf/2310.04268v3.pdf","comment":"Camera-ready version accepted to EDBT 2024"},{"id":"http://arxiv.org/abs/2403.13492v1","updated":"2024-03-20T10:48:29Z","published":"2024-03-20T10:48:29Z","title":"Secure Query Processing with Linear Complexity","summary":"  We present LINQ, the first join protocol with linear complexity (in both\nrunning time and communication) under the secure multi-party computation model\n(MPC). It can also be extended to support all free-connex queries, a large\nclass of select-join-aggregate queries, still with linear complexity. This\nmatches the plaintext result for the query processing problem, as free-connex\nqueries are the largest class of queries known to be solvable in linear time in\nplaintext. We have then built a query processing system based on LINQ, and the\nexperimental results show that LINQ significantly outperforms the state of the\nart. For example, it can finish a query on three relations with an output size\nof 1 million tuples in around 100s in the LAN setting, while existing protocols\nthat support the query cannot finish in an hour. Thus LINQ brings MPC query\nprocessing closer to practicality.\n","authors":["Qiyao Luo","Yilei Wang","Wei Dong","Ke Yi"],"pdf_url":"https://arxiv.org/pdf/2403.13492v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13491v1","updated":"2024-03-20T10:48:00Z","published":"2024-03-20T10:48:00Z","title":"Distance Comparison Operators for Approximate Nearest Neighbor Search:\n  Exploration and Benchmark","summary":"  Approximate nearest neighbor search (ANNS) on high-dimensional vectors has\nbecome a fundamental and essential component in various machine learning tasks.\nPrior research has shown that the distance comparison operation is the\nbottleneck of ANNS, which determines the query and indexing performance. To\novercome this challenge, some novel methods have been proposed recently. The\nbasic idea is to estimate the actual distance with fewer calculations, at the\ncost of accuracy loss. Inspired by this, we also propose that some classical\ntechniques and deep learning models can also be adapted to this purpose. In\nthis paper, we systematically categorize the techniques that have been or can\nbe used to accelerate distance approximation. And to help the users understand\nthe pros and cons of different techniques, we design a fair and comprehensive\nbenchmark, Fudist implements these techniques with the same base index and\nevaluates them on 16 real datasets with several evaluation metrics. Designed as\nan independent and portable library, Fudist is orthogonal to the specific index\nstructure and thus can be easily utilized in the current ANNS library to\nachieve significant improvements.\n","authors":["Zeyu Wang","Haoran Xiong","Zhenying He","Peng Wang","Wei wang"],"pdf_url":"https://arxiv.org/pdf/2403.13491v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13286v1","updated":"2024-03-20T03:56:22Z","published":"2024-03-20T03:56:22Z","title":"A Sampling-based Framework for Hypothesis Testing on Large Attributed\n  Graphs","summary":"  Hypothesis testing is a statistical method used to draw conclusions about\npopulations from sample data, typically represented in tables. With the\nprevalence of graph representations in real-life applications, hypothesis\ntesting in graphs is gaining importance. In this work, we formalize node, edge,\nand path hypotheses in attributed graphs. We develop a sampling-based\nhypothesis testing framework, which can accommodate existing\nhypothesis-agnostic graph sampling methods. To achieve accurate and efficient\nsampling, we then propose a Path-Hypothesis-Aware SamplEr, PHASE, an m-\ndimensional random walk that accounts for the paths specified in a hypothesis.\nWe further optimize its time efficiency and propose PHASEopt. Experiments on\nreal datasets demonstrate the ability of our framework to leverage common graph\nsampling methods for hypothesis testing, and the superiority of\nhypothesis-aware sampling in terms of accuracy and time efficiency.\n","authors":["Yun Wang","Chrysanthi Kosyfaki","Sihem Amer-Yahia","Reynold Cheng"],"pdf_url":"https://arxiv.org/pdf/2403.13286v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13268v1","updated":"2024-03-20T03:07:30Z","published":"2024-03-20T03:07:30Z","title":"Unifews: Unified Entry-Wise Sparsification for Efficient Graph Neural\n  Network","summary":"  Graph Neural Networks (GNNs) have shown promising performance in various\ngraph learning tasks, but at the cost of resource-intensive computations. The\nprimary overhead of GNN update stems from graph propagation and weight\ntransformation, both involving operations on graph-scale matrices. Previous\nstudies attempt to reduce the computational budget by leveraging graph-level or\nnetwork-level sparsification techniques, resulting in downsized graph or\nweights. In this work, we propose Unifews, which unifies the two operations in\nan entry-wise manner considering individual matrix elements, and conducts joint\nedge-weight sparsification to enhance learning efficiency. The entry-wise\ndesign of Unifews enables adaptive compression across GNN layers with\nprogressively increased sparsity, and is applicable to a variety of\narchitectural designs with on-the-fly operation simplification. Theoretically,\nwe establish a novel framework to characterize sparsified GNN learning in view\nof a graph optimization process, and prove that Unifews effectively\napproximates the learning objective with bounded error and reduced\ncomputational load. We conduct extensive experiments to evaluate the\nperformance of our method in diverse settings. Unifews is advantageous in\njointly removing more than 90% of edges and weight entries with comparable or\nbetter accuracy than baseline models. The sparsification offers remarkable\nefficiency improvements including 10-20x matrix operation reduction and up to\n100x acceleration in graph propagation time for the largest graph at the\nbillion-edge scale.\n","authors":["Ningyi Liao","Zihao Yu","Siqiang Luo"],"pdf_url":"https://arxiv.org/pdf/2403.13268v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13914v1","updated":"2024-03-20T18:33:33Z","published":"2024-03-20T18:33:33Z","title":"Database Dependencies and Formal Concept Analysis","summary":"  This is an account of the characterization of database dependencies with\nFormal Concept Analysis.\n","authors":["Jaume Baixeries"],"pdf_url":"https://arxiv.org/pdf/2403.13914v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13863v1","updated":"2024-03-20T08:45:31Z","published":"2024-03-20T08:45:31Z","title":"DiffImpute: Tabular Data Imputation With Denoising Diffusion\n  Probabilistic Model","summary":"  Tabular data plays a crucial role in various domains but often suffers from\nmissing values, thereby curtailing its potential utility. Traditional\nimputation techniques frequently yield suboptimal results and impose\nsubstantial computational burdens, leading to inaccuracies in subsequent\nmodeling tasks. To address these challenges, we propose DiffImpute, a novel\nDenoising Diffusion Probabilistic Model (DDPM). Specifically, DiffImpute is\ntrained on complete tabular datasets, ensuring that it can produce credible\nimputations for missing entries without undermining the authenticity of the\nexisting data. Innovatively, it can be applied to various settings of Missing\nCompletely At Random (MCAR) and Missing At Random (MAR). To effectively handle\nthe tabular features in DDPM, we tailor four tabular denoising networks,\nspanning MLP, ResNet, Transformer, and U-Net. We also propose Harmonization to\nenhance coherence between observed and imputed data by infusing the data back\nand denoising them multiple times during the sampling stage. To enable\nefficient inference while maintaining imputation performance, we propose a\nrefined non-Markovian sampling process that works along with Harmonization.\nEmpirical evaluations on seven diverse datasets underscore the prowess of\nDiffImpute. Specifically, when paired with the Transformer as the denoising\nnetwork, it consistently outperforms its competitors, boasting an average\nranking of 1.7 and the most minimal standard deviation. In contrast, the next\nbest method lags with a ranking of 2.8 and a standard deviation of 0.9. The\ncode is available at https://github.com/Dendiiiii/DiffImpute.\n","authors":["Yizhu Wen","Kai Yi","Jing Ke","Yiqing Shen"],"pdf_url":"https://arxiv.org/pdf/2403.13863v1.pdf","comment":"26 pages, 6 figures"},{"id":"http://arxiv.org/abs/2403.14726v1","updated":"2024-03-20T23:23:40Z","published":"2024-03-20T23:23:40Z","title":"On Enforcing Existence and Non-Existence Constraints in MatBase","summary":"  Existence constraints were defined in the Relational Data Model, but,\nunfortunately, are not provided by any Relational Database Management System,\nexcept for their NOT NULL particular case. Our (Elementary) Mathematical Data\nModel extended them to function products and introduced their dual\nnon-existence constraints. MatBase, an intelligent data and knowledge base\nmanagement system prototype based on both these data models, not only provides\nexistence and non-existence constraints, but also automatically generates code\nfor their enforcement. This paper presents and discusses the algorithms used by\nMatBase to enforce these types of constraints.\n","authors":["Christian Mancas"],"pdf_url":"https://arxiv.org/pdf/2403.14726v1.pdf","comment":"Submitted to the BOHR International Journal of Computer Science\n  (BIJCS), ISSN: 2583-455X, on March 20, 2024"}],"Computer Science and Game Theory":[{"id":"http://arxiv.org/abs/2402.08851v3","updated":"2024-03-20T17:07:59Z","published":"2024-02-13T23:37:53Z","title":"Cardinal-Utility Matching Markets: The Quest for Envy-Freeness,\n  Pareto-Optimality, and Efficient Computability","summary":"  Unlike ordinal-utility matching markets, which are well-developed from the\nviewpoint of both theory and practice, recent insights from a computer science\nperspective have left cardinal-utility matching markets in a quandary. The\ncelebrated pricing-based mechanism for one-sided cardinal-utility matching\nmarkets due to Hylland and Zeckhauser, which had long eluded efficient\nalgorithms, was finally shown to be PPAD-complete.\n  This led us to ask the question: is there an alternative, polynomial time,\nmechanism for one-sided cardinal-utility matching markets which achieves the\ndesirable properties of HZ, i.e.\\ (ex-ante) envy-freeness (EF) and\nPareto-optimality (PO)? In this paper we show:\n  1. The problem of finding an EF+PO lottery in a one-sided cardinal-utility\nmatching market is PPAD-complete.\n  2. A $(2 + \\epsilon)$-approximately envy-free and (exactly) Pareto-optimal\nlottery can be found in polynomial time using Nash bargaining. Moreover, the\nresulting mechanism is $(2 + \\epsilon)$-approximately incentive compatible.\n  We also present several results on two-sided cardinal-utility matching\nmarkets, including non-existence of EF+PO lotteries as well as existence of\njustified-envy-free and weak Pareto-optimal lotteries.\n","authors":["Thorben Tröbst","Vijay V. Vazirani"],"pdf_url":"https://arxiv.org/pdf/2402.08851v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.14339v3","updated":"2024-03-20T04:34:54Z","published":"2023-08-28T06:32:04Z","title":"Entropy-Based Strategies for Multi-Bracket Pools","summary":"  Much work in the parimutuel betting literature has discussed estimating event\noutcome probabilities or developing optimal wagering strategies, particularly\nfor horse race betting. Some betting pools, however, involve betting not just\non a single event, but on a tuple of events. For example, pick six betting in\nhorse racing, March Madness bracket challenges, and predicting a randomly drawn\nbitstring each involve making a series of individual forecasts. Although\ntraditional optimal wagering strategies work well when the size of the tuple is\nvery small (e.g., betting on the winner of a horse race), they are intractable\nfor more general betting pools in higher dimensions (e.g., March Madness\nbracket challenges). Hence we pose the multi-brackets problem: supposing we\nwish to predict a tuple of events and that we know the true probabilities of\neach potential outcome of each event, what is the best way to tractably\ngenerate a set of $n$ predicted tuples? The most general version of this\nproblem is extremely difficult, so we begin with a simpler setting. In\nparticular, we generate $n$ independent predicted tuples according to a\ndistribution having optimal entropy. This entropy-based approach is tractable,\nscalable, and performs well.\n","authors":["Ryan S. Brill","Abraham J. Wyner","Ian J. Barnett"],"pdf_url":"https://arxiv.org/pdf/2308.14339v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13275v1","updated":"2024-03-20T03:19:51Z","published":"2024-03-20T03:19:51Z","title":"Analysing Guarantees in Australian Senate Outcomes","summary":"  Single Transferable Vote (STV) is used to elect candidates to the 76 seat\nAustralian Senate across six states and two territories. These eight STV\ncontests are counted using a combination of ballot scanners, manual data entry\nand tabulation software. On election night, some properties of the set of cast\nballots are determined by hand. This includes the first preference tallies of\neach party. This technical report considers whether there are some properties,\nsuch as individual candidates' first preference tallies, that, if assumed to be\naccurate, imply a portion of the election outcome. The paper also presents an\ninteresting example showing that the rules of STV tabulation used for the\nAustralian Senate can allow bizarre behaviour, such as votes increasing in\nvalue over time.\n","authors":["Michelle Blom"],"pdf_url":"https://arxiv.org/pdf/2403.13275v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14011v1","updated":"2024-03-20T22:21:22Z","published":"2024-03-20T22:21:22Z","title":"A Unified Toll Lane Framework for Autonomous and High-Occupancy Vehicles\n  in Interactive Mixed Autonomy","summary":"  In this study, we introduce a toll lane framework that optimizes the mixed\nflow of autonomous and high-occupancy vehicles on freeways, where human-driven\nand autonomous vehicles of varying commuter occupancy share a segment.\nAutonomous vehicles, with their ability to maintain shorter headways, boost\ntraffic throughput. Our framework designates a toll lane for autonomous\nvehicles with high occupancy to use free of charge, while others pay a toll. We\nexplore the lane choice equilibria when all vehicles minimize travel costs, and\ncharacterize the equilibria by ranking vehicles by their mobility enhancement\npotential, a concept we term the mobility degree. Through numerical examples,\nwe demonstrate the framework's utility in addressing design challenges such as\nsetting optimal tolls, determining occupancy thresholds, and designing lane\npolicies, showing how it facilitates the integration of high-occupancy and\nautonomous vehicles. We also propose an algorithm for assigning rational tolls\nto decrease total commuter delay and examine the effects of toll\nnon-compliance. Our findings suggest that self-interest-driven behavior\nmitigates moderate non-compliance impacts, highlighting the framework's\nresilience. This work presents a pioneering comprehensive analysis of a toll\nlane framework that emphasizes the coexistence of autonomous and high-occupancy\nvehicles, offering insights for traffic management improvements and the\nintegration of autonomous vehicles into existing transportation\ninfrastructures.\n","authors":["Ruolin Li","Philip N. Brown","Roberto Horowitz"],"pdf_url":"https://arxiv.org/pdf/2403.14011v1.pdf","comment":null}],"Information Theory":[{"id":"http://arxiv.org/abs/2402.05667v2","updated":"2024-03-20T15:33:23Z","published":"2024-02-08T13:38:23Z","title":"S$Ω$I: Score-based O-INFORMATION Estimation","summary":"  The analysis of scientific data and complex multivariate systems requires\ninformation quantities that capture relationships among multiple random\nvariables. Recently, new information-theoretic measures have been developed to\novercome the shortcomings of classical ones, such as mutual information, that\nare restricted to considering pairwise interactions. Among them, the concept of\ninformation synergy and redundancy is crucial for understanding the high-order\ndependencies between variables. One of the most prominent and versatile\nmeasures based on this concept is O-information, which provides a clear and\nscalable way to quantify the synergy-redundancy balance in multivariate\nsystems. However, its practical application is limited to simplified cases. In\nthis work, we introduce S$\\Omega$I, which allows for the first time to compute\nO-information without restrictive assumptions about the system. Our experiments\nvalidate our approach on synthetic data, and demonstrate the effectiveness of\nS$\\Omega$I in the context of a real-world use case.\n","authors":["Mustapha Bounoua","Giulio Franzese","Pietro Michiardi"],"pdf_url":"https://arxiv.org/pdf/2402.05667v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2103.14053v2","updated":"2024-03-20T14:51:01Z","published":"2021-03-25T18:01:56Z","title":"Quantum-inspired identification of complex cellular automata","summary":"  Elementary cellular automata (ECA) present iconic examples of complex\nsystems. Though described only by one-dimensional strings of binary cells\nevolving according to nearest-neighbour update rules, certain ECA rules\nmanifest complex dynamics capable of universal computation. Yet, the\nclassification of precisely which rules exhibit complex behaviour remains a\nsignificant challenge. Here we approach this question using tools from quantum\nstochastic modelling, where quantum statistical memory -- the memory required\nto model a stochastic process using a class of quantum machines -- can be used\nto quantify the structure of a stochastic process. By viewing ECA rules as\ntransformations of stochastic patterns, we ask: Does an ECA generate structure\nas quantified by the quantum statistical memory, and if so, how quickly? We\nillustrate how the growth of this measure over time correctly distinguishes\nsimple ECA from complex counterparts. Moreover, it provides a more refined\nmeans for quantitatively identifying complex ECAs -- providing a spectrum on\nwhich we can rank the complexity of ECA by the rate in which they generate\nstructure.\n","authors":["Matthew Ho","Andri Pradana","Thomas J. Elliott","Lock Yue Chew","Mile Gu"],"pdf_url":"https://arxiv.org/pdf/2103.14053v2.pdf","comment":"22 pages, 9 figures"},{"id":"http://arxiv.org/abs/2403.13632v1","updated":"2024-03-20T14:30:59Z","published":"2024-03-20T14:30:59Z","title":"Extremality of stabilizer states","summary":"  We investigate the extremality of stabilizer states to reveal their\nexceptional role in the space of all $n$-qubit/qudit states. We establish\nuncertainty principles for the characteristic function and the Wigner function\nof states, respectively. We find that only stabilizer states achieve saturation\nin these principles. Furthermore, we prove a general theorem that stabilizer\nstates are extremal for convex information measures invariant under local\nunitaries. We explore this extremality in the context of various quantum\ninformation and correlation measures, including entanglement entropy,\nconditional entropy and other entanglement measures. Additionally, leveraging\nthe recent discovery that stabilizer states are the limit states under quantum\nconvolution, we establish the monotonicity of the entanglement entropy and\nconditional entropy under quantum convolution. These results highlight the\nremarkable information-theoretic properties of stabilizer states. Their\nextremality provides valuable insights into their ability to capture\ninformation content and correlations, paving the way for further exploration of\ntheir potential in quantum information processing.\n","authors":["Kaifeng Bu"],"pdf_url":"https://arxiv.org/pdf/2403.13632v1.pdf","comment":"6+3 pages"},{"id":"http://arxiv.org/abs/2403.13615v1","updated":"2024-03-20T14:05:17Z","published":"2024-03-20T14:05:17Z","title":"MIMO Channel as a Neural Function: Implicit Neural Representations for\n  Extreme CSI Compression in Massive MIMO Systems","summary":"  Acquiring and utilizing accurate channel state information (CSI) can\nsignificantly improve transmission performance, thereby holding a crucial role\nin realizing the potential advantages of massive multiple-input multiple-output\n(MIMO) technology. Current prevailing CSI feedback approaches improve precision\nby employing advanced deep-learning methods to learn representative CSI\nfeatures for a subsequent compression process. Diverging from previous works,\nwe treat the CSI compression problem in the context of implicit neural\nrepresentations. Specifically, each CSI matrix is viewed as a neural function\nthat maps the CSI coordinates (antenna number and subchannel) to the\ncorresponding channel gains. Instead of transmitting the parameters of the\nimplicit neural functions directly, we transmit modulations based on the CSI\nmatrix derived through a meta-learning algorithm. Modulations are then applied\nto a shared base network to generate the elements of the CSI matrix.\nModulations corresponding to the CSI matrix are quantized and entropy-coded to\nfurther reduce the communication bandwidth, thus achieving extreme CSI\ncompression ratios. Numerical results show that our proposed approach achieves\nstate-of-the-art performance and showcases flexibility in feedback strategies.\n","authors":["Haotian Wu","Maojun Zhang","Yulin Shao","Krystian Mikolajczyk","Deniz Gündüz"],"pdf_url":"https://arxiv.org/pdf/2403.13615v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13573v1","updated":"2024-03-20T13:12:59Z","published":"2024-03-20T13:12:59Z","title":"Movable Antenna Enabled Interference Network: Joint Antenna Position and\n  Beamforming Design","summary":"  This paper investigates the utility of movable antenna (MA) assistance for\nthe multiple-input single-output (MISO) interference channel. We exploit an\nadditional design degree of freedom provided by MA to enhance the desired\nsignal and suppress interference so as to reduce the total transmit power of\ninterference network. To this end, we jointly optimize the MA positions and\ntransmit beamforming, subject to the signal-to-interference-plus-noise ratio\nconstraints of users. To address the non-convex optimization problem, we\npropose an efficient iterative algorithm to alternately optimize the MA\npositions via successive convex approximation method and the transmit\nbeamforming via second-order cone program approach. Numerical results\ndemonstrate that the proposed MA-enabled MISO interference network outperforms\nits conventional counterpart without MA, which significantly enhances the\ncapability of inter-cell frequency reuse and reduces the complexity of\ntransmitter design.\n","authors":["Honghao Wang","Qingqing Wu","Wen Chen"],"pdf_url":"https://arxiv.org/pdf/2403.13573v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13363v1","updated":"2024-03-20T07:45:50Z","published":"2024-03-20T07:45:50Z","title":"Massive MIMO CSI Feedback using Channel Prediction: How to Avoid Machine\n  Learning at UE?","summary":"  In the literature, machine learning (ML) has been implemented at the base\nstation (BS) and user equipment (UE) to improve the precision of downlink\nchannel state information (CSI). However, ML implementation at the UE can be\ninfeasible for various reasons, such as UE power consumption. Motivated by this\nissue, we propose a CSI learning mechanism at BS, called CSILaBS, to avoid ML\nat UE. To this end, by exploiting channel predictor (CP) at BS, a light-weight\npredictor function (PF) is considered for feedback evaluation at the UE.\nCSILaBS reduces over-the-air feedback overhead, improves CSI quality, and\nlowers the computation cost of UE. Besides, in a multiuser environment, we\npropose various mechanisms to select the feedback by exploiting PF while aiming\nto improve CSI accuracy. We also address various ML-based CPs, such as\nNeuralProphet (NP), an ML-inspired statistical algorithm. Furthermore, inspired\nto use a statistical model and ML together, we propose a novel hybrid framework\ncomposed of a recurrent neural network and NP, which yields better prediction\naccuracy than individual models. The performance of CSILaBS is evaluated\nthrough an empirical dataset recorded at Nokia Bell-Labs. The outcomes show\nthat ML elimination at UE can retain performance gains, for example, precoding\nquality.\n","authors":["Muhammad Karam Shehzad","Luca Rose","Mohamad Assaad"],"pdf_url":"https://arxiv.org/pdf/2403.13363v1.pdf","comment":"14 pages, 11 figures"},{"id":"http://arxiv.org/abs/2403.13350v1","updated":"2024-03-20T07:21:49Z","published":"2024-03-20T07:21:49Z","title":"Construction of Minimal Binary Linear Codes of dimension $n+3$","summary":"  In this paper, we will give the generic construction of a binary linear code\nof dimension $n+3$ and derive the necessary and sufficient conditions for the\nconstructed code to be minimal. Using generic construction, a new family of\nminimal binary linear code will be constructed from a special class of Boolean\nfunctions violating the Ashikhmin-Barg condition. We also obtain the weight\ndistribution of the constructed minimal binary linear code.\n","authors":["Wajid M. Shaikh","Rupali S. Jain","B. Surendranath Reddy","Bhagyashri S. Patil"],"pdf_url":"https://arxiv.org/pdf/2403.13350v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13345v1","updated":"2024-03-20T07:07:13Z","published":"2024-03-20T07:07:13Z","title":"Local Approximation of Secrecy Capacity","summary":"  This paper uses Euclidean Information Theory (EIT) to analyze the wiretap\nchannel. We investigate a scenario of efficiently transmitting a small amount\nof information subject to compression rate and secrecy constraints. We\ntransform the information-theoretic problem into a linear algebra problem and\nobtain the perturbed probability distributions such that secrecy is achievable.\nLocal approximations are being used in order to obtain an estimate of the\nsecrecy capacity by solving a generalized eigenvalue problem.\n","authors":["Emmanouil M. Athanasakos","Nicholas Kalouptsidis","Hariprasad Manjunath"],"pdf_url":"https://arxiv.org/pdf/2403.13345v1.pdf","comment":"Submitted to EUSIPCO 2024"},{"id":"http://arxiv.org/abs/2309.11872v3","updated":"2024-03-20T07:03:17Z","published":"2023-09-21T08:19:01Z","title":"Near-Field Beam Training: Joint Angle and Range Estimation with DFT\n  Codebook","summary":"  Prior works on near-field beam training have mostly assumed dedicated\npolar-domain codebook and on-grid range estimation, which, however, may suffer\nlong training overhead and degraded estimation accuracy. To address these\nissues, we propose in this paper new and efficient beam training schemes with\noff-grid range estimation by using conventional discrete Fourier transform\n(DFT) codebook. Specifically, we first analyze the received beam pattern at the\nuser when far-field beamforming vectors are used for beam scanning, and show an\ninteresting result that this beam pattern contains useful user angle and range\ninformation. Then, we propose two efficient schemes to jointly estimate the\nuser angle and range with the DFT codebook. The first scheme estimates the user\nangle based on a defined angular support and resolves the user range by\nleveraging an approximated angular support width, while the second scheme\nestimates the user range by minimizing a power ratio mean square error (MSE) to\nimprove the range estimation accuracy. Finally, numerical simulations show that\nour proposed schemes greatly reduce the near-field beam training overhead and\nimprove the range estimation accuracy as compared to various benchmark schemes.\n","authors":["Xun Wu","Changsheng You","Jiapeng Li","Yunpu Zhang"],"pdf_url":"https://arxiv.org/pdf/2309.11872v3.pdf","comment":"This work is submitted to IEEE for possible publication. Our other\n  works on near-field beam training include: 1) two-phase near-field beam\n  training (arXiv:2209.14798), 2) hierarchical near-field beam training\n  (arXiv:2302.12511), and 3) near-field beam management (arXiv:2306.16206)"},{"id":"http://arxiv.org/abs/2210.17159v2","updated":"2024-03-20T02:21:23Z","published":"2022-10-31T09:10:06Z","title":"PAGE: Prototype-Based Model-Level Explanations for Graph Neural Networks","summary":"  Aside from graph neural networks (GNNs) attracting significant attention as a\npowerful framework revolutionizing graph representation learning, there has\nbeen an increasing demand for explaining GNN models. Although various\nexplanation methods for GNNs have been developed, most studies have focused on\ninstance-level explanations, which produce explanations tailored to a given\ngraph instance. In our study, we propose Prototype-bAsed GNN-Explainer (PAGE),\na novel model-level GNN explanation method that explains what the underlying\nGNN model has learned for graph classification by discovering\nhuman-interpretable prototype graphs. Our method produces explanations for a\ngiven class, thus being capable of offering more concise and comprehensive\nexplanations than those of instance-level explanations. First, PAGE selects\nembeddings of class-discriminative input graphs on the graph-level embedding\nspace after clustering them. Then, PAGE discovers a common subgraph pattern by\niteratively searching for high matching node tuples using node-level embeddings\nvia a prototype scoring function, thereby yielding a prototype graph as our\nexplanation. Using six graph classification datasets, we demonstrate that PAGE\nqualitatively and quantitatively outperforms the state-of-the-art model-level\nexplanation method. We also carry out systematic experimental studies by\ndemonstrating the relationship between PAGE and instance-level explanation\nmethods, the robustness of PAGE to input data scarce environments, and the\ncomputational efficiency of the proposed prototype scoring function in PAGE.\n","authors":["Yong-Min Shin","Sun-Woo Kim","Won-Yong Shin"],"pdf_url":"https://arxiv.org/pdf/2210.17159v2.pdf","comment":"18 pages, 12 figures, 5 tables; to appear in the IEEE Transactions on\n  Pattern Analysis and Machine Intelligence (Please cite our journal version\n  that will appear in an upcoming issue. Its two-page extended summary was\n  presented in the AAAI-22 Student Abstract and Poster Program.)"}]},"2024-03-21T00:00:00Z":{"Distributed, Parallel, and Cluster Computing":[{"id":"http://arxiv.org/abs/2403.14445v1","updated":"2024-03-21T14:52:03Z","published":"2024-03-21T14:52:03Z","title":"History-Independent Concurrent Objects","summary":"  A data structure is called history independent if its internal memory\nrepresentation does not reveal the history of operations applied to it, only\nits current state. In this paper we study history independence for concurrent\ndata structures, and establish foundational possibility and impossibility\nresults. We show that a large class of concurrent objects cannot be implemented\nfrom smaller base objects in a manner that is both wait-free and history\nindependent; but if we settle for either lock-freedom instead of wait-freedom\nor for a weak notion of history independence, then at least one object in the\nclass, multi-valued single-reader single-writer registers, can be implemented\nfrom smaller base objects, binary registers.\n  On the other hand, using large base objects, we give a strong possibility\nresult in the form of a universal construction: an object with $s$ possible\nstates can be implemented in a wait-free, history-independent manner from\ncompare-and-swap base objects that each have $O(s + 2^n)$ possible memory\nstates, where $n$ is the number of processes in the system.\n","authors":["Hagit Attiya","Michael A. Bender","Martin Farach-Colton","Rotem Oshman","Noa Schiller"],"pdf_url":"https://arxiv.org/pdf/2403.14445v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14352v1","updated":"2024-03-21T12:28:24Z","published":"2024-03-21T12:28:24Z","title":"Accelerating Time-to-Science by Streaming Detector Data Directly into\n  Perlmutter Compute Nodes","summary":"  Recent advancements in detector technology have significantly increased the\nsize and complexity of experimental data, and high-performance computing (HPC)\nprovides a path towards more efficient and timely data processing. However,\nmovement of large data sets from acquisition systems to HPC centers introduces\nbottlenecks owing to storage I/O at both ends. This manuscript introduces a\nstreaming workflow designed for an high data rate electron detector that\nstreams data directly to compute node memory at the National Energy Research\nScientific Computing Center (NERSC), thereby avoiding storage I/O. The new\nworkflow deploys ZeroMQ-based services for data production, aggregation, and\ndistribution for on-the-fly processing, all coordinated through a distributed\nkey-value store. The system is integrated with the detector's science gateway\nand utilizes the NERSC Superfacility API to initiate streaming jobs through a\nweb-based frontend. Our approach achieves up to a 14-fold increase in data\nthroughput and enhances predictability and reliability compared to a I/O-heavy\nfile-based transfer workflow. Our work highlights the transformative potential\nof streaming workflows to expedite data analysis for time-sensitive\nexperiments.\n","authors":["Samuel S. Welborn","Bjoern Enders","Chris Harris","Peter Ercius","Deborah J. Bard"],"pdf_url":"https://arxiv.org/pdf/2403.14352v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14342v1","updated":"2024-03-21T12:20:36Z","published":"2024-03-21T12:20:36Z","title":"Adversary-Augmented Simulation to evaluate client-fairness on\n  HyperLedger Fabric","summary":"  This paper presents a novel adversary model specifically tailored to\ndistributed systems, with the aim to asses the security of blockchain\ntechnologies. Building upon literature on adversarial assumptions and\ncapabilities, we include classical notions of failure and communication models\nto classify and bind the use of adversarial actions. We focus on the effect of\nthese actions on properties of distributed protocols. A significant effort of\nour research is the integration of this model into the Multi-Agent eXperimenter\n(MAX) framework. This integration enables realistic simulations of adversarial\nattacks on blockchain systems. In particular, we have simulated attacks\nviolating a form of client-fairness on HyperLedger Fabric.\n","authors":["Erwan Mahe","Rouwaida Abdallah","Sara Tucci-Piergiovanni","Pierre-Yves Piriou"],"pdf_url":"https://arxiv.org/pdf/2403.14342v1.pdf","comment":"10 pages (2 pages of references), 8 figures"},{"id":"http://arxiv.org/abs/2309.10569v4","updated":"2024-03-21T07:12:06Z","published":"2023-09-19T12:26:56Z","title":"Task Graph offloading via Deep Reinforcement Learning in Mobile Edge\n  Computing","summary":"  Various mobile applications that comprise dependent tasks are gaining\nwidespread popularity and are increasingly complex. These applications often\nhave low-latency requirements, resulting in a significant surge in demand for\ncomputing resources. With the emergence of mobile edge computing (MEC), it\nbecomes the most significant issue to offload the application tasks onto\nsmall-scale devices deployed at the edge of the mobile network for obtaining a\nhigh-quality user experience. However, since the environment of MEC is dynamic,\nmost existing works focusing on task graph offloading, which rely heavily on\nexpert knowledge or accurate analytical models, fail to fully adapt to such\nenvironmental changes, resulting in the reduction of user experience. This\npaper investigates the task graph offloading in MEC, considering the\ntime-varying computation capabilities of edge computing devices. To adapt to\nenvironmental changes, we model the task graph scheduling for computation\noffloading as a Markov Decision Process (MDP). Then, we design a deep\nreinforcement learning algorithm (SATA-DRL) to learn the task scheduling\nstrategy from the interaction with the environment, to improve user experience.\nExtensive simulations validate that SATA-DRL is superior to existing strategies\nin terms of reducing average makespan and deadline violation.\n","authors":["Jiagang Liu","Yun Mi","Xinyu Zhang","Xiaocui Li"],"pdf_url":"https://arxiv.org/pdf/2309.10569v4.pdf","comment":"13 figures"},{"id":"http://arxiv.org/abs/2306.09427v2","updated":"2024-03-21T05:47:40Z","published":"2023-06-15T18:21:02Z","title":"A new open source framework for multiscale modeling of fibrous materials\n  on heterogeneous supercomputers","summary":"  This article presents MuMFiM, an open source application for multiscale\nmodeling of fibrous materials on massively parallel computers. MuMFiM uses two\nscales to represent fibrous materials such as biological network materials\n(extracellular matrix, connective tissue, etc.). It is designed to make use of\nmultiple levels of parallelism, including distributed parallelism of the macro\nand microscales as well as GPU accelerated data-parallelism of the microscale.\nScaling results of the GPU accelerated microscale show that solving microscale\nproblems concurrently on the GPU can lead to a 1000x speedup over the solution\nof a single RVE on the GPU. In addition, we show nearly optimal strong and weak\nscaling results of MuMFiM on up to 128 nodes of AiMOS (Rensselaer Polytechnic\nInstitute) which is composed of IBM AC922 nodes with 6 Volta V100 GPU and 2 20\ncore Power 9 CPUs each. We also show how MuMFiM can be used to solve problems\nof interest to the broader engineering community, in particular providing an\nexample of the facet capsule ligament (FCL) of the human spine undergoing\nuniaxial extension.\n","authors":["Jacob Merson","Catalin Picu","Mark S. Shephard"],"pdf_url":"https://arxiv.org/pdf/2306.09427v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14108v1","updated":"2024-03-21T03:40:46Z","published":"2024-03-21T03:40:46Z","title":"On the Power of Quantum Distributed Proofs","summary":"  Quantum nondeterministic distributed computing was recently introduced as\ndQMA (distributed quantum Merlin-Arthur) protocols by Fraigniaud, Le Gall,\nNishimura and Paz (ITCS 2021). In dQMA protocols, with the help of quantum\nproofs and local communication, nodes on a network verify a global property of\nthe network. Fraigniaud et al. showed that, when the network size is small,\nthere exists an exponential separation in proof size between distributed\nclassical and quantum verification protocols, for the equality problem, where\nthe verifiers check if all the data owned by a subset of them are identical. In\nthis paper, we further investigate and characterize the power of the dQMA\nprotocols for various decision problems.\n  First, we give a more efficient dQMA protocol for the equality problem with a\nsimpler analysis. This is done by adding a symmetrization step on each node and\nexploiting properties of the permutation test, which is a generalization of the\nSWAP test. We also show a quantum advantage for the equality problem on path\nnetworks still persists even when the network size is large, by considering\n``relay points'' between extreme nodes.\n  Second, we show that even in a general network, there exist efficient dQMA\nprotocols for the ranking verification problem, the Hamming distance problem,\nand more problems that derive from efficient quantum one-way communication\nprotocols. Third, in a line network, we construct an efficient dQMA protocol\nfor a problem that has an efficient two-party QMA communication protocol.\n  Finally, we obtain the first lower bounds on the proof and communication cost\nof dQMA protocols. To prove a lower bound on the equality problem, we show any\ndQMA protocol with an entangled proof between nodes can be simulated with a\ndQMA protocol with a separable proof between nodes by using a QMA\ncommunication-complete problem introduced by Raz and Shpilka (CCC 2004).\n","authors":["Atsuya Hasegawa","Srijita Kundu","Harumichi Nishimura"],"pdf_url":"https://arxiv.org/pdf/2403.14108v1.pdf","comment":"49 pages"},{"id":"http://arxiv.org/abs/2403.14097v1","updated":"2024-03-21T03:08:03Z","published":"2024-03-21T03:08:03Z","title":"Parcae: Proactive, Liveput-Optimized DNN Training on Preemptible\n  Instances","summary":"  Deep neural networks (DNNs) are becoming progressively large and costly to\ntrain. This paper aims to reduce DNN training costs by leveraging preemptible\ninstances on modern clouds, which can be allocated at a much lower price when\nidle but may be preempted by the cloud provider at any time. Prior work that\nsupports DNN training on preemptive instances employs a reactive approach to\nhandling instance preemptions and allocations after their occurrence, which\nonly achieves limited performance and scalability.\n  We present Parcae, a system that enables cheap, fast, and scalable DNN\ntraining on preemptible instances by proactively adjusting the parallelization\nstrategy of a DNN training job to adapt to predicted resource changes before\ninstance preemptions and allocations really happen, which significantly reduces\nthe cost of handling these events. Parcae optimizes liveput, a novel metric\nthat measures the expected training throughput of a DNN job under various\npossible preemption scenarios. Compared to existing reactive,\nthroughput-optimized systems, Parcae's proactive, live-optimized solution\nconsiders both the throughput of a job and its robustness under preemptions. To\noptimize liveput, Parcae supports lightweight instance migration and uses an\navailability predictor to forecast future preemptions. It then uses a liveput\noptimizer to discover an optimal strategy to parallelize DNN training under\npredicted preemptions. We evaluate Parcae on a variety of DNNs and preemption\ntraces and show that Parcae outperforms existing spot-instance DNN training\nsystems by up to 10$\\times$. More importantly, Parcae achieves near-optimal\nperformance for training large DNNs under frequent preemptions, in which case\nexisting approaches cannot make any progress.\n","authors":["Jiangfei Duan","Ziang Song","Xupeng Miao","Xiaoli Xi","Dahua Lin","Harry Xu","Minjia Zhang","Zhihao Jia"],"pdf_url":"https://arxiv.org/pdf/2403.14097v1.pdf","comment":"NSDI '24"},{"id":"http://arxiv.org/abs/2403.14047v1","updated":"2024-03-21T00:09:04Z","published":"2024-03-21T00:09:04Z","title":"Accelerating ViT Inference on FPGA through Static and Dynamic Pruning","summary":"  Vision Transformers (ViTs) have achieved state-of-the-art accuracy on various\ncomputer vision tasks. However, their high computational complexity prevents\nthem from being applied to many real-world applications. Weight and token\npruning are two well-known methods for reducing complexity: weight pruning\nreduces the model size and associated computational demands, while token\npruning further dynamically reduces the computation based on the input.\nCombining these two techniques should significantly reduce computation\ncomplexity and model size; however, naively integrating them results in\nirregular computation patterns, leading to significant accuracy drops and\ndifficulties in hardware acceleration.\n  Addressing the above challenges, we propose a comprehensive\nalgorithm-hardware codesign for accelerating ViT on FPGA through simultaneous\npruning -combining static weight pruning and dynamic token pruning. For\nalgorithm design, we systematically combine a hardware-aware structured\nblock-pruning method for pruning model parameters and a dynamic token pruning\nmethod for removing unimportant token vectors. Moreover, we design a novel\ntraining algorithm to recover the model's accuracy. For hardware design, we\ndevelop a novel hardware accelerator for executing the pruned model. The\nproposed hardware design employs multi-level parallelism with load balancing\nstrategy to efficiently deal with the irregular computation pattern led by the\ntwo pruning approaches. Moreover, we develop an efficient hardware mechanism\nfor efficiently executing the on-the-fly token pruning.\n","authors":["Dhruv Parikh","Shouyi Li","Bingyi Zhang","Rajgopal Kannan","Carl Busart","Viktor Prasanna"],"pdf_url":"https://arxiv.org/pdf/2403.14047v1.pdf","comment":"FCCM 2024"},{"id":"http://arxiv.org/abs/2403.14371v1","updated":"2024-03-21T12:59:24Z","published":"2024-03-21T12:59:24Z","title":"Loop Improvement: An Efficient Approach for Extracting Shared Features\n  from Heterogeneous Data without Central Server","summary":"  In federated learning, data heterogeneity significantly impacts performance.\nA typical solution involves segregating these parameters into shared and\npersonalized components, a concept also relevant in multi-task learning.\nAddressing this, we propose \"Loop Improvement\" (LI), a novel method enhancing\nthis separation and feature extraction without necessitating a central server\nor data interchange among participants. Our experiments reveal LI's superiority\nin several aspects: In personalized federated learning environments, LI\nconsistently outperforms the advanced FedALA algorithm in accuracy across\ndiverse scenarios. Additionally, LI's feature extractor closely matches the\nperformance achieved when aggregating data from all clients. In global model\ncontexts, employing LI with stacked personalized layers and an additional\nnetwork also yields comparable results to combined client data scenarios.\nFurthermore, LI's adaptability extends to multi-task learning, streamlining the\nextraction of common features across tasks and obviating the need for\nsimultaneous training. This approach not only enhances individual task\nperformance but also achieves accuracy levels on par with classic multi-task\nlearning methods where all tasks are trained simultaneously. LI integrates a\nloop topology with layer-wise and end-to-end training, compatible with various\nneural network models. This paper also delves into the theoretical\nunderpinnings of LI's effectiveness, offering insights into its potential\napplications. The code is on https://github.com/axedge1983/LI\n","authors":["Fei Li","Chu Kiong Loo","Wei Shiung Liew","Xiaofeng Liu"],"pdf_url":"https://arxiv.org/pdf/2403.14371v1.pdf","comment":"11 pages, 11 figures"},{"id":"http://arxiv.org/abs/2403.14123v1","updated":"2024-03-21T04:31:59Z","published":"2024-03-21T04:31:59Z","title":"AI and Memory Wall","summary":"  The availability of unprecedented unsupervised training data, along with\nneural scaling laws, has resulted in an unprecedented surge in model size and\ncompute requirements for serving/training LLMs. However, the main performance\nbottleneck is increasingly shifting to memory bandwidth. Over the past 20\nyears, peak server hardware FLOPS has been scaling at 3.0x/2yrs, outpacing the\ngrowth of DRAM and interconnect bandwidth, which have only scaled at 1.6 and\n1.4 times every 2 years, respectively. This disparity has made memory, rather\nthan compute, the primary bottleneck in AI applications, particularly in\nserving. Here, we analyze encoder and decoder Transformer models and show how\nmemory bandwidth can become the dominant bottleneck for decoder models. We\nargue for a redesign in model architecture, training, and deployment strategies\nto overcome this memory limitation.\n","authors":["Amir Gholami","Zhewei Yao","Sehoon Kim","Coleman Hooper","Michael W. Mahoney","Kurt Keutzer"],"pdf_url":"https://arxiv.org/pdf/2403.14123v1.pdf","comment":"Published in IEEE Micro Journal"},{"id":"http://arxiv.org/abs/2403.14854v1","updated":"2024-03-21T21:57:01Z","published":"2024-03-21T21:57:01Z","title":"Blockchain e Sistemas Distribuídos: conceitos básicos e\n  implicações","summary":"  Blockchain technology has emerged as a necessity for the decentralization of\npayment methods and transactions, but it has brought with it many properties of\ndistributed systems that have made it a crucial technology for overcoming some\nof society's challenges, especially in the context of decentralizing services,\ntransparency of information, availability, and security. Its architecture and\ncommunication methods, although possessing some complex nuances to understand,\nparticularly for the lay audience in the field of distributed systems,\nprotocols, and computer networks. In this article, we will explore some topics\nof distributed systems related to blockchain technology.\n","authors":["M. Witter","A. Rodrigo De Vit"],"pdf_url":"https://arxiv.org/pdf/2403.14854v1.pdf","comment":"in Portuguese language"},{"id":"http://arxiv.org/abs/2403.14853v1","updated":"2024-03-21T21:56:44Z","published":"2024-03-21T21:56:44Z","title":"iSpLib: A Library for Accelerating Graph Neural Networks using\n  Auto-tuned Sparse Operations","summary":"  Core computations in Graph Neural Network (GNN) training and inference are\noften mapped to sparse matrix operations such as sparse-dense matrix\nmultiplication (SpMM). These sparse operations are harder to optimize by manual\ntuning because their performance depends significantly on the sparsity of input\ngraphs, GNN models, and computing platforms. To address this challenge, we\npresent iSpLib, a PyTorch-based C++ library equipped with auto-tuned sparse\noperations. iSpLib expedites GNN training with a cache-enabled backpropagation\nthat stores intermediate matrices in local caches. The library offers a\nuser-friendly Python plug-in that allows users to take advantage of our\noptimized PyTorch operations out-of-the-box for any existing linear\nalgebra-based PyTorch implementation of popular GNNs (Graph Convolution\nNetwork, GraphSAGE, Graph Inference Network, etc.) with only two lines of\nadditional code. We demonstrate that iSpLib obtains up to 27x overall training\nspeedup compared to the equivalent PyTorch 2.1.0 and PyTorch Geometric 2.4.0\nimplementations on the CPU. Our library is publicly available at\nhttps://github.com/HipGraph/iSpLib (https://doi.org/10.5281/zenodo.10806511).\n","authors":["Md Saidul Hoque Anik","Pranav Badhe","Rohit Gampa","Ariful Azad"],"pdf_url":"https://arxiv.org/pdf/2403.14853v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14792v1","updated":"2024-03-21T19:13:15Z","published":"2024-03-21T19:13:15Z","title":"CASPER: Carbon-Aware Scheduling and Provisioning for Distributed Web\n  Services","summary":"  There has been a significant societal push towards sustainable practices,\nincluding in computing. Modern interactive workloads such as geo-distributed\nweb-services exhibit various spatiotemporal and performance flexibility,\nenabling the possibility to adapt the location, time, and intensity of\nprocessing to align with the availability of renewable and low-carbon energy.\nAn example is a web application hosted across multiple cloud regions, each with\nvarying carbon intensity based on their local electricity mix. Distributed\nload-balancing enables the exploitation of low-carbon energy through load\nmigration across regions, reducing web applications carbon footprint. In this\npaper, we present CASPER, a carbon-aware scheduling and provisioning system\nthat primarily minimizes the carbon footprint of distributed web services while\nalso respecting their Service Level Objectives (SLO). We formulate CASPER as an\nmulti-objective optimization problem that considers both the variable carbon\nintensity and latency constraints of the network. Our evaluation reveals the\nsignificant potential of CASPER in achieving substantial reductions in carbon\nemissions. Compared to baseline methods, CASPER demonstrates improvements of up\nto 70% with no latency performance degradation.\n","authors":["Abel Souza","Shruti Jasoria","Basundhara Chakrabarty","Alexander Bridgwater","Axel Lundberg","Filip Skogh","Ahmed Ali-Eldin","David Irwin","Prashant Shenoy"],"pdf_url":"https://arxiv.org/pdf/2403.14792v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14737v1","updated":"2024-03-21T13:54:36Z","published":"2024-03-21T13:54:36Z","title":"FedMef: Towards Memory-efficient Federated Dynamic Pruning","summary":"  Federated learning (FL) promotes decentralized training while prioritizing\ndata confidentiality. However, its application on resource-constrained devices\nis challenging due to the high demand for computation and memory resources to\ntrain deep learning models. Neural network pruning techniques, such as dynamic\npruning, could enhance model efficiency, but directly adopting them in FL still\nposes substantial challenges, including post-pruning performance degradation,\nhigh activation memory usage, etc. To address these challenges, we propose\nFedMef, a novel and memory-efficient federated dynamic pruning framework.\nFedMef comprises two key components. First, we introduce the budget-aware\nextrusion that maintains pruning efficiency while preserving post-pruning\nperformance by salvaging crucial information from parameters marked for pruning\nwithin a given budget. Second, we propose scaled activation pruning to\neffectively reduce activation memory footprints, which is particularly\nbeneficial for deploying FL to memory-limited devices. Extensive experiments\ndemonstrate the effectiveness of our proposed FedMef. In particular, it\nachieves a significant reduction of 28.5% in memory footprint compared to\nstate-of-the-art methods while obtaining superior accuracy.\n","authors":["Hong Huang","Weiming Zhuang","Chen Chen","Lingjuan Lyu"],"pdf_url":"https://arxiv.org/pdf/2403.14737v1.pdf","comment":"Accepted by CVPR2024"}],"Artificial Intelligence":[{"id":"http://arxiv.org/abs/2403.14624v1","updated":"2024-03-21T17:59:50Z","published":"2024-03-21T17:59:50Z","title":"MathVerse: Does Your Multi-modal LLM Truly See the Diagrams in Visual\n  Math Problems?","summary":"  The remarkable progress of Multi-modal Large Language Models (MLLMs) has\ngarnered unparalleled attention, due to their superior performance in visual\ncontexts. However, their capabilities in visual math problem-solving remain\ninsufficiently evaluated and understood. We investigate current benchmarks to\nincorporate excessive visual content within textual questions, which\npotentially assist MLLMs in deducing answers without truly interpreting the\ninput diagrams. To this end, we introduce MathVerse, an all-around visual math\nbenchmark designed for an equitable and in-depth evaluation of MLLMs. We\nmeticulously collect 2,612 high-quality, multi-subject math problems with\ndiagrams from publicly available sources. Each problem is then transformed by\nhuman annotators into six distinct versions, each offering varying degrees of\ninformation content in multi-modality, contributing to 15K test samples in\ntotal. This approach allows MathVerse to comprehensively assess whether and how\nmuch MLLMs can truly understand the visual diagrams for mathematical reasoning.\nIn addition, we propose a Chain-of-Thought (CoT) evaluation strategy for a\nfine-grained assessment of the output answers. Rather than naively judging True\nor False, we employ GPT-4(V) to adaptively extract crucial reasoning steps, and\nthen score each step with detailed error analysis, which can reveal the\nintermediate CoT reasoning quality by MLLMs. We hope the MathVerse benchmark\nmay provide unique insights to guide the future development of MLLMs. Project\npage: https://mathverse-cuhk.github.io\n","authors":["Renrui Zhang","Dongzhi Jiang","Yichi Zhang","Haokun Lin","Ziyu Guo","Pengshuo Qiu","Aojun Zhou","Pan Lu","Kai-Wei Chang","Peng Gao","Hongsheng Li"],"pdf_url":"https://arxiv.org/pdf/2403.14624v1.pdf","comment":"46 Pages, Work in Progress, Benchmark Project Page:\n  https://mathverse-cuhk.github.io"},{"id":"http://arxiv.org/abs/2403.14617v1","updated":"2024-03-21T17:59:03Z","published":"2024-03-21T17:59:03Z","title":"Videoshop: Localized Semantic Video Editing with Noise-Extrapolated\n  Diffusion Inversion","summary":"  We introduce Videoshop, a training-free video editing algorithm for localized\nsemantic edits. Videoshop allows users to use any editing software, including\nPhotoshop and generative inpainting, to modify the first frame; it\nautomatically propagates those changes, with semantic, spatial, and temporally\nconsistent motion, to the remaining frames. Unlike existing methods that enable\nedits only through imprecise textual instructions, Videoshop allows users to\nadd or remove objects, semantically change objects, insert stock photos into\nvideos, etc. with fine-grained control over locations and appearance. We\nachieve this through image-based video editing by inverting latents with noise\nextrapolation, from which we generate videos conditioned on the edited image.\nVideoshop produces higher quality edits against 6 baselines on 2 editing\nbenchmarks using 10 evaluation metrics.\n","authors":["Xiang Fan","Anand Bhattad","Ranjay Krishna"],"pdf_url":"https://arxiv.org/pdf/2403.14617v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.16828v2","updated":"2024-03-21T17:56:19Z","published":"2023-10-25T17:57:07Z","title":"TD-MPC2: Scalable, Robust World Models for Continuous Control","summary":"  TD-MPC is a model-based reinforcement learning (RL) algorithm that performs\nlocal trajectory optimization in the latent space of a learned implicit\n(decoder-free) world model. In this work, we present TD-MPC2: a series of\nimprovements upon the TD-MPC algorithm. We demonstrate that TD-MPC2 improves\nsignificantly over baselines across 104 online RL tasks spanning 4 diverse task\ndomains, achieving consistently strong results with a single set of\nhyperparameters. We further show that agent capabilities increase with model\nand data size, and successfully train a single 317M parameter agent to perform\n80 tasks across multiple task domains, embodiments, and action spaces. We\nconclude with an account of lessons, opportunities, and risks associated with\nlarge TD-MPC2 agents. Explore videos, models, data, code, and more at\nhttps://tdmpc2.com\n","authors":["Nicklas Hansen","Hao Su","Xiaolong Wang"],"pdf_url":"https://arxiv.org/pdf/2310.16828v2.pdf","comment":"ICLR 2024. Explore videos, models, data, code, and more at\n  https://tdmpc2.com"},{"id":"http://arxiv.org/abs/2403.14592v1","updated":"2024-03-21T17:47:28Z","published":"2024-03-21T17:47:28Z","title":"Envisioning the Next-Generation AI Coding Assistants: Insights &\n  Proposals","summary":"  As a research-product hybrid group in AI for Software Engineering (AI4SE), we\npresent four key takeaways from our experience developing in-IDE AI coding\nassistants. AI coding assistants should set clear expectations for usage,\nintegrate with advanced IDE capabilities and existing extensions, use\nextendable backend designs, and collect app data responsibly for downstream\nanalyses. We propose open questions and challenges that academia and industry\nshould address to realize the vision of next-generation AI coding assistants.\n","authors":["Khanh Nghiem","Anh Minh Nguyen","Nghi D. Q. Bui"],"pdf_url":"https://arxiv.org/pdf/2403.14592v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14589v1","updated":"2024-03-21T17:43:44Z","published":"2024-03-21T17:43:44Z","title":"ReAct Meets ActRe: Autonomous Annotations of Agent Trajectories for\n  Contrastive Self-Training","summary":"  Language agents have demonstrated autonomous decision-making abilities by\nreasoning with foundation models. Recently, efforts have been made to train\nlanguage agents for performance improvement, with multi-step reasoning and\naction trajectories as the training data. However, collecting such trajectories\nstill requires considerable human effort, by either artificial annotations or\nimplementations of diverse prompting frameworks. In this work, we propose\nA$^3$T, a framework that enables the Autonomous Annotation of Agent\nTrajectories in the style of ReAct. The central role is an ActRe prompting\nagent, which explains the reason for an arbitrary action. When randomly\nsampling an external action, the ReAct-style agent could query the ActRe agent\nwith the action to obtain its textual rationales. Novel trajectories are then\nsynthesized by prepending the posterior reasoning from ActRe to the sampled\naction. In this way, the ReAct-style agent executes multiple trajectories for\nthe failed tasks, and selects the successful ones to supplement its failed\ntrajectory for contrastive self-training. Realized by policy gradient methods\nwith binarized rewards, the contrastive self-training with accumulated\ntrajectories facilitates a closed loop for multiple rounds of language agent\nself-improvement. We conduct experiments using QLoRA fine-tuning with the\nopen-sourced Mistral-7B-Instruct-v0.2. In AlfWorld, the agent trained with\nA$^3$T obtains a 1-shot success rate of 96%, and 100% success with 4 iterative\nrounds. In WebShop, the 1-shot performance of the A$^3$T agent matches human\naverage, and 4 rounds of iterative refinement lead to the performance\napproaching human experts. A$^3$T agents significantly outperform existing\ntechniques, including prompting with GPT-4, advanced agent frameworks, and\nfully fine-tuned LLMs.\n","authors":["Zonghan Yang","Peng Li","Ming Yan","Ji Zhang","Fei Huang","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2403.14589v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14582v1","updated":"2024-03-21T17:36:08Z","published":"2024-03-21T17:36:08Z","title":"Large Language Models for Multi-Choice Question Classification of\n  Medical Subjects","summary":"  The aim of this paper is to evaluate whether large language models trained on\nmulti-choice question data can be used to discriminate between medical\nsubjects. This is an important and challenging task for automatic question\nanswering. To achieve this goal, we train deep neural networks for multi-class\nclassification of questions into the inferred medical subjects. Using our\nMulti-Question (MQ) Sequence-BERT method, we outperform the state-of-the-art\nresults on the MedMCQA dataset with an accuracy of 0.68 and 0.60 on their\ndevelopment and test sets, respectively. In this sense, we show the capability\nof AI and LLMs in particular for multi-classification tasks in the Healthcare\ndomain.\n","authors":["Víctor Ponce-López"],"pdf_url":"https://arxiv.org/pdf/2403.14582v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.12258v5","updated":"2024-03-21T17:29:37Z","published":"2024-01-21T16:59:45Z","title":"Emergent Dominance Hierarchies in Reinforcement Learning Agents","summary":"  Modern Reinforcement Learning (RL) algorithms are able to outperform humans\nin a wide variety of tasks. Multi-agent reinforcement learning (MARL) settings\npresent additional challenges, and successful cooperation in mixed-motive\ngroups of agents depends on a delicate balancing act between individual and\ngroup objectives. Social conventions and norms, often inspired by human\ninstitutions, are used as tools for striking this balance.\n  In this paper, we examine a fundamental, well-studied social convention that\nunderlies cooperation in both animal and human societies: dominance\nhierarchies.\n  We adapt the ethological theory of dominance hierarchies to artificial\nagents, borrowing the established terminology and definitions with as few\namendments as possible. We demonstrate that populations of RL agents, operating\nwithout explicit programming or intrinsic rewards, can invent, learn, enforce,\nand transmit a dominance hierarchy to new populations. The dominance\nhierarchies that emerge have a similar structure to those studied in chickens,\nmice, fish, and other species.\n","authors":["Ram Rachum","Yonatan Nakar","Bill Tomlinson","Nitay Alon","Reuth Mirsky"],"pdf_url":"https://arxiv.org/pdf/2401.12258v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14566v1","updated":"2024-03-21T17:09:20Z","published":"2024-03-21T17:09:20Z","title":"A survey on Concept-based Approaches For Model Improvement","summary":"  The focus of recent research has shifted from merely increasing the Deep\nNeural Networks (DNNs) performance in various tasks to DNNs, which are more\ninterpretable to humans. The field of eXplainable Artificial Intelligence (XAI)\nhas observed various techniques, including saliency-based and concept-based\napproaches. Concept-based approaches explain the model's decisions in simple\nhuman understandable terms called Concepts. Concepts are human interpretable\nunits of data and are the thinking ground of humans. Explanations in terms of\nconcepts enable detecting spurious correlations, inherent biases, or\nclever-hans. With the advent of concept-based explanations, there have been\nvarious concept representation methods and automatic concept discovery\nalgorithms. Some recent methods use concepts for post-hoc model disentanglement\nevaluation, while others use them for ante-hoc training. The concept-based\napproaches are new, with many representations coming up, and there is very\nlimited work on Concept-based Model improvement. We provide a systematic review\nand taxonomy of various concept representations and their discovery algorithms\nin DNNs, specifically in vision. We also provide details on concept-based model\nimprovement literature, which is the first to survey concept-based model\nimprovement methods.\n","authors":["Avani Gupta","P J Narayanan"],"pdf_url":"https://arxiv.org/pdf/2403.14566v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14562v1","updated":"2024-03-21T17:06:17Z","published":"2024-03-21T17:06:17Z","title":"The Era of Semantic Decoding","summary":"  Recent work demonstrated great promise in the idea of orchestrating\ncollaborations between LLMs, human input, and various tools to address the\ninherent limitations of LLMs. We propose a novel perspective called semantic\ndecoding, which frames these collaborative processes as optimization procedures\nin semantic space. Specifically, we conceptualize LLMs as semantic processors\nthat manipulate meaningful pieces of information that we call semantic tokens\n(known thoughts). LLMs are among a large pool of other semantic processors,\nincluding humans and tools, such as search engines or code executors.\nCollectively, semantic processors engage in dynamic exchanges of semantic\ntokens to progressively construct high-utility outputs. We refer to these\norchestrated interactions among semantic processors, optimizing and searching\nin semantic space, as semantic decoding algorithms. This concept draws a direct\nparallel to the well-studied problem of syntactic decoding, which involves\ncrafting algorithms to best exploit auto-regressive language models for\nextracting high-utility sequences of syntactic tokens. By focusing on the\nsemantic level and disregarding syntactic details, we gain a fresh perspective\non the engineering of AI systems, enabling us to imagine systems with much\ngreater complexity and capabilities. In this position paper, we formalize the\ntransition from syntactic to semantic tokens as well as the analogy between\nsyntactic and semantic decoding. Subsequently, we explore the possibilities of\noptimizing within the space of semantic tokens via semantic decoding\nalgorithms. We conclude with a list of research opportunities and questions\narising from this fresh perspective. The semantic decoding perspective offers a\npowerful abstraction for search and optimization directly in the space of\nmeaningful concepts, with semantic tokens as the fundamental units of a new\ntype of computation.\n","authors":["Maxime Peyrard","Martin Josifoski","Robert West"],"pdf_url":"https://arxiv.org/pdf/2403.14562v1.pdf","comment":"25 pages, 3 figures"},{"id":"http://arxiv.org/abs/2403.14551v1","updated":"2024-03-21T16:52:01Z","published":"2024-03-21T16:52:01Z","title":"Lexicon-Level Contrastive Visual-Grounding Improves Language Modeling","summary":"  Today's most accurate language models are trained on orders of magnitude more\nlanguage data than human language learners receive - but with no supervision\nfrom other sensory modalities that play a crucial role in human learning. Can\nwe make LMs' representations and predictions more accurate (and more\nhuman-like) with more ecologically plausible supervision? This paper describes\nLexiContrastive Grounding (LCG), a grounded language learning procedure that\nleverages visual supervision to improve textual representations.\nLexiContrastive Grounding combines a next token prediction strategy with a\ncontrastive visual grounding objective, focusing on early-layer representations\nthat encode lexical information. Across multiple word-learning and\nsentence-understanding benchmarks, LexiContrastive Grounding not only\noutperforms standard language-only models in learning efficiency, but also\nimproves upon vision-and-language learning procedures including CLIP, GIT,\nFlamingo, and Vokenization. Moreover, LexiContrastive Grounding improves\nperplexity by around 5% on multiple language modeling tasks. This work\nunderscores the potential of incorporating visual grounding into language\nmodels, aligning more closely with the multimodal nature of human language\nacquisition.\n","authors":["Chengxu Zhuang","Evelina Fedorenko","Jacob Andreas"],"pdf_url":"https://arxiv.org/pdf/2403.14551v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14550v1","updated":"2024-03-21T16:50:12Z","published":"2024-03-21T16:50:12Z","title":"Dynamic Explanation Emphasis in Human-XAI Interaction with Communication\n  Robot","summary":"  Communication robots have the potential to contribute to effective human-XAI\ninteraction as an interface that goes beyond textual or graphical explanations.\nOne of their strengths is that they can use physical and vocal expressions to\nadd detailed nuances to explanations. However, it is not clear how a robot can\napply such expressions, or in particular, how we can develop a strategy to\nadaptively use such expressions depending on the task and user in dynamic\ninteractions. To address this question, this paper proposes DynEmph, a method\nfor a communication robot to decide where to emphasize XAI-generated\nexplanations with physical expressions. It predicts the effect of emphasizing\ncertain points on a user and aims to minimize the expected difference between\npredicted user decisions and AI-suggested ones. DynEmph features a strategy for\ndeciding where to emphasize in a data-driven manner, relieving engineers from\nthe need to manually design a strategy. We further conducted experiments to\ninvestigate how emphasis selection strategies affect the performance of user\ndecisions. The results suggest that, while a naive strategy (emphasizing\nexplanations for an AI's most probable class) does not necessarily work better,\nDynEmph effectively guides users to better decisions under the condition that\nthe performance of the AI suggestion is high.\n","authors":["Yosuke Fukuchi","Seiji Yamada"],"pdf_url":"https://arxiv.org/pdf/2403.14550v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.08812v3","updated":"2024-03-21T16:44:41Z","published":"2024-02-13T21:33:12Z","title":"Intelligent Canvas: Enabling Design-Like Exploratory Visual Data\n  Analysis with Generative AI through Rapid Prototyping, Iteration and Curation","summary":"  Complex data analysis inherently seeks unexpected insights through\nexploratory visual analysis methods, transcending logical, step-by-step\nprocessing. However, existing interfaces such as notebooks and dashboards have\nlimitations in exploration and comparison for visual data analysis. Addressing\nthese limitations, we introduce a \"design-like\" intelligent canvas environment\nintegrating generative AI into data analysis, offering rapid prototyping,\niteration, and comparative visualization management. Our dual contributions\ninclude the integration of generative AI components into a canvas interface,\nand empirical findings from a user study (N=10) evaluating the effectiveness of\nthe canvas interface.\n","authors":["Zijian Ding","Joel Chan"],"pdf_url":"https://arxiv.org/pdf/2402.08812v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14539v1","updated":"2024-03-21T16:40:10Z","published":"2024-03-21T16:40:10Z","title":"Object-Centric Domain Randomization for 3D Shape Reconstruction in the\n  Wild","summary":"  One of the biggest challenges in single-view 3D shape reconstruction in the\nwild is the scarcity of <3D shape, 2D image>-paired data from real-world\nenvironments. Inspired by remarkable achievements via domain randomization, we\npropose ObjectDR which synthesizes such paired data via a random simulation of\nvisual variations in object appearances and backgrounds. Our data synthesis\nframework exploits a conditional generative model (e.g., ControlNet) to\ngenerate images conforming to spatial conditions such as 2.5D sketches, which\nare obtainable through a rendering process of 3D shapes from object collections\n(e.g., Objaverse-XL). To simulate diverse variations while preserving object\nsilhouettes embedded in spatial conditions, we also introduce a disentangled\nframework which leverages an initial object guidance. After synthesizing a wide\nrange of data, we pre-train a model on them so that it learns to capture a\ndomain-invariant geometry prior which is consistent across various domains. We\nvalidate its effectiveness by substantially improving 3D shape reconstruction\nmodels on a real-world benchmark. In a scale-up evaluation, our pre-training\nachieves 23.6% superior results compared with the pre-training on high-quality\ncomputer graphics renderings.\n","authors":["Junhyeong Cho","Kim Youwang","Hunmin Yang","Tae-Hyun Oh"],"pdf_url":"https://arxiv.org/pdf/2403.14539v1.pdf","comment":"Project Page: https://ObjectDR.github.io"},{"id":"http://arxiv.org/abs/2403.14526v1","updated":"2024-03-21T16:26:19Z","published":"2024-03-21T16:26:19Z","title":"Click to Grasp: Zero-Shot Precise Manipulation via Visual Diffusion\n  Descriptors","summary":"  Precise manipulation that is generalizable across scenes and objects remains\na persistent challenge in robotics. Current approaches for this task heavily\ndepend on having a significant number of training instances to handle objects\nwith pronounced visual and/or geometric part ambiguities. Our work explores the\ngrounding of fine-grained part descriptors for precise manipulation in a\nzero-shot setting by utilizing web-trained text-to-image diffusion-based\ngenerative models. We tackle the problem by framing it as a dense semantic part\ncorrespondence task. Our model returns a gripper pose for manipulating a\nspecific part, using as reference a user-defined click from a source image of a\nvisually different instance of the same object. We require no manual grasping\ndemonstrations as we leverage the intrinsic object geometry and features.\nPractical experiments in a real-world tabletop scenario validate the efficacy\nof our approach, demonstrating its potential for advancing semantic-aware\nrobotics manipulation. Web page: https://tsagkas.github.io/click2grasp\n","authors":["Nikolaos Tsagkas","Jack Rome","Subramanian Ramamoorthy","Oisin Mac Aodha","Chris Xiaoxuan Lu"],"pdf_url":"https://arxiv.org/pdf/2403.14526v1.pdf","comment":"8 pages, 4 figures"},{"id":"http://arxiv.org/abs/2403.11879v2","updated":"2024-03-21T16:15:52Z","published":"2024-03-18T15:32:02Z","title":"Unimodal Multi-Task Fusion for Emotional Mimicry Prediciton","summary":"  In this study, we propose a methodology for the Emotional Mimicry Intensity\n(EMI) Estimation task within the context of the 6th Workshop and Competition on\nAffective Behavior Analysis in-the-wild. Our approach leverages the Wav2Vec 2.0\nframework, pre-trained on a comprehensive podcast dataset, to extract a broad\nrange of audio features encompassing both linguistic and paralinguistic\nelements. We enhance feature representation through a fusion technique that\nintegrates individual features with a global mean vector, introducing global\ncontextual insights into our analysis. Additionally, we incorporate a\npre-trained valence-arousal-dominance (VAD) module from the Wav2Vec 2.0 model.\nOur fusion employs a Long Short-Term Memory (LSTM) architecture for efficient\ntemporal analysis of audio data. Utilizing only the provided audio data, our\napproach demonstrates significant improvements over the established baseline.\n","authors":["Tobias Hallmen","Fabian Deuser","Norbert Oswald","Elisabeth André"],"pdf_url":"https://arxiv.org/pdf/2403.11879v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14508v1","updated":"2024-03-21T16:02:52Z","published":"2024-03-21T16:02:52Z","title":"Constrained Reinforcement Learning with Smoothed Log Barrier Function","summary":"  Reinforcement Learning (RL) has been widely applied to many control tasks and\nsubstantially improved the performances compared to conventional control\nmethods in many domains where the reward function is well defined. However, for\nmany real-world problems, it is often more convenient to formulate optimization\nproblems in terms of rewards and constraints simultaneously. Optimizing such\nconstrained problems via reward shaping can be difficult as it requires tedious\nmanual tuning of reward functions with several interacting terms. Recent\nformulations which include constraints mostly require a pre-training phase,\nwhich often needs human expertise to collect data or assumes having a\nsub-optimal policy readily available. We propose a new constrained RL method\ncalled CSAC-LB (Constrained Soft Actor-Critic with Log Barrier Function), which\nachieves competitive performance without any pre-training by applying a linear\nsmoothed log barrier function to an additional safety critic. It implements an\nadaptive penalty for policy learning and alleviates the numerical issues that\nare known to complicate the application of the log barrier function method. As\na result, we show that with CSAC-LB, we achieve state-of-the-art performance on\nseveral constrained control tasks with different levels of difficulty and\nevaluate our methods in a locomotion task on a real quadruped robot platform.\n","authors":["Baohe Zhang","Yuan Zhang","Lilli Frison","Thomas Brox","Joschka Bödecker"],"pdf_url":"https://arxiv.org/pdf/2403.14508v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14504v1","updated":"2024-03-21T15:56:15Z","published":"2024-03-21T15:56:15Z","title":"Soft Learning Probabilistic Circuits","summary":"  Probabilistic Circuits (PCs) are prominent tractable probabilistic models,\nallowing for a range of exact inferences. This paper focuses on the main\nalgorithm for training PCs, LearnSPN, a gold standard due to its efficiency,\nperformance, and ease of use, in particular for tabular data. We show that\nLearnSPN is a greedy likelihood maximizer under mild assumptions. While\ninferences in PCs may use the entire circuit structure for processing queries,\nLearnSPN applies a hard method for learning them, propagating at each sum node\na data point through one and only one of the children/edges as in a hard\nclustering process. We propose a new learning procedure named SoftLearn, that\ninduces a PC using a soft clustering process. We investigate the effect of this\nlearning-inference compatibility in PCs. Our experiments show that SoftLearn\noutperforms LearnSPN in many situations, yielding better likelihoods and\narguably better samples. We also analyze comparable tractable models to\nhighlight the differences between soft/hard learning and model querying.\n","authors":["Soroush Ghandi","Benjamin Quost","Cassio de Campos"],"pdf_url":"https://arxiv.org/pdf/2403.14504v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14496v1","updated":"2024-03-21T15:44:56Z","published":"2024-03-21T15:44:56Z","title":"How Human-Centered Explainable AI Interface Are Designed and Evaluated:\n  A Systematic Survey","summary":"  Despite its technological breakthroughs, eXplainable Artificial Intelligence\n(XAI) research has limited success in producing the {\\em effective\nexplanations} needed by users. In order to improve XAI systems' usability,\npractical interpretability, and efficacy for real users, the emerging area of\n{\\em Explainable Interfaces} (EIs) focuses on the user interface and user\nexperience design aspects of XAI. This paper presents a systematic survey of 53\npublications to identify current trends in human-XAI interaction and promising\ndirections for EI design and development. This is among the first systematic\nsurvey of EI research.\n","authors":["Thu Nguyen","Alessandro Canossa","Jichen Zhu"],"pdf_url":"https://arxiv.org/pdf/2403.14496v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14494v1","updated":"2024-03-21T15:42:17Z","published":"2024-03-21T15:42:17Z","title":"Learning to Project for Cross-Task Knowledge Distillation","summary":"  Traditional knowledge distillation (KD) relies on a proficient teacher\ntrained on the target task, which is not always available. In this setting,\ncross-task distillation can be used, enabling the use of any teacher model\ntrained on a different task. However, many KD methods prove ineffective when\napplied to this cross-task setting. To address this limitation, we propose a\nsimple modification: the use of an inverted projection. We show that this\ndrop-in replacement for a standard projector is effective by learning to\ndisregard any task-specific features which might degrade the student's\nperformance. We find that this simple modification is sufficient for extending\nmany KD methods to the cross-task setting, where the teacher and student tasks\ncan be very different. In doing so, we obtain up to a 1.9% improvement in the\ncross-task setting compared to the traditional projection, at no additional\ncost. Our method can obtain significant performance improvements (up to 7%)\nwhen using even a randomly-initialised teacher on various tasks such as depth\nestimation, image translation, and semantic segmentation, despite the lack of\nany learned knowledge to transfer. To provide conceptual and analytical\ninsights into this result, we show that using an inverted projection allows the\ndistillation loss to be decomposed into a knowledge transfer and a spectral\nregularisation component. Through this analysis we are additionally able to\npropose a novel regularisation loss that allows teacher-free distillation,\nenabling performance improvements of up to 8.57% on ImageNet with no additional\ntraining costs.\n","authors":["Dylan Auty","Roy Miles","Benedikt Kolbeinsson","Krystian Mikolajczyk"],"pdf_url":"https://arxiv.org/pdf/2403.14494v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14488v1","updated":"2024-03-21T15:36:26Z","published":"2024-03-21T15:36:26Z","title":"Physics-Based Causal Reasoning for Safe & Robust Next-Best Action\n  Selection in Robot Manipulation Tasks","summary":"  Safe and efficient object manipulation is a key enabler of many real-world\nrobot applications. However, this is challenging because robot operation must\nbe robust to a range of sensor and actuator uncertainties. In this paper, we\npresent a physics-informed causal-inference-based framework for a robot to\nprobabilistically reason about candidate actions in a block stacking task in a\npartially observable setting. We integrate a physics-based simulation of the\nrigid-body system dynamics with a causal Bayesian network (CBN) formulation to\ndefine a causal generative probabilistic model of the robot decision-making\nprocess. Using simulation-based Monte Carlo experiments, we demonstrate our\nframework's ability to successfully: (1) predict block tower stability with\nhigh accuracy (Pred Acc: 88.6%); and, (2) select an approximate next-best\naction for the block stacking task, for execution by an integrated robot\nsystem, achieving 94.2% task success rate. We also demonstrate our framework's\nsuitability for real-world robot systems by demonstrating successful task\nexecutions with a domestic support robot, with perception and manipulation\nsub-system integration. Hence, we show that by embedding physics-based causal\nreasoning into robots' decision-making processes, we can make robot task\nexecution safer, more reliable, and more robust to various types of\nuncertainty.\n","authors":["Ricardo Cannizzaro","Michael Groom","Jonathan Routley","Robert Osazuwa Ness","Lars Kunze"],"pdf_url":"https://arxiv.org/pdf/2403.14488v1.pdf","comment":"8 pages, 9 figures, submitted to 2024 IEEE/RSJ International\n  Conference on Intelligent Robots and Systems (IROS)"},{"id":"http://arxiv.org/abs/2402.03049v3","updated":"2024-03-21T15:33:34Z","published":"2024-02-05T14:33:56Z","title":"EasyInstruct: An Easy-to-use Instruction Processing Framework for Large\n  Language Models","summary":"  In recent years, instruction tuning has gained increasing attention and\nemerged as a crucial technique to enhance the capabilities of Large Language\nModels (LLMs). To construct high-quality instruction datasets, many instruction\nprocessing approaches have been proposed, aiming to achieve a delicate balance\nbetween data quantity and data quality. Nevertheless, due to inconsistencies\nthat persist among various instruction processing methods, there is no standard\nopen-source instruction processing implementation framework available for the\ncommunity, which hinders practitioners from further developing and advancing.\nTo facilitate instruction processing research and development, we present\nEasyInstruct, an easy-to-use instruction processing framework for LLMs, which\nmodularizes instruction generation, selection, and prompting, while also\nconsidering their combination and interaction. EasyInstruct is publicly\nreleased and actively maintained at https://github.com/zjunlp/EasyInstruct,\nalong with an online demo app and a demo video for quick-start, calling for\nbroader research centered on instruction data and synthetic data.\n","authors":["Yixin Ou","Ningyu Zhang","Honghao Gui","Ziwen Xu","Shuofei Qiao","Yida Xue","Runnan Fang","Kangwei Liu","Lei Li","Zhen Bi","Guozhou Zheng","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2402.03049v3.pdf","comment":"Project website: https://zjunlp.github.io/project/EasyInstruct Code:\n  https://github.com/zjunlp/EasyInstruct Video: https://youtu.be/rfQOWYfziFo\n  Demo: https://huggingface.co/spaces/zjunlp/EasyInstruct"},{"id":"http://arxiv.org/abs/2403.14484v1","updated":"2024-03-21T15:31:28Z","published":"2024-03-21T15:31:28Z","title":"HyperGALE: ASD Classification via Hypergraph Gated Attention with\n  Learnable Hyperedges","summary":"  Autism Spectrum Disorder (ASD) is a neurodevelopmental condition\ncharacterized by varied social cognitive challenges and repetitive behavioral\npatterns. Identifying reliable brain imaging-based biomarkers for ASD has been\na persistent challenge due to the spectrum's diverse symptomatology. Existing\nbaselines in the field have made significant strides in this direction, yet\nthere remains room for improvement in both performance and interpretability. We\npropose \\emph{HyperGALE}, which builds upon the hypergraph by incorporating\nlearned hyperedges and gated attention mechanisms. This approach has led to\nsubstantial improvements in the model's ability to interpret complex brain\ngraph data, offering deeper insights into ASD biomarker characterization.\nEvaluated on the extensive ABIDE II dataset, \\emph{HyperGALE} not only improves\ninterpretability but also demonstrates statistically significant enhancements\nin key performance metrics compared to both previous baselines and the\nfoundational hypergraph model. The advancement \\emph{HyperGALE} brings to ASD\nresearch highlights the potential of sophisticated graph-based techniques in\nneurodevelopmental studies. The source code and implementation instructions are\navailable at GitHub:https://github.com/mehular0ra/HyperGALE.\n","authors":["Mehul Arora","Chirag Shantilal Jain","Lalith Bharadwaj Baru","Kamalaker Dadi","Bapi Raju Surampudi"],"pdf_url":"https://arxiv.org/pdf/2403.14484v1.pdf","comment":"Accepted to IJCNN 2024"},{"id":"http://arxiv.org/abs/2403.14483v1","updated":"2024-03-21T15:29:24Z","published":"2024-03-21T15:29:24Z","title":"Utilizing the LightGBM Algorithm for Operator User Credit Assessment\n  Research","summary":"  Mobile Internet user credit assessment is an important way for communication\noperators to establish decisions and formulate measures, and it is also a\nguarantee for operators to obtain expected benefits. However, credit evaluation\nmethods have long been monopolized by financial industries such as banks and\ncredit. As supporters and providers of platform network technology and network\nresources, communication operators are also builders and maintainers of\ncommunication networks. Internet data improves the user's credit evaluation\nstrategy. This paper uses the massive data provided by communication operators\nto carry out research on the operator's user credit evaluation model based on\nthe fusion LightGBM algorithm. First, for the massive data related to user\nevaluation provided by operators, key features are extracted by data\npreprocessing and feature engineering methods, and a multi-dimensional feature\nset with statistical significance is constructed; then, linear regression,\ndecision tree, LightGBM, and other machine learning algorithms build multiple\nbasic models to find the best basic model; finally, integrates Averaging,\nVoting, Blending, Stacking and other integrated algorithms to refine multiple\nfusion models, and finally establish the most suitable fusion model for\noperator user evaluation.\n","authors":["Shaojie Li","Xinqi Dong","Danqing Ma","Bo Dang","Hengyi Zang","Yulu Gong"],"pdf_url":"https://arxiv.org/pdf/2403.14483v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14472v1","updated":"2024-03-21T15:18:30Z","published":"2024-03-21T15:18:30Z","title":"Detoxifying Large Language Models via Knowledge Editing","summary":"  This paper investigates using knowledge editing techniques to detoxify Large\nLanguage Models (LLMs). We construct a benchmark, SafeEdit, which covers nine\nunsafe categories with various powerful attack prompts and equips comprehensive\nmetrics for systematic evaluation. We conduct experiments to compare knowledge\nediting approaches with previous baselines, indicating that knowledge editing\nhas the potential to efficiently detoxify LLMs with limited impact on general\nperformance. Then, we propose a simple yet effective baseline, dubbed\nDetoxifying with Intraoperative Neural Monitoring (DINM), to diminish the\ntoxicity of LLMs within a few tuning steps via only one instance. We further\nprovide an in-depth analysis of the internal mechanism for various detoxify\napproaches, demonstrating that previous methods like SFT and DPO may merely\nsuppress the activations of toxic parameters, while DINM mitigates the toxicity\nof the toxic parameters to a certain extent, making permanent adjustments. We\nhope that these insights could shed light on future work of developing\ndetoxifying approaches and the underlying knowledge mechanisms of LLMs. Code\nand benchmark are available at https://github.com/zjunlp/EasyEdit.\n","authors":["Mengru Wang","Ningyu Zhang","Ziwen Xu","Zekun Xi","Shumin Deng","Yunzhi Yao","Qishen Zhang","Linyi Yang","Jindong Wang","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2403.14472v1.pdf","comment":"Ongoing work. Project website:\n  https://zjunlp.github.io/project/SafeEdit Benchmark:\n  https://huggingface.co/datasets/zjunlp/SafeEdit Code:\n  https://github.com/zjunlp/EasyEdit"},{"id":"http://arxiv.org/abs/2403.14469v1","updated":"2024-03-21T15:16:50Z","published":"2024-03-21T15:16:50Z","title":"ChatGPT Alternative Solutions: Large Language Models Survey","summary":"  In recent times, the grandeur of Large Language Models (LLMs) has not only\nshone in the realm of natural language processing but has also cast its\nbrilliance across a vast array of applications. This remarkable display of LLM\ncapabilities has ignited a surge in research contributions within this domain,\nspanning a diverse spectrum of topics. These contributions encompass\nadvancements in neural network architecture, context length enhancements, model\nalignment, training datasets, benchmarking, efficiency improvements, and more.\nRecent years have witnessed a dynamic synergy between academia and industry,\npropelling the field of LLM research to new heights. A notable milestone in\nthis journey is the introduction of ChatGPT, a powerful AI chatbot grounded in\nLLMs, which has garnered widespread societal attention. The evolving technology\nof LLMs has begun to reshape the landscape of the entire AI community,\npromising a revolutionary shift in the way we create and employ AI algorithms.\nGiven this swift-paced technical evolution, our survey embarks on a journey to\nencapsulate the recent strides made in the world of LLMs. Through an\nexploration of the background, key discoveries, and prevailing methodologies,\nwe offer an up-to-the-minute review of the literature. By examining multiple\nLLM models, our paper not only presents a comprehensive overview but also\ncharts a course that identifies existing challenges and points toward potential\nfuture research trajectories. This survey furnishes a well-rounded perspective\non the current state of generative AI, shedding light on opportunities for\nfurther exploration, enhancement, and innovation.\n","authors":["Hanieh Alipour","Nick Pendar","Kohinoor Roy"],"pdf_url":"https://arxiv.org/pdf/2403.14469v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14468v1","updated":"2024-03-21T15:15:00Z","published":"2024-03-21T15:15:00Z","title":"AnyV2V: A Plug-and-Play Framework For Any Video-to-Video Editing Tasks","summary":"  Video-to-video editing involves editing a source video along with additional\ncontrol (such as text prompts, subjects, or styles) to generate a new video\nthat aligns with the source video and the provided control. Traditional methods\nhave been constrained to certain editing types, limiting their ability to meet\nthe wide range of user demands. In this paper, we introduce AnyV2V, a novel\ntraining-free framework designed to simplify video editing into two primary\nsteps: (1) employing an off-the-shelf image editing model (e.g.\nInstructPix2Pix, InstantID, etc) to modify the first frame, (2) utilizing an\nexisting image-to-video generation model (e.g. I2VGen-XL) for DDIM inversion\nand feature injection. In the first stage, AnyV2V can plug in any existing\nimage editing tools to support an extensive array of video editing tasks.\nBeyond the traditional prompt-based editing methods, AnyV2V also can support\nnovel video editing tasks, including reference-based style transfer,\nsubject-driven editing, and identity manipulation, which were unattainable by\nprevious methods. In the second stage, AnyV2V can plug in any existing\nimage-to-video models to perform DDIM inversion and intermediate feature\ninjection to maintain the appearance and motion consistency with the source\nvideo. On the prompt-based editing, we show that AnyV2V can outperform the\nprevious best approach by 35\\% on prompt alignment, and 25\\% on human\npreference. On the three novel tasks, we show that AnyV2V also achieves a high\nsuccess rate. We believe AnyV2V will continue to thrive due to its ability to\nseamlessly integrate the fast-evolving image editing methods. Such\ncompatibility can help AnyV2V to increase its versatility to cater to diverse\nuser demands.\n","authors":["Max Ku","Cong Wei","Weiming Ren","Huan Yang","Wenhu Chen"],"pdf_url":"https://arxiv.org/pdf/2403.14468v1.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2309.00903v2","updated":"2024-03-21T15:12:36Z","published":"2023-09-02T10:46:05Z","title":"An explainable three dimension framework to uncover learning patterns: A\n  unified look in variable sulci recognition","summary":"  Explainable AI is crucial in medical imaging. In the challenging field of\nneuroscience, visual topics present a high level of complexity, particularly\nwithin three-dimensional space. The application of neuroscience, which involves\nidentifying brain sulcal features from MRI, faces significant hurdles due to\nvarying annotation protocols among experts and the intricate three-dimension\nfunctionality of the brain. Consequently, traditional explainability approaches\nfall short in effectively validating and evaluating these networks. To address\nthis, we first present a mathematical formulation delineating various\ncategories of explanation needs across diverse computer vision tasks,\ncategorized into self-explanatory, semi-explanatory, non-explanatory, and\nnew-pattern learning applications based on the reliability of the validation\nprotocol. With respect to this mathematical formulation, we propose a 3D\nexplainability framework aimed at validating the outputs of deep learning\nnetworks in detecting the paracingulate sulcus an essential brain anatomical\nfeature. The framework integrates local 3D explanations, global explanations\nthrough dimensionality reduction, concatenated global explanations, and\nstatistical shape features, unveiling new insights into pattern learning. We\ntrained and tested two advanced 3D deep learning networks on the challenging\nTOP-OSLO dataset, significantly improving sulcus detection accuracy,\nparticularly on the left hemisphere. During evaluation with diverse annotation\nprotocols for this dataset, we highlighted the crucial role of an unbiased\nannotation process in achieving precise predictions and effective pattern\nlearning within our proposed 3D framework. The proposed framework not only\nannotates the variable sulcus but also uncovers hidden AI knowledge, promising\nto advance our understanding of brain anatomy and function.\n","authors":["Michail Mamalakis","Heloise de Vareilles","Atheer AI-Manea","Samantha C. Mitchell","Ingrid Arartz","Lynn Egeland Morch-Johnsen","Jane Garrison","Jon Simons","Pietro Lio","John Suckling","Graham Murray"],"pdf_url":"https://arxiv.org/pdf/2309.00903v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14460v1","updated":"2024-03-21T15:07:57Z","published":"2024-03-21T15:07:57Z","title":"Towards Single-System Illusion in Software-Defined Vehicles --\n  Automated, AI-Powered Workflow","summary":"  We propose a novel model- and feature-based approach to development of\nvehicle software systems, where the end architecture is not explicitly defined.\nInstead, it emerges from an iterative process of search and optimization given\ncertain constraints, requirements and hardware architecture, while retaining\nthe property of single-system illusion, where applications run in a logically\nuniform environment. One of the key points of the presented approach is the\ninclusion of modern generative AI, specifically Large Language Models (LLMs),\nin the loop. With the recent advances in the field, we expect that the LLMs\nwill be able to assist in processing of requirements, generation of formal\nsystem models, as well as generation of software deployment specification and\ntest code. The resulting pipeline is automated to a large extent, with feedback\nbeing generated at each step.\n","authors":["Krzysztof Lebioda","Viktor Vorobev","Nenad Petrovic","Fengjunjie Pan","Vahid Zolfaghari","Alois Knoll"],"pdf_url":"https://arxiv.org/pdf/2403.14460v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14459v1","updated":"2024-03-21T15:06:14Z","published":"2024-03-21T15:06:14Z","title":"Multi-Level Explanations for Generative Language Models","summary":"  Perturbation-based explanation methods such as LIME and SHAP are commonly\napplied to text classification. This work focuses on their extension to\ngenerative language models. To address the challenges of text as output and\nlong text inputs, we propose a general framework called MExGen that can be\ninstantiated with different attribution algorithms. To handle text output, we\nintroduce the notion of scalarizers for mapping text to real numbers and\ninvestigate multiple possibilities. To handle long inputs, we take a\nmulti-level approach, proceeding from coarser levels of granularity to finer\nones, and focus on algorithms with linear scaling in model queries. We conduct\na systematic evaluation, both automated and human, of perturbation-based\nattribution methods for summarization and context-grounded question answering.\nThe results show that our framework can provide more locally faithful\nexplanations of generated outputs.\n","authors":["Lucas Monteiro Paes","Dennis Wei","Hyo Jin Do","Hendrik Strobelt","Ronny Luss","Amit Dhurandhar","Manish Nagireddy","Karthikeyan Natesan Ramamurthy","Prasanna Sattigeri","Werner Geyer","Soumya Ghosh"],"pdf_url":"https://arxiv.org/pdf/2403.14459v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10882v2","updated":"2024-03-21T14:50:18Z","published":"2024-03-16T10:26:38Z","title":"Optimizing Language Augmentation for Multilingual Large Language Models:\n  A Case Study on Korean","summary":"  Large language models (LLMs) use pretraining to predict the subsequent word;\nhowever, their expansion requires significant computing resources. Numerous big\ntech companies and research institutes have developed multilingual LLMs (MLLMs)\nto meet current demands, overlooking less-resourced languages (LRLs). This\nstudy proposed three strategies to enhance the performance of LRLs based on the\npublicly available MLLMs. First, the MLLM vocabularies of LRLs were expanded to\nenhance expressiveness. Second, bilingual data were used for pretraining to\nalign the high- and less-resourced languages. Third, a high-quality small-scale\ninstruction dataset was constructed and instruction-tuning was performed to\naugment the LRL. The experiments employed the Llama2 model and Korean was used\nas the LRL, which was quantitatively evaluated against other developed LLMs\nacross eight tasks. Furthermore, a qualitative assessment was performed based\non human evaluation and GPT4. Experimental results showed that our proposed\nBllossom model exhibited superior performance in qualitative analyses compared\nto previously proposed Korean monolingual models.\n","authors":["ChangSu Choi","Yongbin Jeong","Seoyoon Park","InHo Won","HyeonSeok Lim","SangMin Kim","Yejee Kang","Chanhyuk Yoon","Jaewan Park","Yiseul Lee","HyeJin Lee","Younggyun Hahm","Hansaem Kim","KyungTae Lim"],"pdf_url":"https://arxiv.org/pdf/2403.10882v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14443v1","updated":"2024-03-21T14:48:37Z","published":"2024-03-21T14:48:37Z","title":"Language Models Can Reduce Asymmetry in Information Markets","summary":"  This work addresses the buyer's inspection paradox for information markets.\nThe paradox is that buyers need to access information to determine its value,\nwhile sellers need to limit access to prevent theft. To study this, we\nintroduce an open-source simulated digital marketplace where intelligent\nagents, powered by language models, buy and sell information on behalf of\nexternal participants. The central mechanism enabling this marketplace is the\nagents' dual capabilities: they not only have the capacity to assess the\nquality of privileged information but also come equipped with the ability to\nforget. This ability to induce amnesia allows vendors to grant temporary access\nto proprietary information, significantly reducing the risk of unauthorized\nretention while enabling agents to accurately gauge the information's relevance\nto specific queries or tasks. To perform well, agents must make rational\ndecisions, strategically explore the marketplace through generated sub-queries,\nand synthesize answers from purchased information. Concretely, our experiments\n(a) uncover biases in language models leading to irrational behavior and\nevaluate techniques to mitigate these biases, (b) investigate how price affects\ndemand in the context of informational goods, and (c) show that inspection and\nhigher budgets both lead to higher quality outcomes.\n","authors":["Nasim Rahaman","Martin Weiss","Manuel Wüthrich","Yoshua Bengio","Li Erran Li","Chris Pal","Bernhard Schölkopf"],"pdf_url":"https://arxiv.org/pdf/2403.14443v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14440v1","updated":"2024-03-21T14:45:54Z","published":"2024-03-21T14:45:54Z","title":"Analysing Diffusion Segmentation for Medical Images","summary":"  Denoising Diffusion Probabilistic models have become increasingly popular due\nto their ability to offer probabilistic modeling and generate diverse outputs.\nThis versatility inspired their adaptation for image segmentation, where\nmultiple predictions of the model can produce segmentation results that not\nonly achieve high quality but also capture the uncertainty inherent in the\nmodel. Here, powerful architectures were proposed for improving diffusion\nsegmentation performance. However, there is a notable lack of analysis and\ndiscussions on the differences between diffusion segmentation and image\ngeneration, and thorough evaluations are missing that distinguish the\nimprovements these architectures provide for segmentation in general from their\nbenefit for diffusion segmentation specifically. In this work, we critically\nanalyse and discuss how diffusion segmentation for medical images differs from\ndiffusion image generation, with a particular focus on the training behavior.\nFurthermore, we conduct an assessment how proposed diffusion segmentation\narchitectures perform when trained directly for segmentation. Lastly, we\nexplore how different medical segmentation tasks influence the diffusion\nsegmentation behavior and the diffusion process could be adapted accordingly.\nWith these analyses, we aim to provide in-depth insights into the behavior of\ndiffusion segmentation that allow for a better design and evaluation of\ndiffusion segmentation methods in the future.\n","authors":["Mathias Öttl","Siyuan Mei","Frauke Wilm","Jana Steenpass","Matthias Rübner","Arndt Hartmann","Matthias Beckmann","Peter Fasching","Andreas Maier","Ramona Erber","Katharina Breininger"],"pdf_url":"https://arxiv.org/pdf/2403.14440v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14435v1","updated":"2024-03-21T14:41:58Z","published":"2024-03-21T14:41:58Z","title":"Biased Binary Attribute Classifiers Ignore the Majority Classes","summary":"  To visualize the regions of interest that classifiers base their decisions\non, different Class Activation Mapping (CAM) methods have been developed.\nHowever, all of these techniques target categorical classifiers only, though\nmost real-world tasks are binary classification. In this paper, we extend\ngradient-based CAM techniques to work with binary classifiers and visualize the\nactive regions for binary facial attribute classifiers. When training an\nunbalanced binary classifier on an imbalanced dataset, it is well-known that\nthe majority class, i.e. the class with many training samples, is mostly\npredicted much better than minority class with few training instances. In our\nexperiments on the CelebA dataset, we verify these results, when training an\nunbalanced classifier to extract 40 facial attributes simultaneously. One would\nexpect that the biased classifier has learned to extract features mainly for\nthe majority classes and that the proportional energy of the activations mainly\nreside in certain specific regions of the image where the attribute is located.\nHowever, we find very little regular activation for samples of majority\nclasses, while the active regions for minority classes seem mostly reasonable\nand overlap with our expectations. These results suggest that biased\nclassifiers mainly rely on bias activation for majority classes. When training\na balanced classifier on the imbalanced data by employing attribute-specific\nclass weights, majority and minority classes are classified similarly well and\nshow expected activations for almost all attributes\n","authors":["Xinyi Zhang","Johanna Sophie Bieri","Manuel Günther"],"pdf_url":"https://arxiv.org/pdf/2403.14435v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14432v1","updated":"2024-03-21T14:39:28Z","published":"2024-03-21T14:39:28Z","title":"On the continuity and smoothness of the value function in reinforcement\n  learning and optimal control","summary":"  The value function plays a crucial role as a measure for the cumulative\nfuture reward an agent receives in both reinforcement learning and optimal\ncontrol. It is therefore of interest to study how similar the values of\nneighboring states are, i.e., to investigate the continuity of the value\nfunction. We do so by providing and verifying upper bounds on the value\nfunction's modulus of continuity. Additionally, we show that the value function\nis always H\\\"older continuous under relatively weak assumptions on the\nunderlying system and that non-differentiable value functions can be made\ndifferentiable by slightly \"disturbing\" the system.\n","authors":["Hans Harder","Sebastian Peitz"],"pdf_url":"https://arxiv.org/pdf/2403.14432v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14429v1","updated":"2024-03-21T14:36:59Z","published":"2024-03-21T14:36:59Z","title":"Style-Extracting Diffusion Models for Semi-Supervised Histopathology\n  Segmentation","summary":"  Deep learning-based image generation has seen significant advancements with\ndiffusion models, notably improving the quality of generated images. Despite\nthese developments, generating images with unseen characteristics beneficial\nfor downstream tasks has received limited attention. To bridge this gap, we\npropose Style-Extracting Diffusion Models, featuring two conditioning\nmechanisms. Specifically, we utilize 1) a style conditioning mechanism which\nallows to inject style information of previously unseen images during image\ngeneration and 2) a content conditioning which can be targeted to a downstream\ntask, e.g., layout for segmentation. We introduce a trainable style encoder to\nextract style information from images, and an aggregation block that merges\nstyle information from multiple style inputs. This architecture enables the\ngeneration of images with unseen styles in a zero-shot manner, by leveraging\nstyles from unseen images, resulting in more diverse generations. In this work,\nwe use the image layout as target condition and first show the capability of\nour method on a natural image dataset as a proof-of-concept. We further\ndemonstrate its versatility in histopathology, where we combine prior knowledge\nabout tissue composition and unannotated data to create diverse synthetic\nimages with known layouts. This allows us to generate additional synthetic data\nto train a segmentation network in a semi-supervised fashion. We verify the\nadded value of the generated images by showing improved segmentation results\nand lower performance variability between patients when synthetic images are\nincluded during segmentation training. Our code will be made publicly available\nat [LINK].\n","authors":["Mathias Öttl","Frauke Wilm","Jana Steenpass","Jingna Qiu","Matthias Rübner","Arndt Hartmann","Matthias Beckmann","Peter Fasching","Andreas Maier","Ramona Erber","Bernhard Kainz","Katharina Breininger"],"pdf_url":"https://arxiv.org/pdf/2403.14429v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14410v1","updated":"2024-03-21T13:57:45Z","published":"2024-03-21T13:57:45Z","title":"GLC++: Source-Free Universal Domain Adaptation through Global-Local\n  Clustering and Contrastive Affinity Learning","summary":"  Deep neural networks often exhibit sub-optimal performance under covariate\nand category shifts. Source-Free Domain Adaptation (SFDA) presents a promising\nsolution to this dilemma, yet most SFDA approaches are restricted to closed-set\nscenarios. In this paper, we explore Source-Free Universal Domain Adaptation\n(SF-UniDA) aiming to accurately classify \"known\" data belonging to common\ncategories and segregate them from target-private \"unknown\" data. We propose a\nnovel Global and Local Clustering (GLC) technique, which comprises an adaptive\none-vs-all global clustering algorithm to discern between target classes,\ncomplemented by a local k-NN clustering strategy to mitigate negative transfer.\nDespite the effectiveness, the inherent closed-set source architecture leads to\nuniform treatment of \"unknown\" data, impeding the identification of distinct\n\"unknown\" categories. To address this, we evolve GLC to GLC++, integrating a\ncontrastive affinity learning strategy. We examine the superiority of GLC and\nGLC++ across multiple benchmarks and category shift scenarios. Remarkably, in\nthe most challenging open-partial-set scenarios, GLC and GLC++ surpass GATE by\n16.7% and 18.6% in H-score on VisDA, respectively. GLC++ enhances the novel\ncategory clustering accuracy of GLC by 4.3% in open-set scenarios on\nOffice-Home. Furthermore, the introduced contrastive learning strategy not only\nenhances GLC but also significantly facilitates existing methodologies.\n","authors":["Sanqing Qu","Tianpei Zou","Florian Röhrbein","Cewu Lu","Guang Chen","Dacheng Tao","Changjun Jiang"],"pdf_url":"https://arxiv.org/pdf/2403.14410v1.pdf","comment":"This is a substantial extension of the CVPR 2023 paper \"Upcycling\n  Models under Domain and Category Shift\""},{"id":"http://arxiv.org/abs/2403.14409v1","updated":"2024-03-21T13:57:43Z","published":"2024-03-21T13:57:43Z","title":"Locating and Mitigating Gender Bias in Large Language Models","summary":"  Large language models(LLM) are pre-trained on extensive corpora to learn\nfacts and human cognition which contain human preferences. However, this\nprocess can inadvertently lead to these models acquiring biases and stereotypes\nprevalent in society. Prior research has typically tackled the issue of bias\nthrough a one-dimensional perspective, concentrating either on locating or\nmitigating it. This limited perspective has created obstacles in facilitating\nresearch on bias to synergistically complement and progressively build upon one\nanother. In this study, we integrate the processes of locating and mitigating\nbias within a unified framework. Initially, we use causal mediation analysis to\ntrace the causal effects of different components' activation within a large\nlanguage model. Building on this, we propose the LSDM (Least Square Debias\nMethod), a knowledge-editing based method for mitigating gender bias in\noccupational pronouns, and compare it against two baselines on three gender\nbias datasets and seven knowledge competency test datasets. The experimental\nresults indicate that the primary contributors to gender bias are the bottom\nMLP modules acting on the last token of occupational pronouns and the top\nattention module acting on the final word in the sentence. Furthermore, LSDM\nmitigates gender bias in the model more effectively than the other baselines,\nwhile fully preserving the model's capabilities in all other aspects.\n","authors":["Yuchen Cai","Ding Cao","Rongxi Guo","Yaqin Wen","Guiquan Liu","Enhong Chen"],"pdf_url":"https://arxiv.org/pdf/2403.14409v1.pdf","comment":"23 pages, 5 figures"},{"id":"http://arxiv.org/abs/2403.14403v1","updated":"2024-03-21T13:52:30Z","published":"2024-03-21T13:52:30Z","title":"Adaptive-RAG: Learning to Adapt Retrieval-Augmented Large Language\n  Models through Question Complexity","summary":"  Retrieval-Augmented Large Language Models (LLMs), which incorporate the\nnon-parametric knowledge from external knowledge bases into LLMs, have emerged\nas a promising approach to enhancing response accuracy in several tasks, such\nas Question-Answering (QA). However, even though there are various approaches\ndealing with queries of different complexities, they either handle simple\nqueries with unnecessary computational overhead or fail to adequately address\ncomplex multi-step queries; yet, not all user requests fall into only one of\nthe simple or complex categories. In this work, we propose a novel adaptive QA\nframework, that can dynamically select the most suitable strategy for\n(retrieval-augmented) LLMs from the simplest to the most sophisticated ones\nbased on the query complexity. Also, this selection process is operationalized\nwith a classifier, which is a smaller LM trained to predict the complexity\nlevel of incoming queries with automatically collected labels, obtained from\nactual predicted outcomes of models and inherent inductive biases in datasets.\nThis approach offers a balanced strategy, seamlessly adapting between the\niterative and single-step retrieval-augmented LLMs, as well as the no-retrieval\nmethods, in response to a range of query complexities. We validate our model on\na set of open-domain QA datasets, covering multiple query complexities, and\nshow that ours enhances the overall efficiency and accuracy of QA systems,\ncompared to relevant baselines including the adaptive retrieval approaches.\nCode is available at: https://github.com/starsuzi/Adaptive-RAG.\n","authors":["Soyeong Jeong","Jinheon Baek","Sukmin Cho","Sung Ju Hwang","Jong C. Park"],"pdf_url":"https://arxiv.org/pdf/2403.14403v1.pdf","comment":"NAACL 2024"},{"id":"http://arxiv.org/abs/2403.14399v1","updated":"2024-03-21T13:47:40Z","published":"2024-03-21T13:47:40Z","title":"Building Accurate Translation-Tailored LLMs with Language Aware\n  Instruction Tuning","summary":"  Translation-tailored Large language models (LLMs) exhibit remarkable\ntranslation capabilities, even competing with supervised-trained commercial\ntranslation systems. However, off-target translation remains an unsolved\nproblem, especially for low-resource languages, hindering us from developing\naccurate LLMs-based translation models. To mitigate the off-target translation\nproblem and enhance the performance of LLMs on translation, recent works have\neither designed advanced prompting strategies to highlight the functionality of\ntranslation instructions or exploited the in-context learning ability of LLMs\nby feeding few-shot demonstrations. However, these methods essentially do not\nimprove LLM's ability to follow translation instructions, especially the\nlanguage direction information. In this work, we design a two-stage fine-tuning\nalgorithm to improve the instruction-following ability (especially the\ntranslation direction) of LLMs. Specifically, we first tune LLMs with the\nmaximum likelihood estimation loss on the translation dataset to elicit the\nbasic translation capabilities. In the second stage, we construct\ninstruction-conflicting samples by randomly replacing the translation\ndirections with a wrong one within the instruction, and then introduce an extra\nunlikelihood loss to learn those samples. Experiments on IWSLT and WMT\nbenchmarks upon the LLaMA model spanning 16 zero-shot directions show that,\ncompared to the competitive baseline -- translation-finetuned LLama, our method\ncould effectively reduce the off-target translation ratio (averagely -53.3\\%),\nthus improving translation quality with average +5.7 SacreBLEU and +16.4\nBLEURT. Analysis shows that our method could preserve the model's general task\nperformance on AlpacaEval. Code and models will be released at\n\\url{https://github.com/alphadl/LanguageAware_Tuning}.\n","authors":["Changtong Zan","Liang Ding","Li Shen","Yibing Zhen","Weifeng Liu","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2403.14399v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.11259v2","updated":"2024-03-21T13:41:35Z","published":"2023-09-20T12:35:19Z","title":"Sequence-to-Sequence Spanish Pre-trained Language Models","summary":"  In recent years, significant advancements in pre-trained language models have\ndriven the creation of numerous non-English language variants, with a\nparticular emphasis on encoder-only and decoder-only architectures. While\nSpanish language models based on BERT and GPT have demonstrated proficiency in\nnatural language understanding and generation, there remains a noticeable\nscarcity of encoder-decoder models explicitly designed for sequence-to-sequence\ntasks, which aim to map input sequences to generate output sequences\nconditionally. This paper breaks new ground by introducing the implementation\nand evaluation of renowned encoder-decoder architectures exclusively\npre-trained on Spanish corpora. Specifically, we present Spanish versions of\nBART, T5, and BERT2BERT-style models and subject them to a comprehensive\nassessment across various sequence-to-sequence tasks, including summarization,\nquestion answering, split-and-rephrase, dialogue, and translation. Our findings\nunderscore the competitive performance of all models, with the BART- and\nT5-based models emerging as top performers across all tasks. We have made all\nmodels publicly available to the research community to foster future\nexplorations and advancements in Spanish NLP:\nhttps://github.com/vgaraujov/Seq2Seq-Spanish-PLMs.\n","authors":["Vladimir Araujo","Maria Mihaela Trusca","Rodrigo Tufiño","Marie-Francine Moens"],"pdf_url":"https://arxiv.org/pdf/2309.11259v2.pdf","comment":"Accepted paper at LREC-Coling2024"},{"id":"http://arxiv.org/abs/2306.00618v2","updated":"2024-03-21T13:37:23Z","published":"2023-06-01T12:44:33Z","title":"Effective Structured Prompting by Meta-Learning and Representative\n  Verbalizer","summary":"  Prompt tuning for pre-trained masked language models (MLM) has shown\npromising performance in natural language processing tasks with few labeled\nexamples. It tunes a prompt for the downstream task, and a verbalizer is used\nto bridge the predicted token and label prediction. Due to the limited training\ndata, prompt initialization is crucial for prompt tuning. Recently,\nMetaPrompting (Hou et al., 2022) uses meta-learning to learn a shared\ninitialization for all task-specific prompts. However, a single initialization\nis insufficient to obtain good prompts for all tasks and samples when the tasks\nare complex. Moreover, MetaPrompting requires tuning the whole MLM, causing a\nheavy burden on computation and memory as the MLM is usually large. To address\nthese issues, we use a prompt pool to extract more task knowledge and construct\ninstance-dependent prompts via attention. We further propose a novel soft\nverbalizer (RepVerb) which constructs label embedding from feature embeddings\ndirectly. Combining meta-learning the prompt pool and RepVerb, we propose\nMetaPrompter for effective structured prompting. MetaPrompter is\nparameter-efficient as only the pool is required to be tuned. Experimental\nresults demonstrate that MetaPrompter performs better than the recent\nstate-of-the-arts and RepVerb outperforms existing soft verbalizers.\n","authors":["Weisen Jiang","Yu Zhang","James T. Kwok"],"pdf_url":"https://arxiv.org/pdf/2306.00618v2.pdf","comment":"Accepted at ICML 2023"},{"id":"http://arxiv.org/abs/2302.03788v3","updated":"2024-03-21T13:30:22Z","published":"2023-02-07T22:56:58Z","title":"Toward a Theory of Causation for Interpreting Neural Code Models","summary":"  Neural Language Models of Code, or Neural Code Models (NCMs), are rapidly\nprogressing from research prototypes to commercial developer tools. As such,\nunderstanding the capabilities and limitations of such models is becoming\ncritical. However, the abilities of these models are typically measured using\nautomated metrics that often only reveal a portion of their real-world\nperformance. While, in general, the performance of NCMs appears promising,\ncurrently much is unknown about how such models arrive at decisions. To this\nend, this paper introduces $do_{code}$, a post hoc interpretability method\nspecific to NCMs that is capable of explaining model predictions. $do_{code}$\nis based upon causal inference to enable programming language-oriented\nexplanations. While the theoretical underpinnings of $do_{code}$ are extensible\nto exploring different model properties, we provide a concrete instantiation\nthat aims to mitigate the impact of spurious correlations by grounding\nexplanations of model behavior in properties of programming languages. To\ndemonstrate the practical benefit of $do_{code}$, we illustrate the insights\nthat our framework can provide by performing a case study on two popular deep\nlearning architectures and ten NCMs. The results of this case study illustrate\nthat our studied NCMs are sensitive to changes in code syntax. All our NCMs,\nexcept for the BERT-like model, statistically learn to predict tokens related\nto blocks of code (\\eg brackets, parenthesis, semicolon) with less confounding\nbias as compared to other programming language constructs. These insights\ndemonstrate the potential of $do_{code}$ as a useful method to detect and\nfacilitate the elimination of confounding bias in NCMs.\n","authors":["David N. Palacio","Alejandro Velasco","Nathan Cooper","Alvaro Rodriguez","Kevin Moran","Denys Poshyvanyk"],"pdf_url":"https://arxiv.org/pdf/2302.03788v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14381v1","updated":"2024-03-21T13:15:25Z","published":"2024-03-21T13:15:25Z","title":"Editing Knowledge Representation of Language Lodel via Rephrased Prefix\n  Prompts","summary":"  Neural language models (LMs) have been extensively trained on vast corpora to\nstore factual knowledge about various aspects of the world described in texts.\nCurrent technologies typically employ knowledge editing methods or specific\nprompts to modify LM outputs. However, existing knowledge editing methods are\ncostly and inefficient, struggling to produce appropriate text. Additionally,\nprompt engineering is opaque and requires significant effort to find suitable\nprompts. To address these issues, we introduce a new method called PSPEM\n(Prefix Soft Prompt Editing Method), that can be used for a lifetime with just\none training. It resolves the inefficiencies and generalizability issues in\nknowledge editing methods and overcomes the opacity of prompt engineering by\nautomatically seeking optimal soft prompts. Specifically, PSPEM utilizes a\nprompt encoder and an encoding converter to refine key information in prompts\nand uses prompt alignment techniques to guide model generation, ensuring text\nconsistency and adherence to the intended structure and content, thereby\nmaintaining an optimal balance between efficiency and accuracy. We have\nvalidated the effectiveness of PSPEM through knowledge editing and attribute\ninserting. On the COUNTERFACT dataset, PSPEM achieved nearly 100\\% editing\naccuracy and demonstrated the highest level of fluency. We further analyzed the\nsimilarities between PSPEM and original prompts and their impact on the model's\ninternals. The results indicate that PSPEM can serve as an alternative to\noriginal prompts, supporting the model in effective editing.\n","authors":["Yuchen Cai","Ding Cao","Rongxi Guo","Yaqin Wen","Guiquan Liu","Enhong Chen"],"pdf_url":"https://arxiv.org/pdf/2403.14381v1.pdf","comment":"19pages,3figures"},{"id":"http://arxiv.org/abs/2403.14371v1","updated":"2024-03-21T12:59:24Z","published":"2024-03-21T12:59:24Z","title":"Loop Improvement: An Efficient Approach for Extracting Shared Features\n  from Heterogeneous Data without Central Server","summary":"  In federated learning, data heterogeneity significantly impacts performance.\nA typical solution involves segregating these parameters into shared and\npersonalized components, a concept also relevant in multi-task learning.\nAddressing this, we propose \"Loop Improvement\" (LI), a novel method enhancing\nthis separation and feature extraction without necessitating a central server\nor data interchange among participants. Our experiments reveal LI's superiority\nin several aspects: In personalized federated learning environments, LI\nconsistently outperforms the advanced FedALA algorithm in accuracy across\ndiverse scenarios. Additionally, LI's feature extractor closely matches the\nperformance achieved when aggregating data from all clients. In global model\ncontexts, employing LI with stacked personalized layers and an additional\nnetwork also yields comparable results to combined client data scenarios.\nFurthermore, LI's adaptability extends to multi-task learning, streamlining the\nextraction of common features across tasks and obviating the need for\nsimultaneous training. This approach not only enhances individual task\nperformance but also achieves accuracy levels on par with classic multi-task\nlearning methods where all tasks are trained simultaneously. LI integrates a\nloop topology with layer-wise and end-to-end training, compatible with various\nneural network models. This paper also delves into the theoretical\nunderpinnings of LI's effectiveness, offering insights into its potential\napplications. The code is on https://github.com/axedge1983/LI\n","authors":["Fei Li","Chu Kiong Loo","Wei Shiung Liew","Xiaofeng Liu"],"pdf_url":"https://arxiv.org/pdf/2403.14371v1.pdf","comment":"11 pages, 11 figures"},{"id":"http://arxiv.org/abs/2310.10404v6","updated":"2024-03-21T12:59:04Z","published":"2023-10-16T13:49:46Z","title":"LLM4SGG: Large Language Model for Weakly Supervised Scene Graph\n  Generation","summary":"  Weakly-Supervised Scene Graph Generation (WSSGG) research has recently\nemerged as an alternative to the fully-supervised approach that heavily relies\non costly annotations. In this regard, studies on WSSGG have utilized image\ncaptions to obtain unlocalized triplets while primarily focusing on grounding\nthe unlocalized triplets over image regions. However, they have overlooked the\ntwo issues involved in the triplet formation process from the captions: 1)\nSemantic over-simplification issue arises when extracting triplets from\ncaptions, where fine-grained predicates in captions are undesirably converted\ninto coarse-grained predicates, resulting in a long-tailed predicate\ndistribution, and 2) Low-density scene graph issue arises when aligning the\ntriplets in the caption with entity/predicate classes of interest, where many\ntriplets are discarded and not used in training, leading to insufficient\nsupervision. To tackle the two issues, we propose a new approach, i.e., Large\nLanguage Model for weakly-supervised SGG (LLM4SGG), where we mitigate the two\nissues by leveraging the LLM's in-depth understanding of language and reasoning\nability during the extraction of triplets from captions and alignment of\nentity/predicate classes with target data. To further engage the LLM in these\nprocesses, we adopt the idea of Chain-of-Thought and the in-context few-shot\nlearning strategy. To validate the effectiveness of LLM4SGG, we conduct\nextensive experiments on Visual Genome and GQA datasets, showing significant\nimprovements in both Recall@K and mean Recall@K compared to the\nstate-of-the-art WSSGG methods. A further appeal is that LLM4SGG is\ndata-efficient, enabling effective model training with a small amount of\ntraining images.\n","authors":["Kibum Kim","Kanghoon Yoon","Jaehyeong Jeon","Yeonjun In","Jinyoung Moon","Donghyun Kim","Chanyoung Park"],"pdf_url":"https://arxiv.org/pdf/2310.10404v6.pdf","comment":"8 pages; CVPR 2024"},{"id":"http://arxiv.org/abs/2312.12274v2","updated":"2024-03-21T12:51:31Z","published":"2023-12-19T15:56:19Z","title":"Intrinsic Image Diffusion for Indoor Single-view Material Estimation","summary":"  We present Intrinsic Image Diffusion, a generative model for appearance\ndecomposition of indoor scenes. Given a single input view, we sample multiple\npossible material explanations represented as albedo, roughness, and metallic\nmaps. Appearance decomposition poses a considerable challenge in computer\nvision due to the inherent ambiguity between lighting and material properties\nand the lack of real datasets. To address this issue, we advocate for a\nprobabilistic formulation, where instead of attempting to directly predict the\ntrue material properties, we employ a conditional generative model to sample\nfrom the solution space. Furthermore, we show that utilizing the strong learned\nprior of recent diffusion models trained on large-scale real-world images can\nbe adapted to material estimation and highly improves the generalization to\nreal images. Our method produces significantly sharper, more consistent, and\nmore detailed materials, outperforming state-of-the-art methods by $1.5dB$ on\nPSNR and by $45\\%$ better FID score on albedo prediction. We demonstrate the\neffectiveness of our approach through experiments on both synthetic and\nreal-world datasets.\n","authors":["Peter Kocsis","Vincent Sitzmann","Matthias Nießner"],"pdf_url":"https://arxiv.org/pdf/2312.12274v2.pdf","comment":"Project page: https://peter-kocsis.github.io/IntrinsicImageDiffusion/\n  Video: https://youtu.be/lz0meJlj5cA"},{"id":"http://arxiv.org/abs/2311.14758v2","updated":"2024-03-21T12:43:32Z","published":"2023-11-23T15:57:41Z","title":"Point2RBox: Combine Knowledge from Synthetic Visual Patterns for\n  End-to-end Oriented Object Detection with Single Point Supervision","summary":"  With the rapidly increasing demand for oriented object detection (OOD),\nrecent research involving weakly-supervised detectors for learning rotated box\n(RBox) from the horizontal box (HBox) has attracted more and more attention. In\nthis paper, we explore a more challenging yet label-efficient setting, namely\nsingle point-supervised OOD, and present our approach called Point2RBox.\nSpecifically, we propose to leverage two principles: 1) Synthetic pattern\nknowledge combination: By sampling around each labeled point on the image, we\nspread the object feature to synthetic visual patterns with known boxes to\nprovide the knowledge for box regression. 2) Transform self-supervision: With a\ntransformed input image (e.g. scaled/rotated), the output RBoxes are trained to\nfollow the same transformation so that the network can perceive the relative\nsize/rotation between objects. The detector is further enhanced by a few\ndevised techniques to cope with peripheral issues, e.g. the anchor/layer\nassignment as the size of the object is not available in our point supervision\nsetting. To our best knowledge, Point2RBox is the first end-to-end solution for\npoint-supervised OOD. In particular, our method uses a lightweight paradigm,\nyet it achieves a competitive performance among point-supervised alternatives,\n41.05%/27.62%/80.01% on DOTA/DIOR/HRSC datasets.\n","authors":["Yi Yu","Xue Yang","Qingyun Li","Feipeng Da","Jifeng Dai","Yu Qiao","Junchi Yan"],"pdf_url":"https://arxiv.org/pdf/2311.14758v2.pdf","comment":"10 pages, 3 figures, 5 tables, code:\n  https://github.com/yuyi1005/point2rbox-mmrotate"},{"id":"http://arxiv.org/abs/2402.07234v3","updated":"2024-03-21T12:39:09Z","published":"2024-02-11T15:56:03Z","title":"CPSDBench: A Large Language Model Evaluation Benchmark and Baseline for\n  Chinese Public Security Domain","summary":"  Large Language Models (LLMs) have demonstrated significant potential and\neffectiveness across multiple application domains. To assess the performance of\nmainstream LLMs in public security tasks, this study aims to construct a\nspecialized evaluation benchmark tailored to the Chinese public security\ndomain--CPSDbench. CPSDbench integrates datasets related to public security\ncollected from real-world scenarios, supporting a comprehensive assessment of\nLLMs across four key dimensions: text classification, information extraction,\nquestion answering, and text generation. Furthermore, this study introduces a\nset of innovative evaluation metrics designed to more precisely quantify the\nefficacy of LLMs in executing tasks related to public security. Through the\nin-depth analysis and evaluation conducted in this research, we not only\nenhance our understanding of the performance strengths and limitations of\nexisting models in addressing public security issues but also provide\nreferences for the future development of more accurate and customized LLM\nmodels targeted at applications in this field.\n","authors":["Xin Tong","Bo Jin","Zhi Lin","Binjun Wang","Ting Yu","Qiang Cheng"],"pdf_url":"https://arxiv.org/pdf/2402.07234v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14358v1","updated":"2024-03-21T12:37:54Z","published":"2024-03-21T12:37:54Z","title":"Exploring the Potential of Large Language Models in Graph Generation","summary":"  Large language models (LLMs) have achieved great success in many fields, and\nrecent works have studied exploring LLMs for graph discriminative tasks such as\nnode classification. However, the abilities of LLMs for graph generation remain\nunexplored in the literature. Graph generation requires the LLM to generate\ngraphs with given properties, which has valuable real-world applications such\nas drug discovery, while tends to be more challenging. In this paper, we\npropose LLM4GraphGen to explore the ability of LLMs for graph generation with\nsystematical task designs and extensive experiments. Specifically, we propose\nseveral tasks tailored with comprehensive experiments to address key questions\nregarding LLMs' understanding of different graph structure rules, their ability\nto capture structural type distributions, and their utilization of domain\nknowledge for property-based graph generation. Our evaluations demonstrate that\nLLMs, particularly GPT-4, exhibit preliminary abilities in graph generation\ntasks, including rule-based and distribution-based generation. We also observe\nthat popular prompting methods, such as few-shot and chain-of-thought\nprompting, do not consistently enhance performance. Besides, LLMs show\npotential in generating molecules with specific properties. These findings may\nserve as foundations for designing good LLMs based models for graph generation\nand provide valuable insights and further research.\n","authors":["Yang Yao","Xin Wang","Zeyang Zhang","Yijian Qin","Ziwei Zhang","Xu Chu","Yuekui Yang","Wenwu Zhu","Hong Mei"],"pdf_url":"https://arxiv.org/pdf/2403.14358v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.13927v2","updated":"2024-03-21T12:37:21Z","published":"2023-12-21T15:22:07Z","title":"On the convergence of loss and uncertainty-based active learning\n  algorithms","summary":"  We consider the convergence rates of loss and uncertainty-based active\nlearning algorithms under various assumptions. Firstly, we establish a set of\nconditions that ensure convergence rates when applied to linear classifiers and\nlinearly separable datasets. This includes demonstrating convergence rate\nguarantees for loss-based sampling with various loss functions. Secondly, we\nintroduce a framework that allows us to derive convergence rate bounds for\nloss-based sampling by leveraging known convergence rate bounds for stochastic\ngradient descent algorithms. Lastly, we propose a new algorithm that combines\npoint sampling and stochastic Polyak's step size. We establish a condition on\nthe sampling process, ensuring a convergence rate guarantee for this algorithm,\nparticularly in the case of smooth convex loss functions. Our numerical results\nshowcase the efficiency of the proposed algorithm.\n","authors":["Daniel Haimovich","Dima Karamshuk","Fridolin Linder","Niek Tax","Milan Vojnovic"],"pdf_url":"https://arxiv.org/pdf/2312.13927v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.01753v2","updated":"2024-03-21T12:36:22Z","published":"2023-11-03T07:18:36Z","title":"RiskQ: Risk-sensitive Multi-Agent Reinforcement Learning Value\n  Factorization","summary":"  Multi-agent systems are characterized by environmental uncertainty, varying\npolicies of agents, and partial observability, which result in significant\nrisks. In the context of Multi-Agent Reinforcement Learning (MARL), learning\ncoordinated and decentralized policies that are sensitive to risk is\nchallenging. To formulate the coordination requirements in risk-sensitive MARL,\nwe introduce the Risk-sensitive Individual-Global-Max (RIGM) principle as a\ngeneralization of the Individual-Global-Max (IGM) and Distributional IGM (DIGM)\nprinciples. This principle requires that the collection of risk-sensitive\naction selections of each agent should be equivalent to the risk-sensitive\naction selection of the central policy. Current MARL value factorization\nmethods do not satisfy the RIGM principle for common risk metrics such as the\nValue at Risk (VaR) metric or distorted risk measurements. Therefore, we\npropose RiskQ to address this limitation, which models the joint return\ndistribution by modeling quantiles of it as weighted quantile mixtures of\nper-agent return distribution utilities. RiskQ satisfies the RIGM principle for\nthe VaR and distorted risk metrics. We show that RiskQ can obtain promising\nperformance through extensive experiments. The source code of RiskQ is\navailable in https://github.com/xmu-rl-3dv/RiskQ.\n","authors":["Siqi Shen","Chennan Ma","Chao Li","Weiquan Liu","Yongquan Fu","Songzhu Mei","Xinwang Liu","Cheng Wang"],"pdf_url":"https://arxiv.org/pdf/2311.01753v2.pdf","comment":"Accepted at NeurIPS 2023"},{"id":"http://arxiv.org/abs/2403.14606v1","updated":"2024-03-21T17:55:16Z","published":"2024-03-21T17:55:16Z","title":"The Elements of Differentiable Programming","summary":"  Artificial intelligence has recently experienced remarkable advances, fueled\nby large models, vast datasets, accelerated hardware, and, last but not least,\nthe transformative power of differentiable programming. This new programming\nparadigm enables end-to-end differentiation of complex computer programs\n(including those with control flows and data structures), making gradient-based\noptimization of program parameters possible. As an emerging paradigm,\ndifferentiable programming builds upon several areas of computer science and\napplied mathematics, including automatic differentiation, graphical models,\noptimization and statistics. This book presents a comprehensive review of the\nfundamental concepts useful for differentiable programming. We adopt two main\nperspectives, that of optimization and that of probability, with clear\nanalogies between the two. Differentiable programming is not merely the\ndifferentiation of programs, but also the thoughtful design of programs\nintended for differentiation. By making programs differentiable, we inherently\nintroduce probability distributions over their execution, providing a means to\nquantify the uncertainty associated with program outputs.\n","authors":["Mathieu Blondel","Vincent Roulet"],"pdf_url":"https://arxiv.org/pdf/2403.14606v1.pdf","comment":"Draft version 1"},{"id":"http://arxiv.org/abs/2305.00969v7","updated":"2024-03-21T17:52:22Z","published":"2023-05-01T17:56:32Z","title":"CryCeleb: A Speaker Verification Dataset Based on Infant Cry Sounds","summary":"  This paper describes the Ubenwa CryCeleb dataset - a labeled collection of\ninfant cries - and the accompanying CryCeleb 2023 task, which is a public\nspeaker verification challenge based on cry sounds. We released more than 6\nhours of manually segmented cry sounds from 786 newborns for academic use,\naiming to encourage research in infant cry analysis. The inaugural public\ncompetition attracted 59 participants, 11 of whom improved the baseline\nperformance. The top-performing system achieved a significant improvement\nscoring 25.8% equal error rate, which is still far from the performance of\nstate-of-the-art adult speaker verification systems. Therefore, we believe\nthere is room for further research on this dataset, potentially extending\nbeyond the verification task.\n","authors":["David Budaghyan","Charles C. Onu","Arsenii Gorin","Cem Subakan","Doina Precup"],"pdf_url":"https://arxiv.org/pdf/2305.00969v7.pdf","comment":"ICASSP 2024"},{"id":"http://arxiv.org/abs/2403.14578v1","updated":"2024-03-21T17:30:59Z","published":"2024-03-21T17:30:59Z","title":"RAmBLA: A Framework for Evaluating the Reliability of LLMs as Assistants\n  in the Biomedical Domain","summary":"  Large Language Models (LLMs) increasingly support applications in a wide\nrange of domains, some with potential high societal impact such as biomedicine,\nyet their reliability in realistic use cases is under-researched. In this work\nwe introduce the Reliability AssesMent for Biomedical LLM Assistants (RAmBLA)\nframework and evaluate whether four state-of-the-art foundation LLMs can serve\nas reliable assistants in the biomedical domain. We identify prompt robustness,\nhigh recall, and a lack of hallucinations as necessary criteria for this use\ncase. We design shortform tasks and tasks requiring LLM freeform responses\nmimicking real-world user interactions. We evaluate LLM performance using\nsemantic similarity with a ground truth response, through an evaluator LLM.\n","authors":["William James Bolton","Rafael Poyiadzi","Edward R. Morrell","Gabriela van Bergen Gonzalez Bueno","Lea Goetz"],"pdf_url":"https://arxiv.org/pdf/2403.14578v1.pdf","comment":"Published at ICLR 2024 Workshop on Reliable and Responsible\n  Foundation Models"},{"id":"http://arxiv.org/abs/2403.14377v1","updated":"2024-03-21T13:09:23Z","published":"2024-03-21T13:09:23Z","title":"Knowledge-Enhanced Recommendation with User-Centric Subgraph Network","summary":"  Recommendation systems, as widely implemented nowadays on various platforms,\nrecommend relevant items to users based on their preferences. The classical\nmethods which rely on user-item interaction matrices has limitations,\nespecially in scenarios where there is a lack of interaction data for new\nitems. Knowledge graph (KG)-based recommendation systems have emerged as a\npromising solution. However, most KG-based methods adopt node embeddings, which\ndo not provide personalized recommendations for different users and cannot\ngeneralize well to the new items. To address these limitations, we propose\nKnowledge-enhanced User-Centric subgraph Network (KUCNet), a subgraph learning\napproach with graph neural network (GNN) for effective recommendation. KUCNet\nconstructs a U-I subgraph for each user-item pair that captures both the\nhistorical information of user-item interactions and the side information\nprovided in KG. An attention-based GNN is designed to encode the U-I subgraphs\nfor recommendation. Considering efficiency, the pruned user-centric computation\ngraph is further introduced such that multiple U-I subgraphs can be\nsimultaneously computed and that the size can be pruned by Personalized\nPageRank. Our proposed method achieves accurate, efficient, and interpretable\nrecommendations especially for new items. Experimental results demonstrate the\nsuperiority of KUCNet over state-of-the-art KG-based and collaborative\nfiltering (CF)-based methods.\n","authors":["Guangyi Liu","Quanming Yao","Yongqi Zhang","Lei Chen"],"pdf_url":"https://arxiv.org/pdf/2403.14377v1.pdf","comment":null}],"Databases":[{"id":"http://arxiv.org/abs/2403.14441v1","updated":"2024-03-21T14:46:45Z","published":"2024-03-21T14:46:45Z","title":"Quantifying Semantic Query Similarity for Automated Linear SQL Grading:\n  A Graph-based Approach","summary":"  Quantifying the semantic similarity between database queries is a critical\nchallenge with broad applications, ranging from query log analysis to automated\neducational assessment of SQL skills. Traditional methods often rely solely on\nsyntactic comparisons or are limited to checking for semantic equivalence.\n  This paper introduces a novel graph-based approach to measure the semantic\ndissimilarity between SQL queries. Queries are represented as nodes in an\nimplicit graph, while the transitions between nodes are called edits, which are\nweighted by semantic dissimilarity. We employ shortest path algorithms to\nidentify the lowest-cost edit sequence between two given queries, thereby\ndefining a quantifiable measure of semantic distance.\n  A prototype implementation of this technique has been evaluated through an\nempirical study, which strongly suggests that our method provides more accurate\nand comprehensible grading compared to existing techniques. Moreover, the\nresults indicate that our approach comes close to the quality of manual\ngrading, making it a robust tool for diverse database query comparison tasks.\n","authors":["Leo Köberlein","Dominik Probst","Richard Lenz"],"pdf_url":"https://arxiv.org/pdf/2403.14441v1.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2403.14256v1","updated":"2024-03-21T09:29:18Z","published":"2024-03-21T09:29:18Z","title":"Space-Efficient Indexes for Uncertain Strings","summary":"  Strings in the real world are often encoded with some level of uncertainty.\nIn the character-level uncertainty model, an uncertain string $X$ of length $n$\non an alphabet $\\Sigma$ is a sequence of $n$ probability distributions over\n$\\Sigma$. Given an uncertain string $X$ and a weight threshold\n$\\frac{1}{z}\\in(0,1]$, we say that pattern $P$ occurs in $X$ at position $i$,\nif the product of probabilities of the letters of $P$ at positions\n$i,\\ldots,i+|P|-1$ is at least $\\frac{1}{z}$. While indexing standard strings\nfor online pattern searches can be performed in linear time and space, indexing\nuncertain strings is much more challenging. Specifically, the state-of-the-art\nindex for uncertain strings has $\\mathcal{O}(nz)$ size, requires\n$\\mathcal{O}(nz)$ time and $\\mathcal{O}(nz)$ space to be constructed, and\nanswers pattern matching queries in the optimal $\\mathcal{O}(m+|\\text{Occ}|)$\ntime, where $m$ is the length of $P$ and $|\\text{Occ}|$ is the total number of\noccurrences of $P$ in $X$. For large $n$ and (moderate) $z$ values, this index\nis completely impractical to construct, which outweighs the benefit of the\nsupported optimal pattern matching queries. We were thus motivated to design a\nspace-efficient index at the expense of slower yet competitive pattern matching\nqueries. We propose an index of $\\mathcal{O}(\\frac{nz}{\\ell}\\log z)$ expected\nsize, which can be constructed using $\\mathcal{O}(\\frac{nz}{\\ell}\\log z)$\nexpected space, and supports very fast pattern matching queries in expectation,\nfor patterns of length $m\\geq \\ell$. We have implemented and evaluated several\nversions of our index. The best-performing version of our index is up to two\norders of magnitude smaller than the state of the art in terms of both index\nsize and construction space, while offering faster or very competitive query\nand construction times.\n","authors":["Esteban Gabory","Chang Liu","Grigorios Loukides","Solon P. Pissis","Wiktor Zuba"],"pdf_url":"https://arxiv.org/pdf/2403.14256v1.pdf","comment":"Accepted to ICDE 2024. Abstract abridged to satisfy arXiv\n  requirements"},{"id":"http://arxiv.org/abs/2309.02094v3","updated":"2024-03-21T09:03:48Z","published":"2023-09-05T10:00:33Z","title":"TensorBank: Tensor Lakehouse for Foundation Model Training","summary":"  Storing and streaming high dimensional data for foundation model training\nbecame a critical requirement with the rise of foundation models beyond natural\nlanguage. In this paper we introduce TensorBank, a petabyte scale tensor\nlakehouse capable of streaming tensors from Cloud Object Store (COS) to GPU\nmemory at wire speed based on complex relational queries. We use Hierarchical\nStatistical Indices (HSI) for query acceleration. Our architecture allows to\ndirectly address tensors on block level using HTTP range reads. Once in GPU\nmemory, data can be transformed using PyTorch transforms. We provide a generic\nPyTorch dataset type with a corresponding dataset factory translating\nrelational queries and requested transformations as an instance. By making use\nof the HSI, irrelevant blocks can be skipped without reading them as those\nindices contain statistics on their content at different hierarchical\nresolution levels. This is an opinionated architecture powered by open\nstandards and making heavy use of open-source technology. Although, hardened\nfor production use using geospatial-temporal data, this architecture\ngeneralizes to other use case like computer vision, computational neuroscience,\nbiological sequence analysis and more.\n","authors":["Romeo Kienzler","Leonardo Pondian Tizzei","Benedikt Blumenstiel","Zoltan Arnold Nagy","S. Karthik Mukkavilli","Johannes Schmude","Marcus Freitag","Michael Behrendt","Daniel Salles Civitarese","Naomi Simumba","Daiki Kimura","Hendrik Hamann"],"pdf_url":"https://arxiv.org/pdf/2309.02094v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.04877v3","updated":"2024-03-21T07:22:54Z","published":"2023-12-08T07:27:26Z","title":"Generating Explanations to Understand and Repair Embedding-based Entity\n  Alignment","summary":"  Entity alignment (EA) seeks identical entities in different knowledge graphs,\nwhich is a long-standing task in the database research. Recent work leverages\ndeep learning to embed entities in vector space and align them via nearest\nneighbor search. Although embedding-based EA has gained marked success in\nrecent years, it lacks explanations for alignment decisions. In this paper, we\npresent the first framework that can generate explanations for understanding\nand repairing embedding-based EA results. Given an EA pair produced by an\nembedding model, we first compare its neighbor entities and relations to build\na matching subgraph as a local explanation. We then construct an alignment\ndependency graph to understand the pair from an abstract perspective. Finally,\nwe repair the pair by resolving three types of alignment conflicts based on\ndependency graphs. Experiments on a variety of EA datasets demonstrate the\neffectiveness, generalization, and robustness of our framework in explaining\nand repairing embedding-based EA results.\n","authors":["Xiaobin Tian","Zequn Sun","Wei Hu"],"pdf_url":"https://arxiv.org/pdf/2312.04877v3.pdf","comment":"Accepted in the 40th IEEE International Conference on Data\n  Engineering (ICDE 2024)"},{"id":"http://arxiv.org/abs/2403.14151v1","updated":"2024-03-21T05:57:27Z","published":"2024-03-21T05:57:27Z","title":"Deep Learning for Trajectory Data Management and Mining: A Survey and\n  Beyond","summary":"  Trajectory computing is a pivotal domain encompassing trajectory data\nmanagement and mining, garnering widespread attention due to its crucial role\nin various practical applications such as location services, urban traffic, and\npublic safety. Traditional methods, focusing on simplistic spatio-temporal\nfeatures, face challenges of complex calculations, limited scalability, and\ninadequate adaptability to real-world complexities. In this paper, we present a\ncomprehensive review of the development and recent advances in deep learning\nfor trajectory computing (DL4Traj). We first define trajectory data and provide\na brief overview of widely-used deep learning models. Systematically, we\nexplore deep learning applications in trajectory management (pre-processing,\nstorage, analysis, and visualization) and mining (trajectory-related\nforecasting, trajectory-related recommendation, trajectory classification,\ntravel time estimation, anomaly detection, and mobility generation). Notably,\nwe encapsulate recent advancements in Large Language Models (LLMs) that hold\nthe potential to augment trajectory computing. Additionally, we summarize\napplication scenarios, public datasets, and toolkits. Finally, we outline\ncurrent challenges in DL4Traj research and propose future directions. Relevant\npapers and open-source resources have been collated and are continuously\nupdated at:\n\\href{https://github.com/yoshall/Awesome-Trajectory-Computing}{DL4Traj Repo}.\n","authors":["Wei Chen","Yuxuan Liang","Yuanshao Zhu","Yanchuan Chang","Kang Luo","Haomin Wen","Lei Li","Yanwei Yu","Qingsong Wen","Chao Chen","Kai Zheng","Yunjun Gao","Xiaofang Zhou","Yu Zheng"],"pdf_url":"https://arxiv.org/pdf/2403.14151v1.pdf","comment":"25 pages, 12 figures, 5 tables"},{"id":"http://arxiv.org/abs/2403.14128v1","updated":"2024-03-21T04:48:08Z","published":"2024-03-21T04:48:08Z","title":"Gen-T: Table Reclamation in Data Lakes","summary":"  We introduce the problem of Table Reclamation. Given a Source Table and a\nlarge table repository, reclamation finds a set of tables that, when\nintegrated, reproduce the source table as closely as possible. Unlike query\ndiscovery problems like Query-by-Example or by-Target, Table Reclamation\nfocuses on reclaiming the data in the Source Table as fully as possible using\nreal tables that may be incomplete or inconsistent. To do this, we define a new\nmeasure of table similarity, called error-aware instance similarity, to measure\nhow close a reclaimed table is to a Source Table, a measure grounded in\ninstance similarity used in data exchange. Our search covers not only\nSELECT-PROJECT- JOIN queries, but integration queries with unions, outerjoins,\nand the unary operators subsumption and complementation that have been shown to\nbe important in data integration and fusion. Using reclamation, a data\nscientist can understand if any tables in a repository can be used to exactly\nreclaim a tuple in the Source. If not, one can understand if this is due to\ndifferences in values or to incompleteness in the data. Our solution, Gen-T,\nperforms table discovery to retrieve a set of candidate tables from the table\nrepository, filters these down to a set of originating tables, then integrates\nthese tables to reclaim the Source as closely as possible. We show that our\nsolution, while approximate, is accurate, efficient and scalable in the size of\nthe table repository with experiments on real data lakes containing up to 15K\ntables, where the average number of tuples varies from small (web tables) to\nextremely large (open data tables) up to 1M tuples.\n","authors":["Grace Fan","Roee Shraga","Renée J. Miller"],"pdf_url":"https://arxiv.org/pdf/2403.14128v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.06502v6","updated":"2024-03-21T08:25:08Z","published":"2023-12-11T16:27:04Z","title":"On enforcing dyadic-type homogeneous binary function product constraints\n  in MatBase","summary":"  Homogeneous binary function products are often encountered in the\nsub-universes modeled by databases, from genealogical trees to sports, from\neducation to healthcare, etc. Their properties must be discovered and enforced\nby the software applications managing such data to guarantee plausibility. The\n(Elementary) Mathematical Data Model provides 18 dyadic-type homogeneous binary\nfunction product constraint types. MatBase, an intelligent data and knowledge\nbase management system prototype, allows database designers to simply declare\nthem by only clicking corresponding checkboxes and automatically generates code\nfor enforcing them. This paper describes the algorithms that MatBase uses for\nenforcing all these 18 homogeneous binary function product constraint types,\nwhich may also be used by developers not having access to MatBase.\n","authors":["Christian Mancas"],"pdf_url":"https://arxiv.org/pdf/2312.06502v6.pdf","comment":"submitted on Dec. 7, 2023, to the Journal of Data Science and\n  Intelligent Systems (JDSIS), on Dec. 20, 2023, to the Journal of\n  Computational and Cognitive Engineering, both of Bon View Publishing,\n  Singapore, on Dec. 30 to the Journal of Current Research and Studies, and on\n  Jan. 25, 2024, to the Journal of Computer Science Research, Bilingual\n  Publishing Group, Singapore"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2403.14624v1","updated":"2024-03-21T17:59:50Z","published":"2024-03-21T17:59:50Z","title":"MathVerse: Does Your Multi-modal LLM Truly See the Diagrams in Visual\n  Math Problems?","summary":"  The remarkable progress of Multi-modal Large Language Models (MLLMs) has\ngarnered unparalleled attention, due to their superior performance in visual\ncontexts. However, their capabilities in visual math problem-solving remain\ninsufficiently evaluated and understood. We investigate current benchmarks to\nincorporate excessive visual content within textual questions, which\npotentially assist MLLMs in deducing answers without truly interpreting the\ninput diagrams. To this end, we introduce MathVerse, an all-around visual math\nbenchmark designed for an equitable and in-depth evaluation of MLLMs. We\nmeticulously collect 2,612 high-quality, multi-subject math problems with\ndiagrams from publicly available sources. Each problem is then transformed by\nhuman annotators into six distinct versions, each offering varying degrees of\ninformation content in multi-modality, contributing to 15K test samples in\ntotal. This approach allows MathVerse to comprehensively assess whether and how\nmuch MLLMs can truly understand the visual diagrams for mathematical reasoning.\nIn addition, we propose a Chain-of-Thought (CoT) evaluation strategy for a\nfine-grained assessment of the output answers. Rather than naively judging True\nor False, we employ GPT-4(V) to adaptively extract crucial reasoning steps, and\nthen score each step with detailed error analysis, which can reveal the\nintermediate CoT reasoning quality by MLLMs. We hope the MathVerse benchmark\nmay provide unique insights to guide the future development of MLLMs. Project\npage: https://mathverse-cuhk.github.io\n","authors":["Renrui Zhang","Dongzhi Jiang","Yichi Zhang","Haokun Lin","Ziyu Guo","Pengshuo Qiu","Aojun Zhou","Pan Lu","Kai-Wei Chang","Peng Gao","Hongsheng Li"],"pdf_url":"https://arxiv.org/pdf/2403.14624v1.pdf","comment":"46 Pages, Work in Progress, Benchmark Project Page:\n  https://mathverse-cuhk.github.io"},{"id":"http://arxiv.org/abs/2403.14623v1","updated":"2024-03-21T17:59:41Z","published":"2024-03-21T17:59:41Z","title":"Simplified Diffusion Schrödinger Bridge","summary":"  This paper introduces a novel theoretical simplification of the Diffusion\nSchr\\\"odinger Bridge (DSB) that facilitates its unification with Score-based\nGenerative Models (SGMs), addressing the limitations of DSB in complex data\ngeneration and enabling faster convergence and enhanced performance. By\nemploying SGMs as an initial solution for DSB, our approach capitalizes on the\nstrengths of both frameworks, ensuring a more efficient training process and\nimproving the performance of SGM. We also propose a reparameterization\ntechnique that, despite theoretical approximations, practically improves the\nnetwork's fitting capabilities. Our extensive experimental evaluations confirm\nthe effectiveness of the simplified DSB, demonstrating its significant\nimprovements. We believe the contributions of this work pave the way for\nadvanced generative modeling. The code is available at\nhttps://github.com/tzco/Simplified-Diffusion-Schrodinger-Bridge.\n","authors":["Zhicong Tang","Tiankai Hang","Shuyang Gu","Dong Chen","Baining Guo"],"pdf_url":"https://arxiv.org/pdf/2403.14623v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14617v1","updated":"2024-03-21T17:59:03Z","published":"2024-03-21T17:59:03Z","title":"Videoshop: Localized Semantic Video Editing with Noise-Extrapolated\n  Diffusion Inversion","summary":"  We introduce Videoshop, a training-free video editing algorithm for localized\nsemantic edits. Videoshop allows users to use any editing software, including\nPhotoshop and generative inpainting, to modify the first frame; it\nautomatically propagates those changes, with semantic, spatial, and temporally\nconsistent motion, to the remaining frames. Unlike existing methods that enable\nedits only through imprecise textual instructions, Videoshop allows users to\nadd or remove objects, semantically change objects, insert stock photos into\nvideos, etc. with fine-grained control over locations and appearance. We\nachieve this through image-based video editing by inverting latents with noise\nextrapolation, from which we generate videos conditioned on the edited image.\nVideoshop produces higher quality edits against 6 baselines on 2 editing\nbenchmarks using 10 evaluation metrics.\n","authors":["Xiang Fan","Anand Bhattad","Ranjay Krishna"],"pdf_url":"https://arxiv.org/pdf/2403.14617v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14613v1","updated":"2024-03-21T17:58:04Z","published":"2024-03-21T17:58:04Z","title":"DreamReward: Text-to-3D Generation with Human Preference","summary":"  3D content creation from text prompts has shown remarkable success recently.\nHowever, current text-to-3D methods often generate 3D results that do not align\nwell with human preferences. In this paper, we present a comprehensive\nframework, coined DreamReward, to learn and improve text-to-3D models from\nhuman preference feedback. To begin with, we collect 25k expert comparisons\nbased on a systematic annotation pipeline including rating and ranking. Then,\nwe build Reward3D -- the first general-purpose text-to-3D human preference\nreward model to effectively encode human preferences. Building upon the 3D\nreward model, we finally perform theoretical analysis and present the Reward3D\nFeedback Learning (DreamFL), a direct tuning algorithm to optimize the\nmulti-view diffusion models with a redefined scorer. Grounded by theoretical\nproof and extensive experiment comparisons, our DreamReward successfully\ngenerates high-fidelity and 3D consistent results with significant boosts in\nprompt alignment with human intention. Our results demonstrate the great\npotential for learning from human feedback to improve text-to-3D models.\n","authors":["Junliang Ye","Fangfu Liu","Qixiu Li","Zhengyi Wang","Yikai Wang","Xinzhou Wang","Yueqi Duan","Jun Zhu"],"pdf_url":"https://arxiv.org/pdf/2403.14613v1.pdf","comment":"Project page: https://jamesyjl.github.io/DreamReward"},{"id":"http://arxiv.org/abs/2310.16828v2","updated":"2024-03-21T17:56:19Z","published":"2023-10-25T17:57:07Z","title":"TD-MPC2: Scalable, Robust World Models for Continuous Control","summary":"  TD-MPC is a model-based reinforcement learning (RL) algorithm that performs\nlocal trajectory optimization in the latent space of a learned implicit\n(decoder-free) world model. In this work, we present TD-MPC2: a series of\nimprovements upon the TD-MPC algorithm. We demonstrate that TD-MPC2 improves\nsignificantly over baselines across 104 online RL tasks spanning 4 diverse task\ndomains, achieving consistently strong results with a single set of\nhyperparameters. We further show that agent capabilities increase with model\nand data size, and successfully train a single 317M parameter agent to perform\n80 tasks across multiple task domains, embodiments, and action spaces. We\nconclude with an account of lessons, opportunities, and risks associated with\nlarge TD-MPC2 agents. Explore videos, models, data, code, and more at\nhttps://tdmpc2.com\n","authors":["Nicklas Hansen","Hao Su","Xiaolong Wang"],"pdf_url":"https://arxiv.org/pdf/2310.16828v2.pdf","comment":"ICLR 2024. Explore videos, models, data, code, and more at\n  https://tdmpc2.com"},{"id":"http://arxiv.org/abs/2403.14608v1","updated":"2024-03-21T17:55:50Z","published":"2024-03-21T17:55:50Z","title":"Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey","summary":"  Large models represent a groundbreaking advancement in multiple application\nfields, enabling remarkable achievements across various tasks. However, their\nunprecedented scale comes with significant computational costs. These models,\noften consisting of billions of parameters, require vast amounts of\ncomputational resources for execution. Especially, the expansive scale and\ncomputational demands pose considerable challenges when customizing them for\nparticular downstream tasks, particularly over the hardware platforms\nconstrained by computational capabilities. Parameter Efficient Fine-Tuning\n(PEFT) provides a practical solution by efficiently adapt the large models over\nthe various downstream tasks. In particular, PEFT refers to the process of\nadjusting the parameters of a pre-trained large models to adapt it to a\nspecific task while minimizing the number of additional parameters introduced\nor computational resources required. This approach is particularly important\nwhen dealing with large language models with high parameter counts, as\nfine-tuning these models from scratch can be computationally expensive and\nresource-intensive, posing considerable challenges in the supporting system\nplatform design. In this survey, we present comprehensive studies of various\nPEFT algorithms, examining their performance and computational overhead.\nMoreover, we provide an overview of applications developed using different PEFT\nalgorithms and discuss common techniques employed to mitigate computation costs\nfor PEFT. In addition to the algorithmic perspective, we overview various\nreal-world system designs to investigate the implementation costs associated\nwith different PEFT algorithms. This survey serves as an indispensable resource\nfor researchers aiming to understand both the PEFT algorithm and its system\nimplementation, offering detailed insights into recent advancements and\npractical applications.\n","authors":["Zeyu Han","Chao Gao","Jinyang Liu"," Jeff"," Zhang","Sai Qian Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.14608v1.pdf","comment":"25 pages, 13 figures"},{"id":"http://arxiv.org/abs/2403.14606v1","updated":"2024-03-21T17:55:16Z","published":"2024-03-21T17:55:16Z","title":"The Elements of Differentiable Programming","summary":"  Artificial intelligence has recently experienced remarkable advances, fueled\nby large models, vast datasets, accelerated hardware, and, last but not least,\nthe transformative power of differentiable programming. This new programming\nparadigm enables end-to-end differentiation of complex computer programs\n(including those with control flows and data structures), making gradient-based\noptimization of program parameters possible. As an emerging paradigm,\ndifferentiable programming builds upon several areas of computer science and\napplied mathematics, including automatic differentiation, graphical models,\noptimization and statistics. This book presents a comprehensive review of the\nfundamental concepts useful for differentiable programming. We adopt two main\nperspectives, that of optimization and that of probability, with clear\nanalogies between the two. Differentiable programming is not merely the\ndifferentiation of programs, but also the thoughtful design of programs\nintended for differentiation. By making programs differentiable, we inherently\nintroduce probability distributions over their execution, providing a means to\nquantify the uncertainty associated with program outputs.\n","authors":["Mathieu Blondel","Vincent Roulet"],"pdf_url":"https://arxiv.org/pdf/2403.14606v1.pdf","comment":"Draft version 1"},{"id":"http://arxiv.org/abs/2403.14602v1","updated":"2024-03-21T17:52:08Z","published":"2024-03-21T17:52:08Z","title":"ReNoise: Real Image Inversion Through Iterative Noising","summary":"  Recent advancements in text-guided diffusion models have unlocked powerful\nimage manipulation capabilities. However, applying these methods to real images\nnecessitates the inversion of the images into the domain of the pretrained\ndiffusion model. Achieving faithful inversion remains a challenge, particularly\nfor more recent models trained to generate images with a small number of\ndenoising steps. In this work, we introduce an inversion method with a high\nquality-to-operation ratio, enhancing reconstruction accuracy without\nincreasing the number of operations. Building on reversing the diffusion\nsampling process, our method employs an iterative renoising mechanism at each\ninversion sampling step. This mechanism refines the approximation of a\npredicted point along the forward diffusion trajectory, by iteratively applying\nthe pretrained diffusion model, and averaging these predictions. We evaluate\nthe performance of our ReNoise technique using various sampling algorithms and\nmodels, including recent accelerated diffusion models. Through comprehensive\nevaluations and comparisons, we show its effectiveness in terms of both\naccuracy and speed. Furthermore, we confirm that our method preserves\neditability by demonstrating text-driven image editing on real images.\n","authors":["Daniel Garibi","Or Patashnik","Andrey Voynov","Hadar Averbuch-Elor","Daniel Cohen-Or"],"pdf_url":"https://arxiv.org/pdf/2403.14602v1.pdf","comment":"project page at: https://garibida.github.io/ReNoise-Inversion/"},{"id":"http://arxiv.org/abs/2403.14597v1","updated":"2024-03-21T17:50:22Z","published":"2024-03-21T17:50:22Z","title":"Extended Reality for Enhanced Human-Robot Collaboration: a\n  Human-in-the-Loop Approach","summary":"  The rise of automation has provided an opportunity to achieve higher\nefficiency in manufacturing processes, yet it often compromises the flexibility\nrequired to promptly respond to evolving market needs and meet the demand for\ncustomization. Human-robot collaboration attempts to tackle these challenges by\ncombining the strength and precision of machines with human ingenuity and\nperceptual understanding. In this paper, we conceptualize and propose an\nimplementation framework for an autonomous, machine learning-based manipulator\nthat incorporates human-in-the-loop principles and leverages Extended Reality\n(XR) to facilitate intuitive communication and programming between humans and\nrobots. Furthermore, the conceptual framework foresees human involvement\ndirectly in the robot learning process, resulting in higher adaptability and\ntask generalization. The paper highlights key technologies enabling the\nproposed framework, emphasizing the importance of developing the digital\necosystem as a whole. Additionally, we review the existent implementation\napproaches of XR in human-robot collaboration, showcasing diverse perspectives\nand methodologies. The challenges and future outlooks are discussed, delving\ninto the major obstacles and potential research avenues of XR for more natural\nhuman-robot interaction and integration in the industrial landscape.\n","authors":["Yehor Karpichev","Todd Charter","Homayoun Najjaran"],"pdf_url":"https://arxiv.org/pdf/2403.14597v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14593v1","updated":"2024-03-21T17:48:38Z","published":"2024-03-21T17:48:38Z","title":"Rethinking Adversarial Inverse Reinforcement Learning: From the Angles\n  of Policy Imitation and Transferable Reward Recovery","summary":"  Adversarial inverse reinforcement learning (AIRL) stands as a cornerstone\napproach in imitation learning. This paper rethinks the two different angles of\nAIRL: policy imitation and transferable reward recovery. We begin with\nsubstituting the built-in algorithm in AIRL with soft actor-critic (SAC) during\nthe policy optimization process to enhance sample efficiency, thanks to the\noff-policy formulation of SAC and identifiable Markov decision process (MDP)\nmodels with respect to AIRL. It indeed exhibits a significant improvement in\npolicy imitation but accidentally brings drawbacks to transferable reward\nrecovery. To learn this issue, we illustrate that the SAC algorithm itself is\nnot feasible to disentangle the reward function comprehensively during the AIRL\ntraining process, and propose a hybrid framework, PPO-AIRL + SAC, for\nsatisfactory transfer effect. Additionally, we analyze the capability of\nenvironments to extract disentangled rewards from an algebraic theory\nperspective.\n","authors":["Yangchun Zhang","Yirui Zhou"],"pdf_url":"https://arxiv.org/pdf/2403.14593v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14589v1","updated":"2024-03-21T17:43:44Z","published":"2024-03-21T17:43:44Z","title":"ReAct Meets ActRe: Autonomous Annotations of Agent Trajectories for\n  Contrastive Self-Training","summary":"  Language agents have demonstrated autonomous decision-making abilities by\nreasoning with foundation models. Recently, efforts have been made to train\nlanguage agents for performance improvement, with multi-step reasoning and\naction trajectories as the training data. However, collecting such trajectories\nstill requires considerable human effort, by either artificial annotations or\nimplementations of diverse prompting frameworks. In this work, we propose\nA$^3$T, a framework that enables the Autonomous Annotation of Agent\nTrajectories in the style of ReAct. The central role is an ActRe prompting\nagent, which explains the reason for an arbitrary action. When randomly\nsampling an external action, the ReAct-style agent could query the ActRe agent\nwith the action to obtain its textual rationales. Novel trajectories are then\nsynthesized by prepending the posterior reasoning from ActRe to the sampled\naction. In this way, the ReAct-style agent executes multiple trajectories for\nthe failed tasks, and selects the successful ones to supplement its failed\ntrajectory for contrastive self-training. Realized by policy gradient methods\nwith binarized rewards, the contrastive self-training with accumulated\ntrajectories facilitates a closed loop for multiple rounds of language agent\nself-improvement. We conduct experiments using QLoRA fine-tuning with the\nopen-sourced Mistral-7B-Instruct-v0.2. In AlfWorld, the agent trained with\nA$^3$T obtains a 1-shot success rate of 96%, and 100% success with 4 iterative\nrounds. In WebShop, the 1-shot performance of the A$^3$T agent matches human\naverage, and 4 rounds of iterative refinement lead to the performance\napproaching human experts. A$^3$T agents significantly outperform existing\ntechniques, including prompting with GPT-4, advanced agent frameworks, and\nfully fine-tuned LLMs.\n","authors":["Zonghan Yang","Peng Li","Ming Yan","Ji Zhang","Fei Huang","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2403.14589v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14587v1","updated":"2024-03-21T17:42:45Z","published":"2024-03-21T17:42:45Z","title":"An Analysis of Linear Time Series Forecasting Models","summary":"  Despite their simplicity, linear models perform well at time series\nforecasting, even when pitted against deeper and more expensive models. A\nnumber of variations to the linear model have been proposed, often including\nsome form of feature normalisation that improves model generalisation. In this\npaper we analyse the sets of functions expressible using these linear model\narchitectures. In so doing we show that several popular variants of linear\nmodels for time series forecasting are equivalent and functionally\nindistinguishable from standard, unconstrained linear regression. We\ncharacterise the model classes for each linear variant. We demonstrate that\neach model can be reinterpreted as unconstrained linear regression over a\nsuitably augmented feature set, and therefore admit closed-form solutions when\nusing a mean-squared loss function. We provide experimental evidence that the\nmodels under inspection learn nearly identical solutions, and finally\ndemonstrate that the simpler closed form solutions are superior forecasters\nacross 72% of test settings.\n","authors":["William Toner","Luke Darlow"],"pdf_url":"https://arxiv.org/pdf/2403.14587v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14583v1","updated":"2024-03-21T17:37:43Z","published":"2024-03-21T17:37:43Z","title":"Co-Optimization of Environment and Policies for Decentralized\n  Multi-Agent Navigation","summary":"  This work views the multi-agent system and its surrounding environment as a\nco-evolving system, where the behavior of one affects the other. The goal is to\ntake both agent actions and environment configurations as decision variables,\nand optimize these two components in a coordinated manner to improve some\nmeasure of interest. Towards this end, we consider the problem of decentralized\nmulti-agent navigation in cluttered environments. By introducing two\nsub-objectives of multi-agent navigation and environment optimization, we\npropose an $\\textit{agent-environment co-optimization}$ problem and develop a\n$\\textit{coordinated algorithm}$ that alternates between these sub-objectives\nto search for an optimal synthesis of agent actions and obstacle configurations\nin the environment; ultimately, improving the navigation performance. Due to\nthe challenge of explicitly modeling the relation between agents, environment\nand performance, we leverage policy gradient to formulate a model-free learning\nmechanism within the coordinated framework. A formal convergence analysis shows\nthat our coordinated algorithm tracks the local minimum trajectory of an\nassociated time-varying non-convex optimization problem. Extensive numerical\nresults corroborate theoretical findings and show the benefits of\nco-optimization over baselines. Interestingly, the results also indicate that\noptimized environment configurations are able to offer structural guidance that\nis key to de-conflicting agents in motion.\n","authors":["Zhan Gao","Guang Yang","Amanda Prorok"],"pdf_url":"https://arxiv.org/pdf/2403.14583v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14578v1","updated":"2024-03-21T17:30:59Z","published":"2024-03-21T17:30:59Z","title":"RAmBLA: A Framework for Evaluating the Reliability of LLMs as Assistants\n  in the Biomedical Domain","summary":"  Large Language Models (LLMs) increasingly support applications in a wide\nrange of domains, some with potential high societal impact such as biomedicine,\nyet their reliability in realistic use cases is under-researched. In this work\nwe introduce the Reliability AssesMent for Biomedical LLM Assistants (RAmBLA)\nframework and evaluate whether four state-of-the-art foundation LLMs can serve\nas reliable assistants in the biomedical domain. We identify prompt robustness,\nhigh recall, and a lack of hallucinations as necessary criteria for this use\ncase. We design shortform tasks and tasks requiring LLM freeform responses\nmimicking real-world user interactions. We evaluate LLM performance using\nsemantic similarity with a ground truth response, through an evaluator LLM.\n","authors":["William James Bolton","Rafael Poyiadzi","Edward R. Morrell","Gabriela van Bergen Gonzalez Bueno","Lea Goetz"],"pdf_url":"https://arxiv.org/pdf/2403.14578v1.pdf","comment":"Published at ICLR 2024 Workshop on Reliable and Responsible\n  Foundation Models"},{"id":"http://arxiv.org/abs/2401.12258v5","updated":"2024-03-21T17:29:37Z","published":"2024-01-21T16:59:45Z","title":"Emergent Dominance Hierarchies in Reinforcement Learning Agents","summary":"  Modern Reinforcement Learning (RL) algorithms are able to outperform humans\nin a wide variety of tasks. Multi-agent reinforcement learning (MARL) settings\npresent additional challenges, and successful cooperation in mixed-motive\ngroups of agents depends on a delicate balancing act between individual and\ngroup objectives. Social conventions and norms, often inspired by human\ninstitutions, are used as tools for striking this balance.\n  In this paper, we examine a fundamental, well-studied social convention that\nunderlies cooperation in both animal and human societies: dominance\nhierarchies.\n  We adapt the ethological theory of dominance hierarchies to artificial\nagents, borrowing the established terminology and definitions with as few\namendments as possible. We demonstrate that populations of RL agents, operating\nwithout explicit programming or intrinsic rewards, can invent, learn, enforce,\nand transmit a dominance hierarchy to new populations. The dominance\nhierarchies that emerge have a similar structure to those studied in chickens,\nmice, fish, and other species.\n","authors":["Ram Rachum","Yonatan Nakar","Bill Tomlinson","Nitay Alon","Reuth Mirsky"],"pdf_url":"https://arxiv.org/pdf/2401.12258v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14566v1","updated":"2024-03-21T17:09:20Z","published":"2024-03-21T17:09:20Z","title":"A survey on Concept-based Approaches For Model Improvement","summary":"  The focus of recent research has shifted from merely increasing the Deep\nNeural Networks (DNNs) performance in various tasks to DNNs, which are more\ninterpretable to humans. The field of eXplainable Artificial Intelligence (XAI)\nhas observed various techniques, including saliency-based and concept-based\napproaches. Concept-based approaches explain the model's decisions in simple\nhuman understandable terms called Concepts. Concepts are human interpretable\nunits of data and are the thinking ground of humans. Explanations in terms of\nconcepts enable detecting spurious correlations, inherent biases, or\nclever-hans. With the advent of concept-based explanations, there have been\nvarious concept representation methods and automatic concept discovery\nalgorithms. Some recent methods use concepts for post-hoc model disentanglement\nevaluation, while others use them for ante-hoc training. The concept-based\napproaches are new, with many representations coming up, and there is very\nlimited work on Concept-based Model improvement. We provide a systematic review\nand taxonomy of various concept representations and their discovery algorithms\nin DNNs, specifically in vision. We also provide details on concept-based model\nimprovement literature, which is the first to survey concept-based model\nimprovement methods.\n","authors":["Avani Gupta","P J Narayanan"],"pdf_url":"https://arxiv.org/pdf/2403.14566v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06563v2","updated":"2024-03-21T17:08:43Z","published":"2024-03-11T10:05:29Z","title":"Unraveling the Mystery of Scaling Laws: Part I","summary":"  Scaling law principles indicate a power-law correlation between loss and\nvariables such as model size, dataset size, and computational resources\nutilized during training. These principles play a vital role in optimizing\nvarious aspects of model pre-training, ultimately contributing to the success\nof large language models such as GPT-4, Llama and Gemini. However, the original\nscaling law paper by OpenAI did not disclose the complete details necessary to\nderive the precise scaling law formulas, and their conclusions are only based\non models containing up to 1.5 billion parameters. Though some subsequent works\nattempt to unveil these details and scale to larger models, they often neglect\nthe training dependency of important factors such as the learning rate, context\nlength and batch size, leading to their failure to establish a reliable formula\nfor predicting the test loss trajectory. In this technical report, we confirm\nthat the scaling law formulations proposed in the original OpenAI paper remain\nvalid when scaling the model size up to 33 billion, but the constant\ncoefficients in these formulas vary significantly with the experiment setup. We\nmeticulously identify influential factors and provide transparent, step-by-step\ninstructions to estimate all constant terms in scaling-law formulas by training\non models with only 1M~60M parameters. Using these estimated formulas, we\nshowcase the capability to accurately predict various attributes for models\nwith up to 33B parameters before their training, including (1) the minimum\npossible test loss; (2) the minimum required training steps and processed\ntokens to achieve a specific loss; (3) the critical batch size with an optimal\ntime/computation trade-off at any loss value; and (4) the complete test loss\ntrajectory with arbitrary batch size.\n","authors":["Hui Su","Zhi Tian","Xiaoyu Shen","Xunliang Cai"],"pdf_url":"https://arxiv.org/pdf/2403.06563v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06860v2","updated":"2024-03-21T17:06:49Z","published":"2024-03-11T16:13:58Z","title":"A Geospatial Approach to Predicting Desert Locust Breeding Grounds in\n  Africa","summary":"  Desert locust swarms present a major threat to agriculture and food security.\nAddressing this challenge, our study develops an operationally-ready model for\npredicting locust breeding grounds, which has the potential to enhance early\nwarning systems and targeted control measures. We curated a dataset from the\nUnited Nations Food and Agriculture Organization's (UN-FAO) locust observation\nrecords and analyzed it using two types of spatio-temporal input features:\nremotely-sensed environmental and climate data as well as multi-spectral earth\nobservation images. Our approach employed custom deep learning models\n(three-dimensional and LSTM-based recurrent convolutional networks), along with\nthe geospatial foundational model Prithvi recently released by Jakubik et al.,\n2023. These models notably outperformed existing baselines, with the\nPrithvi-based model, fine-tuned on multi-spectral images from NASA's Harmonized\nLandsat and Sentinel-2 (HLS) dataset, achieving the highest accuracy, F1 and\nROC-AUC scores (83.03%, 81.53% and 87.69%, respectively). A significant finding\nfrom our research is that multi-spectral earth observation images alone are\nsufficient for effective locust breeding ground prediction without the need to\nexplicitly incorporate climatic or environmental features.\n","authors":["Ibrahim Salihu Yusuf","Mukhtar Opeyemi Yusuf","Kobby Panford-Quainoo","Arnu Pretorius"],"pdf_url":"https://arxiv.org/pdf/2403.06860v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14551v1","updated":"2024-03-21T16:52:01Z","published":"2024-03-21T16:52:01Z","title":"Lexicon-Level Contrastive Visual-Grounding Improves Language Modeling","summary":"  Today's most accurate language models are trained on orders of magnitude more\nlanguage data than human language learners receive - but with no supervision\nfrom other sensory modalities that play a crucial role in human learning. Can\nwe make LMs' representations and predictions more accurate (and more\nhuman-like) with more ecologically plausible supervision? This paper describes\nLexiContrastive Grounding (LCG), a grounded language learning procedure that\nleverages visual supervision to improve textual representations.\nLexiContrastive Grounding combines a next token prediction strategy with a\ncontrastive visual grounding objective, focusing on early-layer representations\nthat encode lexical information. Across multiple word-learning and\nsentence-understanding benchmarks, LexiContrastive Grounding not only\noutperforms standard language-only models in learning efficiency, but also\nimproves upon vision-and-language learning procedures including CLIP, GIT,\nFlamingo, and Vokenization. Moreover, LexiContrastive Grounding improves\nperplexity by around 5% on multiple language modeling tasks. This work\nunderscores the potential of incorporating visual grounding into language\nmodels, aligning more closely with the multimodal nature of human language\nacquisition.\n","authors":["Chengxu Zhuang","Evelina Fedorenko","Jacob Andreas"],"pdf_url":"https://arxiv.org/pdf/2403.14551v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.03849v2","updated":"2024-03-21T16:49:20Z","published":"2024-03-06T16:49:33Z","title":"MedMamba: Vision Mamba for Medical Image Classification","summary":"  Medical image classification is a very fundamental and crucial task in the\nfield of computer vision. These years, CNN-based and Transformer-based models\nhave been widely used to classify various medical images. Unfortunately, The\nlimitation of CNNs in long-range modeling capabilities prevents them from\neffectively extracting features in medical images, while Transformers are\nhampered by their quadratic computational complexity. Recent research has shown\nthat the state space model (SSM) represented by Mamba can efficiently model\nlong-range interactions while maintaining linear computational complexity.\nInspired by this, we propose Vision Mamba for medical image classification\n(MedMamba). More specifically, we introduce a novel Conv-SSM module. Conv-SSM\ncombines the local feature extraction ability of convolutional layers with the\nability of SSM to capture long-range dependency, thereby modeling medical\nimages with different modalities. To demonstrate the potential of MedMamba, we\nconducted extensive experiments using 14 publicly available medical datasets\nwith different imaging techniques and two private datasets built by ourselves.\nExtensive experimental results demonstrate that the proposed MedMamba performs\nwell in detecting lesions in various medical images. To the best of our\nknowledge, this is the first Vision Mamba tailored for medical image\nclassification. The purpose of this work is to establish a new baseline for\nmedical image classification tasks and provide valuable insights for the future\ndevelopment of more efficient and effective SSM-based artificial intelligence\nalgorithms and application systems in the medical. Source code has been\navailable at https://github.com/YubiaoYue/MedMamba.\n","authors":["Yubiao Yue","Zhenzhang Li"],"pdf_url":"https://arxiv.org/pdf/2403.03849v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14547v1","updated":"2024-03-21T16:48:45Z","published":"2024-03-21T16:48:45Z","title":"Estimating Physical Information Consistency of Channel Data Augmentation\n  for Remote Sensing Images","summary":"  The application of data augmentation for deep learning (DL) methods plays an\nimportant role in achieving state-of-the-art results in supervised,\nsemi-supervised, and self-supervised image classification. In particular,\nchannel transformations (e.g., solarize, grayscale, brightness adjustments) are\nintegrated into data augmentation pipelines for remote sensing (RS) image\nclassification tasks. However, contradicting beliefs exist about their proper\napplications to RS images. A common point of critique is that the application\nof channel augmentation techniques may lead to physically inconsistent spectral\ndata (i.e., pixel signatures). To shed light on the open debate, we propose an\napproach to estimate whether a channel augmentation technique affects the\nphysical information of RS images. To this end, the proposed approach estimates\na score that measures the alignment of a pixel signature within a time series\nthat can be naturally subject to deviations caused by factors such as\nacquisition conditions or phenological states of vegetation. We compare the\nscores associated with original and augmented pixel signatures to evaluate the\nphysical consistency. Experimental results on a multi-label image\nclassification task show that channel augmentations yielding a score that\nexceeds the expected deviation of original pixel signatures can not improve the\nperformance of a baseline model trained without augmentation.\n","authors":["Tom Burgert","Begüm Demir"],"pdf_url":"https://arxiv.org/pdf/2403.14547v1.pdf","comment":"Accepted at the IEEE International Geoscience and Remote Sensing\n  Symposium"},{"id":"http://arxiv.org/abs/2403.14539v1","updated":"2024-03-21T16:40:10Z","published":"2024-03-21T16:40:10Z","title":"Object-Centric Domain Randomization for 3D Shape Reconstruction in the\n  Wild","summary":"  One of the biggest challenges in single-view 3D shape reconstruction in the\nwild is the scarcity of <3D shape, 2D image>-paired data from real-world\nenvironments. Inspired by remarkable achievements via domain randomization, we\npropose ObjectDR which synthesizes such paired data via a random simulation of\nvisual variations in object appearances and backgrounds. Our data synthesis\nframework exploits a conditional generative model (e.g., ControlNet) to\ngenerate images conforming to spatial conditions such as 2.5D sketches, which\nare obtainable through a rendering process of 3D shapes from object collections\n(e.g., Objaverse-XL). To simulate diverse variations while preserving object\nsilhouettes embedded in spatial conditions, we also introduce a disentangled\nframework which leverages an initial object guidance. After synthesizing a wide\nrange of data, we pre-train a model on them so that it learns to capture a\ndomain-invariant geometry prior which is consistent across various domains. We\nvalidate its effectiveness by substantially improving 3D shape reconstruction\nmodels on a real-world benchmark. In a scale-up evaluation, our pre-training\nachieves 23.6% superior results compared with the pre-training on high-quality\ncomputer graphics renderings.\n","authors":["Junhyeong Cho","Kim Youwang","Hunmin Yang","Tae-Hyun Oh"],"pdf_url":"https://arxiv.org/pdf/2403.14539v1.pdf","comment":"Project Page: https://ObjectDR.github.io"},{"id":"http://arxiv.org/abs/2312.09234v3","updated":"2024-03-21T16:26:09Z","published":"2023-12-14T18:57:16Z","title":"Let's do the time-warp-attend: Learning topological invariants of\n  dynamical systems","summary":"  Dynamical systems across the sciences, from electrical circuits to ecological\nnetworks, undergo qualitative and often catastrophic changes in behavior,\ncalled bifurcations, when their underlying parameters cross a threshold.\nExisting methods predict oncoming catastrophes in individual systems but are\nprimarily time-series-based and struggle both to categorize qualitative\ndynamical regimes across diverse systems and to generalize to real data. To\naddress this challenge, we propose a data-driven, physically-informed\ndeep-learning framework for classifying dynamical regimes and characterizing\nbifurcation boundaries based on the extraction of topologically invariant\nfeatures. We focus on the paradigmatic case of the supercritical Hopf\nbifurcation, which is used to model periodic dynamics across a wide range of\napplications. Our convolutional attention method is trained with data\naugmentations that encourage the learning of topological invariants which can\nbe used to detect bifurcation boundaries in unseen systems and to design models\nof biological systems like oscillatory gene regulatory networks. We further\ndemonstrate our method's use in analyzing real data by recovering distinct\nproliferation and differentiation dynamics along pancreatic endocrinogenesis\ntrajectory in gene expression space based on single-cell data. Our method\nprovides valuable insights into the qualitative, long-term behavior of a wide\nrange of dynamical systems, and can detect bifurcations or catastrophic\ntransitions in large-scale physical and biological systems.\n","authors":["Noa Moriel","Matthew Ricci","Mor Nitzan"],"pdf_url":"https://arxiv.org/pdf/2312.09234v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.03049v2","updated":"2024-03-21T16:21:45Z","published":"2023-10-04T02:18:28Z","title":"QuATON: Quantization Aware Training of Optical Neurons","summary":"  Optical processors, built with \"optical neurons\", can efficiently perform\nhigh-dimensional linear operations at the speed of light. Thus they are a\npromising avenue to accelerate large-scale linear computations. With the\ncurrent advances in micro-fabrication, such optical processors can now be 3D\nfabricated, but with a limited precision. This limitation translates to\nquantization of learnable parameters in optical neurons, and should be handled\nduring the design of the optical processor in order to avoid a model mismatch.\nSpecifically, optical neurons should be trained or designed within the\nphysical-constraints at a predefined quantized precision level. To address this\ncritical issues we propose a physics-informed quantization-aware training\nframework. Our approach accounts for physical constraints during the training\nprocess, leading to robust designs. We demonstrate that our approach can design\nstate of the art optical processors using diffractive networks for multiple\nphysics based tasks despite quantized learnable parameters. We thus lay the\nfoundation upon which improved optical processors may be 3D fabricated in the\nfuture.\n","authors":["Hasindu Kariyawasam","Ramith Hettiarachchi","Quansan Yang","Alex Matlock","Takahiro Nambara","Hiroyuki Kusaka","Yuichiro Kunai","Peter T C So","Edward S Boyden","Dushan Wadduwage"],"pdf_url":"https://arxiv.org/pdf/2310.03049v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.16584v3","updated":"2024-03-21T16:21:23Z","published":"2023-09-28T16:44:18Z","title":"Collaborative Distributed Machine Learning","summary":"  Various collaborative distributed machine learning (CDML) systems, including\nfederated learning systems and swarm learning systems, with different key\ntraits were developed to leverage resources for development and use of machine\nlearning (ML) models in a confidentiality-preserving way. To meet use case\nrequirements, suitable CDML systems need to be selected. However, comparison\nbetween CDML systems regarding their suitability for use cases is often\ndifficult. This work presents a CDML system conceptualization and CDML\narchetypes to support comparison of CDML systems and introduce scientific and\npractical audiences to the principal functioning and key traits of CDML\nsystems.\n","authors":["David Jin","Niclas Kannengießer","Sascha Rank","Ali Sunyaev"],"pdf_url":"https://arxiv.org/pdf/2309.16584v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.11287v2","updated":"2024-03-21T16:11:17Z","published":"2023-10-17T14:09:45Z","title":"Assessing the Causal Impact of Humanitarian Aid on Food Security","summary":"  In the face of climate change-induced droughts, vulnerable regions encounter\nsevere threats to food security, demanding urgent humanitarian assistance. This\npaper introduces a causal inference framework for the Horn of Africa, aiming to\nassess the impact of cash-based interventions on food crises. Our contributions\ninclude identifying causal relationships within the food security system,\nharmonizing a comprehensive database including socio-economic, weather and\nremote sensing data, and estimating the causal effect of humanitarian\ninterventions on malnutrition. On a country level, our results revealed no\nsignificant effects, likely due to limited sample size, suboptimal data\nquality, and an imperfect causal graph resulting from our limited understanding\nof multidisciplinary systems like food security. Instead, on a district level,\nresults revealed significant effects, further implying the context-specific\nnature of the system. This underscores the need to enhance data collection and\nrefine causal models with domain experts for more effective future\ninterventions and policies, improving transparency and accountability in\nhumanitarian aid.\n","authors":["Jordi Cerdà-Bautista","José María Tárraga","Vasileios Sitokonstantinou","Gustau Camps-Valls"],"pdf_url":"https://arxiv.org/pdf/2310.11287v2.pdf","comment":"Accepted for publication and presentation at the International\n  Geoscience and Remote Sensing Symposium (IGARSS) 2024"},{"id":"http://arxiv.org/abs/2403.14514v1","updated":"2024-03-21T16:10:42Z","published":"2024-03-21T16:10:42Z","title":"Machine-learning invariant foliations in forced systems for reduced\n  order modelling","summary":"  We identify reduced order models (ROM) of forced systems from data using\ninvariant foliations. The forcing can be external, parametric, periodic or\nquasi-periodic. The process has four steps: 1. identify an approximate\ninvariant torus and the linear dynamics about the torus; 2. identify a globally\ndefined invariant foliation about the torus; 3. identify a local foliation\nabout an invariant manifold that complements the global foliation 4. extract\nthe invariant manifold as the leaf going through the torus and interpret the\nresult. We combine steps 2 and 3, so that we can track the location of the\ninvariant torus and scale the invariance equations appropriately. We highlight\nsome fundamental limitations of invariant manifolds and foliations when fitting\nthem to data, that require further mathematics to resolve.\n","authors":["Robert Szalai"],"pdf_url":"https://arxiv.org/pdf/2403.14514v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12157v2","updated":"2024-03-21T16:09:57Z","published":"2023-03-21T19:34:20Z","title":"Learning a Depth Covariance Function","summary":"  We propose learning a depth covariance function with applications to\ngeometric vision tasks. Given RGB images as input, the covariance function can\nbe flexibly used to define priors over depth functions, predictive\ndistributions given observations, and methods for active point selection. We\nleverage these techniques for a selection of downstream tasks: depth\ncompletion, bundle adjustment, and monocular dense visual odometry.\n","authors":["Eric Dexheimer","Andrew J. Davison"],"pdf_url":"https://arxiv.org/pdf/2303.12157v2.pdf","comment":"CVPR 2023. Project page: https://edexheim.github.io/DepthCov/"},{"id":"http://arxiv.org/abs/2403.14508v1","updated":"2024-03-21T16:02:52Z","published":"2024-03-21T16:02:52Z","title":"Constrained Reinforcement Learning with Smoothed Log Barrier Function","summary":"  Reinforcement Learning (RL) has been widely applied to many control tasks and\nsubstantially improved the performances compared to conventional control\nmethods in many domains where the reward function is well defined. However, for\nmany real-world problems, it is often more convenient to formulate optimization\nproblems in terms of rewards and constraints simultaneously. Optimizing such\nconstrained problems via reward shaping can be difficult as it requires tedious\nmanual tuning of reward functions with several interacting terms. Recent\nformulations which include constraints mostly require a pre-training phase,\nwhich often needs human expertise to collect data or assumes having a\nsub-optimal policy readily available. We propose a new constrained RL method\ncalled CSAC-LB (Constrained Soft Actor-Critic with Log Barrier Function), which\nachieves competitive performance without any pre-training by applying a linear\nsmoothed log barrier function to an additional safety critic. It implements an\nadaptive penalty for policy learning and alleviates the numerical issues that\nare known to complicate the application of the log barrier function method. As\na result, we show that with CSAC-LB, we achieve state-of-the-art performance on\nseveral constrained control tasks with different levels of difficulty and\nevaluate our methods in a locomotion task on a real quadruped robot platform.\n","authors":["Baohe Zhang","Yuan Zhang","Lilli Frison","Thomas Brox","Joschka Bödecker"],"pdf_url":"https://arxiv.org/pdf/2403.14508v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.01181v2","updated":"2024-03-21T15:57:29Z","published":"2023-06-01T22:29:28Z","title":"TMI! Finetuned Models Leak Private Information from their Pretraining\n  Data","summary":"  Transfer learning has become an increasingly popular technique in machine\nlearning as a way to leverage a pretrained model trained for one task to assist\nwith building a finetuned model for a related task. This paradigm has been\nespecially popular for $\\textit{privacy}$ in machine learning, where the\npretrained model is considered public, and only the data for finetuning is\nconsidered sensitive. However, there are reasons to believe that the data used\nfor pretraining is still sensitive, making it essential to understand how much\ninformation the finetuned model leaks about the pretraining data. In this work\nwe propose a new membership-inference threat model where the adversary only has\naccess to the finetuned model and would like to infer the membership of the\npretraining data. To realize this threat model, we implement a novel\nmetaclassifier-based attack, $\\textbf{TMI}$, that leverages the influence of\nmemorized pretraining samples on predictions in the downstream task. We\nevaluate $\\textbf{TMI}$ on both vision and natural language tasks across\nmultiple transfer learning settings, including finetuning with differential\nprivacy. Through our evaluation, we find that $\\textbf{TMI}$ can successfully\ninfer membership of pretraining examples using query access to the finetuned\nmodel. An open-source implementation of $\\textbf{TMI}$ can be found\n$\\href{https://github.com/johnmath/tmi-pets24}{\\text{on GitHub}}$.\n","authors":["John Abascal","Stanley Wu","Alina Oprea","Jonathan Ullman"],"pdf_url":"https://arxiv.org/pdf/2306.01181v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14504v1","updated":"2024-03-21T15:56:15Z","published":"2024-03-21T15:56:15Z","title":"Soft Learning Probabilistic Circuits","summary":"  Probabilistic Circuits (PCs) are prominent tractable probabilistic models,\nallowing for a range of exact inferences. This paper focuses on the main\nalgorithm for training PCs, LearnSPN, a gold standard due to its efficiency,\nperformance, and ease of use, in particular for tabular data. We show that\nLearnSPN is a greedy likelihood maximizer under mild assumptions. While\ninferences in PCs may use the entire circuit structure for processing queries,\nLearnSPN applies a hard method for learning them, propagating at each sum node\na data point through one and only one of the children/edges as in a hard\nclustering process. We propose a new learning procedure named SoftLearn, that\ninduces a PC using a soft clustering process. We investigate the effect of this\nlearning-inference compatibility in PCs. Our experiments show that SoftLearn\noutperforms LearnSPN in many situations, yielding better likelihoods and\narguably better samples. We also analyze comparable tractable models to\nhighlight the differences between soft/hard learning and model querying.\n","authors":["Soroush Ghandi","Benjamin Quost","Cassio de Campos"],"pdf_url":"https://arxiv.org/pdf/2403.14504v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10132v2","updated":"2024-03-21T15:42:06Z","published":"2023-12-15T17:02:19Z","title":"Closing the Gap: Achieving Better Accuracy-Robustness Tradeoffs against\n  Query-Based Attacks","summary":"  Although promising, existing defenses against query-based attacks share a\ncommon limitation: they offer increased robustness against attacks at the price\nof a considerable accuracy drop on clean samples. In this work, we show how to\nefficiently establish, at test-time, a solid tradeoff between robustness and\naccuracy when mitigating query-based attacks. Given that these attacks\nnecessarily explore low-confidence regions, our insight is that activating\ndedicated defenses, such as random noise defense and random image\ntransformations, only for low-confidence inputs is sufficient to prevent them.\nOur approach is independent of training and supported by theory. We verify the\neffectiveness of our approach for various existing defenses by conducting\nextensive experiments on CIFAR-10, CIFAR-100, and ImageNet. Our results confirm\nthat our proposal can indeed enhance these defenses by providing better\ntradeoffs between robustness and accuracy when compared to state-of-the-art\napproaches while being completely training-free.\n","authors":["Pascal Zimmer","Sébastien Andreina","Giorgia Azzurra Marson","Ghassan Karame"],"pdf_url":"https://arxiv.org/pdf/2312.10132v2.pdf","comment":"To appear in the Proceedings of the AAAI Conference on Artificial\n  Intelligence (AAAI) 2024"},{"id":"http://arxiv.org/abs/2403.14488v1","updated":"2024-03-21T15:36:26Z","published":"2024-03-21T15:36:26Z","title":"Physics-Based Causal Reasoning for Safe & Robust Next-Best Action\n  Selection in Robot Manipulation Tasks","summary":"  Safe and efficient object manipulation is a key enabler of many real-world\nrobot applications. However, this is challenging because robot operation must\nbe robust to a range of sensor and actuator uncertainties. In this paper, we\npresent a physics-informed causal-inference-based framework for a robot to\nprobabilistically reason about candidate actions in a block stacking task in a\npartially observable setting. We integrate a physics-based simulation of the\nrigid-body system dynamics with a causal Bayesian network (CBN) formulation to\ndefine a causal generative probabilistic model of the robot decision-making\nprocess. Using simulation-based Monte Carlo experiments, we demonstrate our\nframework's ability to successfully: (1) predict block tower stability with\nhigh accuracy (Pred Acc: 88.6%); and, (2) select an approximate next-best\naction for the block stacking task, for execution by an integrated robot\nsystem, achieving 94.2% task success rate. We also demonstrate our framework's\nsuitability for real-world robot systems by demonstrating successful task\nexecutions with a domestic support robot, with perception and manipulation\nsub-system integration. Hence, we show that by embedding physics-based causal\nreasoning into robots' decision-making processes, we can make robot task\nexecution safer, more reliable, and more robust to various types of\nuncertainty.\n","authors":["Ricardo Cannizzaro","Michael Groom","Jonathan Routley","Robert Osazuwa Ness","Lars Kunze"],"pdf_url":"https://arxiv.org/pdf/2403.14488v1.pdf","comment":"8 pages, 9 figures, submitted to 2024 IEEE/RSJ International\n  Conference on Intelligent Robots and Systems (IROS)"},{"id":"http://arxiv.org/abs/2402.03049v3","updated":"2024-03-21T15:33:34Z","published":"2024-02-05T14:33:56Z","title":"EasyInstruct: An Easy-to-use Instruction Processing Framework for Large\n  Language Models","summary":"  In recent years, instruction tuning has gained increasing attention and\nemerged as a crucial technique to enhance the capabilities of Large Language\nModels (LLMs). To construct high-quality instruction datasets, many instruction\nprocessing approaches have been proposed, aiming to achieve a delicate balance\nbetween data quantity and data quality. Nevertheless, due to inconsistencies\nthat persist among various instruction processing methods, there is no standard\nopen-source instruction processing implementation framework available for the\ncommunity, which hinders practitioners from further developing and advancing.\nTo facilitate instruction processing research and development, we present\nEasyInstruct, an easy-to-use instruction processing framework for LLMs, which\nmodularizes instruction generation, selection, and prompting, while also\nconsidering their combination and interaction. EasyInstruct is publicly\nreleased and actively maintained at https://github.com/zjunlp/EasyInstruct,\nalong with an online demo app and a demo video for quick-start, calling for\nbroader research centered on instruction data and synthetic data.\n","authors":["Yixin Ou","Ningyu Zhang","Honghao Gui","Ziwen Xu","Shuofei Qiao","Yida Xue","Runnan Fang","Kangwei Liu","Lei Li","Zhen Bi","Guozhou Zheng","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2402.03049v3.pdf","comment":"Project website: https://zjunlp.github.io/project/EasyInstruct Code:\n  https://github.com/zjunlp/EasyInstruct Video: https://youtu.be/rfQOWYfziFo\n  Demo: https://huggingface.co/spaces/zjunlp/EasyInstruct"},{"id":"http://arxiv.org/abs/2403.14484v1","updated":"2024-03-21T15:31:28Z","published":"2024-03-21T15:31:28Z","title":"HyperGALE: ASD Classification via Hypergraph Gated Attention with\n  Learnable Hyperedges","summary":"  Autism Spectrum Disorder (ASD) is a neurodevelopmental condition\ncharacterized by varied social cognitive challenges and repetitive behavioral\npatterns. Identifying reliable brain imaging-based biomarkers for ASD has been\na persistent challenge due to the spectrum's diverse symptomatology. Existing\nbaselines in the field have made significant strides in this direction, yet\nthere remains room for improvement in both performance and interpretability. We\npropose \\emph{HyperGALE}, which builds upon the hypergraph by incorporating\nlearned hyperedges and gated attention mechanisms. This approach has led to\nsubstantial improvements in the model's ability to interpret complex brain\ngraph data, offering deeper insights into ASD biomarker characterization.\nEvaluated on the extensive ABIDE II dataset, \\emph{HyperGALE} not only improves\ninterpretability but also demonstrates statistically significant enhancements\nin key performance metrics compared to both previous baselines and the\nfoundational hypergraph model. The advancement \\emph{HyperGALE} brings to ASD\nresearch highlights the potential of sophisticated graph-based techniques in\nneurodevelopmental studies. The source code and implementation instructions are\navailable at GitHub:https://github.com/mehular0ra/HyperGALE.\n","authors":["Mehul Arora","Chirag Shantilal Jain","Lalith Bharadwaj Baru","Kamalaker Dadi","Bapi Raju Surampudi"],"pdf_url":"https://arxiv.org/pdf/2403.14484v1.pdf","comment":"Accepted to IJCNN 2024"},{"id":"http://arxiv.org/abs/2403.14483v1","updated":"2024-03-21T15:29:24Z","published":"2024-03-21T15:29:24Z","title":"Utilizing the LightGBM Algorithm for Operator User Credit Assessment\n  Research","summary":"  Mobile Internet user credit assessment is an important way for communication\noperators to establish decisions and formulate measures, and it is also a\nguarantee for operators to obtain expected benefits. However, credit evaluation\nmethods have long been monopolized by financial industries such as banks and\ncredit. As supporters and providers of platform network technology and network\nresources, communication operators are also builders and maintainers of\ncommunication networks. Internet data improves the user's credit evaluation\nstrategy. This paper uses the massive data provided by communication operators\nto carry out research on the operator's user credit evaluation model based on\nthe fusion LightGBM algorithm. First, for the massive data related to user\nevaluation provided by operators, key features are extracted by data\npreprocessing and feature engineering methods, and a multi-dimensional feature\nset with statistical significance is constructed; then, linear regression,\ndecision tree, LightGBM, and other machine learning algorithms build multiple\nbasic models to find the best basic model; finally, integrates Averaging,\nVoting, Blending, Stacking and other integrated algorithms to refine multiple\nfusion models, and finally establish the most suitable fusion model for\noperator user evaluation.\n","authors":["Shaojie Li","Xinqi Dong","Danqing Ma","Bo Dang","Hengyi Zang","Yulu Gong"],"pdf_url":"https://arxiv.org/pdf/2403.14483v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14472v1","updated":"2024-03-21T15:18:30Z","published":"2024-03-21T15:18:30Z","title":"Detoxifying Large Language Models via Knowledge Editing","summary":"  This paper investigates using knowledge editing techniques to detoxify Large\nLanguage Models (LLMs). We construct a benchmark, SafeEdit, which covers nine\nunsafe categories with various powerful attack prompts and equips comprehensive\nmetrics for systematic evaluation. We conduct experiments to compare knowledge\nediting approaches with previous baselines, indicating that knowledge editing\nhas the potential to efficiently detoxify LLMs with limited impact on general\nperformance. Then, we propose a simple yet effective baseline, dubbed\nDetoxifying with Intraoperative Neural Monitoring (DINM), to diminish the\ntoxicity of LLMs within a few tuning steps via only one instance. We further\nprovide an in-depth analysis of the internal mechanism for various detoxify\napproaches, demonstrating that previous methods like SFT and DPO may merely\nsuppress the activations of toxic parameters, while DINM mitigates the toxicity\nof the toxic parameters to a certain extent, making permanent adjustments. We\nhope that these insights could shed light on future work of developing\ndetoxifying approaches and the underlying knowledge mechanisms of LLMs. Code\nand benchmark are available at https://github.com/zjunlp/EasyEdit.\n","authors":["Mengru Wang","Ningyu Zhang","Ziwen Xu","Zekun Xi","Shumin Deng","Yunzhi Yao","Qishen Zhang","Linyi Yang","Jindong Wang","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2403.14472v1.pdf","comment":"Ongoing work. Project website:\n  https://zjunlp.github.io/project/SafeEdit Benchmark:\n  https://huggingface.co/datasets/zjunlp/SafeEdit Code:\n  https://github.com/zjunlp/EasyEdit"},{"id":"http://arxiv.org/abs/2403.14466v1","updated":"2024-03-21T15:13:54Z","published":"2024-03-21T15:13:54Z","title":"Universal Feature Selection for Simultaneous Interpretability of\n  Multitask Datasets","summary":"  Extracting meaningful features from complex, high-dimensional datasets across\nscientific domains remains challenging. Current methods often struggle with\nscalability, limiting their applicability to large datasets, or make\nrestrictive assumptions about feature-property relationships, hindering their\nability to capture complex interactions. BoUTS's general and scalable feature\nselection algorithm surpasses these limitations to identify both universal\nfeatures relevant to all datasets and task-specific features predictive for\nspecific subsets. Evaluated on seven diverse chemical regression datasets,\nBoUTS achieves state-of-the-art feature sparsity while maintaining prediction\naccuracy comparable to specialized methods. Notably, BoUTS's universal features\nenable domain-specific knowledge transfer between datasets, and suggest deep\nconnections in seemingly-disparate chemical datasets. We expect these results\nto have important repercussions in manually-guided inverse problems. Beyond its\ncurrent application, BoUTS holds immense potential for elucidating data-poor\nsystems by leveraging information from similar data-rich systems. BoUTS\nrepresents a significant leap in cross-domain feature selection, potentially\nleading to advancements in various scientific fields.\n","authors":["Matt Raymond","Jacob Charles Saldinger","Paolo Elvati","Clayton Scott","Angela Violi"],"pdf_url":"https://arxiv.org/pdf/2403.14466v1.pdf","comment":"Main text: 14 pages, 3 figures, 1 table; SI: 7 pages, 1 figure, 4\n  tables, 3 algorithms"},{"id":"http://arxiv.org/abs/2403.14457v1","updated":"2024-03-21T15:04:32Z","published":"2024-03-21T15:04:32Z","title":"gTBLS: Generating Tables from Text by Conditional Question Answering","summary":"  Distilling large, unstructured text into a structured, condensed form such as\ntables is an open research problem. One of the primary challenges in\nautomatically generating tables is ensuring their syntactic validity. Prior\napproaches address this challenge by including additional parameters in the\nTransformer's attention mechanism to attend to specific rows and column\nheaders. In contrast to this single-stage method, this paper presents a\ntwo-stage approach called Generative Tables (gTBLS). The first stage infers\ntable structure (row and column headers) from the text. The second stage\nformulates questions using these headers and fine-tunes a causal language model\nto answer them. Furthermore, the gTBLS approach is amenable to the utilization\nof pre-trained Large Language Models in a zero-shot configuration, presenting a\nsolution for table generation in situations where fine-tuning is not feasible.\ngTBLS improves prior approaches by up to 10% in BERTScore on the table\nconstruction task and up to 20% on the table content generation task of the\nE2E, WikiTableText, WikiBio, and RotoWire datasets.\n","authors":["Anirudh Sundar","Christopher Richardson","Larry Heck"],"pdf_url":"https://arxiv.org/pdf/2403.14457v1.pdf","comment":"12 pages, 1 figure"},{"id":"http://arxiv.org/abs/2403.14443v1","updated":"2024-03-21T14:48:37Z","published":"2024-03-21T14:48:37Z","title":"Language Models Can Reduce Asymmetry in Information Markets","summary":"  This work addresses the buyer's inspection paradox for information markets.\nThe paradox is that buyers need to access information to determine its value,\nwhile sellers need to limit access to prevent theft. To study this, we\nintroduce an open-source simulated digital marketplace where intelligent\nagents, powered by language models, buy and sell information on behalf of\nexternal participants. The central mechanism enabling this marketplace is the\nagents' dual capabilities: they not only have the capacity to assess the\nquality of privileged information but also come equipped with the ability to\nforget. This ability to induce amnesia allows vendors to grant temporary access\nto proprietary information, significantly reducing the risk of unauthorized\nretention while enabling agents to accurately gauge the information's relevance\nto specific queries or tasks. To perform well, agents must make rational\ndecisions, strategically explore the marketplace through generated sub-queries,\nand synthesize answers from purchased information. Concretely, our experiments\n(a) uncover biases in language models leading to irrational behavior and\nevaluate techniques to mitigate these biases, (b) investigate how price affects\ndemand in the context of informational goods, and (c) show that inspection and\nhigher budgets both lead to higher quality outcomes.\n","authors":["Nasim Rahaman","Martin Weiss","Manuel Wüthrich","Yoshua Bengio","Li Erran Li","Chris Pal","Bernhard Schölkopf"],"pdf_url":"https://arxiv.org/pdf/2403.14443v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14440v1","updated":"2024-03-21T14:45:54Z","published":"2024-03-21T14:45:54Z","title":"Analysing Diffusion Segmentation for Medical Images","summary":"  Denoising Diffusion Probabilistic models have become increasingly popular due\nto their ability to offer probabilistic modeling and generate diverse outputs.\nThis versatility inspired their adaptation for image segmentation, where\nmultiple predictions of the model can produce segmentation results that not\nonly achieve high quality but also capture the uncertainty inherent in the\nmodel. Here, powerful architectures were proposed for improving diffusion\nsegmentation performance. However, there is a notable lack of analysis and\ndiscussions on the differences between diffusion segmentation and image\ngeneration, and thorough evaluations are missing that distinguish the\nimprovements these architectures provide for segmentation in general from their\nbenefit for diffusion segmentation specifically. In this work, we critically\nanalyse and discuss how diffusion segmentation for medical images differs from\ndiffusion image generation, with a particular focus on the training behavior.\nFurthermore, we conduct an assessment how proposed diffusion segmentation\narchitectures perform when trained directly for segmentation. Lastly, we\nexplore how different medical segmentation tasks influence the diffusion\nsegmentation behavior and the diffusion process could be adapted accordingly.\nWith these analyses, we aim to provide in-depth insights into the behavior of\ndiffusion segmentation that allow for a better design and evaluation of\ndiffusion segmentation methods in the future.\n","authors":["Mathias Öttl","Siyuan Mei","Frauke Wilm","Jana Steenpass","Matthias Rübner","Arndt Hartmann","Matthias Beckmann","Peter Fasching","Andreas Maier","Ramona Erber","Katharina Breininger"],"pdf_url":"https://arxiv.org/pdf/2403.14440v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14438v1","updated":"2024-03-21T14:44:03Z","published":"2024-03-21T14:44:03Z","title":"A Multimodal Approach to Device-Directed Speech Detection with Large\n  Language Models","summary":"  Interactions with virtual assistants typically start with a predefined\ntrigger phrase followed by the user command. To make interactions with the\nassistant more intuitive, we explore whether it is feasible to drop the\nrequirement that users must begin each command with a trigger phrase. We\nexplore this task in three ways: First, we train classifiers using only\nacoustic information obtained from the audio waveform. Second, we take the\ndecoder outputs of an automatic speech recognition (ASR) system, such as 1-best\nhypotheses, as input features to a large language model (LLM). Finally, we\nexplore a multimodal system that combines acoustic and lexical features, as\nwell as ASR decoder signals in an LLM. Using multimodal information yields\nrelative equal-error-rate improvements over text-only and audio-only models of\nup to 39% and 61%. Increasing the size of the LLM and training with low-rank\nadaption leads to further relative EER reductions of up to 18% on our dataset.\n","authors":["Dominik Wager","Alexander Churchill","Siddharth Sigtia","Panayiotis Georgiou","Matt Mirsamadi","Aarshee Mishra","Erik Marchi"],"pdf_url":"https://arxiv.org/pdf/2403.14438v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2312.03632"},{"id":"http://arxiv.org/abs/2403.14435v1","updated":"2024-03-21T14:41:58Z","published":"2024-03-21T14:41:58Z","title":"Biased Binary Attribute Classifiers Ignore the Majority Classes","summary":"  To visualize the regions of interest that classifiers base their decisions\non, different Class Activation Mapping (CAM) methods have been developed.\nHowever, all of these techniques target categorical classifiers only, though\nmost real-world tasks are binary classification. In this paper, we extend\ngradient-based CAM techniques to work with binary classifiers and visualize the\nactive regions for binary facial attribute classifiers. When training an\nunbalanced binary classifier on an imbalanced dataset, it is well-known that\nthe majority class, i.e. the class with many training samples, is mostly\npredicted much better than minority class with few training instances. In our\nexperiments on the CelebA dataset, we verify these results, when training an\nunbalanced classifier to extract 40 facial attributes simultaneously. One would\nexpect that the biased classifier has learned to extract features mainly for\nthe majority classes and that the proportional energy of the activations mainly\nreside in certain specific regions of the image where the attribute is located.\nHowever, we find very little regular activation for samples of majority\nclasses, while the active regions for minority classes seem mostly reasonable\nand overlap with our expectations. These results suggest that biased\nclassifiers mainly rely on bias activation for majority classes. When training\na balanced classifier on the imbalanced data by employing attribute-specific\nclass weights, majority and minority classes are classified similarly well and\nshow expected activations for almost all attributes\n","authors":["Xinyi Zhang","Johanna Sophie Bieri","Manuel Günther"],"pdf_url":"https://arxiv.org/pdf/2403.14435v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14429v1","updated":"2024-03-21T14:36:59Z","published":"2024-03-21T14:36:59Z","title":"Style-Extracting Diffusion Models for Semi-Supervised Histopathology\n  Segmentation","summary":"  Deep learning-based image generation has seen significant advancements with\ndiffusion models, notably improving the quality of generated images. Despite\nthese developments, generating images with unseen characteristics beneficial\nfor downstream tasks has received limited attention. To bridge this gap, we\npropose Style-Extracting Diffusion Models, featuring two conditioning\nmechanisms. Specifically, we utilize 1) a style conditioning mechanism which\nallows to inject style information of previously unseen images during image\ngeneration and 2) a content conditioning which can be targeted to a downstream\ntask, e.g., layout for segmentation. We introduce a trainable style encoder to\nextract style information from images, and an aggregation block that merges\nstyle information from multiple style inputs. This architecture enables the\ngeneration of images with unseen styles in a zero-shot manner, by leveraging\nstyles from unseen images, resulting in more diverse generations. In this work,\nwe use the image layout as target condition and first show the capability of\nour method on a natural image dataset as a proof-of-concept. We further\ndemonstrate its versatility in histopathology, where we combine prior knowledge\nabout tissue composition and unannotated data to create diverse synthetic\nimages with known layouts. This allows us to generate additional synthetic data\nto train a segmentation network in a semi-supervised fashion. We verify the\nadded value of the generated images by showing improved segmentation results\nand lower performance variability between patients when synthetic images are\nincluded during segmentation training. Our code will be made publicly available\nat [LINK].\n","authors":["Mathias Öttl","Frauke Wilm","Jana Steenpass","Jingna Qiu","Matthias Rübner","Arndt Hartmann","Matthias Beckmann","Peter Fasching","Andreas Maier","Ramona Erber","Bernhard Kainz","Katharina Breininger"],"pdf_url":"https://arxiv.org/pdf/2403.14429v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.12032v2","updated":"2024-03-21T14:36:26Z","published":"2023-10-18T15:16:24Z","title":"Exact and general decoupled solutions of the LMC Multitask Gaussian\n  Process model","summary":"  The Linear Model of Co-regionalization (LMC) is a very general model of\nmultitask gaussian process for regression or classification. While its\nexpressivity and conceptual simplicity are appealing, naive implementations\nhave cubic complexity in the number of datapoints and number of tasks, making\napproximations mandatory for most applications. However, recent work has shown\nthat under some conditions the latent processes of the model can be decoupled,\nleading to a complexity that is only linear in the number of said processes. We\nhere extend these results, showing from the most general assumptions that the\nonly condition necessary to an efficient exact computation of the LMC is a mild\nhypothesis on the noise model. We introduce a full parametrization of the\nresulting \\emph{projected LMC} model, and an expression of the marginal\nlikelihood enabling efficient optimization. We perform a parametric study on\nsynthetic data to show the excellent performance of our approach, compared to\nan unrestricted exact LMC and approximations of the latter. Overall, the\nprojected LMC appears as a credible and simpler alternative to state-of-the art\nmodels, which greatly facilitates some computations such as leave-one-out\ncross-validation and fantasization.\n","authors":["Olivier Truffinet","Karim Ammar","Jean-Philippe Argaud","Bertrand Bouriquet"],"pdf_url":"https://arxiv.org/pdf/2310.12032v2.pdf","comment":"29 pages, 10 figures, submitted to UAI"},{"id":"http://arxiv.org/abs/2403.14425v1","updated":"2024-03-21T14:28:43Z","published":"2024-03-21T14:28:43Z","title":"Task-optimal data-driven surrogate models for eNMPC via differentiable\n  simulation and optimization","summary":"  We present a method for end-to-end learning of Koopman surrogate models for\noptimal performance in control. In contrast to previous contributions that\nemploy standard reinforcement learning (RL) algorithms, we use a training\nalgorithm that exploits the potential differentiability of environments based\non mechanistic simulation models. We evaluate the performance of our method by\ncomparing it to that of other controller type and training algorithm\ncombinations on a literature known eNMPC case study. Our method exhibits\nsuperior performance on this problem, thereby constituting a promising avenue\ntowards more capable controllers that employ dynamic surrogate models.\n","authors":["Daniel Mayfrank","Na Young Ahn","Alexander Mitsos","Manuel Dahmen"],"pdf_url":"https://arxiv.org/pdf/2403.14425v1.pdf","comment":"6 pages, 4 figures, 1 table"},{"id":"http://arxiv.org/abs/2403.14421v1","updated":"2024-03-21T14:17:28Z","published":"2024-03-21T14:17:28Z","title":"DP-RDM: Adapting Diffusion Models to Private Domains Without Fine-Tuning","summary":"  Text-to-image diffusion models have been shown to suffer from sample-level\nmemorization, possibly reproducing near-perfect replica of images that they are\ntrained on, which may be undesirable. To remedy this issue, we develop the\nfirst differentially private (DP) retrieval-augmented generation algorithm that\nis capable of generating high-quality image samples while providing provable\nprivacy guarantees. Specifically, we assume access to a text-to-image diffusion\nmodel trained on a small amount of public data, and design a DP retrieval\nmechanism to augment the text prompt with samples retrieved from a private\nretrieval dataset. Our \\emph{differentially private retrieval-augmented\ndiffusion model} (DP-RDM) requires no fine-tuning on the retrieval dataset to\nadapt to another domain, and can use state-of-the-art generative models to\ngenerate high-quality image samples while satisfying rigorous DP guarantees.\nFor instance, when evaluated on MS-COCO, our DP-RDM can generate samples with a\nprivacy budget of $\\epsilon=10$, while providing a $3.5$ point improvement in\nFID compared to public-only retrieval for up to $10,000$ queries.\n","authors":["Jonathan Lebensold","Maziar Sanjabi","Pietro Astolfi","Adriana Romero-Soriano","Kamalika Chaudhuri","Mike Rabbat","Chuan Guo"],"pdf_url":"https://arxiv.org/pdf/2403.14421v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.06137v2","updated":"2024-03-21T14:03:39Z","published":"2024-02-09T02:11:25Z","title":"On the Privacy of Selection Mechanisms with Gaussian Noise","summary":"  Report Noisy Max and Above Threshold are two classical differentially private\n(DP) selection mechanisms. Their output is obtained by adding noise to a\nsequence of low-sensitivity queries and reporting the identity of the query\nwhose (noisy) answer satisfies a certain condition. Pure DP guarantees for\nthese mechanisms are easy to obtain when Laplace noise is added to the queries.\nOn the other hand, when instantiated using Gaussian noise, standard analyses\nonly yield approximate DP guarantees despite the fact that the outputs of these\nmechanisms lie in a discrete space. In this work, we revisit the analysis of\nReport Noisy Max and Above Threshold with Gaussian noise and show that, under\nthe additional assumption that the underlying queries are bounded, it is\npossible to provide pure ex-ante DP bounds for Report Noisy Max and pure\nex-post DP bounds for Above Threshold. The resulting bounds are tight and\ndepend on closed-form expressions that can be numerically evaluated using\nstandard methods. Empirically we find these lead to tighter privacy accounting\nin the high privacy, low data regime. Further, we propose a simple privacy\nfilter for composing pure ex-post DP guarantees, and use it to derive a fully\nadaptive Gaussian Sparse Vector Technique mechanism. Finally, we provide\nexperiments on mobility and energy consumption datasets demonstrating that our\nSparse Vector Technique is practically competitive with previous approaches and\nrequires less hyper-parameter tuning.\n","authors":["Jonathan Lebensold","Doina Precup","Borja Balle"],"pdf_url":"https://arxiv.org/pdf/2402.06137v2.pdf","comment":"AISTATS 2024"},{"id":"http://arxiv.org/abs/2403.14413v1","updated":"2024-03-21T13:59:19Z","published":"2024-03-21T13:59:19Z","title":"Model Uncertainty in Evolutionary Optimization and Bayesian\n  Optimization: A Comparative Analysis","summary":"  Black-box optimization problems, which are common in many real-world\napplications, require optimization through input-output interactions without\naccess to internal workings. This often leads to significant computational\nresources being consumed for simulations. Bayesian Optimization (BO) and\nSurrogate-Assisted Evolutionary Algorithm (SAEA) are two widely used\ngradient-free optimization techniques employed to address such challenges. Both\napproaches follow a similar iterative procedure that relies on surrogate models\nto guide the search process. This paper aims to elucidate the similarities and\ndifferences in the utilization of model uncertainty between these two methods,\nas well as the impact of model inaccuracies on algorithmic performance. A novel\nmodel-assisted strategy is introduced, which utilizes unevaluated solutions to\ngenerate offspring, leveraging the population-based search capabilities of\nevolutionary algorithm to enhance the effectiveness of model-assisted\noptimization. Experimental results demonstrate that the proposed approach\noutperforms mainstream Bayesian optimization algorithms in terms of accuracy\nand efficiency.\n","authors":["Hao Hao","Xiaoqun Zhang","Aimin Zhou"],"pdf_url":"https://arxiv.org/pdf/2403.14413v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14410v1","updated":"2024-03-21T13:57:45Z","published":"2024-03-21T13:57:45Z","title":"GLC++: Source-Free Universal Domain Adaptation through Global-Local\n  Clustering and Contrastive Affinity Learning","summary":"  Deep neural networks often exhibit sub-optimal performance under covariate\nand category shifts. Source-Free Domain Adaptation (SFDA) presents a promising\nsolution to this dilemma, yet most SFDA approaches are restricted to closed-set\nscenarios. In this paper, we explore Source-Free Universal Domain Adaptation\n(SF-UniDA) aiming to accurately classify \"known\" data belonging to common\ncategories and segregate them from target-private \"unknown\" data. We propose a\nnovel Global and Local Clustering (GLC) technique, which comprises an adaptive\none-vs-all global clustering algorithm to discern between target classes,\ncomplemented by a local k-NN clustering strategy to mitigate negative transfer.\nDespite the effectiveness, the inherent closed-set source architecture leads to\nuniform treatment of \"unknown\" data, impeding the identification of distinct\n\"unknown\" categories. To address this, we evolve GLC to GLC++, integrating a\ncontrastive affinity learning strategy. We examine the superiority of GLC and\nGLC++ across multiple benchmarks and category shift scenarios. Remarkably, in\nthe most challenging open-partial-set scenarios, GLC and GLC++ surpass GATE by\n16.7% and 18.6% in H-score on VisDA, respectively. GLC++ enhances the novel\ncategory clustering accuracy of GLC by 4.3% in open-set scenarios on\nOffice-Home. Furthermore, the introduced contrastive learning strategy not only\nenhances GLC but also significantly facilitates existing methodologies.\n","authors":["Sanqing Qu","Tianpei Zou","Florian Röhrbein","Cewu Lu","Guang Chen","Dacheng Tao","Changjun Jiang"],"pdf_url":"https://arxiv.org/pdf/2403.14410v1.pdf","comment":"This is a substantial extension of the CVPR 2023 paper \"Upcycling\n  Models under Domain and Category Shift\""},{"id":"http://arxiv.org/abs/2312.02914v3","updated":"2024-03-21T13:53:48Z","published":"2023-12-05T17:39:19Z","title":"Unsupervised Video Domain Adaptation with Masked Pre-Training and\n  Collaborative Self-Training","summary":"  In this work, we tackle the problem of unsupervised domain adaptation (UDA)\nfor video action recognition. Our approach, which we call UNITE, uses an image\nteacher model to adapt a video student model to the target domain. UNITE first\nemploys self-supervised pre-training to promote discriminative feature learning\non target domain videos using a teacher-guided masked distillation objective.\nWe then perform self-training on masked target data, using the video student\nmodel and image teacher model together to generate improved pseudolabels for\nunlabeled target videos. Our self-training process successfully leverages the\nstrengths of both models to achieve strong transfer performance across domains.\nWe evaluate our approach on multiple video domain adaptation benchmarks and\nobserve significant improvements upon previously reported results.\n","authors":["Arun Reddy","William Paul","Corban Rivera","Ketul Shah","Celso M. de Melo","Rama Chellappa"],"pdf_url":"https://arxiv.org/pdf/2312.02914v3.pdf","comment":"Accepted at CVPR 2024. 13 pages, 4 figures"},{"id":"http://arxiv.org/abs/2009.09213v6","updated":"2024-03-21T16:24:05Z","published":"2020-09-19T11:26:01Z","title":"Dodging DeepFake Detection via Implicit Spatial-Domain Notch Filtering","summary":"  The current high-fidelity generation and high-precision detection of DeepFake\nimages are at an arms race. We believe that producing DeepFakes that are highly\nrealistic and 'detection evasive' can serve the ultimate goal of improving\nfuture generation DeepFake detection capabilities. In this paper, we propose a\nsimple yet powerful pipeline to reduce the artifact patterns of fake images\nwithout hurting image quality by performing implicit spatial-domain notch\nfiltering. We first demonstrate that frequency-domain notch filtering, although\nfamously shown to be effective in removing periodic noise in the spatial\ndomain, is infeasible for our task at hand due to the manual designs required\nfor the notch filters. We, therefore, resort to a learning-based approach to\nreproduce the notch filtering effects, but solely in the spatial domain. We\nadopt a combination of adding overwhelming spatial noise for breaking the\nperiodic noise pattern and deep image filtering to reconstruct the noise-free\nfake images, and we name our method DeepNotch. Deep image filtering provides a\nspecialized filter for each pixel in the noisy image, producing filtered images\nwith high fidelity compared to their DeepFake counterparts. Moreover, we also\nuse the semantic information of the image to generate an adversarial guidance\nmap to add noise intelligently. Our large-scale evaluation on 3 representative\nstate-of-the-art DeepFake detection methods (tested on 16 types of DeepFakes)\nhas demonstrated that our technique significantly reduces the accuracy of these\n3 fake image detection methods, 36.79% on average and up to 97.02% in the best\ncase.\n","authors":["Yihao Huang","Felix Juefei-Xu","Qing Guo","Yang Liu","Geguang Pu"],"pdf_url":"https://arxiv.org/pdf/2009.09213v6.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2205.14116v3","updated":"2024-03-21T16:14:01Z","published":"2022-05-27T17:28:54Z","title":"Don't Explain Noise: Robust Counterfactuals for Randomized Ensembles","summary":"  Counterfactual explanations describe how to modify a feature vector in order\nto flip the outcome of a trained classifier. Obtaining robust counterfactual\nexplanations is essential to provide valid algorithmic recourse and meaningful\nexplanations. We study the robustness of explanations of randomized ensembles,\nwhich are always subject to algorithmic uncertainty even when the training data\nis fixed. We formalize the generation of robust counterfactual explanations as\na probabilistic problem and show the link between the robustness of ensemble\nmodels and the robustness of base learners. We develop a practical method with\ngood empirical performance and support it with theoretical guarantees for\nensembles of convex base learners. Our results show that existing methods give\nsurprisingly low robustness: the validity of naive counterfactuals is below\n$50\\%$ on most data sets and can fall to $20\\%$ on problems with many features.\nIn contrast, our method achieves high robustness with only a small increase in\nthe distance from counterfactual explanations to their initial observations.\n","authors":["Alexandre Forel","Axel Parmentier","Thibaut Vidal"],"pdf_url":"https://arxiv.org/pdf/2205.14116v3.pdf","comment":null}],"Computer Science and Game Theory":[{"id":"http://arxiv.org/abs/2401.12258v5","updated":"2024-03-21T17:29:37Z","published":"2024-01-21T16:59:45Z","title":"Emergent Dominance Hierarchies in Reinforcement Learning Agents","summary":"  Modern Reinforcement Learning (RL) algorithms are able to outperform humans\nin a wide variety of tasks. Multi-agent reinforcement learning (MARL) settings\npresent additional challenges, and successful cooperation in mixed-motive\ngroups of agents depends on a delicate balancing act between individual and\ngroup objectives. Social conventions and norms, often inspired by human\ninstitutions, are used as tools for striking this balance.\n  In this paper, we examine a fundamental, well-studied social convention that\nunderlies cooperation in both animal and human societies: dominance\nhierarchies.\n  We adapt the ethological theory of dominance hierarchies to artificial\nagents, borrowing the established terminology and definitions with as few\namendments as possible. We demonstrate that populations of RL agents, operating\nwithout explicit programming or intrinsic rewards, can invent, learn, enforce,\nand transmit a dominance hierarchy to new populations. The dominance\nhierarchies that emerge have a similar structure to those studied in chickens,\nmice, fish, and other species.\n","authors":["Ram Rachum","Yonatan Nakar","Bill Tomlinson","Nitay Alon","Reuth Mirsky"],"pdf_url":"https://arxiv.org/pdf/2401.12258v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.03501v2","updated":"2024-03-21T10:48:26Z","published":"2023-10-05T12:25:48Z","title":"Designing Digital Voting Systems for Citizens: Achieving Fairness and\n  Legitimacy in Participatory Budgeting","summary":"  Participatory Budgeting (PB) has evolved into a key democratic instrument for\nresource allocation in cities. Enabled by digital platforms, cities now have\nthe opportunity to let citizens directly propose and vote on urban projects,\nusing different voting input and aggregation rules. However, the choices cities\nmake in terms of the rules of their PB have often not been informed by academic\nstudies on voter behaviour and preferences. Therefore, this work presents the\nresults of behavioural experiments where participants were asked to vote in a\nfictional PB setting. We identified approaches to designing PB voting that\nminimise cognitive load and enhance the perceived fairness and legitimacy of\nthe digital process from the citizens' perspective. In our study, participants\npreferred voting input formats that are more expressive (like rankings and\ndistributing points) over simpler formats (like approval voting). Participants\nalso indicated a desire for the budget to be fairly distributed across city\ndistricts and project categories. Participants found the Method of Equal Shares\nvoting rule to be fairer than the conventional Greedy voting rule. These\nfindings offer actionable insights for digital governance, contributing to the\ndevelopment of fairer and more transparent digital systems and collective\ndecision-making processes for citizens.\n","authors":["Joshua C. Yang","Carina I. Hausladen","Dominik Peters","Evangelos Pournaras","Regula Hänggli Fricker","Dirk Helbing"],"pdf_url":"https://arxiv.org/pdf/2310.03501v2.pdf","comment":"Under review in ACM Digital Government: Research and Practice"},{"id":"http://arxiv.org/abs/2307.09478v2","updated":"2024-03-21T10:28:38Z","published":"2023-07-14T09:16:24Z","title":"The Role of Transparency in Repeated First-Price Auctions with Unknown\n  Valuations","summary":"  We study the problem of regret minimization for a single bidder in a sequence\nof first-price auctions where the bidder discovers the item's value only if the\nauction is won. Our main contribution is a complete characterization, up to\nlogarithmic factors, of the minimax regret in terms of the auction's\n\\emph{transparency}, which controls the amount of information on competing bids\ndisclosed by the auctioneer at the end of each auction. Our results hold under\ndifferent assumptions (stochastic, adversarial, and their smoothed variants) on\nthe environment generating the bidder's valuations and competing bids. These\nminimax rates reveal how the interplay between transparency and the nature of\nthe environment affects how fast one can learn to bid optimally in first-price\nauctions.\n","authors":["Nicolò Cesa-Bianchi","Tommaso Cesari","Roberto Colomboni","Federico Fusco","Stefano Leonardi"],"pdf_url":"https://arxiv.org/pdf/2307.09478v2.pdf","comment":"Accepted at STOC 2024"},{"id":"http://arxiv.org/abs/2310.12928v2","updated":"2024-03-21T10:19:36Z","published":"2023-10-19T17:24:48Z","title":"Resolving social dilemmas with minimal reward transfer","summary":"  Multi-agent cooperation is an important topic, and is particularly\nchallenging in mixed-motive situations where it does not pay to be nice to\nothers. Consequently, self-interested agents often avoid collective behaviour,\nresulting in suboptimal outcomes for the group. In response, in this paper we\nintroduce a metric to quantify the disparity between what is rational for\nindividual agents and what is rational for the group, which we call the general\nself-interest level. This metric represents the maximum proportion of\nindividual rewards that all agents can retain while ensuring that achieving\nsocial welfare optimum becomes a dominant strategy. By aligning the individual\nand group incentives, rational agents acting to maximise their own reward will\nsimultaneously maximise the collective reward. As agents transfer their rewards\nto motivate others to consider their welfare, we diverge from traditional\nconcepts of altruism or prosocial behaviours. The general self-interest level\nis a property of a game that is useful for assessing the propensity of players\nto cooperate and understanding how features of a game impact this. We\nillustrate the effectiveness of our method on several novel games\nrepresentations of social dilemmas with arbitrary numbers of players.\n","authors":["Richard Willis","Yali Du","Joel Z Leibo","Michael Luck"],"pdf_url":"https://arxiv.org/pdf/2310.12928v2.pdf","comment":"34 pages, 13 tables, submitted to the Journal of Autonomous Agents\n  and Multi-Agent Systems: Special Issue on Citizen-Centric AI Systems"},{"id":"http://arxiv.org/abs/2403.14443v1","updated":"2024-03-21T14:48:37Z","published":"2024-03-21T14:48:37Z","title":"Language Models Can Reduce Asymmetry in Information Markets","summary":"  This work addresses the buyer's inspection paradox for information markets.\nThe paradox is that buyers need to access information to determine its value,\nwhile sellers need to limit access to prevent theft. To study this, we\nintroduce an open-source simulated digital marketplace where intelligent\nagents, powered by language models, buy and sell information on behalf of\nexternal participants. The central mechanism enabling this marketplace is the\nagents' dual capabilities: they not only have the capacity to assess the\nquality of privileged information but also come equipped with the ability to\nforget. This ability to induce amnesia allows vendors to grant temporary access\nto proprietary information, significantly reducing the risk of unauthorized\nretention while enabling agents to accurately gauge the information's relevance\nto specific queries or tasks. To perform well, agents must make rational\ndecisions, strategically explore the marketplace through generated sub-queries,\nand synthesize answers from purchased information. Concretely, our experiments\n(a) uncover biases in language models leading to irrational behavior and\nevaluate techniques to mitigate these biases, (b) investigate how price affects\ndemand in the context of informational goods, and (c) show that inspection and\nhigher budgets both lead to higher quality outcomes.\n","authors":["Nasim Rahaman","Martin Weiss","Manuel Wüthrich","Yoshua Bengio","Li Erran Li","Chris Pal","Bernhard Schölkopf"],"pdf_url":"https://arxiv.org/pdf/2403.14443v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.09470v2","updated":"2024-03-21T19:12:08Z","published":"2023-07-13T19:05:11Z","title":"Multi-Player Zero-Sum Markov Games with Networked Separable Interactions","summary":"  We study a new class of Markov games, \\emph(multi-player) zero-sum Markov\nGames} with \\emph{Networked separable interactions} (zero-sum NMGs), to model\nthe local interaction structure in non-cooperative multi-agent sequential\ndecision-making. We define a zero-sum NMG as a model where {the payoffs of the\nauxiliary games associated with each state are zero-sum and} have some\nseparable (i.e., polymatrix) structure across the neighbors over some\ninteraction network. We first identify the necessary and sufficient conditions\nunder which an MG can be presented as a zero-sum NMG, and show that the set of\nMarkov coarse correlated equilibrium (CCE) collapses to the set of Markov Nash\nequilibrium (NE) in these games, in that the product of per-state\nmarginalization of the former for all players yields the latter. Furthermore,\nwe show that finding approximate Markov \\emph{stationary} CCE in\ninfinite-horizon discounted zero-sum NMGs is \\texttt{PPAD}-hard, unless the\nunderlying network has a ``star topology''. Then, we propose\nfictitious-play-type dynamics, the classical learning dynamics in normal-form\ngames, for zero-sum NMGs, and establish convergence guarantees to Markov\nstationary NE under a star-shaped network structure. Finally, in light of the\nhardness result, we focus on computing a Markov \\emph{non-stationary} NE and\nprovide finite-iteration guarantees for a series of value-iteration-based\nalgorithms. We also provide numerical experiments to corroborate our\ntheoretical results.\n","authors":["Chanwoo Park","Kaiqing Zhang","Asuman Ozdaglar"],"pdf_url":"https://arxiv.org/pdf/2307.09470v2.pdf","comment":null}],"Information Theory":[{"id":"http://arxiv.org/abs/2401.00947v2","updated":"2024-03-21T17:21:14Z","published":"2024-01-01T20:08:58Z","title":"On SAT information content, its polynomial-time solvability and fixed\n  code algorithms","summary":"  The amount of information in satisfiability problem (SAT) is considered. SAT\ncan be polynomial-time solvable when the solving algorithm holds an exponential\namount of information. It is also established that SAT Kolmogorov complexity is\nconstant. It is argued that the amount of information in SAT grows at least\nexponentially with the size of the input instance. The amount of information in\nSAT is compared with the amount of information in the fixed code algorithms and\ngenerated over runtime.\n","authors":["Maciej Drozdowski"],"pdf_url":"https://arxiv.org/pdf/2401.00947v2.pdf","comment":"11 pages, 1 table, 0 figures"},{"id":"http://arxiv.org/abs/2308.02583v2","updated":"2024-03-21T16:01:09Z","published":"2023-08-03T17:17:06Z","title":"Postselected communication over quantum channels","summary":"  The single-letter characterisation of the entanglement-assisted capacity of a\nquantum channel is one of the seminal results of quantum information theory. In\nthis paper, we consider a modified communication scenario in which the receiver\nis allowed an additional, `inconclusive' measurement outcome, and we employ an\nerror metric given by the error probability in decoding the transmitted message\nconditioned on a conclusive measurement result. We call this setting\npostselected communication and the ensuing highest achievable rates the\npostselected capacities. Here, we provide a precise single-letter\ncharacterisation of postselected capacities in the setting of entanglement\nassistance as well as the more general nonsignalling assistance, establishing\nthat they are both equal to the channel's projective mutual information -- a\nvariant of mutual information based on the Hilbert projective metric. We do so\nby establishing bounds on the one-shot postselected capacities, with a lower\nbound that makes use of a postselected teleportation protocol and an upper\nbound in terms of the postselected hypothesis testing relative entropy. As\nsuch, we obtain fundamental limits on a channel's ability to communicate even\nwhen this strong resource of postselection is allowed, implying limitations on\ncommunication even when the receiver has access to postselected closed timelike\ncurves.\n","authors":["Kaiyuan Ji","Bartosz Regula","Mark M. Wilde"],"pdf_url":"https://arxiv.org/pdf/2308.02583v2.pdf","comment":"38 pages, 5 figures, submitted to International Journal of Quantum\n  Information (IJQI) as part of a special issue dedicated to Alexander S.\n  Holevo on the occasion of his 80th birthday"},{"id":"http://arxiv.org/abs/2403.14450v1","updated":"2024-03-21T14:58:07Z","published":"2024-03-21T14:58:07Z","title":"Maximal $α$-Leakage for Quantum Privacy Mechanisms","summary":"  In this work, maximal $\\alpha$-leakage is introduced to quantify how much a\nquantum adversary can learn about any sensitive information of data upon\nobserving its disturbed version via a quantum privacy mechanism. We first show\nthat an adversary's maximal expected $\\alpha$-gain using optimal measurement is\ncharacterized by measured conditional R\\'enyi entropy. This can be viewed as a\nparametric generalization of K\\\"onig et al.'s famous guessing probability\nformula [IEEE Trans. Inf. Theory, 55(9), 2009]. Then, we prove that the\n$\\alpha$-leakage and maximal $\\alpha$-leakage for a quantum privacy mechanism\nare determined by measured Arimoto information and measured R\\'enyi capacity,\nrespectively. Various properties of maximal $\\alpha$-leakage, such as data\nprocessing inequality and composition property are established as well.\nMoreover, we show that regularized $\\alpha$-leakage and regularized maximal\n$\\alpha$-leakage for identical and independent quantum privacy mechanisms\ncoincide with $\\alpha$-tilted sandwiched R\\'enyi information and sandwiched\nR\\'enyi capacity, respectively.\n","authors":["Bo-Yu Yang","Hsuan Yu","Hao-Chung Cheng"],"pdf_url":"https://arxiv.org/pdf/2403.14450v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.12433v2","updated":"2024-03-21T14:51:25Z","published":"2023-11-21T08:43:55Z","title":"Constellation Shaping under Phase Noise Impairment for Sub-THz\n  Communications","summary":"  The large untapped spectrum in the sub-THz allows for ultra-high throughput\ncommunication to realize many seemingly impossible applications in 6G. One of\nthe challenges in radio communications in sub-THz is the hardware impairments.\nSpecifically, phase noise is one key hardware impairment, which is accentuated\nas we increase the frequency and bandwidth. Furthermore, the moderate output\npower of the sub-THz power amplifier demands limits on peak to average power\nratio (PAPR) signal design. Single carrier frequency domain equalization\n(SC-FDE) has been identified as a suitable candidate for sub-THz, although some\nchallenges such as phase noise and PAPR still remain to be tackled. In this\nwork, we design a phase noise robust, modest PAPR SC waveform by geometrically\nshaping the constellation under practical conditions. We formulate the waveform\noptimization problem in its augmented Lagrangian form and use a\nback-propagation-inspired technique to obtain a constellation design that is\nnumerically robust to phase noise, while maintaining a relatively low PAPR\ncompared to the conventional waveforms.\n","authors":["Dileepa Marasinghe","Le Hang Nguyen","Jafar Mohammadi","Yejian Chen","Thorsten Wild","Nandana Rajatheva"],"pdf_url":"https://arxiv.org/pdf/2311.12433v2.pdf","comment":"To appear in IEEE ICC 2024"},{"id":"http://arxiv.org/abs/2403.14416v1","updated":"2024-03-21T14:05:32Z","published":"2024-03-21T14:05:32Z","title":"Quantum Channel Simulation under Purified Distance is no more difficult\n  than State Splitting","summary":"  Characterizing the minimal communication needed for the quantum channel\nsimulation is a fundamental task in the quantum information theory. In this\npaper, we show that, under the purified distance, the quantum channel\nsimulation can be directly achieved via quantum state splitting without using a\ntechnique known as the de Finetti reduction, and thus provide a pair of tighter\none-shot bounds. Using the bounds, we also recover the quantum reverse Shannon\ntheorem in a much simpler way.\n","authors":["Michael X. Cao","Rahul Jain","Marco Tomamichel"],"pdf_url":"https://arxiv.org/pdf/2403.14416v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14360v1","updated":"2024-03-21T12:41:18Z","published":"2024-03-21T12:41:18Z","title":"An Achievability Bound for Variable-Length Stop-Feedback Coding over the\n  Gaussian Channel","summary":"  Feedback holds a pivotal role in practical communication schemes, even though\nit does not enhance channel capacity. Its main attribute includes adaptability\nin transmission that allows for a higher rate of convergence of the error\nprobability to zero with respect to blocklength. Motivated by this fact, we\npresent a non-asymptotic achievability bound for variable-length coding with\nstop-feedback. Specifically, a general achievability bound is derived, that\nemploys a random coding ensemble in combination with minimum distance decoding.\nThe general bound is particularized for the Gaussian channel. Numerical\nevaluation of the bound confirms the significant value of feedback compared to\ntransmission with fixed blocklength coding and without feedback.\n","authors":["Ioannis Papoutsidakis","Robert J. Piechocki","Angela Doufexi"],"pdf_url":"https://arxiv.org/pdf/2403.14360v1.pdf","comment":"Submitted to the 2024 International Symposium on Information Theory\n  (ISIT)"},{"id":"http://arxiv.org/abs/2403.14338v1","updated":"2024-03-21T12:06:30Z","published":"2024-03-21T12:06:30Z","title":"Optimal Second-Order Rates for Quantum Information Decoupling","summary":"  In this paper, we consider the standard quantum information decoupling, in\nwhich Alice aims to decouple her system from the environment by local\noperations and discarding some of her systems. To achieve an\n$\\varepsilon$-decoupling with trace distance as the error criterion, we\nestablish a near-optimal one-shot characterization for the largest dimension of\nthe remainder system in terms of the conditional\n$(1-\\varepsilon)$-hypothesis-testing entropy. When the underlying system is\nindependent and identically prepared, our result leads to the matched\nsecond-order rate as well as the matched moderate deviation rate. As an\napplication, we find an achievability bound in entanglement distillation\nprotocol, where the objective is for Alice and Bob to transform their quantum\nstate to maximally entangled state with largest possible dimension using only\nlocal operations and one-way classical communications.\n","authors":["Yu-Chen Shen","Li Gao","Hao-Chung Cheng"],"pdf_url":"https://arxiv.org/pdf/2403.14338v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14192v1","updated":"2024-03-21T07:35:53Z","published":"2024-03-21T07:35:53Z","title":"Fundamentals of Delay-Doppler Communications: Practical Implementation\n  and Extensions to OTFS","summary":"  The recently proposed orthogonal time frequency space (OTFS) modulation,\nwhich is a typical Delay-Doppler (DD) communication scheme, has attracted\nsignificant attention thanks to its appealing performance over doubly-selective\nchannels. In this paper, we present the fundamentals of general DD\ncommunications from the viewpoint of the Zak transform. We start our study by\nconstructing DD domain basis functions aligning with the time-frequency\n(TF)-consistency condition, which are globally quasi-periodic and locally\ntwisted-shifted. We unveil that these features are translated to unique signal\nstructures in both time and frequency, which are beneficial for communication\npurposes. Then, we focus on the practical implementations of DD Nyquist\ncommunications, where we show that rectangular windows achieve perfect DD\northogonality, while truncated periodic signals can obtain sufficient DD\northogonality. Particularly, smoothed rectangular window with excess bandwidth\ncan result in a slightly worse orthogonality but better pulse localization in\nthe DD domain. Furthermore, we present a practical pulse shaping framework for\ngeneral DD communications and derive the corresponding input-output relation\nunder various shaping pulses. Our numerical results agree with our derivations\nand also demonstrate advantages of DD communications over conventional\northogonal frequency-division multiplexing (OFDM).\n","authors":["Shuangyang Li","Peter Jung","Weijie Yuan","Zhiqiang Wei","Jinhong Yuan","Baoming Bai","Giuseppe Caire"],"pdf_url":"https://arxiv.org/pdf/2403.14192v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.08298v3","updated":"2024-03-21T06:24:52Z","published":"2022-12-16T06:28:16Z","title":"Exploring Hybrid Active-Passive RIS-Aided MEC Systems: From the\n  Mode-Switching Perspective","summary":"  Mobile edge computing (MEC) has been regarded as a promising technique to\nsupport latencysensitivity and computation-intensive serves. However, the low\noffloading rate caused by the random channel fading characteristic becomes a\nmajor bottleneck in restricting the performance of the MEC. Fortunately,\nreconfigurable intelligent surface (RIS) can alleviate this problem since it\ncan boost both the spectrum- and energy- efficiency. Different from the\nexisting works adopting either fully active or fully passive RIS, we propose a\nnovel hybrid RIS in which reflecting units can flexibly switch between active\nand passive modes. To achieve a tradeoff between the latency and energy\nconsumption, an optimization problem is formulated by minimizing the total\ncost. In light of the intractability of the problem, we develop an alternating\noptimization-based iterative algorithm by combining the successive convex\napproximation method, the variable substitution, and the singular value\ndecomposition (SVD) to obtain sub-optimal solutions. Furthermore, in order to\ngain more insight into the problem, we consider two special cases involving a\nlatency minimization problem and an energy consumption minimization problem,\nand respectively analyze the tradeoff between the number of active and passive\nunits. Simulation results verify that the proposed algorithm can achieve\nflexible mode switching and significantly outperforms existing algorithms.\n","authors":["Hao Xie","Dong Li","Bowen Gu"],"pdf_url":"https://arxiv.org/pdf/2212.08298v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08727v2","updated":"2024-03-21T05:41:15Z","published":"2024-03-13T17:28:06Z","title":"The q-ary Gilbert-Varshamov bound can be improved for all but finitely\n  many positive integers q","summary":"  For any positive integer $q\\geq 2$ and any real number $\\delta\\in(0,1)$, let\n$\\alpha_q(n,\\delta n)$ denote the maximum size of a subset of $\\mathbb{Z}_q^n$\nwith minimum Hamming distance at least $\\delta n$, where\n$\\mathbb{Z}_q=\\{0,1,\\dotsc,q-1\\}$ and $n\\in\\mathbb{N}$. The asymptotic rate\nfunction is defined by $ R_q(\\delta) =\n\\limsup_{n\\rightarrow\\infty}\\frac{1}{n}\\log_q\\alpha_q(n,\\delta n).$ The famous\n$q$-ary asymptotic Gilbert-Varshamov bound, obtained in the 1950s, states that\n\\[ R_q(\\delta) \\geq 1 -\n\\delta\\log_q(q-1)-\\delta\\log_q\\frac{1}{\\delta}-(1-\\delta)\\log_q\\frac{1}{1-\\delta}\n\\stackrel{\\mathrm{def}}{=}R_\\mathrm{GV}(\\delta,q) \\] for all positive integers\n$q\\geq 2$ and $0<\\delta<1-q^{-1}$. In the case that $q$ is an even power of a\nprime with $q\\geq 49$, the $q$-ary Gilbert-Varshamov bound was firstly improved\nby using algebraic geometry codes in the works of Tsfasman, Vladut, and Zink\nand of Ihara in the 1980s. These algebraic geometry codes have been modified to\nimprove the $q$-ary Gilbert-Varshamov bound $R_\\mathrm{GV}(\\delta,q)$ at a\nspecific tangent point $\\delta=\\delta_0\\in (0,1)$ of the curve\n$R_\\mathrm{GV}(\\delta,q)$ for each given integer $q\\geq 46$. However, the\n$q$-ary Gilbert-Varshamov bound $R_\\mathrm{GV}(\\delta,q)$ at $\\delta=1/2$,\ni.e., $R_\\mathrm{GV}(1/2,q)$, remains the largest known lower bound of\n$R_q(1/2)$ for infinitely many positive integers $q$ which is a generic prime\nand which is a generic non-prime-power integer. In this paper, by using codes\nfrom geometry of numbers introduced by Lenstra in the 1980s, we prove that the\n$q$-ary Gilbert-Varshamov bound $R_\\mathrm{GV}(\\delta,q)$ with $\\delta\\in(0,1)$\ncan be improved for all but finitely many positive integers $q$. It is shown\nthat the growth defined by $\\eta(\\delta)=\n\\liminf_{q\\rightarrow\\infty}\\frac{1}{\\log q}\\log[1-\\delta-R_q(\\delta)]^{-1}$\nfor every $\\delta\\in(0,1)$ has actually a nontrivial lower bound.\n","authors":["Xue-Bin Liang"],"pdf_url":"https://arxiv.org/pdf/2403.08727v2.pdf","comment":"35 pages; more relevant and connected works are mentioned and\n  referenced, with a suitable but slight adjustment for presentation in\n  Abstract and Introduction"}],"Performance":[{"id":"http://arxiv.org/abs/2403.14853v1","updated":"2024-03-21T21:56:44Z","published":"2024-03-21T21:56:44Z","title":"iSpLib: A Library for Accelerating Graph Neural Networks using\n  Auto-tuned Sparse Operations","summary":"  Core computations in Graph Neural Network (GNN) training and inference are\noften mapped to sparse matrix operations such as sparse-dense matrix\nmultiplication (SpMM). These sparse operations are harder to optimize by manual\ntuning because their performance depends significantly on the sparsity of input\ngraphs, GNN models, and computing platforms. To address this challenge, we\npresent iSpLib, a PyTorch-based C++ library equipped with auto-tuned sparse\noperations. iSpLib expedites GNN training with a cache-enabled backpropagation\nthat stores intermediate matrices in local caches. The library offers a\nuser-friendly Python plug-in that allows users to take advantage of our\noptimized PyTorch operations out-of-the-box for any existing linear\nalgebra-based PyTorch implementation of popular GNNs (Graph Convolution\nNetwork, GraphSAGE, Graph Inference Network, etc.) with only two lines of\nadditional code. We demonstrate that iSpLib obtains up to 27x overall training\nspeedup compared to the equivalent PyTorch 2.1.0 and PyTorch Geometric 2.4.0\nimplementations on the CPU. Our library is publicly available at\nhttps://github.com/HipGraph/iSpLib (https://doi.org/10.5281/zenodo.10806511).\n","authors":["Md Saidul Hoque Anik","Pranav Badhe","Rohit Gampa","Ariful Azad"],"pdf_url":"https://arxiv.org/pdf/2403.14853v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14792v1","updated":"2024-03-21T19:13:15Z","published":"2024-03-21T19:13:15Z","title":"CASPER: Carbon-Aware Scheduling and Provisioning for Distributed Web\n  Services","summary":"  There has been a significant societal push towards sustainable practices,\nincluding in computing. Modern interactive workloads such as geo-distributed\nweb-services exhibit various spatiotemporal and performance flexibility,\nenabling the possibility to adapt the location, time, and intensity of\nprocessing to align with the availability of renewable and low-carbon energy.\nAn example is a web application hosted across multiple cloud regions, each with\nvarying carbon intensity based on their local electricity mix. Distributed\nload-balancing enables the exploitation of low-carbon energy through load\nmigration across regions, reducing web applications carbon footprint. In this\npaper, we present CASPER, a carbon-aware scheduling and provisioning system\nthat primarily minimizes the carbon footprint of distributed web services while\nalso respecting their Service Level Objectives (SLO). We formulate CASPER as an\nmulti-objective optimization problem that considers both the variable carbon\nintensity and latency constraints of the network. Our evaluation reveals the\nsignificant potential of CASPER in achieving substantial reductions in carbon\nemissions. Compared to baseline methods, CASPER demonstrates improvements of up\nto 70% with no latency performance degradation.\n","authors":["Abel Souza","Shruti Jasoria","Basundhara Chakrabarty","Alexander Bridgwater","Axel Lundberg","Filip Skogh","Ahmed Ali-Eldin","David Irwin","Prashant Shenoy"],"pdf_url":"https://arxiv.org/pdf/2403.14792v1.pdf","comment":null}]},"2024-03-22T00:00:00Z":{"Distributed, Parallel, and Cluster Computing":[{"id":"http://arxiv.org/abs/2403.11522v2","updated":"2024-03-22T10:28:05Z","published":"2024-03-18T07:22:31Z","title":"LOOPer: A Learned Automatic Code Optimizer For Polyhedral Compilers","summary":"  While polyhedral compilers have shown success in implementing advanced code\ntransformations, they still have challenges in selecting the most profitable\ntransformations that lead to the best speedups. This has motivated the use of\nmachine learning to build cost models to guide the search for polyhedral\noptimizations. State-of-the-art polyhedral compilers have demonstrated a viable\nproof-of-concept of this approach. While such a proof-of-concept has shown\npromise, it still has significant limitations. State-of-the-art polyhedral\ncompilers that use a deep-learning cost model only support a small subset of\naffine transformations, limiting their ability to apply complex code\ntransformations. They also only support simple programs that have a single loop\nnest and a rectangular iteration domain, limiting their applicability to many\nprograms. These limitations significantly impact the generality of such\ncompilers and autoschedulers and put into question the whole approach. In this\npaper, we introduce LOOPer, the first polyhedral autoscheduler that uses a\ndeep-learning based cost model and covers a large set of affine transformations\nand programs. It supports the exploration of a large set of affine\ntransformations, allowing the application of complex sequences of polyhedral\ntransformations. It also supports the optimization of programs with multiple\nloop nests and with rectangular and non-rectangular iteration domains, allowing\nthe optimization of an extensive set of programs. We implement and evaluate\nLOOPer and show that it achieves speedups over the state-of-the-art. On the\nPolybench benchmark, LOOPer achieves a geometric mean speedup of 1.59x over\nTiramisu. LOOPer also achieves competitive speedups with a geometric mean\nspeedup of 1.34x over Pluto, a state-of-the-art polyhedral compiler that does\nnot use a machine-learning based cost model.\n","authors":["Massinissa Merouani","Khaled Afif Boudaoud","Iheb Nassim Aouadj","Nassim Tchoulak","Islem Kara Bernou","Hamza Benyamina","Fatima Benbouzid-Si Tayeb","Karima Benatchba","Hugh Leather","Riyadh Baghdadi"],"pdf_url":"https://arxiv.org/pdf/2403.11522v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15366v1","updated":"2024-03-22T17:36:19Z","published":"2024-03-22T17:36:19Z","title":"Fourier Transform-based Estimators for Data Sketches","summary":"  In this paper we consider the problem of estimating the $f$-moment\n($\\sum_{v\\in [n]} (f(\\mathbf{x}(v))-f(0))$) of a dynamic vector $\\mathbf{x}\\in\n\\mathbb{G}^n$ over some abelian group $(\\mathbb{G},+)$, where the\n$\\|f\\|_\\infty$ norm is bounded. We propose a simple sketch and new estimation\nframework based on the \\emph{Fourier transform} of $f$. I.e., we decompose $f$\ninto a linear combination of homomorphisms $f_1,f_2,\\ldots$ from\n$(\\mathbb{G},+)$ to $(\\mathbb{C},\\times)$, estimate the $f_k$-moment for each\n$f_k$, and synthesize them to obtain an estimate of the $f$-moment. Our\nestimators are asymptotically unbiased and have variance asymptotic to\n$\\|\\mathbf{x}\\|_0^2 (\\|f\\|_\\infty^2 m^{-1} + \\|\\hat{f}\\|_1^2 m^{-2})$, where\nthe size of the sketch is $O(m\\log n\\log|\\mathbb{G}|)$ bits.\n  When $\\mathbb{G}=\\mathbb{Z}$ this problem can also be solved using\noff-the-shelf $\\ell_0$-samplers with space $O(m\\log^2 n)$ bits, which does not\nobviously generalize to finite groups. As a concrete benchmark, we extend\nGanguly, Garofalakis, and Rastogi's singleton-detector-based sampler to work\nover $\\mathbb{G}$ using $O(m\\log n\\log|\\mathbb{G}|\\log(m\\log n))$ bits.\n  We give some experimental evidence that the Fourier-based estimation\nframework is significantly more accurate than sampling-based approaches at the\nsame memory footprint.\n","authors":["Seth Pettie","Dingyu Wang"],"pdf_url":"https://arxiv.org/pdf/2403.15366v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15324v1","updated":"2024-03-22T16:25:34Z","published":"2024-03-22T16:25:34Z","title":"ProvDeploy: Provenance-oriented Containerization of High Performance\n  Computing Scientific Workflows","summary":"  Many existing scientific workflows require High Performance Computing\nenvironments to produce results in a timely manner. These workflows have\nseveral software library components and use different environments, making the\ndeployment and execution of the software stack not trivial. This complexity\nincreases if the user needs to add provenance data capture services to the\nworkflow. This manuscript introduces ProvDeploy to assist the user in\nconfiguring containers for scientific workflows with integrated provenance data\ncapture. ProvDeploy was evaluated with a Scientific Machine Learning workflow,\nexploring containerization strategies focused on provenance in two distinct HPC\nenvironments\n","authors":["Liliane Kunstmann","Débora Pina","Daniel de Oliveira","Marta Mattoso"],"pdf_url":"https://arxiv.org/pdf/2403.15324v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.12973v2","updated":"2024-03-22T15:00:47Z","published":"2023-12-20T12:31:28Z","title":"Sparse Mean Field Load Balancing in Large Localized Queueing Systems","summary":"  Scalable load balancing algorithms are of great interest in cloud networks\nand data centers, necessitating the use of tractable techniques to compute\noptimal load balancing policies for good performance. However, most existing\nscalable techniques, especially asymptotically scaling methods based on mean\nfield theory, have not been able to model large queueing networks with strong\nlocality. Meanwhile, general multi-agent reinforcement learning techniques can\nbe hard to scale and usually lack a theoretical foundation. In this work, we\naddress this challenge by leveraging recent advances in sparse mean field\ntheory to learn a near-optimal load balancing policy in sparsely connected\nqueueing networks in a tractable manner, which may be preferable to global\napproaches in terms of wireless communication overhead. Importantly, we obtain\na general load balancing framework for a large class of sparse bounded-degree\nwireless topologies. By formulating a novel mean field control problem in the\ncontext of graphs with bounded degree, we reduce the otherwise difficult\nmulti-agent problem to a single-agent problem. Theoretically, the approach is\njustified by approximation guarantees. Empirically, the proposed methodology\nperforms well on several realistic and scalable wireless network topologies as\ncompared to a number of well-known load balancing heuristics and existing\nscalable multi-agent reinforcement learning methods.\n","authors":["Anam Tahir","Kai Cui","Heinz Koeppl"],"pdf_url":"https://arxiv.org/pdf/2312.12973v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.09444v3","updated":"2024-03-22T14:26:33Z","published":"2023-07-18T17:17:27Z","title":"No distributed quantum advantage for approximate graph coloring","summary":"  We give an almost complete characterization of the hardness of $c$-coloring\n$\\chi$-chromatic graphs with distributed algorithms, for a wide range of models\nof distributed computing. In particular, we show that these problems do not\nadmit any distributed quantum advantage. To do that: 1) We give a new\ndistributed algorithm that finds a $c$-coloring in $\\chi$-chromatic graphs in\n$\\tilde{\\mathcal{O}}(n^{\\frac{1}{\\alpha}})$ rounds, with $\\alpha =\n\\bigl\\lfloor\\frac{c-1}{\\chi - 1}\\bigr\\rfloor$. 2) We prove that any distributed\nalgorithm for this problem requires $\\Omega(n^{\\frac{1}{\\alpha}})$ rounds.\n  Our upper bound holds in the classical, deterministic LOCAL model, while the\nnear-matching lower bound holds in the non-signaling model. This model,\nintroduced by Arfaoui and Fraigniaud in 2014, captures all models of\ndistributed graph algorithms that obey physical causality; this includes not\nonly classical deterministic LOCAL and randomized LOCAL but also quantum-LOCAL,\neven with a pre-shared quantum state.\n  We also show that similar arguments can be used to prove that, e.g.,\n3-coloring 2-dimensional grids or $c$-coloring trees remain hard problems even\nfor the non-signaling model, and in particular do not admit any quantum\nadvantage. Our lower-bound arguments are purely graph-theoretic at heart; no\nbackground on quantum information theory is needed to establish the proofs.\n","authors":["Xavier Coiteux-Roy","Francesco d'Amore","Rishikesh Gajjala","Fabian Kuhn","François Le Gall","Henrik Lievonen","Augusto Modanese","Marc-Olivier Renou","Gustav Schmid","Jukka Suomela"],"pdf_url":"https://arxiv.org/pdf/2307.09444v3.pdf","comment":"Accepted to STOC 2024"},{"id":"http://arxiv.org/abs/2403.15195v1","updated":"2024-03-22T13:31:24Z","published":"2024-03-22T13:31:24Z","title":"FSD-Inference: Fully Serverless Distributed Inference with Scalable\n  Cloud Communication","summary":"  Serverless computing offers attractive scalability, elasticity and\ncost-effectiveness. However, constraints on memory, CPU and function runtime\nhave hindered its adoption for data-intensive applications and machine learning\n(ML) workloads. Traditional 'server-ful' platforms enable distributed\ncomputation via fast networks and well-established inter-process communication\n(IPC) mechanisms such as MPI and shared memory. In the absence of such\nsolutions in the serverless domain, parallel computation with significant IPC\nrequirements is challenging. We present FSD-Inference, the first fully\nserverless and highly scalable system for distributed ML inference. We explore\npotential communication channels, in conjunction with Function-as-a-Service\n(FaaS) compute, to design a state-of-the-art solution for distributed ML within\nthe context of serverless data-intensive computing. We introduce novel fully\nserverless communication schemes for ML inference workloads, leveraging both\ncloud-based publish-subscribe/queueing and object storage offerings. We\ndemonstrate how publish-subscribe/queueing services can be adapted for FaaS IPC\nwith comparable performance to object storage, while offering significantly\nreduced cost at high parallelism levels. We conduct in-depth experiments on\nbenchmark DNNs of various sizes. The results show that when compared to\nserver-based alternatives, FSD-Inference is significantly more cost-effective\nand scalable, and can even achieve competitive performance against optimized\nHPC solutions. Experiments also confirm that our serverless solution can handle\nlarge distributed workloads and leverage high degrees of FaaS parallelism.\n","authors":["Joe Oakley","Hakan Ferhatosmanoglu"],"pdf_url":"https://arxiv.org/pdf/2403.15195v1.pdf","comment":"In Proceedings of 2024 IEEE 40th International Conference on Data\n  Engineering (ICDE) (to appear)"},{"id":"http://arxiv.org/abs/2403.15191v1","updated":"2024-03-22T13:21:09Z","published":"2024-03-22T13:21:09Z","title":"ECHO: Efficient Off-Chain Payments and Cross-Chain Swaps for\n  Cryptocurrencies","summary":"  In this paper, we present ECHO, a TEE-based layer-2 solution that tackles two\ncrucial challenges in the realm of cryptocurrencies: off-chain payments and\ncross-chain swaps. It offers three notable features: - Channel-free off-chain\npayments: it allows a payer to make direct payments to anyone without requiring\nany on-chain relationship or intermediary channels. - Real-time yet\ndecentralized cross-chain swaps: it is the first known solution that enables\nreal-time cross-chain swaps without relying on a central server. This novel\nfeature is made possible through a ground-breaking fair exchange protocol. -\nTEE crash-tolerance: it offers two solutions to handle TEE crashes, one of\nwhich involves an innovative application of time-lock puzzles in this context.\nWe evaluate ECHO on a network consists of 1000 nodes and the evaluation results\nshow that ECHO can achieve 7000 TPS\n","authors":["Di Wu","Jian Liu","Zhengwei Hou","Wu Wen","Kui Ren"],"pdf_url":"https://arxiv.org/pdf/2403.15191v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14903v1","updated":"2024-03-22T01:21:06Z","published":"2024-03-22T01:21:06Z","title":"Modeling Distributed Computing Infrastructures for HEP Applications","summary":"  Predicting the performance of various infrastructure design options in\ncomplex federated infrastructures with computing sites distributed over a wide\narea network that support a plethora of users and workflows, such as the\nWorldwide LHC Computing Grid (WLCG), is not trivial. Due to the complexity and\nsize of these infrastructures, it is not feasible to deploy experimental\ntest-beds at large scales merely for the purpose of comparing and evaluating\nalternate designs. An alternative is to study the behaviours of these systems\nusing simulation. This approach has been used successfully in the past to\nidentify efficient and practical infrastructure designs for High Energy Physics\n(HEP). A prominent example is the Monarc simulation framework, which was used\nto study the initial structure of the WLCG. New simulation capabilities are\nneeded to simulate large-scale heterogeneous computing systems with complex\nnetworks, data access and caching patterns. A modern tool to simulate HEP\nworkloads that execute on distributed computing infrastructures based on the\nSimGrid and WRENCH simulation frameworks is outlined. Studies of its accuracy\nand scalability are presented using HEP as a case-study. Hypothetical\nadjustments to prevailing computing architectures in HEP are studied providing\ninsights into the dynamics of a part of the WLCG and candidates for\nimprovements.\n","authors":["Maximilian Horzela","Henri Casanova","Manuel Giffels","Artur Gottmann","Robin Hofsaess","Günter Quast","Simone Rossi Tisbeni","Achim Streit","Frédéric Suter"],"pdf_url":"https://arxiv.org/pdf/2403.14903v1.pdf","comment":"To appear in Proceedings of the 26th International Conference on\n  Computing in High Energy & Nuclear Physics (CHEP2023)"},{"id":"http://arxiv.org/abs/2403.15582v1","updated":"2024-03-22T19:14:04Z","published":"2024-03-22T19:14:04Z","title":"Fast real-time arbitrary waveform generation using graphic processing\n  units","summary":"  Real-time Arbitrary Waveform Generation (AWG) is essential in various\nengineering and research applications, and often requires complex bespoke\nhardware and software. This paper introduces an AWG framework using an NVIDIA\nGraphics Processing Unit (GPU) and a commercially available high-speed\nDigital-to-Analog Converter (DAC) card, both running on a desktop personal\ncomputer (PC). The GPU accelerates the \"embarrassingly\" data parallel additive\nwaveform synthesis framework for AWG, and the DAC reconstructs the generated\nwaveform in the analog domain at high speed. The AWG framework is programmed\nusing the developer-friendly Compute Unified Device Architecture (CUDA) runtime\napplication programming interface from NVIDIA and is readily customizable, and\nscalable with additional parallel hardware. We present and characterize two\ndifferent pathways for computing modulated radio-frequency (rf) waveforms: one\npathway offers high-complexity simultaneous chirping of 1000 individual\nNyquist-limited single-frequency tones for 35 ms at a sampling rate of 560\nMB/s, and the other pathway allows simultaneous continuous chirping of 194\nindividual Nyquist-limited single-frequency tones at 100 MB/s, or 20 individual\ntones at 560 MB/s. This AWG framework is designed for fast on-the-fly\nrearrangement of a large stochastically-loaded optical tweezer array of single\natoms or molecules into a defect-free array needed for quantum simulation and\nquantum computation applications.\n","authors":["Juntian Tu","Sarthak Subhankar"],"pdf_url":"https://arxiv.org/pdf/2403.15582v1.pdf","comment":"13 pages, 10 figures"}],"Artificial Intelligence":[{"id":"http://arxiv.org/abs/2403.11879v3","updated":"2024-03-22T10:08:51Z","published":"2024-03-18T15:32:02Z","title":"Unimodal Multi-Task Fusion for Emotional Mimicry Prediction","summary":"  In this study, we propose a methodology for the Emotional Mimicry Intensity\n(EMI) Estimation task within the context of the 6th Workshop and Competition on\nAffective Behavior Analysis in-the-wild. Our approach leverages the Wav2Vec 2.0\nframework, pre-trained on a comprehensive podcast dataset, to extract a broad\nrange of audio features encompassing both linguistic and paralinguistic\nelements. We enhance feature representation through a fusion technique that\nintegrates individual features with a global mean vector, introducing global\ncontextual insights into our analysis. Additionally, we incorporate a\npre-trained valence-arousal-dominance (VAD) module from the Wav2Vec 2.0 model.\nOur fusion employs a Long Short-Term Memory (LSTM) architecture for efficient\ntemporal analysis of audio data. Utilizing only the provided audio data, our\napproach demonstrates significant improvements over the established baseline.\n","authors":["Tobias Hallmen","Fabian Deuser","Norbert Oswald","Elisabeth André"],"pdf_url":"https://arxiv.org/pdf/2403.11879v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14468v2","updated":"2024-03-22T02:16:40Z","published":"2024-03-21T15:15:00Z","title":"AnyV2V: A Plug-and-Play Framework For Any Video-to-Video Editing Tasks","summary":"  Video-to-video editing involves editing a source video along with additional\ncontrol (such as text prompts, subjects, or styles) to generate a new video\nthat aligns with the source video and the provided control. Traditional methods\nhave been constrained to certain editing types, limiting their ability to meet\nthe wide range of user demands. In this paper, we introduce AnyV2V, a novel\ntraining-free framework designed to simplify video editing into two primary\nsteps: (1) employing an off-the-shelf image editing model (e.g.\nInstructPix2Pix, InstantID, etc) to modify the first frame, (2) utilizing an\nexisting image-to-video generation model (e.g. I2VGen-XL) for DDIM inversion\nand feature injection. In the first stage, AnyV2V can plug in any existing\nimage editing tools to support an extensive array of video editing tasks.\nBeyond the traditional prompt-based editing methods, AnyV2V also can support\nnovel video editing tasks, including reference-based style transfer,\nsubject-driven editing, and identity manipulation, which were unattainable by\nprevious methods. In the second stage, AnyV2V can plug in any existing\nimage-to-video models to perform DDIM inversion and intermediate feature\ninjection to maintain the appearance and motion consistency with the source\nvideo. On the prompt-based editing, we show that AnyV2V can outperform the\nprevious best approach by 35\\% on prompt alignment, and 25\\% on human\npreference. On the three novel tasks, we show that AnyV2V also achieves a high\nsuccess rate. We believe AnyV2V will continue to thrive due to its ability to\nseamlessly integrate the fast-evolving image editing methods. Such\ncompatibility can help AnyV2V to increase its versatility to cater to diverse\nuser demands.\n","authors":["Max Ku","Cong Wei","Weiming Ren","Harry Yang","Wenhu Chen"],"pdf_url":"https://arxiv.org/pdf/2403.14468v2.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2403.15388v1","updated":"2024-03-22T17:59:52Z","published":"2024-03-22T17:59:52Z","title":"LLaVA-PruMerge: Adaptive Token Reduction for Efficient Large Multimodal\n  Models","summary":"  Large Multimodal Models (LMMs) have shown significant reasoning capabilities\nby connecting a visual encoder and a large language model. LMMs typically use a\nfixed amount of visual tokens, such as the penultimate layer features in the\nCLIP visual encoder, as the prefix content. Recent LMMs incorporate more\ncomplex visual inputs, such as high-resolution images and videos, which\nincrease the number of visual tokens significantly. However, due to the design\nof the Transformer architecture, computational costs associated with these\nmodels tend to increase quadratically with the number of input tokens. To\ntackle this problem, we explore a token reduction mechanism and find, similar\nto prior work, that many visual tokens are spatially redundant. Based on this,\nwe propose PruMerge, a novel adaptive visual token reduction approach, which\nlargely reduces the number of visual tokens while maintaining comparable model\nperformance. We first select the unpruned visual tokens based on their\nsimilarity to class tokens and spatial tokens. We then cluster the pruned\ntokens based on key similarity and merge the clustered tokens with the unpruned\ntokens to supplement their information. Empirically, when applied to LLaVA-1.5,\nour approach can compress the visual tokens by 14.4 times on average, and\nachieve comparable performance across diverse visual question-answering and\nreasoning tasks. Code and checkpoints are at https://llava-prumerge.github.io/.\n","authors":["Yuzhang Shang","Mu Cai","Bingxin Xu","Yong Jae Lee","Yan Yan"],"pdf_url":"https://arxiv.org/pdf/2403.15388v1.pdf","comment":"Project page: https://llava-prumerge.github.io/"},{"id":"http://arxiv.org/abs/2403.15385v1","updated":"2024-03-22T17:59:37Z","published":"2024-03-22T17:59:37Z","title":"LATTE3D: Large-scale Amortized Text-To-Enhanced3D Synthesis","summary":"  Recent text-to-3D generation approaches produce impressive 3D results but\nrequire time-consuming optimization that can take up to an hour per prompt.\nAmortized methods like ATT3D optimize multiple prompts simultaneously to\nimprove efficiency, enabling fast text-to-3D synthesis. However, they cannot\ncapture high-frequency geometry and texture details and struggle to scale to\nlarge prompt sets, so they generalize poorly. We introduce LATTE3D, addressing\nthese limitations to achieve fast, high-quality generation on a significantly\nlarger prompt set. Key to our method is 1) building a scalable architecture and\n2) leveraging 3D data during optimization through 3D-aware diffusion priors,\nshape regularization, and model initialization to achieve robustness to diverse\nand complex training prompts. LATTE3D amortizes both neural field and textured\nsurface generation to produce highly detailed textured meshes in a single\nforward pass. LATTE3D generates 3D objects in 400ms, and can be further\nenhanced with fast test-time optimization.\n","authors":["Kevin Xie","Jonathan Lorraine","Tianshi Cao","Jun Gao","James Lucas","Antonio Torralba","Sanja Fidler","Xiaohui Zeng"],"pdf_url":"https://arxiv.org/pdf/2403.15385v1.pdf","comment":"See the project website at\n  https://research.nvidia.com/labs/toronto-ai/LATTE3D/"},{"id":"http://arxiv.org/abs/2403.08763v2","updated":"2024-03-22T17:56:38Z","published":"2024-03-13T17:58:57Z","title":"Simple and Scalable Strategies to Continually Pre-train Large Language\n  Models","summary":"  Large language models (LLMs) are routinely pre-trained on billions of tokens,\nonly to start the process over again once new data becomes available. A much\nmore efficient solution is to continually pre-train these models, saving\nsignificant compute compared to re-training. However, the distribution shift\ninduced by new data typically results in degraded performance on previous data\nor poor adaptation to the new data. In this work, we show that a simple and\nscalable combination of learning rate (LR) re-warming, LR re-decaying, and\nreplay of previous data is sufficient to match the performance of fully\nre-training from scratch on all available data, as measured by final loss and\nlanguage model (LM) evaluation benchmarks. Specifically, we show this for a\nweak but realistic distribution shift between two commonly used LLM\npre-training datasets (English$\\rightarrow$English) and a stronger distribution\nshift (English$\\rightarrow$German) at the $405$M parameter model scale with\nlarge dataset sizes (hundreds of billions of tokens). Selecting the weak but\nrealistic shift for larger-scale experiments, we also find that our continual\nlearning strategies match the re-training baseline for a 10B parameter LLM. Our\nresults demonstrate that LLMs can be successfully updated via simple and\nscalable continual learning strategies, matching the re-training baseline using\nonly a fraction of the compute. Finally, inspired by previous work, we propose\nalternatives to the cosine learning rate schedule that help circumvent\nforgetting induced by LR re-warming and that are not bound to a fixed token\nbudget.\n","authors":["Adam Ibrahim","Benjamin Thérien","Kshitij Gupta","Mats L. Richter","Quentin Anthony","Timothée Lesort","Eugene Belilovsky","Irina Rish"],"pdf_url":"https://arxiv.org/pdf/2403.08763v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.00652v2","updated":"2024-03-22T17:56:05Z","published":"2023-03-01T16:54:48Z","title":"Finding the right XAI method -- A Guide for the Evaluation and Ranking\n  of Explainable AI Methods in Climate Science","summary":"  Explainable artificial intelligence (XAI) methods shed light on the\npredictions of machine learning algorithms. Several different approaches exist\nand have already been applied in climate science. However, usually missing\nground truth explanations complicate their evaluation and comparison,\nsubsequently impeding the choice of the XAI method. Therefore, in this work, we\nintroduce XAI evaluation in the climate context and discuss different desired\nexplanation properties, namely robustness, faithfulness, randomization,\ncomplexity, and localization. To this end, we chose previous work as a case\nstudy where the decade of annual-mean temperature maps is predicted. After\ntraining both a multi-layer perceptron (MLP) and a convolutional neural network\n(CNN), multiple XAI methods are applied and their skill scores in reference to\na random uniform explanation are calculated for each property. Independent of\nthe network, we find that XAI methods Integrated Gradients, layer-wise\nrelevance propagation, and input times gradients exhibit considerable\nrobustness, faithfulness, and complexity while sacrificing randomization\nperformance. Sensitivity methods -- gradient, SmoothGrad, NoiseGrad, and\nFusionGrad, match the robustness skill but sacrifice faithfulness and\ncomplexity for randomization skill. We find architecture-dependent performance\ndifferences regarding robustness, complexity and localization skills of\ndifferent XAI methods, highlighting the necessity for research task-specific\nevaluation. Overall, our work offers an overview of different evaluation\nproperties in the climate science context and shows how to compare and\nbenchmark different explanation methods, assessing their suitability based on\nstrengths and weaknesses, for the specific research problem at hand. By that,\nwe aim to support climate researchers in the selection of a suitable XAI\nmethod.\n","authors":["Philine Bommer","Marlene Kretschmer","Anna Hedström","Dilyara Bareeva","Marina M. -C. Höhne"],"pdf_url":"https://arxiv.org/pdf/2303.00652v2.pdf","comment":"19 pages, 10 figure, accepted at AIES journal by AMS"},{"id":"http://arxiv.org/abs/2403.15371v1","updated":"2024-03-22T17:50:43Z","published":"2024-03-22T17:50:43Z","title":"Can large language models explore in-context?","summary":"  We investigate the extent to which contemporary Large Language Models (LLMs)\ncan engage in exploration, a core capability in reinforcement learning and\ndecision making. We focus on native performance of existing LLMs, without\ntraining interventions. We deploy LLMs as agents in simple multi-armed bandit\nenvironments, specifying the environment description and interaction history\nentirely in-context, i.e., within the LLM prompt. We experiment with GPT-3.5,\nGPT-4, and Llama2, using a variety of prompt designs, and find that the models\ndo not robustly engage in exploration without substantial interventions: i)\nAcross all of our experiments, only one configuration resulted in satisfactory\nexploratory behavior: GPT-4 with chain-of-thought reasoning and an externally\nsummarized interaction history, presented as sufficient statistics; ii) All\nother configurations did not result in robust exploratory behavior, including\nthose with chain-of-thought reasoning but unsummarized history. Although these\nfindings can be interpreted positively, they suggest that external\nsummarization -- which may not be possible in more complex settings -- is\nimportant for obtaining desirable behavior from LLM agents. We conclude that\nnon-trivial algorithmic interventions, such as fine-tuning or dataset curation,\nmay be required to empower LLM-based decision making agents in complex\nsettings.\n","authors":["Akshay Krishnamurthy","Keegan Harris","Dylan J. Foster","Cyril Zhang","Aleksandrs Slivkins"],"pdf_url":"https://arxiv.org/pdf/2403.15371v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14617v2","updated":"2024-03-22T17:45:52Z","published":"2024-03-21T17:59:03Z","title":"Videoshop: Localized Semantic Video Editing with Noise-Extrapolated\n  Diffusion Inversion","summary":"  We introduce Videoshop, a training-free video editing algorithm for localized\nsemantic edits. Videoshop allows users to use any editing software, including\nPhotoshop and generative inpainting, to modify the first frame; it\nautomatically propagates those changes, with semantic, spatial, and temporally\nconsistent motion, to the remaining frames. Unlike existing methods that enable\nedits only through imprecise textual instructions, Videoshop allows users to\nadd or remove objects, semantically change objects, insert stock photos into\nvideos, etc. with fine-grained control over locations and appearance. We\nachieve this through image-based video editing by inverting latents with noise\nextrapolation, from which we generate videos conditioned on the edited image.\nVideoshop produces higher quality edits against 6 baselines on 2 editing\nbenchmarks using 10 evaluation metrics.\n","authors":["Xiang Fan","Anand Bhattad","Ranjay Krishna"],"pdf_url":"https://arxiv.org/pdf/2403.14617v2.pdf","comment":"Project page at https://videoshop-editing.github.io/"},{"id":"http://arxiv.org/abs/2312.00812v4","updated":"2024-03-22T17:29:01Z","published":"2023-11-28T03:13:09Z","title":"Empowering Autonomous Driving with Large Language Models: A Safety\n  Perspective","summary":"  Autonomous Driving (AD) encounters significant safety hurdles in long-tail\nunforeseen driving scenarios, largely stemming from the non-interpretability\nand poor generalization of the deep neural networks within the AD system,\nparticularly in out-of-distribution and uncertain data. To this end, this paper\nexplores the integration of Large Language Models (LLMs) into AD systems,\nleveraging their robust common-sense knowledge and reasoning abilities. The\nproposed methodologies employ LLMs as intelligent decision-makers in behavioral\nplanning, augmented with a safety verifier shield for contextual safety\nlearning, for enhancing driving performance and safety. We present two key\nstudies in a simulated environment: an adaptive LLM-conditioned Model\nPredictive Control (MPC) and an LLM-enabled interactive behavior planning\nscheme with a state machine. Demonstrating superior performance and safety\nmetrics compared to state-of-the-art approaches, our approach shows the\npromising potential for using LLMs for autonomous vehicles.\n","authors":["Yixuan Wang","Ruochen Jiao","Sinong Simon Zhan","Chengtian Lang","Chao Huang","Zhaoran Wang","Zhuoran Yang","Qi Zhu"],"pdf_url":"https://arxiv.org/pdf/2312.00812v4.pdf","comment":"Accepted to LLMAgent workshop @ICLR2024"},{"id":"http://arxiv.org/abs/2309.12276v3","updated":"2024-03-22T17:28:17Z","published":"2023-09-21T17:37:01Z","title":"LLMR: Real-time Prompting of Interactive Worlds using Large Language\n  Models","summary":"  We present Large Language Model for Mixed Reality (LLMR), a framework for the\nreal-time creation and modification of interactive Mixed Reality experiences\nusing LLMs. LLMR leverages novel strategies to tackle difficult cases where\nideal training data is scarce, or where the design goal requires the synthesis\nof internal dynamics, intuitive analysis, or advanced interactivity. Our\nframework relies on text interaction and the Unity game engine. By\nincorporating techniques for scene understanding, task planning,\nself-debugging, and memory management, LLMR outperforms the standard GPT-4 by\n4x in average error rate. We demonstrate LLMR's cross-platform interoperability\nwith several example worlds, and evaluate it on a variety of creation and\nmodification tasks to show that it can produce and edit diverse objects, tools,\nand scenes. Finally, we conducted a usability study (N=11) with a diverse set\nthat revealed participants had positive experiences with the system and would\nuse it again.\n","authors":["Fernanda De La Torre","Cathy Mengying Fang","Han Huang","Andrzej Banburski-Fahey","Judith Amores Fernandez","Jaron Lanier"],"pdf_url":"https://arxiv.org/pdf/2309.12276v3.pdf","comment":"46 pages, 18 figures; Matching version accepted at CHI 2024"},{"id":"http://arxiv.org/abs/2309.16512v4","updated":"2024-03-22T17:26:53Z","published":"2023-09-28T15:19:30Z","title":"From Complexity to Clarity: Analytical Expressions of Deep Neural\n  Network Weights via Clifford's Geometric Algebra and Convexity","summary":"  In this paper, we introduce a novel analysis of neural networks based on\ngeometric (Clifford) algebra and convex optimization. We show that optimal\nweights of deep ReLU neural networks are given by the wedge product of training\nsamples when trained with standard regularized loss. Furthermore, the training\nproblem reduces to convex optimization over wedge product features, which\nencode the geometric structure of the training dataset. This structure is given\nin terms of signed volumes of triangles and parallelotopes generated by data\nvectors. The convex problem finds a small subset of samples via $\\ell_1$\nregularization to discover only relevant wedge product features. Our analysis\nprovides a novel perspective on the inner workings of deep neural networks and\nsheds light on the role of the hidden layers.\n","authors":["Mert Pilanci"],"pdf_url":"https://arxiv.org/pdf/2309.16512v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15362v1","updated":"2024-03-22T17:26:05Z","published":"2024-03-22T17:26:05Z","title":"CoLLEGe: Concept Embedding Generation for Large Language Models","summary":"  Current language models are unable to quickly learn new concepts on the fly,\noften requiring a more involved finetuning process to learn robustly. Prompting\nin-context is not robust to context distractions, and often fails to confer\nmuch information about the new concepts. Classic methods for few-shot word\nlearning in NLP, relying on global word vectors, are less applicable to large\nlanguage models. In this paper, we introduce a novel approach named CoLLEGe\n(Concept Learning with Language Embedding Generation) to modernize few-shot\nconcept learning. CoLLEGe is a meta-learning framework capable of generating\nflexible embeddings for new concepts using a small number of example sentences\nor definitions. Our primary meta-learning objective is simply to facilitate a\nlanguage model to make next word predictions in forthcoming sentences, making\nit compatible with language model pretraining. We design a series of tasks to\ntest new concept learning in challenging real-world scenarios, including new\nword acquisition, definition inference, and verbal reasoning, and demonstrate\nthat our method succeeds in each setting without task-specific training.\n","authors":["Ryan Teehan","Brenden Lake","Mengye Ren"],"pdf_url":"https://arxiv.org/pdf/2403.15362v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.17543v2","updated":"2024-03-22T17:12:49Z","published":"2023-12-29T10:18:36Z","title":"Building Efficient Universal Classifiers with Natural Language Inference","summary":"  Generative Large Language Models (LLMs) have become the mainstream choice for\nfewshot and zeroshot learning thanks to the universality of text generation.\nMany users, however, do not need the broad capabilities of generative LLMs when\nthey only want to automate a classification task. Smaller BERT-like models can\nalso learn universal tasks, which allow them to do any text classification task\nwithout requiring fine-tuning (zeroshot classification) or to learn new tasks\nwith only a few examples (fewshot), while being significantly more efficient\nthan generative LLMs. This paper (1) explains how Natural Language Inference\n(NLI) can be used as a universal classification task that follows similar\nprinciples as instruction fine-tuning of generative LLMs, (2) provides a\nstep-by-step guide with reusable Jupyter notebooks for building a universal\nclassifier, and (3) shares the resulting universal classifier that is trained\non 33 datasets with 389 diverse classes. Parts of the code we share has been\nused to train our older zeroshot classifiers that have been downloaded more\nthan 55 million times via the Hugging Face Hub as of December 2023. Our new\nclassifier improves zeroshot performance by 9.4%.\n","authors":["Moritz Laurer","Wouter van Atteveldt","Andreu Casas","Kasper Welbers"],"pdf_url":"https://arxiv.org/pdf/2312.17543v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.14125v3","updated":"2024-03-22T17:06:53Z","published":"2023-12-21T18:46:41Z","title":"VideoPoet: A Large Language Model for Zero-Shot Video Generation","summary":"  We present VideoPoet, a language model capable of synthesizing high-quality\nvideo, with matching audio, from a large variety of conditioning signals.\nVideoPoet employs a decoder-only transformer architecture that processes\nmultimodal inputs -- including images, videos, text, and audio. The training\nprotocol follows that of Large Language Models (LLMs), consisting of two\nstages: pretraining and task-specific adaptation. During pretraining, VideoPoet\nincorporates a mixture of multimodal generative objectives within an\nautoregressive Transformer framework. The pretrained LLM serves as a foundation\nthat can be adapted for a range of video generation tasks. We present empirical\nresults demonstrating the model's state-of-the-art capabilities in zero-shot\nvideo generation, specifically highlighting VideoPoet's ability to generate\nhigh-fidelity motions. Project page: http://sites.research.google/videopoet/\n","authors":["Dan Kondratyuk","Lijun Yu","Xiuye Gu","José Lezama","Jonathan Huang","Grant Schindler","Rachel Hornung","Vighnesh Birodkar","Jimmy Yan","Ming-Chang Chiu","Krishna Somandepalli","Hassan Akbari","Yair Alon","Yong Cheng","Josh Dillon","Agrim Gupta","Meera Hahn","Anja Hauth","David Hendon","Alonso Martinez","David Minnen","Mikhail Sirotenko","Kihyuk Sohn","Xuan Yang","Hartwig Adam","Ming-Hsuan Yang","Irfan Essa","Huisheng Wang","David A. Ross","Bryan Seybold","Lu Jiang"],"pdf_url":"https://arxiv.org/pdf/2312.14125v3.pdf","comment":"Project page: http://sites.research.google/videopoet/"},{"id":"http://arxiv.org/abs/2403.15341v1","updated":"2024-03-22T16:50:56Z","published":"2024-03-22T16:50:56Z","title":"Collaborative AI Teaming in Unknown Environments via Active Goal\n  Deduction","summary":"  With the advancements of artificial intelligence (AI), we're seeing more\nscenarios that require AI to work closely with other agents, whose goals and\nstrategies might not be known beforehand. However, existing approaches for\ntraining collaborative agents often require defined and known reward signals\nand cannot address the problem of teaming with unknown agents that often have\nlatent objectives/rewards. In response to this challenge, we propose teaming\nwith unknown agents framework, which leverages kernel density Bayesian inverse\nlearning method for active goal deduction and utilizes pre-trained,\ngoal-conditioned policies to enable zero-shot policy adaptation. We prove that\nunbiased reward estimates in our framework are sufficient for optimal teaming\nwith unknown agents. We further evaluate the framework of redesigned\nmulti-agent particle and StarCraft II micromanagement environments with diverse\nunknown agents of different behaviors/rewards. Empirical results demonstrate\nthat our framework significantly advances the teaming performance of AI and\nunknown agents in a wide range of collaborative scenarios.\n","authors":["Zuyuan Zhang","Hanhan Zhou","Mahdi Imani","Taeyoung Lee","Tian Lan"],"pdf_url":"https://arxiv.org/pdf/2403.15341v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.00094v2","updated":"2024-03-22T16:38:34Z","published":"2023-11-30T13:07:19Z","title":"Fast ODE-based Sampling for Diffusion Models in Around 5 Steps","summary":"  Sampling from diffusion models can be treated as solving the corresponding\nordinary differential equations (ODEs), with the aim of obtaining an accurate\nsolution with as few number of function evaluations (NFE) as possible.\nRecently, various fast samplers utilizing higher-order ODE solvers have emerged\nand achieved better performance than the initial first-order one. However,\nthese numerical methods inherently result in certain approximation errors,\nwhich significantly degrades sample quality with extremely small NFE (e.g.,\naround 5). In contrast, based on the geometric observation that each sampling\ntrajectory almost lies in a two-dimensional subspace embedded in the ambient\nspace, we propose Approximate MEan-Direction Solver (AMED-Solver) that\neliminates truncation errors by directly learning the mean direction for fast\ndiffusion sampling. Besides, our method can be easily used as a plugin to\nfurther improve existing ODE-based samplers. Extensive experiments on image\nsynthesis with the resolution ranging from 32 to 512 demonstrate the\neffectiveness of our method. With only 5 NFE, we achieve 6.61 FID on CIFAR-10,\n10.74 FID on ImageNet 64$\\times$64, and 13.20 FID on LSUN Bedroom. Our code is\navailable at https://github.com/zju-pi/diff-sampler.\n","authors":["Zhenyu Zhou","Defang Chen","Can Wang","Chun Chen"],"pdf_url":"https://arxiv.org/pdf/2312.00094v2.pdf","comment":"Accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2403.15325v1","updated":"2024-03-22T16:30:58Z","published":"2024-03-22T16:30:58Z","title":"A Technological Perspective on Misuse of Available AI","summary":"  Potential malicious misuse of civilian artificial intelligence (AI) poses\nserious threats to security on a national and international level. Besides\ndefining autonomous systems from a technological viewpoint and explaining how\nAI development is characterized, we show how already existing and openly\navailable AI technology could be misused. To underline this, we developed three\nexemplary use cases of potentially misused AI that threaten political, digital\nand physical security. The use cases can be built from existing AI technologies\nand components from academia, the private sector and the developer-community.\nThis shows how freely available AI can be combined into autonomous weapon\nsystems. Based on the use cases, we deduce points of control and further\nmeasures to prevent the potential threat through misused AI. Further, we\npromote the consideration of malicious misuse of civilian AI systems in the\ndiscussion on autonomous weapon systems (AWS).\n","authors":["Lukas Pöhler","Valentin Schrader","Alexander Ladwein","Florian von Keller"],"pdf_url":"https://arxiv.org/pdf/2403.15325v1.pdf","comment":"Presented at the UN Meeting of the Group of Governmental Experts on\n  Lethal Autonomous Weapons Systems, 30 August 2018"},{"id":"http://arxiv.org/abs/2310.08731v2","updated":"2024-03-22T16:30:48Z","published":"2023-10-12T21:38:07Z","title":"Novelty Detection in Reinforcement Learning with World Models","summary":"  Reinforcement learning (RL) using world models has found significant recent\nsuccesses. However, when a sudden change to world mechanics or properties\noccurs then agent performance and reliability can dramatically decline. We\nrefer to the sudden change in visual properties or state transitions as\nnovelties. Implementing novelty detection within generated world model\nframeworks is a crucial task for protecting the agent when deployed. In this\npaper, we propose straightforward bounding approaches to incorporate novelty\ndetection into world model RL agents, by utilizing the misalignment of the\nworld model's hallucinated states and the true observed states as an anomaly\nscore. We provide effective approaches to detecting novelties in a distribution\nof transitions learned by an agent in a world model. Finally, we show the\nadvantage of our work in a novel environment compared to traditional machine\nlearning novelty detection methods as well as currently accepted RL focused\nnovelty detection algorithms.\n","authors":["Geigh Zollicoffer","Kenneth Eaton","Jonathan Balloch","Julia Kim","Mark O. Riedl","Robert Wright"],"pdf_url":"https://arxiv.org/pdf/2310.08731v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.04690v2","updated":"2024-03-22T16:26:40Z","published":"2024-03-07T17:35:58Z","title":"Faster Neighborhood Attention: Reducing the O(n^2) Cost of Self\n  Attention at the Threadblock Level","summary":"  Neighborhood attention reduces the cost of self attention by restricting each\ntoken's attention span to its nearest neighbors. This restriction,\nparameterized by a window size and dilation factor, draws a spectrum of\npossible attention patterns between linear projection and self attention.\nNeighborhood attention, and more generally sliding window attention patterns,\nhave long been bounded by infrastructure, particularly in higher-rank spaces\n(2-D and 3-D), calling for the development of custom kernels, which have been\nlimited in either functionality, or performance, if not both. In this work, we\nfirst show that neighborhood attention can be represented as a batched GEMM\nproblem, similar to standard attention, and implement it for 1-D and 2-D\nneighborhood attention. These kernels on average provide 895% and 272%\nimprovement in full precision latency compared to existing naive kernels for\n1-D and 2-D neighborhood attention respectively. We find certain inherent\ninefficiencies in all unfused neighborhood attention kernels that bound their\nperformance and lower-precision scalability. We also developed fused\nneighborhood attention; an adaptation of fused dot-product attention kernels\nthat allow fine-grained control over attention across different spatial axes.\nKnown for reducing the quadratic time complexity of self attention to a linear\ncomplexity, neighborhood attention can now enjoy a reduced and constant memory\nfootprint, and record-breaking half precision latency. We observe that our\nfused kernels successfully circumvent some of the unavoidable inefficiencies in\nunfused implementations. While our unfused GEMM-based kernels only improve half\nprecision performance compared to naive kernels by an average of 496% and 113%\nin 1-D and 2-D problems respectively, our fused kernels improve naive kernels\nby an average of 1607% and 581% in 1-D and 2-D problems respectively.\n","authors":["Ali Hassani","Wen-Mei Hwu","Humphrey Shi"],"pdf_url":"https://arxiv.org/pdf/2403.04690v2.pdf","comment":"Project page: https://github.com/SHI-Labs/NATTEN"},{"id":"http://arxiv.org/abs/2403.15317v1","updated":"2024-03-22T16:11:29Z","published":"2024-03-22T16:11:29Z","title":"Point-DETR3D: Leveraging Imagery Data with Spatial Point Prior for\n  Weakly Semi-supervised 3D Object Detection","summary":"  Training high-accuracy 3D detectors necessitates massive labeled 3D\nannotations with 7 degree-of-freedom, which is laborious and time-consuming.\nTherefore, the form of point annotations is proposed to offer significant\nprospects for practical applications in 3D detection, which is not only more\naccessible and less expensive but also provides strong spatial information for\nobject localization.In this paper, we empirically discover that it is\nnon-trivial to merely adapt Point-DETR to its 3D form, encountering two main\nbottlenecks: 1) it fails to encode strong 3D prior into the model, and 2) it\ngenerates low-quality pseudo labels in distant regions due to the extreme\nsparsity of LiDAR points. To overcome these challenges, we introduce\nPoint-DETR3D, a teacher-student framework for weakly semi-supervised 3D\ndetection, designed to fully capitalize on point-wise supervision within a\nconstrained instance-wise annotation budget.Different from Point-DETR which\nencodes 3D positional information solely through a point encoder, we propose an\nexplicit positional query initialization strategy to enhance the positional\nprior. Considering the low quality of pseudo labels at distant regions produced\nby the teacher model, we enhance the detector's perception by incorporating\ndense imagery data through a novel Cross-Modal Deformable RoI Fusion\n(D-RoI).Moreover, an innovative point-guided self-supervised learning technique\nis proposed to allow for fully exploiting point priors, even in student\nmodels.Extensive experiments on representative nuScenes dataset demonstrate our\nPoint-DETR3D obtains significant improvements compared to previous works.\nNotably, with only 5% of labeled data, Point-DETR3D achieves over 90%\nperformance of its fully supervised counterpart.\n","authors":["Hongzhi Gao","Zheng Chen","Zehui Chen","Lin Chen","Jiaming Liu","Shanghang Zhang","Feng Zhao"],"pdf_url":"https://arxiv.org/pdf/2403.15317v1.pdf","comment":"Accepted by AAAI2024"},{"id":"http://arxiv.org/abs/2403.15313v1","updated":"2024-03-22T16:06:05Z","published":"2024-03-22T16:06:05Z","title":"CR3DT: Camera-RADAR Fusion for 3D Detection and Tracking","summary":"  Accurate detection and tracking of surrounding objects is essential to enable\nself-driving vehicles. While Light Detection and Ranging (LiDAR) sensors have\nset the benchmark for high performance, the appeal of camera-only solutions\nlies in their cost-effectiveness. Notably, despite the prevalent use of Radio\nDetection and Ranging (RADAR) sensors in automotive systems, their potential in\n3D detection and tracking has been largely disregarded due to data sparsity and\nmeasurement noise. As a recent development, the combination of RADARs and\ncameras is emerging as a promising solution. This paper presents Camera-RADAR\n3D Detection and Tracking (CR3DT), a camera-RADAR fusion model for 3D object\ndetection, and Multi-Object Tracking (MOT). Building upon the foundations of\nthe State-of-the-Art (SotA) camera-only BEVDet architecture, CR3DT demonstrates\nsubstantial improvements in both detection and tracking capabilities, by\nincorporating the spatial and velocity information of the RADAR sensor.\nExperimental results demonstrate an absolute improvement in detection\nperformance of 5.3% in mean Average Precision (mAP) and a 14.9% increase in\nAverage Multi-Object Tracking Accuracy (AMOTA) on the nuScenes dataset when\nleveraging both modalities. CR3DT bridges the gap between high-performance and\ncost-effective perception systems in autonomous driving, by capitalizing on the\nubiquitous presence of RADAR in automotive applications.\n","authors":["Nicolas Baumann","Michael Baumgartner","Edoardo Ghignone","Jonas Kühne","Tobias Fischer","Yung-Hsu Yang","Marc Pollefeys","Michele Magno"],"pdf_url":"https://arxiv.org/pdf/2403.15313v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10581v2","updated":"2024-03-22T16:00:24Z","published":"2024-03-15T13:25:09Z","title":"Large Language Model-informed ECG Dual Attention Network for Heart\n  Failure Risk Prediction","summary":"  Heart failure (HF) poses a significant public health challenge, with a rising\nglobal mortality rate. Early detection and prevention of HF could significantly\nreduce its impact. We introduce a novel methodology for predicting HF risk\nusing 12-lead electrocardiograms (ECGs). We present a novel, lightweight\ndual-attention ECG network designed to capture complex ECG features essential\nfor early HF risk prediction, despite the notable imbalance between low and\nhigh-risk groups. This network incorporates a cross-lead attention module and\ntwelve lead-specific temporal attention modules, focusing on cross-lead\ninteractions and each lead's local dynamics. To further alleviate model\noverfitting, we leverage a large language model (LLM) with a public ECG-Report\ndataset for pretraining on an ECG-report alignment task. The network is then\nfine-tuned for HF risk prediction using two specific cohorts from the UK\nBiobank study, focusing on patients with hypertension (UKB-HYP) and those who\nhave had a myocardial infarction (UKB-MI).The results reveal that LLM-informed\npre-training substantially enhances HF risk prediction in these cohorts. The\ndual-attention design not only improves interpretability but also predictive\naccuracy, outperforming existing competitive methods with C-index scores of\n0.6349 for UKB-HYP and 0.5805 for UKB-MI. This demonstrates our method's\npotential in advancing HF risk assessment with clinical complex ECG data.\n","authors":["Chen Chen","Lei Li","Marcel Beetz","Abhirup Banerjee","Ramneek Gupta","Vicente Grau"],"pdf_url":"https://arxiv.org/pdf/2403.10581v2.pdf","comment":"Under journal revision"},{"id":"http://arxiv.org/abs/2403.13969v2","updated":"2024-03-22T15:57:20Z","published":"2024-03-20T20:46:41Z","title":"\"This is not a data problem\": Algorithms and Power in Public Higher\n  Education in Canada","summary":"  Algorithmic decision-making is increasingly being adopted across public\nhigher education. The expansion of data-driven practices by post-secondary\ninstitutions has occurred in parallel with the adoption of New Public\nManagement approaches by neoliberal administrations. In this study, we conduct\na qualitative analysis of an in-depth ethnographic case study of data and\nalgorithms in use at a public college in Ontario, Canada. We identify the data,\nalgorithms, and outcomes in use at the college. We assess how the college's\nprocesses and relationships support those outcomes and the different\nstakeholders' perceptions of the college's data-driven systems. In addition, we\nfind that the growing reliance on algorithmic decisions leads to increased\nstudent surveillance, exacerbation of existing inequities, and the automation\nof the faculty-student relationship. Finally, we identify a cycle of increased\ninstitutional power perpetuated by algorithmic decision-making, and driven by a\npush towards financial sustainability.\n","authors":["Kelly McConvey","Shion Guha"],"pdf_url":"https://arxiv.org/pdf/2403.13969v2.pdf","comment":"In CHI '24 Proceedings of the CHI Conference on Human Factors in\n  Computing Systems Honolulu, HI, USA"},{"id":"http://arxiv.org/abs/2403.15304v1","updated":"2024-03-22T15:54:30Z","published":"2024-03-22T15:54:30Z","title":"KTbench: A Novel Data Leakage-Free Framework for Knowledge Tracing","summary":"  Knowledge Tracing (KT) is concerned with predicting students' future\nperformance on learning items in intelligent tutoring systems. Learning items\nare tagged with skill labels called knowledge concepts (KCs). Many KT models\nexpand the sequence of item-student interactions into KC-student interactions\nby replacing learning items with their constituting KCs. This often results in\na longer sequence length. This approach addresses the issue of sparse\nitem-student interactions and minimises model parameters. However, two problems\nhave been identified with such models.\n  The first problem is the model's ability to learn correlations between KCs\nbelonging to the same item, which can result in the leakage of ground truth\nlabels and hinder performance. This problem can lead to a significant decrease\nin performance on datasets with a higher number of KCs per item. The second\nproblem is that the available benchmark implementations ignore accounting for\nchanges in sequence length when expanding KCs, leading to different models\nbeing tested with varying sequence lengths but still compared against the same\nbenchmark.\n  To address these problems, we introduce a general masking framework that\nmitigates the first problem and enhances the performance of such KT models\nwhile preserving the original model architecture without significant\nalterations. Additionally, we introduce KTbench, an open-source benchmark\nlibrary designed to ensure the reproducibility of this work while mitigating\nthe second problem.\n","authors":["Yahya Badran","Christine Preisach"],"pdf_url":"https://arxiv.org/pdf/2403.15304v1.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2307.08309v3","updated":"2024-03-22T15:52:47Z","published":"2023-07-17T08:09:40Z","title":"LogPrécis: Unleashing Language Models for Automated Malicious Log\n  Analysis","summary":"  The collection of security-related logs holds the key to understanding attack\nbehaviors and diagnosing vulnerabilities. Still, their analysis remains a\ndaunting challenge. Recently, Language Models (LMs) have demonstrated unmatched\npotential in understanding natural and programming languages. The question\narises whether and how LMs could be also useful for security experts since\ntheir logs contain intrinsically confused and obfuscated information. In this\npaper, we systematically study how to benefit from the state-of-the-art in LM\nto automatically analyze text-like Unix shell attack logs. We present a\nthorough design methodology that leads to LogPr\\'ecis. It receives as input raw\nshell sessions and automatically identifies and assigns the attacker tactic to\neach portion of the session, i.e., unveiling the sequence of the attacker's\ngoals. We demonstrate LogPr\\'ecis capability to support the analysis of two\nlarge datasets containing about 400,000 unique Unix shell attacks. LogPr\\'ecis\nreduces them into about 3,000 fingerprints, each grouping sessions with the\nsame sequence of tactics. The abstraction it provides lets the analyst better\nunderstand attacks, identify fingerprints, detect novelty, link similar\nattacks, and track families and mutations. Overall, LogPr\\'ecis, released as\nopen source, paves the way for better and more responsive defense against\ncyberattacks.\n","authors":["Matteo Boffa","Rodolfo Vieira Valentim","Luca Vassio","Danilo Giordano","Idilio Drago","Marco Mellia","Zied Ben Houidi"],"pdf_url":"https://arxiv.org/pdf/2307.08309v3.pdf","comment":"18 pages, Computer&Security\n  (https://www.sciencedirect.com/science/article/pii/S0167404824001068), code\n  available at https://github.com/SmartData-Polito/logprecis, models available\n  at https://huggingface.co/SmartDataPolito"},{"id":"http://arxiv.org/abs/2403.15301v1","updated":"2024-03-22T15:51:39Z","published":"2024-03-22T15:51:39Z","title":"Planning with a Learned Policy Basis to Optimally Solve Complex Tasks","summary":"  Conventional reinforcement learning (RL) methods can successfully solve a\nwide range of sequential decision problems. However, learning policies that can\ngeneralize predictably across multiple tasks in a setting with non-Markovian\nreward specifications is a challenging problem. We propose to use successor\nfeatures to learn a policy basis so that each (sub)policy in it solves a\nwell-defined subproblem. In a task described by a finite state automaton (FSA)\nthat involves the same set of subproblems, the combination of these\n(sub)policies can then be used to generate an optimal solution without\nadditional learning. In contrast to other methods that combine (sub)policies\nvia planning, our method asymptotically attains global optimality, even in\nstochastic environments.\n","authors":["Guillermo Infante","David Kuric","Anders Jonsson","Vicenç Gómez","Herke van Hoof"],"pdf_url":"https://arxiv.org/pdf/2403.15301v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15297v1","updated":"2024-03-22T15:44:59Z","published":"2024-03-22T15:44:59Z","title":"Sphere Neural-Networks for Rational Reasoning","summary":"  The success of Large Language Models (LLMs), e.g., ChatGPT, is witnessed by\ntheir planetary popularity, their capability of human-like question-answering,\nand also by their steadily improved reasoning performance. However, it remains\nunclear whether LLMs reason. It is an open problem how traditional neural\nnetworks can be qualitatively extended to go beyond the statistic paradigm and\nachieve high-level cognition. Here, we present a minimalist qualitative\nextension by generalising computational building blocks from vectors to\nspheres. We propose Sphere Neural Networks (SphNNs) for human-like reasoning\nthrough model construction and inspection, and develop SphNN for syllogistic\nreasoning, a microcosm of human rationality. Instead of training data, SphNN\nuses a neuro-symbolic transition map of neighbourhood spatial relations to\nguide transformations from the current sphere configuration towards the target.\nSphNN is the first neural model that can determine the validity of long-chained\nsyllogistic reasoning in one epoch by constructing sphere configurations as\nEuler diagrams, with the worst computational complexity of O(N^2). SphNN can\nevolve into various types of reasoning, such as spatio-temporal reasoning,\nlogical reasoning with negation and disjunction, event reasoning,\nneuro-symbolic reasoning, and humour understanding (the highest level of\ncognition). All these suggest a new kind of Herbert A. Simon's scissors with\ntwo neural blades. SphNNs will tremendously enhance interdisciplinary\ncollaborations to develop the two neural blades and realise deterministic\nneural reasoning and human-bounded rationality and elevate LLMs to reliable\npsychological AI. This work suggests that the non-zero radii of spheres are the\nmissing components that prevent traditional deep-learning systems from reaching\nthe realm of rational reasoning and cause LLMs to be trapped in the swamp of\nhallucination.\n","authors":["Tiansi Dong","Mateja Jamnik","Pietro Liò"],"pdf_url":"https://arxiv.org/pdf/2403.15297v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09530v2","updated":"2024-03-22T15:26:05Z","published":"2024-03-14T16:13:00Z","title":"VisionGPT-3D: A Generalized Multimodal Agent for Enhanced 3D Vision\n  Understanding","summary":"  The evolution of text to visual components facilitates people's daily lives,\nsuch as generating image, videos from text and identifying the desired elements\nwithin the images. Computer vision models involving the multimodal abilities in\nthe previous days are focused on image detection, classification based on\nwell-defined objects. Large language models (LLMs) introduces the\ntransformation from nature language to visual objects, which present the visual\nlayout for text contexts. OpenAI GPT-4 has emerged as the pinnacle in LLMs,\nwhile the computer vision (CV) domain boasts a plethora of state-of-the-art\n(SOTA) models and algorithms to convert 2D images to their 3D representations.\nHowever, the mismatching between the algorithms with the problem could lead to\nundesired results. In response to this challenge, we propose an unified\nVisionGPT-3D framework to consolidate the state-of-the-art vision models,\nthereby facilitating the development of vision-oriented AI. VisionGPT-3D\nprovides a versatile multimodal framework building upon the strengths of\nmultimodal foundation models. It seamlessly integrates various SOTA vision\nmodels and brings the automation in the selection of SOTA vision models,\nidentifies the suitable 3D mesh creation algorithms corresponding to 2D depth\nmaps analysis, generates optimal results based on diverse multimodal inputs\nsuch as text prompts.\n  Keywords: VisionGPT-3D, 3D vision understanding, Multimodal agent\n","authors":["Chris Kelly","Luhui Hu","Jiayin Hu","Yu Tian","Deshun Yang","Bang Yang","Cindy Yang","Zihao Li","Zaoshan Huang","Yuexian Zou"],"pdf_url":"https://arxiv.org/pdf/2403.09530v2.pdf","comment":"12 pages, 7 figures, pending conference"},{"id":"http://arxiv.org/abs/2403.15274v1","updated":"2024-03-22T15:16:23Z","published":"2024-03-22T15:16:23Z","title":"Bioinformatics and Biomedical Informatics with ChatGPT: Year One Review","summary":"  The year 2023 marked a significant surge in the exploration of applying large\nlanguage model (LLM) chatbots, notably ChatGPT, across various disciplines. We\nsurveyed the applications of ChatGPT in various sectors of bioinformatics and\nbiomedical informatics throughout the year, covering omics, genetics,\nbiomedical text mining, drug discovery, biomedical image understanding,\nbioinformatics programming, and bioinformatics education. Our survey delineates\nthe current strengths and limitations of this chatbot in bioinformatics and\noffers insights into potential avenues for future development.\n","authors":["Jinge Wang","Zien Cheng","Qiuming Yao","Li Liu","Dong Xu","Gangqing Hu"],"pdf_url":"https://arxiv.org/pdf/2403.15274v1.pdf","comment":"19 pages, 3 Figures, 1 Table"},{"id":"http://arxiv.org/abs/2403.15257v1","updated":"2024-03-22T14:57:27Z","published":"2024-03-22T14:57:27Z","title":"Hierarchical Information Enhancement Network for Cascade Prediction in\n  Social Networks","summary":"  Understanding information cascades in networks is a fundamental issue in\nnumerous applications. Current researches often sample cascade information into\nseveral independent paths or subgraphs to learn a simple cascade\nrepresentation. However, these approaches fail to exploit the hierarchical\nsemantic associations between different modalities, limiting their predictive\nperformance. In this work, we propose a novel Hierarchical Information\nEnhancement Network (HIENet) for cascade prediction. Our approach integrates\nfundamental cascade sequence, user social graphs, and sub-cascade graph into a\nunified framework. Specifically, HIENet utilizes DeepWalk to sample cascades\ninformation into a series of sequences. It then gathers path information\nbetween users to extract the social relationships of propagators. Additionally,\nwe employ a time-stamped graph convolutional network to aggregate sub-cascade\ngraph information effectively. Ultimately, we introduce a Multi-modal Cascade\nTransformer to powerfully fuse these clues, providing a comprehensive\nunderstanding of cascading process. Extensive experiments have demonstrated the\neffectiveness of the proposed method.\n","authors":["Fanrui Zhang","Jiawei Liu","Qiang Zhang","Xiaoling Zhu","Zheng-Jun Zha"],"pdf_url":"https://arxiv.org/pdf/2403.15257v1.pdf","comment":"7 pages, 2 figures"},{"id":"http://arxiv.org/abs/2403.15251v1","updated":"2024-03-22T14:49:49Z","published":"2024-03-22T14:49:49Z","title":"Safe Learning of PDDL Domains with Conditional Effects -- Extended\n  Version","summary":"  Powerful domain-independent planners have been developed to solve various\ntypes of planning problems. These planners often require a model of the acting\nagent's actions, given in some planning domain description language. Manually\ndesigning such an action model is a notoriously challenging task. An\nalternative is to automatically learn action models from observation. Such an\naction model is called safe if every plan created with it is consistent with\nthe real, unknown action model. Algorithms for learning such safe action models\nexist, yet they cannot handle domains with conditional or universal effects,\nwhich are common constructs in many planning problems. We prove that learning\nnon-trivial safe action models with conditional effects may require an\nexponential number of samples. Then, we identify reasonable assumptions under\nwhich such learning is tractable and propose SAM Learning of Conditional\nEffects (Conditional-SAM), the first algorithm capable of doing so. We analyze\nConditional-SAM theoretically and evaluate it experimentally. Our results show\nthat the action models learned by Conditional-SAM can be used to solve\nperfectly most of the test set problems in most of the experimented domains.\n","authors":["Argaman Mordoch","Enrico Scala","Roni Stern","Brendan Juba"],"pdf_url":"https://arxiv.org/pdf/2403.15251v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.04025v3","updated":"2024-03-22T14:49:31Z","published":"2023-08-08T03:43:24Z","title":"MSAC: Multiple Speech Attribute Control Method for Reliable Speech\n  Emotion Recognition","summary":"  Despite notable progress, speech emotion recognition (SER) remains\nchallenging due to the intricate and ambiguous nature of speech emotion,\nparticularly in wild world. While current studies primarily focus on\nrecognition and generalization abilities, our research pioneers an\ninvestigation into the reliability of SER methods in the presence of semantic\ndata shifts and explores how to exert fine-grained control over various\nattributes inherent in speech signals to enhance speech emotion modeling. In\nthis paper, we first introduce MSAC-SERNet, a novel unified SER framework\ncapable of simultaneously handling both single-corpus and cross-corpus SER.\nSpecifically, concentrating exclusively on the speech emotion attribute, a\nnovel CNN-based SER model is presented to extract discriminative emotional\nrepresentations, guided by additive margin softmax loss. Considering\ninformation overlap between various speech attributes, we propose a novel\nlearning paradigm based on correlations of different speech attributes, termed\nMultiple Speech Attribute Control (MSAC), which empowers the proposed SER model\nto simultaneously capture fine-grained emotion-related features while\nmitigating the negative impact of emotion-agnostic representations.\nFurthermore, we make a first attempt to examine the reliability of the\nMSAC-SERNet framework using out-of-distribution detection methods. Experiments\non both single-corpus and cross-corpus SER scenarios indicate that MSAC-SERNet\nnot only consistently outperforms the baseline in all aspects, but achieves\nsuperior performance compared to state-of-the-art SER approaches.\n","authors":["Yu Pan","Yuguang Yang","Yuheng Huang","Jixun Yao","Jingjing Yin","Yanni Hu","Heng Lu","Lei Ma","Jianjun Zhao"],"pdf_url":"https://arxiv.org/pdf/2308.04025v3.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2403.15250v1","updated":"2024-03-22T14:47:35Z","published":"2024-03-22T14:47:35Z","title":"Comprehensive Reassessment of Large-Scale Evaluation Outcomes in LLMs: A\n  Multifaceted Statistical Approach","summary":"  Amidst the rapid evolution of LLMs, the significance of evaluation in\ncomprehending and propelling these models forward is increasingly paramount.\nEvaluations have revealed that factors such as scaling, training types,\narchitectures and other factors profoundly impact the performance of LLMs.\nHowever, the extent and nature of these impacts continue to be subjects of\ndebate because most assessments have been restricted to a limited number of\nmodels and data points. Clarifying the effects of these factors on performance\nscores can be more effectively achieved through a statistical lens. Our study\nembarks on a thorough re-examination of these LLMs, targeting the inadequacies\nin current evaluation methods. With the advent of a uniform evaluation\nframework, our research leverages an expansive dataset of evaluation results,\nintroducing a comprehensive statistical methodology. This includes the\napplication of ANOVA, Tukey HSD tests, GAMM, and clustering technique, offering\na robust and transparent approach to deciphering LLM performance data. Contrary\nto prevailing findings, our results challenge assumptions about emergent\nabilities and the influence of given training types and architectures in LLMs.\nThese findings furnish new perspectives on the characteristics, intrinsic\nnature, and developmental trajectories of LLMs. By providing straightforward\nand reliable methods to scrutinize and reassess LLM performance data, this\nstudy contributes a nuanced perspective on LLM efficiency and potentials.\n","authors":["Kun Sun","Rong Wang","Haitao Liu","Anders Søgaard"],"pdf_url":"https://arxiv.org/pdf/2403.15250v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15249v1","updated":"2024-03-22T14:47:18Z","published":"2024-03-22T14:47:18Z","title":"Spectral Motion Alignment for Video Motion Transfer using Diffusion\n  Models","summary":"  The evolution of diffusion models has greatly impacted video generation and\nunderstanding. Particularly, text-to-video diffusion models (VDMs) have\nsignificantly facilitated the customization of input video with target\nappearance, motion, etc. Despite these advances, challenges persist in\naccurately distilling motion information from video frames. While existing\nworks leverage the consecutive frame residual as the target motion vector, they\ninherently lack global motion context and are vulnerable to frame-wise\ndistortions. To address this, we present Spectral Motion Alignment (SMA), a\nnovel framework that refines and aligns motion vectors using Fourier and\nwavelet transforms. SMA learns motion patterns by incorporating\nfrequency-domain regularization, facilitating the learning of whole-frame\nglobal motion dynamics, and mitigating spatial artifacts. Extensive experiments\ndemonstrate SMA's efficacy in improving motion transfer while maintaining\ncomputational efficiency and compatibility across various video customization\nframeworks.\n","authors":["Geon Yeong Park","Hyeonho Jeong","Sang Wan Lee","Jong Chul Ye"],"pdf_url":"https://arxiv.org/pdf/2403.15249v1.pdf","comment":"Project page:\n  https://geonyeong-park.github.io/spectral-motion-alignment/"},{"id":"http://arxiv.org/abs/2403.15248v1","updated":"2024-03-22T14:46:51Z","published":"2024-03-22T14:46:51Z","title":"Self-Supervised Backbone Framework for Diverse Agricultural Vision Tasks","summary":"  Computer vision in agriculture is game-changing with its ability to transform\nfarming into a data-driven, precise, and sustainable industry. Deep learning\nhas empowered agriculture vision to analyze vast, complex visual data, but\nheavily rely on the availability of large annotated datasets. This remains a\nbottleneck as manual labeling is error-prone, time-consuming, and expensive.\nThe lack of efficient labeling approaches inspired us to consider\nself-supervised learning as a paradigm shift, learning meaningful feature\nrepresentations from raw agricultural image data. In this work, we explore how\nself-supervised representation learning unlocks the potential applicability to\ndiverse agriculture vision tasks by eliminating the need for large-scale\nannotated datasets. We propose a lightweight framework utilizing SimCLR, a\ncontrastive learning approach, to pre-train a ResNet-50 backbone on a large,\nunannotated dataset of real-world agriculture field images. Our experimental\nanalysis and results indicate that the model learns robust features applicable\nto a broad range of downstream agriculture tasks discussed in the paper.\nAdditionally, the reduced reliance on annotated data makes our approach more\ncost-effective and accessible, paving the way for broader adoption of computer\nvision in agriculture.\n","authors":["Sudhir Sornapudi","Rajhans Singh"],"pdf_url":"https://arxiv.org/pdf/2403.15248v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05752v2","updated":"2024-03-22T14:44:17Z","published":"2024-03-09T01:17:26Z","title":"Task-Oriented GNNs Training on Large Knowledge Graphs for Accurate and\n  Efficient Modeling","summary":"  A Knowledge Graph (KG) is a heterogeneous graph encompassing a diverse range\nof node and edge types. Heterogeneous Graph Neural Networks (HGNNs) are popular\nfor training machine learning tasks like node classification and link\nprediction on KGs. However, HGNN methods exhibit excessive complexity\ninfluenced by the KG's size, density, and the number of node and edge types. AI\npractitioners handcraft a subgraph of a KG G relevant to a specific task. We\nrefer to this subgraph as a task-oriented subgraph (TOSG), which contains a\nsubset of task-related node and edge types in G. Training the task using TOSG\ninstead of G alleviates the excessive computation required for a large KG.\nCrafting the TOSG demands a deep understanding of the KG's structure and the\ntask's objectives. Hence, it is challenging and time-consuming. This paper\nproposes KG-TOSA, an approach to automate the TOSG extraction for task-oriented\nHGNN training on a large KG. In KG-TOSA, we define a generic graph pattern that\ncaptures the KG's local and global structure relevant to a specific task. We\nexplore different techniques to extract subgraphs matching our graph pattern:\nnamely (i) two techniques sampling around targeted nodes using biased random\nwalk or influence scores, and (ii) a SPARQL-based extraction method leveraging\nRDF engines' built-in indices. Hence, it achieves negligible preprocessing\noverhead compared to the sampling techniques. We develop a benchmark of real\nKGs of large sizes and various tasks for node classification and link\nprediction. Our experiments show that KG-TOSA helps state-of-the-art HGNN\nmethods reduce training time and memory usage by up to 70% while improving the\nmodel performance, e.g., accuracy and inference time.\n","authors":["Hussein Abdallah","Waleed Afandi","Panos Kalnis","Essam Mansour"],"pdf_url":"https://arxiv.org/pdf/2403.05752v2.pdf","comment":"12 pages,9 Figures, 3 Tables, ICDE:2024"},{"id":"http://arxiv.org/abs/2403.15245v1","updated":"2024-03-22T14:41:55Z","published":"2024-03-22T14:41:55Z","title":"Reasoning-Enhanced Object-Centric Learning for Videos","summary":"  Object-centric learning aims to break down complex visual scenes into more\nmanageable object representations, enhancing the understanding and reasoning\nabilities of machine learning systems toward the physical world. Recently,\nslot-based video models have demonstrated remarkable proficiency in segmenting\nand tracking objects, but they overlook the importance of the effective\nreasoning module. In the real world, reasoning and predictive abilities play a\ncrucial role in human perception and object tracking; in particular, these\nabilities are closely related to human intuitive physics. Inspired by this, we\ndesigned a novel reasoning module called the Slot-based Time-Space Transformer\nwith Memory buffer (STATM) to enhance the model's perception ability in complex\nscenes. The memory buffer primarily serves as storage for slot information from\nupstream modules, the Slot-based Time-Space Transformer makes predictions\nthrough slot-based spatiotemporal attention computations and fusion. Our\nexperiment results on various datasets show that STATM can significantly\nenhance object-centric learning capabilities of slot-based video models.\n","authors":["Jian Li","Pu Ren","Yang Liu","Hao Sun"],"pdf_url":"https://arxiv.org/pdf/2403.15245v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15235v1","updated":"2024-03-22T14:29:03Z","published":"2024-03-22T14:29:03Z","title":"Multi-perspective Memory Enhanced Network for Identifying Key Nodes in\n  Social Networks","summary":"  Identifying key nodes in social networks plays a crucial role in timely\nblocking false information. Existing key node identification methods usually\nconsider node influence only from the propagation structure perspective and\nhave insufficient generalization ability to unknown scenarios. In this paper,\nwe propose a novel Multi-perspective Memory Enhanced Network (MMEN) for\nidentifying key nodes in social networks, which mines key nodes from multiple\nperspectives and utilizes memory networks to store historical information.\nSpecifically, MMEN first constructs two propagation networks from the\nperspectives of user attributes and propagation structure and updates node\nfeature representations using graph attention networks. Meanwhile, the memory\nnetwork is employed to store information of similar subgraphs, enhancing the\nmodel's generalization performance in unknown scenarios. Finally, MMEN applies\nadaptive weights to combine the node influence of the two propagation networks\nto select the ultimate key nodes. Extensive experiments demonstrate that our\nmethod significantly outperforms previous methods.\n","authors":["Qiang Zhang","Jiawei Liu","Fanrui Zhang","Xiaoling Zhu","Zheng-Jun Zha"],"pdf_url":"https://arxiv.org/pdf/2403.15235v1.pdf","comment":"7 pages, 1 figures"},{"id":"http://arxiv.org/abs/2206.00759v3","updated":"2024-03-22T14:13:56Z","published":"2022-06-01T20:48:24Z","title":"Interpretability Guarantees with Merlin-Arthur Classifiers","summary":"  We propose an interactive multi-agent classifier that provides provable\ninterpretability guarantees even for complex agents such as neural networks.\nThese guarantees consist of lower bounds on the mutual information between\nselected features and the classification decision. Our results are inspired by\nthe Merlin-Arthur protocol from Interactive Proof Systems and express these\nbounds in terms of measurable metrics such as soundness and completeness.\nCompared to existing interactive setups, we rely neither on optimal agents nor\non the assumption that features are distributed independently. Instead, we use\nthe relative strength of the agents as well as the new concept of Asymmetric\nFeature Correlation which captures the precise kind of correlations that make\ninterpretability guarantees difficult. We evaluate our results on two\nsmall-scale datasets where high mutual information can be verified explicitly.\n","authors":["Stephan Wäldchen","Kartikey Sharma","Berkant Turan","Max Zimmer","Sebastian Pokutta"],"pdf_url":"https://arxiv.org/pdf/2206.00759v3.pdf","comment":"AISTATS24 Camera-Ready Version, 34 pages total (9 pages main part, 3\n  pages references, 22 pages appendix), 17 figures, 3 tables"},{"id":"http://arxiv.org/abs/2403.15218v1","updated":"2024-03-22T14:07:07Z","published":"2024-03-22T14:07:07Z","title":"Anytime, Anywhere, Anyone: Investigating the Feasibility of Segment\n  Anything Model for Crowd-Sourcing Medical Image Annotations","summary":"  Curating annotations for medical image segmentation is a labor-intensive and\ntime-consuming task that requires domain expertise, resulting in \"narrowly\"\nfocused deep learning (DL) models with limited translational utility. Recently,\nfoundation models like the Segment Anything Model (SAM) have revolutionized\nsemantic segmentation with exceptional zero-shot generalizability across\nvarious domains, including medical imaging, and hold a lot of promise for\nstreamlining the annotation process. However, SAM has yet to be evaluated in a\ncrowd-sourced setting to curate annotations for training 3D DL segmentation\nmodels. In this work, we explore the potential of SAM for crowd-sourcing\n\"sparse\" annotations from non-experts to generate \"dense\" segmentation masks\nfor training 3D nnU-Net models, a state-of-the-art DL segmentation model. Our\nresults indicate that while SAM-generated annotations exhibit high mean Dice\nscores compared to ground-truth annotations, nnU-Net models trained on\nSAM-generated annotations perform significantly worse than nnU-Net models\ntrained on ground-truth annotations ($p<0.001$, all).\n","authors":["Pranav Kulkarni","Adway Kanhere","Dharmam Savani","Andrew Chan","Devina Chatterjee","Paul H. Yi","Vishwa S. Parekh"],"pdf_url":"https://arxiv.org/pdf/2403.15218v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15216v1","updated":"2024-03-22T14:03:37Z","published":"2024-03-22T14:03:37Z","title":"(Un)making AI Magic: a Design Taxonomy","summary":"  This paper examines the role that enchantment plays in the design of AI\nthings by constructing a taxonomy of design approaches that increase or\ndecrease the perception of magic and enchantment. We start from the design\ndiscourse surrounding recent developments in AI technologies, highlighting\nspecific interaction qualities such as algorithmic uncertainties and errors and\narticulating relations to the rhetoric of magic and supernatural thinking.\nThrough analyzing and reflecting upon 52 students' design projects from two\neditions of a Master course in design and AI, we identify seven design\nprinciples and unpack the effects of each in terms of enchantment and\ndisenchantment. We conclude by articulating ways in which this taxonomy can be\napproached and appropriated by design/HCI practitioners, especially to support\nexploration and reflexivity.\n","authors":["Maria Luce Lupetti","Dave Murray-Rust"],"pdf_url":"https://arxiv.org/pdf/2403.15216v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10482v2","updated":"2024-03-22T13:59:34Z","published":"2024-03-15T17:12:57Z","title":"Can a GPT4-Powered AI Agent Be a Good Enough Performance Attribution\n  Analyst?","summary":"  Performance attribution analysis, defined as the process of explaining the\ndrivers of the excess performance of an investment portfolio against a\nbenchmark, stands as a significant feature of portfolio management and plays a\ncrucial role in the investment decision-making process, particularly within the\nfund management industry. Rooted in a solid financial and mathematical\nframework, the importance and methodologies of this analytical technique are\nextensively documented across numerous academic research papers and books. The\nintegration of large language models (LLMs) and AI agents marks a\ngroundbreaking development in this field. These agents are designed to automate\nand enhance the performance attribution analysis by accurately calculating and\nanalyzing portfolio performances against benchmarks. In this study, we\nintroduce the application of an AI Agent for a variety of essential performance\nattribution tasks, including the analysis of performance drivers and utilizing\nLLMs as calculation engine for multi-level attribution analysis and\nquestion-answering (QA) tasks. Leveraging advanced prompt engineering\ntechniques such as Chain-of-Thought (CoT) and Plan and Solve (PS), and\nemploying a standard agent framework from LangChain, the research achieves\npromising results: it achieves accuracy rates exceeding 93% in analyzing\nperformance drivers, attains 100% in multi-level attribution calculations, and\nsurpasses 84% accuracy in QA exercises that simulate official examination\nstandards. These findings affirm the impactful role of AI agents, prompt\nengineering and evaluation in advancing portfolio management processes,\nhighlighting a significant development in the practical application and\nevaluation of Generative AI technologies within the domain.\n","authors":["Bruno de Melo","Jamiel Sheikh"],"pdf_url":"https://arxiv.org/pdf/2403.10482v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15195v1","updated":"2024-03-22T13:31:24Z","published":"2024-03-22T13:31:24Z","title":"FSD-Inference: Fully Serverless Distributed Inference with Scalable\n  Cloud Communication","summary":"  Serverless computing offers attractive scalability, elasticity and\ncost-effectiveness. However, constraints on memory, CPU and function runtime\nhave hindered its adoption for data-intensive applications and machine learning\n(ML) workloads. Traditional 'server-ful' platforms enable distributed\ncomputation via fast networks and well-established inter-process communication\n(IPC) mechanisms such as MPI and shared memory. In the absence of such\nsolutions in the serverless domain, parallel computation with significant IPC\nrequirements is challenging. We present FSD-Inference, the first fully\nserverless and highly scalable system for distributed ML inference. We explore\npotential communication channels, in conjunction with Function-as-a-Service\n(FaaS) compute, to design a state-of-the-art solution for distributed ML within\nthe context of serverless data-intensive computing. We introduce novel fully\nserverless communication schemes for ML inference workloads, leveraging both\ncloud-based publish-subscribe/queueing and object storage offerings. We\ndemonstrate how publish-subscribe/queueing services can be adapted for FaaS IPC\nwith comparable performance to object storage, while offering significantly\nreduced cost at high parallelism levels. We conduct in-depth experiments on\nbenchmark DNNs of various sizes. The results show that when compared to\nserver-based alternatives, FSD-Inference is significantly more cost-effective\nand scalable, and can even achieve competitive performance against optimized\nHPC solutions. Experiments also confirm that our serverless solution can handle\nlarge distributed workloads and leverage high degrees of FaaS parallelism.\n","authors":["Joe Oakley","Hakan Ferhatosmanoglu"],"pdf_url":"https://arxiv.org/pdf/2403.15195v1.pdf","comment":"In Proceedings of 2024 IEEE 40th International Conference on Data\n  Engineering (ICDE) (to appear)"},{"id":"http://arxiv.org/abs/2312.13964v2","updated":"2024-03-22T13:25:53Z","published":"2023-12-21T15:51:12Z","title":"PIA: Your Personalized Image Animator via Plug-and-Play Modules in\n  Text-to-Image Models","summary":"  Recent advancements in personalized text-to-image (T2I) models have\nrevolutionized content creation, empowering non-experts to generate stunning\nimages with unique styles. While promising, adding realistic motions into these\npersonalized images by text poses significant challenges in preserving distinct\nstyles, high-fidelity details, and achieving motion controllability by text. In\nthis paper, we present PIA, a Personalized Image Animator that excels in\naligning with condition images, achieving motion controllability by text, and\nthe compatibility with various personalized T2I models without specific tuning.\nTo achieve these goals, PIA builds upon a base T2I model with well-trained\ntemporal alignment layers, allowing for the seamless transformation of any\npersonalized T2I model into an image animation model. A key component of PIA is\nthe introduction of the condition module, which utilizes the condition frame\nand inter-frame affinity as input to transfer appearance information guided by\nthe affinity hint for individual frame synthesis in the latent space. This\ndesign mitigates the challenges of appearance-related image alignment within\nand allows for a stronger focus on aligning with motion-related guidance.\n","authors":["Yiming Zhang","Zhening Xing","Yanhong Zeng","Youqing Fang","Kai Chen"],"pdf_url":"https://arxiv.org/pdf/2312.13964v2.pdf","comment":"Project page: https://pi-animator.github.io/"},{"id":"http://arxiv.org/abs/2403.15192v1","updated":"2024-03-22T13:24:50Z","published":"2024-03-22T13:24:50Z","title":"SFOD: Spiking Fusion Object Detector","summary":"  Event cameras, characterized by high temporal resolution, high dynamic range,\nlow power consumption, and high pixel bandwidth, offer unique capabilities for\nobject detection in specialized contexts. Despite these advantages, the\ninherent sparsity and asynchrony of event data pose challenges to existing\nobject detection algorithms. Spiking Neural Networks (SNNs), inspired by the\nway the human brain codes and processes information, offer a potential solution\nto these difficulties. However, their performance in object detection using\nevent cameras is limited in current implementations. In this paper, we propose\nthe Spiking Fusion Object Detector (SFOD), a simple and efficient approach to\nSNN-based object detection. Specifically, we design a Spiking Fusion Module,\nachieving the first-time fusion of feature maps from different scales in SNNs\napplied to event cameras. Additionally, through integrating our analysis and\nexperiments conducted during the pretraining of the backbone network on the\nNCAR dataset, we delve deeply into the impact of spiking decoding strategies\nand loss functions on model performance. Thereby, we establish state-of-the-art\nclassification results based on SNNs, achieving 93.7\\% accuracy on the NCAR\ndataset. Experimental results on the GEN1 detection dataset demonstrate that\nthe SFOD achieves a state-of-the-art mAP of 32.1\\%, outperforming existing\nSNN-based approaches. Our research not only underscores the potential of SNNs\nin object detection with event cameras but also propels the advancement of\nSNNs. Code is available at https://github.com/yimeng-fan/SFOD.\n","authors":["Yimeng Fan","Wei Zhang","Changsong Liu","Mingyang Li","Wenrui Lu"],"pdf_url":"https://arxiv.org/pdf/2403.15192v1.pdf","comment":"Accepted by CVPR2024"},{"id":"http://arxiv.org/abs/2306.14899v2","updated":"2024-03-22T13:24:35Z","published":"2023-06-26T17:59:55Z","title":"FunQA: Towards Surprising Video Comprehension","summary":"  Surprising videos, such as funny clips, creative performances, or visual\nillusions, attract significant attention. Enjoyment of these videos is not\nsimply a response to visual stimuli; rather, it hinges on the human capacity to\nunderstand (and appreciate) commonsense violations depicted in these videos. We\nintroduce FunQA, a challenging video question-answering (QA) dataset\nspecifically designed to evaluate and enhance the depth of video reasoning\nbased on counter-intuitive and fun videos. Unlike most video QA benchmarks\nwhich focus on less surprising contexts, e.g., cooking or instructional videos,\nFunQA covers three previously unexplored types of surprising videos: 1)\nHumorQA, 2) CreativeQA, and 3) MagicQA. For each subset, we establish rigorous\nQA tasks designed to assess the model's capability in counter-intuitive\ntimestamp localization, detailed video description, and reasoning around\ncounter-intuitiveness. We also pose higher-level tasks, such as attributing a\nfitting and vivid title to the video and scoring the video creativity. In\ntotal, the FunQA benchmark consists of 312K free-text QA pairs derived from\n4.3K video clips, spanning a total of 24 video hours. Moreover, we propose\nFunMentor, an agent designed for Vision-Language Models (VLMs) that uses\nmulti-turn dialogues to enhance models' understanding of counter-intuitiveness.\nExtensive experiments with existing VLMs demonstrate the effectiveness of\nFunMentor and reveal significant performance gaps for the FunQA videos across\nspatial-temporal reasoning, visual-centered reasoning, and free-text\ngeneration.\n","authors":["Binzhu Xie","Sicheng Zhang","Zitang Zhou","Bo Li","Yuanhan Zhang","Jack Hessel","Jingkang Yang","Ziwei Liu"],"pdf_url":"https://arxiv.org/pdf/2306.14899v2.pdf","comment":"Project Page: https://funqa-benchmark.github.io/ Codebase:\n  https://github.com/Jingkang50/FunQA"},{"id":"http://arxiv.org/abs/2403.15176v1","updated":"2024-03-22T13:01:10Z","published":"2024-03-22T13:01:10Z","title":"Brain-grounding of semantic vectors improves neural decoding of visual\n  stimuli","summary":"  Developing algorithms for accurate and comprehensive neural decoding of\nmental contents is one of the long-cherished goals in the field of neuroscience\nand brain-machine interfaces. Previous studies have demonstrated the\nfeasibility of neural decoding by training machine learning models to map brain\nactivity patterns into a semantic vector representation of stimuli. These\nvectors, hereafter referred as pretrained feature vectors, are usually derived\nfrom semantic spaces based solely on image and/or text features and therefore\nthey might have a totally different characteristics than how visual stimuli is\nrepresented in the human brain, resulting in limiting the capability of brain\ndecoders to learn this mapping. To address this issue, we propose a\nrepresentation learning framework, termed brain-grounding of semantic vectors,\nwhich fine-tunes pretrained feature vectors to better align with the neural\nrepresentation of visual stimuli in the human brain. We trained this model this\nmodel with functional magnetic resonance imaging (fMRI) of 150 different visual\nstimuli categories, and then performed zero-shot brain decoding and\nidentification analyses on 1) fMRI and 2) magnetoencephalography (MEG).\nInterestingly, we observed that by using the brain-grounded vectors, the brain\ndecoding and identification accuracy on brain data from different neuroimaging\nmodalities increases. These findings underscore the potential of incorporating\na richer array of brain-derived features to enhance performance of brain\ndecoding algorithms.\n","authors":["Shirin Vafaei","Ryohei Fukuma","Huixiang Yang","Haruhiko Kishima","Takufumi Yanagisawa"],"pdf_url":"https://arxiv.org/pdf/2403.15176v1.pdf","comment":"16 pages, 4 figures"},{"id":"http://arxiv.org/abs/2403.15170v1","updated":"2024-03-22T12:46:58Z","published":"2024-03-22T12:46:58Z","title":"Exploring the Task-agnostic Trait of Self-supervised Learning in the\n  Context of Detecting Mental Disorders","summary":"  Self-supervised learning (SSL) has been investigated to generate\ntask-agnostic representations across various domains. However, such\ninvestigation has not been conducted for detecting multiple mental disorders.\nThe rationale behind the existence of a task-agnostic representation lies in\nthe overlapping symptoms among multiple mental disorders. Consequently, the\nbehavioural data collected for mental health assessment may carry a mixed bag\nof attributes related to multiple disorders. Motivated by that, in this study,\nwe explore a task-agnostic representation derived through SSL in the context of\ndetecting major depressive disorder (MDD) and post-traumatic stress disorder\n(PTSD) using audio and video data collected during interactive sessions. This\nstudy employs SSL models trained by predicting multiple fixed targets or masked\nframes. We propose a list of fixed targets to make the generated representation\nmore efficient for detecting MDD and PTSD. Furthermore, we modify the\nhyper-parameters of the SSL encoder predicting fixed targets to generate global\nrepresentations that capture varying temporal contexts. Both these innovations\nare noted to yield improved detection performances for considered mental\ndisorders and exhibit task-agnostic traits. In the context of the SSL model\npredicting masked frames, the generated global representations are also noted\nto exhibit task-agnostic traits.\n","authors":["Rohan Kumar Gupta","Rohit Sinha"],"pdf_url":"https://arxiv.org/pdf/2403.15170v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15167v1","updated":"2024-03-22T12:37:14Z","published":"2024-03-22T12:37:14Z","title":"Transition Graph Properties of Target Class Classification","summary":"  Target class classification is a mixed classification and transition model\nwhose integrated goal is to assign objects to a certain, so called target or\nnormal class. The classification process is iterative, and in each step an\nobject in a certain class undergoes an action attached to that class,\ninitiating the transition of the object to one of the classes. The sequence of\ntransitions, which we call class transitions, must be designed to provide the\nfinal assignment of objects to the target class. The transition process can be\ndescribed in the form of a directed graph, and the success of the final\nclassification is mainly due to the properties of this graph. In our previous\nresearch we showed that the desirable structure of the transition graph is an\noriented rooted tree with orientation towards the root vertex, which\ncorresponds to the normal class. It is clear that the transition graph of an\narbitrary algorithm (policy) may not have this property. In this paper we study\nthe structure of realistic transition graphs, which makes it possible to find\nclassification inconsistencies, helping to transfer it into the desired form.\nThe medical interpretation of dynamic treatment regime considered in the\narticle further clarifies the investigated framework.\n","authors":["Levon Aslanyan","Hasmik Sahakyan"],"pdf_url":"https://arxiv.org/pdf/2403.15167v1.pdf","comment":"14pages, 4 figures"},{"id":"http://arxiv.org/abs/2312.16427v3","updated":"2024-03-22T12:05:02Z","published":"2023-12-27T06:23:29Z","title":"Learning to Embed Time Series Patches Independently","summary":"  Masked time series modeling has recently gained much attention as a\nself-supervised representation learning strategy for time series. Inspired by\nmasked image modeling in computer vision, recent works first patchify and\npartially mask out time series, and then train Transformers to capture the\ndependencies between patches by predicting masked patches from unmasked\npatches. However, we argue that capturing such patch dependencies might not be\nan optimal strategy for time series representation learning; rather, learning\nto embed patches independently results in better time series representations.\nSpecifically, we propose to use 1) the simple patch reconstruction task, which\nautoencode each patch without looking at other patches, and 2) the simple\npatch-wise MLP that embeds each patch independently. In addition, we introduce\ncomplementary contrastive learning to hierarchically capture adjacent time\nseries information efficiently. Our proposed method improves time series\nforecasting and classification performance compared to state-of-the-art\nTransformer-based models, while it is more efficient in terms of the number of\nparameters and training/inference time. Code is available at this repository:\nhttps://github.com/seunghan96/pits.\n","authors":["Seunghan Lee","Taeyoung Park","Kibok Lee"],"pdf_url":"https://arxiv.org/pdf/2312.16427v3.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2312.16424v3","updated":"2024-03-22T12:02:42Z","published":"2023-12-27T06:15:00Z","title":"Soft Contrastive Learning for Time Series","summary":"  Contrastive learning has shown to be effective to learn representations from\ntime series in a self-supervised way. However, contrasting similar time series\ninstances or values from adjacent timestamps within a time series leads to\nignore their inherent correlations, which results in deteriorating the quality\nof learned representations. To address this issue, we propose SoftCLT, a simple\nyet effective soft contrastive learning strategy for time series. This is\nachieved by introducing instance-wise and temporal contrastive loss with soft\nassignments ranging from zero to one. Specifically, we define soft assignments\nfor 1) instance-wise contrastive loss by the distance between time series on\nthe data space, and 2) temporal contrastive loss by the difference of\ntimestamps. SoftCLT is a plug-and-play method for time series contrastive\nlearning that improves the quality of learned representations without bells and\nwhistles. In experiments, we demonstrate that SoftCLT consistently improves the\nperformance in various downstream tasks including classification,\nsemi-supervised learning, transfer learning, and anomaly detection, showing\nstate-of-the-art performance. Code is available at this repository:\nhttps://github.com/seunghan96/softclt.\n","authors":["Seunghan Lee","Taeyoung Park","Kibok Lee"],"pdf_url":"https://arxiv.org/pdf/2312.16424v3.pdf","comment":"ICLR 2024 Spotlight"},{"id":"http://arxiv.org/abs/2403.15143v1","updated":"2024-03-22T11:53:03Z","published":"2024-03-22T11:53:03Z","title":"Modular Deep Active Learning Framework for Image Annotation: A Technical\n  Report for the Ophthalmo-AI Project","summary":"  Image annotation is one of the most essential tasks for guaranteeing proper\ntreatment for patients and tracking progress over the course of therapy in the\nfield of medical imaging and disease diagnosis. However, manually annotating a\nlot of 2D and 3D imaging data can be extremely tedious. Deep Learning (DL)\nbased segmentation algorithms have completely transformed this process and made\nit possible to automate image segmentation. By accurately segmenting medical\nimages, these algorithms can greatly minimize the time and effort necessary for\nmanual annotation. Additionally, by incorporating Active Learning (AL) methods,\nthese segmentation algorithms can perform far more effectively with a smaller\namount of ground truth data. We introduce MedDeepCyleAL, an end-to-end\nframework implementing the complete AL cycle. It provides researchers with the\nflexibility to choose the type of deep learning model they wish to employ and\nincludes an annotation tool that supports the classification and segmentation\nof medical images. The user-friendly interface allows for easy alteration of\nthe AL and DL model settings through a configuration file, requiring no prior\nprogramming experience. While MedDeepCyleAL can be applied to any kind of image\ndata, we have specifically applied it to ophthalmology data in this project.\n","authors":["Md Abdul Kadir","Hasan Md Tusfiqur Alam","Pascale Maul","Hans-Jürgen Profitlich","Moritz Wolf","Daniel Sonntag"],"pdf_url":"https://arxiv.org/pdf/2403.15143v1.pdf","comment":"DFKI Technical Report"}],"Performance":[{"id":"http://arxiv.org/abs/2403.15303v1","updated":"2024-03-22T15:54:08Z","published":"2024-03-22T15:54:08Z","title":"Network Calculus Characterization of Congestion Control for Time-Varying\n  Traffic","summary":"  Models for the dynamics of congestion control generally involve systems of\ncoupled differential equations. Universally, these models assume that traffic\nsources saturate the maximum transmissions allowed by the congestion control\nmethod. This is not suitable for studying congestion control of intermittent\nbut bursty traffic sources. In this paper, we present a characterization of\ncongestion control for arbitrary time-varying traffic that applies to\nrate-based as well as window-based congestion control. We leverage the\ncapability of network calculus to precisely describe the input-output\nrelationship at network elements for arbitrary source traffic. We show that our\ncharacterization can closely track the dynamics of even complex congestion\ncontrol algorithms.\n","authors":["Harvinder Lehal","Natchanon Luangsomboon","Jörg Liebeherr"],"pdf_url":"https://arxiv.org/pdf/2403.15303v1.pdf","comment":null}],"Operating Systems":[{"id":"http://arxiv.org/abs/2306.11227v2","updated":"2024-03-22T00:08:00Z","published":"2023-06-20T01:30:08Z","title":"An Introduction to the Compute Express Link (CXL) Interconnect","summary":"  The Compute Express Link (CXL) is an open industry-standard interconnect\nbetween processors and devices such as accelerators, memory buffers, smart\nnetwork interfaces, persistent memory, and solid-state drives. CXL offers\ncoherency and memory semantics with bandwidth that scales with PCIe bandwidth\nwhile achieving significantly lower latency than PCIe. All major CPU vendors,\ndevice vendors, and datacenter operators have adopted CXL as a common standard.\nThis enables an inter-operable ecosystem that supports key computing use cases\nincluding highly efficient accelerators, server memory bandwidth and capacity\nexpansion, multi-server resource pooling and sharing, and efficient\npeer-to-peer communication. This survey provides an introduction to CXL\ncovering the standards CXL 1.0, CXL 2.0, and CXL 3.0. We further survey CXL\nimplementations, discuss CXL's impact on the datacenter landscape, and future\ndirections.\n","authors":["Debendra Das Sharma","Robert Blankenship","Daniel S. Berger"],"pdf_url":"https://arxiv.org/pdf/2306.11227v2.pdf","comment":null}],"Databases":[{"id":"http://arxiv.org/abs/2403.15366v1","updated":"2024-03-22T17:36:19Z","published":"2024-03-22T17:36:19Z","title":"Fourier Transform-based Estimators for Data Sketches","summary":"  In this paper we consider the problem of estimating the $f$-moment\n($\\sum_{v\\in [n]} (f(\\mathbf{x}(v))-f(0))$) of a dynamic vector $\\mathbf{x}\\in\n\\mathbb{G}^n$ over some abelian group $(\\mathbb{G},+)$, where the\n$\\|f\\|_\\infty$ norm is bounded. We propose a simple sketch and new estimation\nframework based on the \\emph{Fourier transform} of $f$. I.e., we decompose $f$\ninto a linear combination of homomorphisms $f_1,f_2,\\ldots$ from\n$(\\mathbb{G},+)$ to $(\\mathbb{C},\\times)$, estimate the $f_k$-moment for each\n$f_k$, and synthesize them to obtain an estimate of the $f$-moment. Our\nestimators are asymptotically unbiased and have variance asymptotic to\n$\\|\\mathbf{x}\\|_0^2 (\\|f\\|_\\infty^2 m^{-1} + \\|\\hat{f}\\|_1^2 m^{-2})$, where\nthe size of the sketch is $O(m\\log n\\log|\\mathbb{G}|)$ bits.\n  When $\\mathbb{G}=\\mathbb{Z}$ this problem can also be solved using\noff-the-shelf $\\ell_0$-samplers with space $O(m\\log^2 n)$ bits, which does not\nobviously generalize to finite groups. As a concrete benchmark, we extend\nGanguly, Garofalakis, and Rastogi's singleton-detector-based sampler to work\nover $\\mathbb{G}$ using $O(m\\log n\\log|\\mathbb{G}|\\log(m\\log n))$ bits.\n  We give some experimental evidence that the Fourier-based estimation\nframework is significantly more accurate than sampling-based approaches at the\nsame memory footprint.\n","authors":["Seth Pettie","Dingyu Wang"],"pdf_url":"https://arxiv.org/pdf/2403.15366v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15324v1","updated":"2024-03-22T16:25:34Z","published":"2024-03-22T16:25:34Z","title":"ProvDeploy: Provenance-oriented Containerization of High Performance\n  Computing Scientific Workflows","summary":"  Many existing scientific workflows require High Performance Computing\nenvironments to produce results in a timely manner. These workflows have\nseveral software library components and use different environments, making the\ndeployment and execution of the software stack not trivial. This complexity\nincreases if the user needs to add provenance data capture services to the\nworkflow. This manuscript introduces ProvDeploy to assist the user in\nconfiguring containers for scientific workflows with integrated provenance data\ncapture. ProvDeploy was evaluated with a Scientific Machine Learning workflow,\nexploring containerization strategies focused on provenance in two distinct HPC\nenvironments\n","authors":["Liliane Kunstmann","Débora Pina","Daniel de Oliveira","Marta Mattoso"],"pdf_url":"https://arxiv.org/pdf/2403.15324v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.00705v2","updated":"2024-03-22T15:13:17Z","published":"2024-02-01T16:00:21Z","title":"Combining the Strengths of Dutch Survey and Register Data in a Data\n  Challenge to Predict Fertility (PreFer)","summary":"  The social sciences have produced an impressive body of research on\ndeterminants of fertility outcomes, or whether and when people have children.\nHowever, the strength of these determinants and underlying theories are rarely\nevaluated on their predictive ability on new data. This prevents us from\nsystematically comparing studies, hindering the evaluation and accumulation of\nknowledge. In this paper, we present two datasets which can be used to study\nthe predictability of fertility outcomes in the Netherlands. One dataset is\nbased on the LISS panel, a longitudinal survey which includes thousands of\nvariables on a wide range of topics, including individual preferences and\nvalues. The other is based on the Dutch register data which lacks attitudinal\ndata but includes detailed information about the life courses of millions of\nDutch residents. We provide information about the datasets and the samples, and\ndescribe the fertility outcome of interest. We also introduce the fertility\nprediction data challenge PreFer which is based on these datasets and will\nstart in Spring 2024. We outline the ways in which measuring the predictability\nof fertility outcomes using these datasets and combining their strengths in the\ndata challenge can advance our understanding of fertility behaviour and\ncomputational social science. We further provide details for participants on\nhow to take part in the data challenge.\n","authors":["Elizaveta Sivak","Paulina Pankowska","Adrienne Mendrik","Tom Emery","Javier Garcia-Bernardo","Seyit Hocuk","Kasia Karpinska","Angelica Maineri","Joris Mulder","Malvina Nissim","Gert Stulp"],"pdf_url":"https://arxiv.org/pdf/2402.00705v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14902v1","updated":"2024-03-22T01:17:07Z","published":"2024-03-22T01:17:07Z","title":"Hydro: Adaptive Query Processing of ML Queries","summary":"  Query optimization in relational database management systems (DBMSs) is\ncritical for fast query processing. The query optimizer relies on precise\nselectivity and cost estimates to effectively optimize queries prior to\nexecution. While this strategy is effective for relational DBMSs, it is not\nsufficient for DBMSs tailored for processing machine learning (ML) queries. In\nML-centric DBMSs, query optimization is challenging for two reasons. First, the\nperformance bottleneck of the queries shifts to user-defined functions (UDFs)\nthat often wrap around deep learning models, making it difficult to accurately\nestimate UDF statistics without profiling the query. This leads to inaccurate\nstatistics and sub-optimal query plans. Second, the optimal query plan for ML\nqueries is data-dependent, necessitating DBMSs to adapt the query plan on the\nfly during execution. So, a static query plan is not sufficient for such\nqueries.\n  In this paper, we present Hydro, an ML-centric DBMS that utilizes adaptive\nquery processing (AQP) for efficiently processing ML queries. Hydro is designed\nto quickly evaluate UDF-based query predicates by ensuring optimal predicate\nevaluation order and improving the scalability of UDF execution. By integrating\nAQP, Hydro continuously monitors UDF statistics, routes data to predicates in\nan optimal order, and dynamically allocates resources for evaluating\npredicates. We demonstrate Hydro's efficacy through four illustrative use\ncases, delivering up to 11.52x speedup over a baseline system.\n","authors":["Gaurav Tarlok Kakkar","Jiashen Cao","Aubhro Sengupta","Joy Arulraj","Hyesoon Kim"],"pdf_url":"https://arxiv.org/pdf/2403.14902v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14128v2","updated":"2024-03-22T23:40:39Z","published":"2024-03-21T04:48:08Z","title":"Gen-T: Table Reclamation in Data Lakes","summary":"  We introduce the problem of Table Reclamation. Given a Source Table and a\nlarge table repository, reclamation finds a set of tables that, when\nintegrated, reproduce the source table as closely as possible. Unlike query\ndiscovery problems like Query-by-Example or by-Target, Table Reclamation\nfocuses on reclaiming the data in the Source Table as fully as possible using\nreal tables that may be incomplete or inconsistent. To do this, we define a new\nmeasure of table similarity, called error-aware instance similarity, to measure\nhow close a reclaimed table is to a Source Table, a measure grounded in\ninstance similarity used in data exchange. Our search covers not only\nSELECT-PROJECT- JOIN queries, but integration queries with unions, outerjoins,\nand the unary operators subsumption and complementation that have been shown to\nbe important in data integration and fusion. Using reclamation, a data\nscientist can understand if any tables in a repository can be used to exactly\nreclaim a tuple in the Source. If not, one can understand if this is due to\ndifferences in values or to incompleteness in the data. Our solution, Gen-T,\nperforms table discovery to retrieve a set of candidate tables from the table\nrepository, filters these down to a set of originating tables, then integrates\nthese tables to reclaim the Source as closely as possible. We show that our\nsolution, while approximate, is accurate, efficient and scalable in the size of\nthe table repository with experiments on real data lakes containing up to 15K\ntables, where the average number of tuples varies from small (web tables) to\nextremely large (open data tables) up to 1M tuples.\n","authors":["Grace Fan","Roee Shraga","Renée J. Miller"],"pdf_url":"https://arxiv.org/pdf/2403.14128v2.pdf","comment":"to appear at ICDE 2024"},{"id":"http://arxiv.org/abs/2403.15553v1","updated":"2024-03-22T18:08:10Z","published":"2024-03-22T18:08:10Z","title":"Efficiently Estimating Mutual Information Between Attributes Across\n  Tables","summary":"  Relational data augmentation is a powerful technique for enhancing data\nanalytics and improving machine learning models by incorporating columns from\nexternal datasets. However, it is challenging to efficiently discover relevant\nexternal tables to join with a given input table. Existing approaches rely on\ndata discovery systems to identify joinable tables from external sources,\ntypically based on overlap or containment. However, the sheer number of tables\nobtained from these systems results in irrelevant joins that need to be\nperformed; this can be computationally expensive or even infeasible in\npractice. We address this limitation by proposing the use of efficient mutual\ninformation (MI) estimation for finding relevant joinable tables. We introduce\na new sketching method that enables efficient evaluation of relationship\ndiscovery queries by estimating MI without materializing the joins and\nreturning a smaller set of tables that are more likely to be relevant. We also\ndemonstrate the effectiveness of our approach at approximating MI in extensive\nexperiments using synthetic and real-world datasets.\n","authors":["Aécio Santos","Flip Korn","Juliana Freire"],"pdf_url":"https://arxiv.org/pdf/2403.15553v1.pdf","comment":"Accepted to IEEE ICDE 2024"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2403.15389v1","updated":"2024-03-22T17:59:58Z","published":"2024-03-22T17:59:58Z","title":"DiffusionMTL: Learning Multi-Task Denoising Diffusion Model from\n  Partially Annotated Data","summary":"  Recently, there has been an increased interest in the practical problem of\nlearning multiple dense scene understanding tasks from partially annotated\ndata, where each training sample is only labeled for a subset of the tasks. The\nmissing of task labels in training leads to low-quality and noisy predictions,\nas can be observed from state-of-the-art methods. To tackle this issue, we\nreformulate the partially-labeled multi-task dense prediction as a pixel-level\ndenoising problem, and propose a novel multi-task denoising diffusion framework\ncoined as DiffusionMTL. It designs a joint diffusion and denoising paradigm to\nmodel a potential noisy distribution in the task prediction or feature maps and\ngenerate rectified outputs for different tasks. To exploit multi-task\nconsistency in denoising, we further introduce a Multi-Task Conditioning\nstrategy, which can implicitly utilize the complementary nature of the tasks to\nhelp learn the unlabeled tasks, leading to an improvement in the denoising\nperformance of the different tasks. Extensive quantitative and qualitative\nexperiments demonstrate that the proposed multi-task denoising diffusion model\ncan significantly improve multi-task prediction maps, and outperform the\nstate-of-the-art methods on three challenging multi-task benchmarks, under two\ndifferent partial-labeling evaluation settings. The code is available at\nhttps://prismformore.github.io/diffusionmtl/.\n","authors":["Hanrong Ye","Dan Xu"],"pdf_url":"https://arxiv.org/pdf/2403.15389v1.pdf","comment":"The paper is accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2403.15385v1","updated":"2024-03-22T17:59:37Z","published":"2024-03-22T17:59:37Z","title":"LATTE3D: Large-scale Amortized Text-To-Enhanced3D Synthesis","summary":"  Recent text-to-3D generation approaches produce impressive 3D results but\nrequire time-consuming optimization that can take up to an hour per prompt.\nAmortized methods like ATT3D optimize multiple prompts simultaneously to\nimprove efficiency, enabling fast text-to-3D synthesis. However, they cannot\ncapture high-frequency geometry and texture details and struggle to scale to\nlarge prompt sets, so they generalize poorly. We introduce LATTE3D, addressing\nthese limitations to achieve fast, high-quality generation on a significantly\nlarger prompt set. Key to our method is 1) building a scalable architecture and\n2) leveraging 3D data during optimization through 3D-aware diffusion priors,\nshape regularization, and model initialization to achieve robustness to diverse\nand complex training prompts. LATTE3D amortizes both neural field and textured\nsurface generation to produce highly detailed textured meshes in a single\nforward pass. LATTE3D generates 3D objects in 400ms, and can be further\nenhanced with fast test-time optimization.\n","authors":["Kevin Xie","Jonathan Lorraine","Tianshi Cao","Jun Gao","James Lucas","Antonio Torralba","Sanja Fidler","Xiaohui Zeng"],"pdf_url":"https://arxiv.org/pdf/2403.15385v1.pdf","comment":"See the project website at\n  https://research.nvidia.com/labs/toronto-ai/LATTE3D/"},{"id":"http://arxiv.org/abs/2403.08763v2","updated":"2024-03-22T17:56:38Z","published":"2024-03-13T17:58:57Z","title":"Simple and Scalable Strategies to Continually Pre-train Large Language\n  Models","summary":"  Large language models (LLMs) are routinely pre-trained on billions of tokens,\nonly to start the process over again once new data becomes available. A much\nmore efficient solution is to continually pre-train these models, saving\nsignificant compute compared to re-training. However, the distribution shift\ninduced by new data typically results in degraded performance on previous data\nor poor adaptation to the new data. In this work, we show that a simple and\nscalable combination of learning rate (LR) re-warming, LR re-decaying, and\nreplay of previous data is sufficient to match the performance of fully\nre-training from scratch on all available data, as measured by final loss and\nlanguage model (LM) evaluation benchmarks. Specifically, we show this for a\nweak but realistic distribution shift between two commonly used LLM\npre-training datasets (English$\\rightarrow$English) and a stronger distribution\nshift (English$\\rightarrow$German) at the $405$M parameter model scale with\nlarge dataset sizes (hundreds of billions of tokens). Selecting the weak but\nrealistic shift for larger-scale experiments, we also find that our continual\nlearning strategies match the re-training baseline for a 10B parameter LLM. Our\nresults demonstrate that LLMs can be successfully updated via simple and\nscalable continual learning strategies, matching the re-training baseline using\nonly a fraction of the compute. Finally, inspired by previous work, we propose\nalternatives to the cosine learning rate schedule that help circumvent\nforgetting induced by LR re-warming and that are not bound to a fixed token\nbudget.\n","authors":["Adam Ibrahim","Benjamin Thérien","Kshitij Gupta","Mats L. Richter","Quentin Anthony","Timothée Lesort","Eugene Belilovsky","Irina Rish"],"pdf_url":"https://arxiv.org/pdf/2403.08763v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.00652v2","updated":"2024-03-22T17:56:05Z","published":"2023-03-01T16:54:48Z","title":"Finding the right XAI method -- A Guide for the Evaluation and Ranking\n  of Explainable AI Methods in Climate Science","summary":"  Explainable artificial intelligence (XAI) methods shed light on the\npredictions of machine learning algorithms. Several different approaches exist\nand have already been applied in climate science. However, usually missing\nground truth explanations complicate their evaluation and comparison,\nsubsequently impeding the choice of the XAI method. Therefore, in this work, we\nintroduce XAI evaluation in the climate context and discuss different desired\nexplanation properties, namely robustness, faithfulness, randomization,\ncomplexity, and localization. To this end, we chose previous work as a case\nstudy where the decade of annual-mean temperature maps is predicted. After\ntraining both a multi-layer perceptron (MLP) and a convolutional neural network\n(CNN), multiple XAI methods are applied and their skill scores in reference to\na random uniform explanation are calculated for each property. Independent of\nthe network, we find that XAI methods Integrated Gradients, layer-wise\nrelevance propagation, and input times gradients exhibit considerable\nrobustness, faithfulness, and complexity while sacrificing randomization\nperformance. Sensitivity methods -- gradient, SmoothGrad, NoiseGrad, and\nFusionGrad, match the robustness skill but sacrifice faithfulness and\ncomplexity for randomization skill. We find architecture-dependent performance\ndifferences regarding robustness, complexity and localization skills of\ndifferent XAI methods, highlighting the necessity for research task-specific\nevaluation. Overall, our work offers an overview of different evaluation\nproperties in the climate science context and shows how to compare and\nbenchmark different explanation methods, assessing their suitability based on\nstrengths and weaknesses, for the specific research problem at hand. By that,\nwe aim to support climate researchers in the selection of a suitable XAI\nmethod.\n","authors":["Philine Bommer","Marlene Kretschmer","Anna Hedström","Dilyara Bareeva","Marina M. -C. Höhne"],"pdf_url":"https://arxiv.org/pdf/2303.00652v2.pdf","comment":"19 pages, 10 figure, accepted at AIES journal by AMS"},{"id":"http://arxiv.org/abs/2403.15371v1","updated":"2024-03-22T17:50:43Z","published":"2024-03-22T17:50:43Z","title":"Can large language models explore in-context?","summary":"  We investigate the extent to which contemporary Large Language Models (LLMs)\ncan engage in exploration, a core capability in reinforcement learning and\ndecision making. We focus on native performance of existing LLMs, without\ntraining interventions. We deploy LLMs as agents in simple multi-armed bandit\nenvironments, specifying the environment description and interaction history\nentirely in-context, i.e., within the LLM prompt. We experiment with GPT-3.5,\nGPT-4, and Llama2, using a variety of prompt designs, and find that the models\ndo not robustly engage in exploration without substantial interventions: i)\nAcross all of our experiments, only one configuration resulted in satisfactory\nexploratory behavior: GPT-4 with chain-of-thought reasoning and an externally\nsummarized interaction history, presented as sufficient statistics; ii) All\nother configurations did not result in robust exploratory behavior, including\nthose with chain-of-thought reasoning but unsummarized history. Although these\nfindings can be interpreted positively, they suggest that external\nsummarization -- which may not be possible in more complex settings -- is\nimportant for obtaining desirable behavior from LLM agents. We conclude that\nnon-trivial algorithmic interventions, such as fine-tuning or dataset curation,\nmay be required to empower LLM-based decision making agents in complex\nsettings.\n","authors":["Akshay Krishnamurthy","Keegan Harris","Dylan J. Foster","Cyril Zhang","Aleksandrs Slivkins"],"pdf_url":"https://arxiv.org/pdf/2403.15371v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15370v1","updated":"2024-03-22T17:49:11Z","published":"2024-03-22T17:49:11Z","title":"Augmented Reality based Simulated Data (ARSim) with multi-view\n  consistency for AV perception networks","summary":"  Detecting a diverse range of objects under various driving scenarios is\nessential for the effectiveness of autonomous driving systems. However, the\nreal-world data collected often lacks the necessary diversity presenting a\nlong-tail distribution. Although synthetic data has been utilized to overcome\nthis issue by generating virtual scenes, it faces hurdles such as a significant\ndomain gap and the substantial efforts required from 3D artists to create\nrealistic environments. To overcome these challenges, we present ARSim, a fully\nautomated, comprehensive, modular framework designed to enhance real multi-view\nimage data with 3D synthetic objects of interest. The proposed method\nintegrates domain adaptation and randomization strategies to address covariate\nshift between real and simulated data by inferring essential domain attributes\nfrom real data and employing simulation-based randomization for other\nattributes. We construct a simplified virtual scene using real data and\nstrategically place 3D synthetic assets within it. Illumination is achieved by\nestimating light distribution from multiple images capturing the surroundings\nof the vehicle. Camera parameters from real data are employed to render\nsynthetic assets in each frame. The resulting augmented multi-view consistent\ndataset is used to train a multi-camera perception network for autonomous\nvehicles. Experimental results on various AV perception tasks demonstrate the\nsuperior performance of networks trained on the augmented dataset.\n","authors":["Aqeel Anwar","Tae Eun Choe","Zian Wang","Sanja Fidler","Minwoo Park"],"pdf_url":"https://arxiv.org/pdf/2403.15370v1.pdf","comment":"17 pages, 15 figures, 7 tables"},{"id":"http://arxiv.org/abs/2403.14617v2","updated":"2024-03-22T17:45:52Z","published":"2024-03-21T17:59:03Z","title":"Videoshop: Localized Semantic Video Editing with Noise-Extrapolated\n  Diffusion Inversion","summary":"  We introduce Videoshop, a training-free video editing algorithm for localized\nsemantic edits. Videoshop allows users to use any editing software, including\nPhotoshop and generative inpainting, to modify the first frame; it\nautomatically propagates those changes, with semantic, spatial, and temporally\nconsistent motion, to the remaining frames. Unlike existing methods that enable\nedits only through imprecise textual instructions, Videoshop allows users to\nadd or remove objects, semantically change objects, insert stock photos into\nvideos, etc. with fine-grained control over locations and appearance. We\nachieve this through image-based video editing by inverting latents with noise\nextrapolation, from which we generate videos conditioned on the edited image.\nVideoshop produces higher quality edits against 6 baselines on 2 editing\nbenchmarks using 10 evaluation metrics.\n","authors":["Xiang Fan","Anand Bhattad","Ranjay Krishna"],"pdf_url":"https://arxiv.org/pdf/2403.14617v2.pdf","comment":"Project page at https://videoshop-editing.github.io/"},{"id":"http://arxiv.org/abs/2403.15365v1","updated":"2024-03-22T17:33:11Z","published":"2024-03-22T17:33:11Z","title":"A Transfer Attack to Image Watermarks","summary":"  Watermark has been widely deployed by industry to detect AI-generated images.\nThe robustness of such watermark-based detector against evasion attacks in the\nwhite-box and black-box settings is well understood in the literature. However,\nthe robustness in the no-box setting is much less understood. In particular,\nmultiple studies claimed that image watermark is robust in such setting. In\nthis work, we propose a new transfer evasion attack to image watermark in the\nno-box setting. Our transfer attack adds a perturbation to a watermarked image\nto evade multiple surrogate watermarking models trained by the attacker itself,\nand the perturbed watermarked image also evades the target watermarking model.\nOur major contribution is to show that, both theoretically and empirically,\nwatermark-based AI-generated image detector is not robust to evasion attacks\neven if the attacker does not have access to the watermarking model nor the\ndetection API.\n","authors":["Yuepeng Hu","Zhengyuan Jiang","Moyang Guo","Neil Gong"],"pdf_url":"https://arxiv.org/pdf/2403.15365v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15363v1","updated":"2024-03-22T17:31:21Z","published":"2024-03-22T17:31:21Z","title":"Cascading Blackout Severity Prediction with Statistically-Augmented\n  Graph Neural Networks","summary":"  Higher variability in grid conditions, resulting from growing renewable\npenetration and increased incidence of extreme weather events, has increased\nthe difficulty of screening for scenarios that may lead to catastrophic\ncascading failures. Traditional power-flow-based tools for assessing cascading\nblackout risk are too slow to properly explore the space of possible failures\nand load/generation patterns. We add to the growing literature of faster\ngraph-neural-network (GNN)-based techniques, developing two novel techniques\nfor the estimation of blackout magnitude from initial grid conditions. First we\npropose several methods for employing an initial classification step to filter\nout safe \"non blackout\" scenarios prior to magnitude estimation. Second, using\ninsights from the statistical properties of cascading blackouts, we propose a\nmethod for facilitating non-local message passing in our GNN models. We\nvalidate these two approaches on a large simulated dataset, and show the\npotential of both to increase blackout size estimation performance.\n","authors":["Joe Gorka","Tim Hsu","Wenting Li","Yury Maximov","Line Roald"],"pdf_url":"https://arxiv.org/pdf/2403.15363v1.pdf","comment":"Accepted to Power Systems Computation Conference (PSCC) 2024"},{"id":"http://arxiv.org/abs/2312.00812v4","updated":"2024-03-22T17:29:01Z","published":"2023-11-28T03:13:09Z","title":"Empowering Autonomous Driving with Large Language Models: A Safety\n  Perspective","summary":"  Autonomous Driving (AD) encounters significant safety hurdles in long-tail\nunforeseen driving scenarios, largely stemming from the non-interpretability\nand poor generalization of the deep neural networks within the AD system,\nparticularly in out-of-distribution and uncertain data. To this end, this paper\nexplores the integration of Large Language Models (LLMs) into AD systems,\nleveraging their robust common-sense knowledge and reasoning abilities. The\nproposed methodologies employ LLMs as intelligent decision-makers in behavioral\nplanning, augmented with a safety verifier shield for contextual safety\nlearning, for enhancing driving performance and safety. We present two key\nstudies in a simulated environment: an adaptive LLM-conditioned Model\nPredictive Control (MPC) and an LLM-enabled interactive behavior planning\nscheme with a state machine. Demonstrating superior performance and safety\nmetrics compared to state-of-the-art approaches, our approach shows the\npromising potential for using LLMs for autonomous vehicles.\n","authors":["Yixuan Wang","Ruochen Jiao","Sinong Simon Zhan","Chengtian Lang","Chao Huang","Zhaoran Wang","Zhuoran Yang","Qi Zhu"],"pdf_url":"https://arxiv.org/pdf/2312.00812v4.pdf","comment":"Accepted to LLMAgent workshop @ICLR2024"},{"id":"http://arxiv.org/abs/2309.16512v4","updated":"2024-03-22T17:26:53Z","published":"2023-09-28T15:19:30Z","title":"From Complexity to Clarity: Analytical Expressions of Deep Neural\n  Network Weights via Clifford's Geometric Algebra and Convexity","summary":"  In this paper, we introduce a novel analysis of neural networks based on\ngeometric (Clifford) algebra and convex optimization. We show that optimal\nweights of deep ReLU neural networks are given by the wedge product of training\nsamples when trained with standard regularized loss. Furthermore, the training\nproblem reduces to convex optimization over wedge product features, which\nencode the geometric structure of the training dataset. This structure is given\nin terms of signed volumes of triangles and parallelotopes generated by data\nvectors. The convex problem finds a small subset of samples via $\\ell_1$\nregularization to discover only relevant wedge product features. Our analysis\nprovides a novel perspective on the inner workings of deep neural networks and\nsheds light on the role of the hidden layers.\n","authors":["Mert Pilanci"],"pdf_url":"https://arxiv.org/pdf/2309.16512v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15361v1","updated":"2024-03-22T17:23:37Z","published":"2024-03-22T17:23:37Z","title":"Learning Topological Representations for Deep Image Understanding","summary":"  In many scenarios, especially biomedical applications, the correct\ndelineation of complex fine-scaled structures such as neurons, tissues, and\nvessels is critical for downstream analysis. Despite the strong predictive\npower of deep learning methods, they do not provide a satisfactory\nrepresentation of these structures, thus creating significant barriers in\nscalable annotation and downstream analysis. In this dissertation, we tackle\nsuch challenges by proposing novel representations of these topological\nstructures in a deep learning framework. We leverage the mathematical tools\nfrom topological data analysis, i.e., persistent homology and discrete Morse\ntheory, to develop principled methods for better segmentation and uncertainty\nestimation, which will become powerful tools for scalable annotation.\n","authors":["Xiaoling Hu"],"pdf_url":"https://arxiv.org/pdf/2403.15361v1.pdf","comment":"Ph.D. thesis from Stony Brook University. This thesis includes works\n  arXiv:1906.05404, arXiv:2110.08335, arXiv:2112.07812, arXiv:2103.09992,\n  arXiv:2206.01742"},{"id":"http://arxiv.org/abs/2403.15360v1","updated":"2024-03-22T17:22:56Z","published":"2024-03-22T17:22:56Z","title":"SiMBA: Simplified Mamba-Based Architecture for Vision and Multivariate\n  Time series","summary":"  Transformers have widely adopted attention networks for sequence mixing and\nMLPs for channel mixing, playing a pivotal role in achieving breakthroughs\nacross domains. However, recent literature highlights issues with attention\nnetworks, including low inductive bias and quadratic complexity concerning\ninput sequence length. State Space Models (SSMs) like S4 and others (Hippo,\nGlobal Convolutions, liquid S4, LRU, Mega, and Mamba), have emerged to address\nthe above issues to help handle longer sequence lengths. Mamba, while being the\nstate-of-the-art SSM, has a stability issue when scaled to large networks for\ncomputer vision datasets. We propose SiMBA, a new architecture that introduces\nEinstein FFT (EinFFT) for channel modeling by specific eigenvalue computations\nand uses the Mamba block for sequence modeling. Extensive performance studies\nacross image and time-series benchmarks demonstrate that SiMBA outperforms\nexisting SSMs, bridging the performance gap with state-of-the-art transformers.\nNotably, SiMBA establishes itself as the new state-of-the-art SSM on ImageNet\nand transfer learning benchmarks such as Stanford Car and Flower as well as\ntask learning benchmarks as well as seven time series benchmark datasets. The\nproject page is available on this website\n~\\url{https://github.com/badripatro/Simba}.\n","authors":["Badri N. Patro","Vijay S. Agneeswaran"],"pdf_url":"https://arxiv.org/pdf/2403.15360v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09611v3","updated":"2024-03-22T17:03:16Z","published":"2024-03-14T17:51:32Z","title":"MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training","summary":"  In this work, we discuss building performant Multimodal Large Language Models\n(MLLMs). In particular, we study the importance of various architecture\ncomponents and data choices. Through careful and comprehensive ablations of the\nimage encoder, the vision language connector, and various pre-training data\nchoices, we identified several crucial design lessons. For example, we\ndemonstrate that for large-scale multimodal pre-training using a careful mix of\nimage-caption, interleaved image-text, and text-only data is crucial for\nachieving state-of-the-art (SOTA) few-shot results across multiple benchmarks,\ncompared to other published pre-training results. Further, we show that the\nimage encoder together with image resolution and the image token count has\nsubstantial impact, while the vision-language connector design is of\ncomparatively negligible importance. By scaling up the presented recipe, we\nbuild MM1, a family of multimodal models up to 30B parameters, including both\ndense models and mixture-of-experts (MoE) variants, that are SOTA in\npre-training metrics and achieve competitive performance after supervised\nfine-tuning on a range of established multimodal benchmarks. Thanks to\nlarge-scale pre-training, MM1 enjoys appealing properties such as enhanced\nin-context learning, and multi-image reasoning, enabling few-shot\nchain-of-thought prompting.\n","authors":["Brandon McKinzie","Zhe Gan","Jean-Philippe Fauconnier","Sam Dodge","Bowen Zhang","Philipp Dufter","Dhruti Shah","Xianzhi Du","Futang Peng","Floris Weers","Anton Belyi","Haotian Zhang","Karanjeet Singh","Doug Kang","Ankur Jain","Hongyu Hè","Max Schwarzer","Tom Gunter","Xiang Kong","Aonan Zhang","Jianyu Wang","Chong Wang","Nan Du","Tao Lei","Sam Wiseman","Guoli Yin","Mark Lee","Zirui Wang","Ruoming Pang","Peter Grasch","Alexander Toshev","Yinfei Yang"],"pdf_url":"https://arxiv.org/pdf/2403.09611v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00885v2","updated":"2024-03-22T16:57:37Z","published":"2023-12-30T15:58:32Z","title":"Attractor reconstruction with reservoir computers: The effect of the\n  reservoir's conditional Lyapunov exponents on faithful attractor\n  reconstruction","summary":"  Reservoir computing is a machine learning framework that has been shown to be\nable to replicate the chaotic attractor, including the fractal dimension and\nthe entire Lyapunov spectrum, of the dynamical system on which it is trained.\nWe quantitatively relate the generalized synchronization dynamics of a driven\nreservoir during the training stage to the performance of the trained reservoir\ncomputer at the attractor reconstruction task. We show that, in order to obtain\nsuccessful attractor reconstruction and Lyapunov spectrum estimation, the\nlargest conditional Lyapunov exponent of the driven reservoir must be\nsignificantly more negative than the most negative Lyapunov exponent of the\ntarget system. We also find that the maximal conditional Lyapunov exponent of\nthe reservoir depends strongly on the spectral radius of the reservoir\nadjacency matrix, and therefore, for attractor reconstruction and Lyapunov\nspectrum estimation, small spectral radius reservoir computers perform better\nin general. Our arguments are supported by numerical examples on well-known\nchaotic systems.\n","authors":["Joseph D. Hart"],"pdf_url":"https://arxiv.org/pdf/2401.00885v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05469v3","updated":"2024-03-22T16:49:06Z","published":"2023-10-09T07:26:35Z","title":"Learning to Predict Structural Vibrations","summary":"  In mechanical structures like airplanes, cars and houses, noise is generated\nand transmitted through vibrations. To take measures to reduce this noise,\nvibrations need to be simulated with expensive numerical computations.\nSurrogate deep learning models present a promising alternative to classical\nnumerical simulations as they can be evaluated magnitudes faster, while\ntrading-off accuracy. To quantify such trade-offs systematically and foster the\ndevelopment of methods, we present a benchmark on the task of predicting the\nvibration of harmonically excited plates. The benchmark features a total of\n12000 plate geometries with varying forms of beadings, material and sizes with\nassociated numerical solutions. To address the benchmark task, we propose a new\nnetwork architecture, named Frequency-Query Operator, which is trained to map\nplate geometries to their vibration pattern given a specific excitation\nfrequency. Applying principles from operator learning and implicit models for\nshape encoding, our approach effectively addresses the prediction of highly\nvariable frequency response functions occurring in dynamic systems. To quantify\nthe prediction quality, we introduce a set of evaluation metrics and evaluate\nthe method on our vibrating-plates benchmark. Our method outperforms DeepONets,\nFourier Neural Operators and more traditional neural network architectures.\nCode, dataset and visualizations: https://eckerlab.org/code/delden2023_plate\n","authors":["Jan van Delden","Julius Schultz","Christopher Blech","Sabine C. Langer","Timo Lüddecke"],"pdf_url":"https://arxiv.org/pdf/2310.05469v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.00401v2","updated":"2024-03-22T16:32:24Z","published":"2023-09-30T14:54:31Z","title":"Learning High-level Semantic-Relational Concepts for SLAM","summary":"  Recent works on SLAM extend their pose graphs with higher-level semantic\nconcepts like Rooms exploiting relationships between them, to provide, not only\na richer representation of the situation/environment but also to improve the\naccuracy of its estimation. Concretely, our previous work, Situational Graphs\n(S-Graphs+), a pioneer in jointly leveraging semantic relationships in the\nfactor optimization process, relies on semantic entities such as Planes and\nRooms, whose relationship is mathematically defined. Nevertheless, there is no\nunique approach to finding all the hidden patterns in lower-level factor-graphs\nthat correspond to high-level concepts of different natures. It is currently\ntackled with ad-hoc algorithms, which limits its graph expressiveness.\n  To overcome this limitation, in this work, we propose an algorithm based on\nGraph Neural Networks for learning high-level semantic-relational concepts that\ncan be inferred from the low-level factor graph. Given a set of mapped Planes\nour algorithm is capable of inferring Room entities relating to the Planes.\nAdditionally, to demonstrate the versatility of our method, our algorithm can\ninfer an additional semantic-relational concept, i.e. Wall, and its\nrelationship with its Planes. We validate our method in both simulated and real\ndatasets demonstrating improved performance over two baseline approaches.\nFurthermore, we integrate our method into the S-Graphs+ algorithm providing\nimproved pose and map accuracy compared to the baseline while further enhancing\nthe scene representation.\n","authors":["Jose Andres Millan-Romera","Hriday Bavle","Muhammad Shaheer","Martin R. Oswald","Holger Voos","Jose Luis Sanchez-Lopez"],"pdf_url":"https://arxiv.org/pdf/2310.00401v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08731v2","updated":"2024-03-22T16:30:48Z","published":"2023-10-12T21:38:07Z","title":"Novelty Detection in Reinforcement Learning with World Models","summary":"  Reinforcement learning (RL) using world models has found significant recent\nsuccesses. However, when a sudden change to world mechanics or properties\noccurs then agent performance and reliability can dramatically decline. We\nrefer to the sudden change in visual properties or state transitions as\nnovelties. Implementing novelty detection within generated world model\nframeworks is a crucial task for protecting the agent when deployed. In this\npaper, we propose straightforward bounding approaches to incorporate novelty\ndetection into world model RL agents, by utilizing the misalignment of the\nworld model's hallucinated states and the true observed states as an anomaly\nscore. We provide effective approaches to detecting novelties in a distribution\nof transitions learned by an agent in a world model. Finally, we show the\nadvantage of our work in a novel environment compared to traditional machine\nlearning novelty detection methods as well as currently accepted RL focused\nnovelty detection algorithms.\n","authors":["Geigh Zollicoffer","Kenneth Eaton","Jonathan Balloch","Julia Kim","Mark O. Riedl","Robert Wright"],"pdf_url":"https://arxiv.org/pdf/2310.08731v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.04690v2","updated":"2024-03-22T16:26:40Z","published":"2024-03-07T17:35:58Z","title":"Faster Neighborhood Attention: Reducing the O(n^2) Cost of Self\n  Attention at the Threadblock Level","summary":"  Neighborhood attention reduces the cost of self attention by restricting each\ntoken's attention span to its nearest neighbors. This restriction,\nparameterized by a window size and dilation factor, draws a spectrum of\npossible attention patterns between linear projection and self attention.\nNeighborhood attention, and more generally sliding window attention patterns,\nhave long been bounded by infrastructure, particularly in higher-rank spaces\n(2-D and 3-D), calling for the development of custom kernels, which have been\nlimited in either functionality, or performance, if not both. In this work, we\nfirst show that neighborhood attention can be represented as a batched GEMM\nproblem, similar to standard attention, and implement it for 1-D and 2-D\nneighborhood attention. These kernels on average provide 895% and 272%\nimprovement in full precision latency compared to existing naive kernels for\n1-D and 2-D neighborhood attention respectively. We find certain inherent\ninefficiencies in all unfused neighborhood attention kernels that bound their\nperformance and lower-precision scalability. We also developed fused\nneighborhood attention; an adaptation of fused dot-product attention kernels\nthat allow fine-grained control over attention across different spatial axes.\nKnown for reducing the quadratic time complexity of self attention to a linear\ncomplexity, neighborhood attention can now enjoy a reduced and constant memory\nfootprint, and record-breaking half precision latency. We observe that our\nfused kernels successfully circumvent some of the unavoidable inefficiencies in\nunfused implementations. While our unfused GEMM-based kernels only improve half\nprecision performance compared to naive kernels by an average of 496% and 113%\nin 1-D and 2-D problems respectively, our fused kernels improve naive kernels\nby an average of 1607% and 581% in 1-D and 2-D problems respectively.\n","authors":["Ali Hassani","Wen-Mei Hwu","Humphrey Shi"],"pdf_url":"https://arxiv.org/pdf/2403.04690v2.pdf","comment":"Project page: https://github.com/SHI-Labs/NATTEN"},{"id":"http://arxiv.org/abs/2402.07586v2","updated":"2024-03-22T16:25:26Z","published":"2024-02-12T11:35:25Z","title":"Unveiling Group-Specific Distributed Concept Drift: A Fairness\n  Imperative in Federated Learning","summary":"  In the evolving field of machine learning, ensuring fairness has become a\ncritical concern, prompting the development of algorithms designed to mitigate\ndiscriminatory outcomes in decision-making processes. However, achieving\nfairness in the presence of group-specific concept drift remains an unexplored\nfrontier, and our research represents pioneering efforts in this regard.\nGroup-specific concept drift refers to situations where one group experiences\nconcept drift over time while another does not, leading to a decrease in\nfairness even if accuracy remains fairly stable. Within the framework of\nfederated learning, where clients collaboratively train models, its distributed\nnature further amplifies these challenges since each client can experience\ngroup-specific concept drift independently while still sharing the same\nunderlying concept, creating a complex and dynamic environment for maintaining\nfairness. One of the significant contributions of our research is the\nformalization and introduction of the problem of group-specific concept drift\nand its distributed counterpart, shedding light on its critical importance in\nthe realm of fairness. In addition, leveraging insights from prior research, we\nadapt an existing distributed concept drift adaptation algorithm to tackle\ngroup-specific distributed concept drift which utilizes a multi-model approach,\na local group-specific drift detection mechanism, and continuous clustering of\nmodels over time. The findings from our experiments highlight the importance of\naddressing group-specific concept drift and its distributed counterpart to\nadvance fairness in machine learning.\n","authors":["Teresa Salazar","João Gama","Helder Araújo","Pedro Henriques Abreu"],"pdf_url":"https://arxiv.org/pdf/2402.07586v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15316v1","updated":"2024-03-22T16:10:38Z","published":"2024-03-22T16:10:38Z","title":"Ultrasound Imaging based on the Variance of a Diffusion Restoration\n  Model","summary":"  Despite today's prevalence of ultrasound imaging in medicine, ultrasound\nsignal-to-noise ratio is still affected by several sources of noise and\nartefacts. Moreover, enhancing ultrasound image quality involves balancing\nconcurrent factors like contrast, resolution, and speckle preservation.\nRecently, there has been progress in both model-based and learning-based\napproaches addressing the problem of ultrasound image reconstruction. Bringing\nthe best from both worlds, we propose a hybrid reconstruction method combining\nan ultrasound linear direct model with a learning-based prior coming from a\ngenerative Denoising Diffusion model. More specifically, we rely on the\nunsupervised fine-tuning of a pre-trained Denoising Diffusion Restoration Model\n(DDRM). Given the nature of multiplicative noise inherent to ultrasound, this\npaper proposes an empirical model to characterize the stochasticity of\ndiffusion reconstruction of ultrasound images, and shows the interest of its\nvariance as an echogenicity map estimator. We conduct experiments on synthetic,\nin-vitro, and in-vivo data, demonstrating the efficacy of our variance imaging\napproach in achieving high-quality image reconstructions from single plane-wave\nacquisitions and in comparison to state-of-the-art methods.\n","authors":["Yuxin Zhang","Clément Huneau","Jérôme Idier","Diana Mateus"],"pdf_url":"https://arxiv.org/pdf/2403.15316v1.pdf","comment":"5 pages; submitted to EUSIPCO 2024. arXiv admin note: text overlap\n  with arXiv:2310.20618"},{"id":"http://arxiv.org/abs/2403.09919v2","updated":"2024-03-22T16:06:42Z","published":"2024-03-14T23:40:56Z","title":"Recurrent Drafter for Fast Speculative Decoding in Large Language Models","summary":"  In this paper, we introduce an improved approach of speculative decoding\naimed at enhancing the efficiency of serving large language models. Our method\ncapitalizes on the strengths of two established techniques: the classic\ntwo-model speculative decoding approach, and the more recent single-model\napproach, Medusa. Drawing inspiration from Medusa, our approach adopts a\nsingle-model strategy for speculative decoding. However, our method\ndistinguishes itself by employing a single, lightweight draft head with a\nrecurrent dependency design, akin in essence to the small, draft model uses in\nclassic speculative decoding, but without the complexities of the full\ntransformer architecture. And because of the recurrent dependency, we can use\nbeam search to swiftly filter out undesired candidates with the draft head. The\noutcome is a method that combines the simplicity of single-model design and\navoids the need to create a data-dependent tree attention structure only for\ninference in Medusa. We empirically demonstrate the effectiveness of the\nproposed method on several popular open source language models, along with a\ncomprehensive analysis of the trade-offs involved in adopting this approach.\n","authors":["Aonan Zhang","Chong Wang","Yi Wang","Xuanyu Zhang","Yunfei Cheng"],"pdf_url":"https://arxiv.org/pdf/2403.09919v2.pdf","comment":"11 pages, 6 figures"},{"id":"http://arxiv.org/abs/2403.15312v1","updated":"2024-03-22T16:04:26Z","published":"2024-03-22T16:04:26Z","title":"A Wasserstein perspective of Vanilla GANs","summary":"  The empirical success of Generative Adversarial Networks (GANs) caused an\nincreasing interest in theoretical research. The statistical literature is\nmainly focused on Wasserstein GANs and generalizations thereof, which\nespecially allow for good dimension reduction properties. Statistical results\nfor Vanilla GANs, the original optimization problem, are still rather limited\nand require assumptions such as smooth activation functions and equal\ndimensions of the latent space and the ambient space. To bridge this gap, we\ndraw a connection from Vanilla GANs to the Wasserstein distance. By doing so,\nexisting results for Wasserstein GANs can be extended to Vanilla GANs. In\nparticular, we obtain an oracle inequality for Vanilla GANs in Wasserstein\ndistance. The assumptions of this oracle inequality are designed to be\nsatisfied by network architectures commonly used in practice, such as\nfeedforward ReLU networks. By providing a quantitative result for the\napproximation of a Lipschitz function by a feedforward ReLU network with\nbounded H\\\"older norm, we conclude a rate of convergence for Vanilla GANs as\nwell as Wasserstein GANs as estimators of the unknown probability distribution.\n","authors":["Lea Kunkel","Mathias Trabs"],"pdf_url":"https://arxiv.org/pdf/2403.15312v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10581v2","updated":"2024-03-22T16:00:24Z","published":"2024-03-15T13:25:09Z","title":"Large Language Model-informed ECG Dual Attention Network for Heart\n  Failure Risk Prediction","summary":"  Heart failure (HF) poses a significant public health challenge, with a rising\nglobal mortality rate. Early detection and prevention of HF could significantly\nreduce its impact. We introduce a novel methodology for predicting HF risk\nusing 12-lead electrocardiograms (ECGs). We present a novel, lightweight\ndual-attention ECG network designed to capture complex ECG features essential\nfor early HF risk prediction, despite the notable imbalance between low and\nhigh-risk groups. This network incorporates a cross-lead attention module and\ntwelve lead-specific temporal attention modules, focusing on cross-lead\ninteractions and each lead's local dynamics. To further alleviate model\noverfitting, we leverage a large language model (LLM) with a public ECG-Report\ndataset for pretraining on an ECG-report alignment task. The network is then\nfine-tuned for HF risk prediction using two specific cohorts from the UK\nBiobank study, focusing on patients with hypertension (UKB-HYP) and those who\nhave had a myocardial infarction (UKB-MI).The results reveal that LLM-informed\npre-training substantially enhances HF risk prediction in these cohorts. The\ndual-attention design not only improves interpretability but also predictive\naccuracy, outperforming existing competitive methods with C-index scores of\n0.6349 for UKB-HYP and 0.5805 for UKB-MI. This demonstrates our method's\npotential in advancing HF risk assessment with clinical complex ECG data.\n","authors":["Chen Chen","Lei Li","Marcel Beetz","Abhirup Banerjee","Ramneek Gupta","Vicente Grau"],"pdf_url":"https://arxiv.org/pdf/2403.10581v2.pdf","comment":"Under journal revision"},{"id":"http://arxiv.org/abs/2403.15309v1","updated":"2024-03-22T15:59:24Z","published":"2024-03-22T15:59:24Z","title":"Controlled Training Data Generation with Diffusion Models","summary":"  In this work, we present a method to control a text-to-image generative model\nto produce training data specifically \"useful\" for supervised learning. Unlike\nprevious works that employ an open-loop approach and pre-define prompts to\ngenerate new data using either a language model or human expertise, we develop\nan automated closed-loop system which involves two feedback mechanisms. The\nfirst mechanism uses feedback from a given supervised model and finds\nadversarial prompts that result in image generations that maximize the model\nloss. While these adversarial prompts result in diverse data informed by the\nmodel, they are not informed of the target distribution, which can be\ninefficient. Therefore, we introduce the second feedback mechanism that guides\nthe generation process towards a certain target distribution. We call the\nmethod combining these two mechanisms Guided Adversarial Prompts. We perform\nour evaluations on different tasks, datasets and architectures, with different\ntypes of distribution shifts (spuriously correlated data, unseen domains) and\ndemonstrate the efficiency of the proposed feedback mechanisms compared to\nopen-loop approaches.\n","authors":["Teresa Yeo","Andrei Atanov","Harold Benoit","Aleksandr Alekseev","Ruchira Ray","Pooya Esmaeil Akhoondi","Amir Zamir"],"pdf_url":"https://arxiv.org/pdf/2403.15309v1.pdf","comment":"Project page at https://adversarial-prompts.epfl.ch/"},{"id":"http://arxiv.org/abs/2403.15304v1","updated":"2024-03-22T15:54:30Z","published":"2024-03-22T15:54:30Z","title":"KTbench: A Novel Data Leakage-Free Framework for Knowledge Tracing","summary":"  Knowledge Tracing (KT) is concerned with predicting students' future\nperformance on learning items in intelligent tutoring systems. Learning items\nare tagged with skill labels called knowledge concepts (KCs). Many KT models\nexpand the sequence of item-student interactions into KC-student interactions\nby replacing learning items with their constituting KCs. This often results in\na longer sequence length. This approach addresses the issue of sparse\nitem-student interactions and minimises model parameters. However, two problems\nhave been identified with such models.\n  The first problem is the model's ability to learn correlations between KCs\nbelonging to the same item, which can result in the leakage of ground truth\nlabels and hinder performance. This problem can lead to a significant decrease\nin performance on datasets with a higher number of KCs per item. The second\nproblem is that the available benchmark implementations ignore accounting for\nchanges in sequence length when expanding KCs, leading to different models\nbeing tested with varying sequence lengths but still compared against the same\nbenchmark.\n  To address these problems, we introduce a general masking framework that\nmitigates the first problem and enhances the performance of such KT models\nwhile preserving the original model architecture without significant\nalterations. Additionally, we introduce KTbench, an open-source benchmark\nlibrary designed to ensure the reproducibility of this work while mitigating\nthe second problem.\n","authors":["Yahya Badran","Christine Preisach"],"pdf_url":"https://arxiv.org/pdf/2403.15304v1.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2403.15301v1","updated":"2024-03-22T15:51:39Z","published":"2024-03-22T15:51:39Z","title":"Planning with a Learned Policy Basis to Optimally Solve Complex Tasks","summary":"  Conventional reinforcement learning (RL) methods can successfully solve a\nwide range of sequential decision problems. However, learning policies that can\ngeneralize predictably across multiple tasks in a setting with non-Markovian\nreward specifications is a challenging problem. We propose to use successor\nfeatures to learn a policy basis so that each (sub)policy in it solves a\nwell-defined subproblem. In a task described by a finite state automaton (FSA)\nthat involves the same set of subproblems, the combination of these\n(sub)policies can then be used to generate an optimal solution without\nadditional learning. In contrast to other methods that combine (sub)policies\nvia planning, our method asymptotically attains global optimality, even in\nstochastic environments.\n","authors":["Guillermo Infante","David Kuric","Anders Jonsson","Vicenç Gómez","Herke van Hoof"],"pdf_url":"https://arxiv.org/pdf/2403.15301v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.04147v2","updated":"2024-03-22T15:37:38Z","published":"2023-11-07T17:18:52Z","title":"Multi-resolution Time-Series Transformer for Long-term Forecasting","summary":"  The performance of transformers for time-series forecasting has improved\nsignificantly. Recent architectures learn complex temporal patterns by\nsegmenting a time-series into patches and using the patches as tokens. The\npatch size controls the ability of transformers to learn the temporal patterns\nat different frequencies: shorter patches are effective for learning localized,\nhigh-frequency patterns, whereas mining long-term seasonalities and trends\nrequires longer patches. Inspired by this observation, we propose a novel\nframework, Multi-resolution Time-Series Transformer (MTST), which consists of a\nmulti-branch architecture for simultaneous modeling of diverse temporal\npatterns at different resolutions. In contrast to many existing time-series\ntransformers, we employ relative positional encoding, which is better suited\nfor extracting periodic components at different scales. Extensive experiments\non several real-world datasets demonstrate the effectiveness of MTST in\ncomparison to state-of-the-art forecasting techniques.\n","authors":["Yitian Zhang","Liheng Ma","Soumyasundar Pal","Yingxue Zhang","Mark Coates"],"pdf_url":"https://arxiv.org/pdf/2311.04147v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15285v1","updated":"2024-03-22T15:31:37Z","published":"2024-03-22T15:31:37Z","title":"Blockchain-based Pseudonym Management for Vehicle Twin Migrations in\n  Vehicular Edge Metaverse","summary":"  Driven by the great advances in metaverse and edge computing technologies,\nvehicular edge metaverses are expected to disrupt the current paradigm of\nintelligent transportation systems. As highly computerized avatars of Vehicular\nMetaverse Users (VMUs), the Vehicle Twins (VTs) deployed in edge servers can\nprovide valuable metaverse services to improve driving safety and on-board\nsatisfaction for their VMUs throughout journeys. To maintain uninterrupted\nmetaverse experiences, VTs must be migrated among edge servers following the\nmovements of vehicles. This can raise concerns about privacy breaches during\nthe dynamic communications among vehicular edge metaverses. To address these\nconcerns and safeguard location privacy, pseudonyms as temporary identifiers\ncan be leveraged by both VMUs and VTs to realize anonymous communications in\nthe physical space and virtual spaces. However, existing pseudonym management\nmethods fall short in meeting the extensive pseudonym demands in vehicular edge\nmetaverses, thus dramatically diminishing the performance of privacy\npreservation. To this end, we present a cross-metaverse empowered dual\npseudonym management framework. We utilize cross-chain technology to enhance\nmanagement efficiency and data security for pseudonyms. Furthermore, we propose\na metric to assess the privacy level and employ a Multi-Agent Deep\nReinforcement Learning (MADRL) approach to obtain an optimal pseudonym\ngenerating strategy. Numerical results demonstrate that our proposed schemes\nare high-efficiency and cost-effective, showcasing their promising applications\nin vehicular edge metaverses.\n","authors":["Jiawen Kang","Xiaofeng Luo","Jiangtian Nie","Tianhao Wu","Haibo Zhou","Yonghua Wang","Dusit Niyato","Shiwen Mao","Shengli Xie"],"pdf_url":"https://arxiv.org/pdf/2403.15285v1.pdf","comment":"14 pages, 9 figures"},{"id":"http://arxiv.org/abs/2308.13712v3","updated":"2024-03-22T15:30:57Z","published":"2023-08-25T23:54:15Z","title":"Residual Denoising Diffusion Models","summary":"  We propose residual denoising diffusion models (RDDM), a novel dual diffusion\nprocess that decouples the traditional single denoising diffusion process into\nresidual diffusion and noise diffusion. This dual diffusion framework expands\nthe denoising-based diffusion models, initially uninterpretable for image\nrestoration, into a unified and interpretable model for both image generation\nand restoration by introducing residuals. Specifically, our residual diffusion\nrepresents directional diffusion from the target image to the degraded input\nimage and explicitly guides the reverse generation process for image\nrestoration, while noise diffusion represents random perturbations in the\ndiffusion process. The residual prioritizes certainty, while the noise\nemphasizes diversity, enabling RDDM to effectively unify tasks with varying\ncertainty or diversity requirements, such as image generation and restoration.\nWe demonstrate that our sampling process is consistent with that of DDPM and\nDDIM through coefficient transformation, and propose a partially\npath-independent generation process to better understand the reverse process.\nNotably, our RDDM enables a generic UNet, trained with only an L1 loss and a\nbatch size of 1, to compete with state-of-the-art image restoration methods. We\nprovide code and pre-trained models to encourage further exploration,\napplication, and development of our innovative framework\n(https://github.com/nachifur/RDDM).\n","authors":["Jiawei Liu","Qiang Wang","Huijie Fan","Yinong Wang","Yandong Tang","Liangqiong Qu"],"pdf_url":"https://arxiv.org/pdf/2308.13712v3.pdf","comment":"Accepted to CVPR2024"},{"id":"http://arxiv.org/abs/2309.09510v2","updated":"2024-03-22T15:25:04Z","published":"2023-09-18T06:43:30Z","title":"Dynamic-SUPERB: Towards A Dynamic, Collaborative, and Comprehensive\n  Instruction-Tuning Benchmark for Speech","summary":"  Text language models have shown remarkable zero-shot capability in\ngeneralizing to unseen tasks when provided with well-formulated instructions.\nHowever, existing studies in speech processing primarily focus on limited or\nspecific tasks. Moreover, the lack of standardized benchmarks hinders a fair\ncomparison across different approaches. Thus, we present Dynamic-SUPERB, a\nbenchmark designed for building universal speech models capable of leveraging\ninstruction tuning to perform multiple tasks in a zero-shot fashion. To achieve\ncomprehensive coverage of diverse speech tasks and harness instruction tuning,\nwe invite the community to collaborate and contribute, facilitating the dynamic\ngrowth of the benchmark. To initiate, Dynamic-SUPERB features 55 evaluation\ninstances by combining 33 tasks and 22 datasets. This spans a broad spectrum of\ndimensions, providing a comprehensive platform for evaluation. Additionally, we\npropose several approaches to establish benchmark baselines. These include the\nutilization of speech models, text language models, and the multimodal encoder.\nEvaluation results indicate that while these baselines perform reasonably on\nseen tasks, they struggle with unseen ones. We release all materials to the\npublic and welcome researchers to collaborate on the project, advancing\ntechnologies in the field together.\n","authors":["Chien-yu Huang","Ke-Han Lu","Shih-Heng Wang","Chi-Yuan Hsiao","Chun-Yi Kuan","Haibin Wu","Siddhant Arora","Kai-Wei Chang","Jiatong Shi","Yifan Peng","Roshan Sharma","Shinji Watanabe","Bhiksha Ramakrishnan","Shady Shehata","Hung-yi Lee"],"pdf_url":"https://arxiv.org/pdf/2309.09510v2.pdf","comment":"To appear in the proceedings of ICASSP 2024"},{"id":"http://arxiv.org/abs/2402.00705v2","updated":"2024-03-22T15:13:17Z","published":"2024-02-01T16:00:21Z","title":"Combining the Strengths of Dutch Survey and Register Data in a Data\n  Challenge to Predict Fertility (PreFer)","summary":"  The social sciences have produced an impressive body of research on\ndeterminants of fertility outcomes, or whether and when people have children.\nHowever, the strength of these determinants and underlying theories are rarely\nevaluated on their predictive ability on new data. This prevents us from\nsystematically comparing studies, hindering the evaluation and accumulation of\nknowledge. In this paper, we present two datasets which can be used to study\nthe predictability of fertility outcomes in the Netherlands. One dataset is\nbased on the LISS panel, a longitudinal survey which includes thousands of\nvariables on a wide range of topics, including individual preferences and\nvalues. The other is based on the Dutch register data which lacks attitudinal\ndata but includes detailed information about the life courses of millions of\nDutch residents. We provide information about the datasets and the samples, and\ndescribe the fertility outcome of interest. We also introduce the fertility\nprediction data challenge PreFer which is based on these datasets and will\nstart in Spring 2024. We outline the ways in which measuring the predictability\nof fertility outcomes using these datasets and combining their strengths in the\ndata challenge can advance our understanding of fertility behaviour and\ncomputational social science. We further provide details for participants on\nhow to take part in the data challenge.\n","authors":["Elizaveta Sivak","Paulina Pankowska","Adrienne Mendrik","Tom Emery","Javier Garcia-Bernardo","Seyit Hocuk","Kasia Karpinska","Angelica Maineri","Joris Mulder","Malvina Nissim","Gert Stulp"],"pdf_url":"https://arxiv.org/pdf/2402.00705v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15267v1","updated":"2024-03-22T15:06:31Z","published":"2024-03-22T15:06:31Z","title":"Parametric PDE Control with Deep Reinforcement Learning and\n  Differentiable L0-Sparse Polynomial Policies","summary":"  Optimal control of parametric partial differential equations (PDEs) is\ncrucial in many applications in engineering and science. In recent years, the\nprogress in scientific machine learning has opened up new frontiers for the\ncontrol of parametric PDEs. In particular, deep reinforcement learning (DRL)\nhas the potential to solve high-dimensional and complex control problems in a\nlarge variety of applications. Most DRL methods rely on deep neural network\n(DNN) control policies. However, for many dynamical systems, DNN-based control\npolicies tend to be over-parametrized, which means they need large amounts of\ntraining data, show limited robustness, and lack interpretability. In this\nwork, we leverage dictionary learning and differentiable L$_0$ regularization\nto learn sparse, robust, and interpretable control policies for parametric\nPDEs. Our sparse policy architecture is agnostic to the DRL method and can be\nused in different policy-gradient and actor-critic DRL algorithms without\nchanging their policy-optimization procedure. We test our approach on the\nchallenging tasks of controlling parametric Kuramoto-Sivashinsky and\nconvection-diffusion-reaction PDEs. We show that our method (1) outperforms\nbaseline DNN-based DRL policies, (2) allows for the derivation of interpretable\nequations of the learned optimal control laws, and (3) generalizes to unseen\nparameters of the PDE without retraining the policies.\n","authors":["Nicolò Botteghi","Urban Fasel"],"pdf_url":"https://arxiv.org/pdf/2403.15267v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15263v1","updated":"2024-03-22T15:02:24Z","published":"2024-03-22T15:02:24Z","title":"Federated Bayesian Deep Learning: The Application of Statistical\n  Aggregation Methods to Bayesian Models","summary":"  Federated learning (FL) is an approach to training machine learning models\nthat takes advantage of multiple distributed datasets while maintaining data\nprivacy and reducing communication costs associated with sharing local\ndatasets. Aggregation strategies have been developed to pool or fuse the\nweights and biases of distributed deterministic models; however, modern\ndeterministic deep learning (DL) models are often poorly calibrated and lack\nthe ability to communicate a measure of epistemic uncertainty in prediction,\nwhich is desirable for remote sensing platforms and safety-critical\napplications. Conversely, Bayesian DL models are often well calibrated and\ncapable of quantifying and communicating a measure of epistemic uncertainty\nalong with a competitive prediction accuracy. Unfortunately, because the\nweights and biases in Bayesian DL models are defined by a probability\ndistribution, simple application of the aggregation methods associated with FL\nschemes for deterministic models is either impossible or results in sub-optimal\nperformance. In this work, we use independent and identically distributed (IID)\nand non-IID partitions of the CIFAR-10 dataset and a fully variational\nResNet-20 architecture to analyze six different aggregation strategies for\nBayesian DL models. Additionally, we analyze the traditional federated\naveraging approach applied to an approximate Bayesian Monte Carlo dropout model\nas a lightweight alternative to more complex variational inference methods in\nFL. We show that aggregation strategy is a key hyperparameter in the design of\na Bayesian FL system with downstream effects on accuracy, calibration,\nuncertainty quantification, training stability, and client compute\nrequirements.\n","authors":["John Fischer","Marko Orescanin","Justin Loomis","Patrick McClure"],"pdf_url":"https://arxiv.org/pdf/2403.15263v1.pdf","comment":"22 pages, 9 figures"},{"id":"http://arxiv.org/abs/2312.12973v2","updated":"2024-03-22T15:00:47Z","published":"2023-12-20T12:31:28Z","title":"Sparse Mean Field Load Balancing in Large Localized Queueing Systems","summary":"  Scalable load balancing algorithms are of great interest in cloud networks\nand data centers, necessitating the use of tractable techniques to compute\noptimal load balancing policies for good performance. However, most existing\nscalable techniques, especially asymptotically scaling methods based on mean\nfield theory, have not been able to model large queueing networks with strong\nlocality. Meanwhile, general multi-agent reinforcement learning techniques can\nbe hard to scale and usually lack a theoretical foundation. In this work, we\naddress this challenge by leveraging recent advances in sparse mean field\ntheory to learn a near-optimal load balancing policy in sparsely connected\nqueueing networks in a tractable manner, which may be preferable to global\napproaches in terms of wireless communication overhead. Importantly, we obtain\na general load balancing framework for a large class of sparse bounded-degree\nwireless topologies. By formulating a novel mean field control problem in the\ncontext of graphs with bounded degree, we reduce the otherwise difficult\nmulti-agent problem to a single-agent problem. Theoretically, the approach is\njustified by approximation guarantees. Empirically, the proposed methodology\nperforms well on several realistic and scalable wireless network topologies as\ncompared to a number of well-known load balancing heuristics and existing\nscalable multi-agent reinforcement learning methods.\n","authors":["Anam Tahir","Kai Cui","Heinz Koeppl"],"pdf_url":"https://arxiv.org/pdf/2312.12973v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15250v1","updated":"2024-03-22T14:47:35Z","published":"2024-03-22T14:47:35Z","title":"Comprehensive Reassessment of Large-Scale Evaluation Outcomes in LLMs: A\n  Multifaceted Statistical Approach","summary":"  Amidst the rapid evolution of LLMs, the significance of evaluation in\ncomprehending and propelling these models forward is increasingly paramount.\nEvaluations have revealed that factors such as scaling, training types,\narchitectures and other factors profoundly impact the performance of LLMs.\nHowever, the extent and nature of these impacts continue to be subjects of\ndebate because most assessments have been restricted to a limited number of\nmodels and data points. Clarifying the effects of these factors on performance\nscores can be more effectively achieved through a statistical lens. Our study\nembarks on a thorough re-examination of these LLMs, targeting the inadequacies\nin current evaluation methods. With the advent of a uniform evaluation\nframework, our research leverages an expansive dataset of evaluation results,\nintroducing a comprehensive statistical methodology. This includes the\napplication of ANOVA, Tukey HSD tests, GAMM, and clustering technique, offering\na robust and transparent approach to deciphering LLM performance data. Contrary\nto prevailing findings, our results challenge assumptions about emergent\nabilities and the influence of given training types and architectures in LLMs.\nThese findings furnish new perspectives on the characteristics, intrinsic\nnature, and developmental trajectories of LLMs. By providing straightforward\nand reliable methods to scrutinize and reassess LLM performance data, this\nstudy contributes a nuanced perspective on LLM efficiency and potentials.\n","authors":["Kun Sun","Rong Wang","Haitao Liu","Anders Søgaard"],"pdf_url":"https://arxiv.org/pdf/2403.15250v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15249v1","updated":"2024-03-22T14:47:18Z","published":"2024-03-22T14:47:18Z","title":"Spectral Motion Alignment for Video Motion Transfer using Diffusion\n  Models","summary":"  The evolution of diffusion models has greatly impacted video generation and\nunderstanding. Particularly, text-to-video diffusion models (VDMs) have\nsignificantly facilitated the customization of input video with target\nappearance, motion, etc. Despite these advances, challenges persist in\naccurately distilling motion information from video frames. While existing\nworks leverage the consecutive frame residual as the target motion vector, they\ninherently lack global motion context and are vulnerable to frame-wise\ndistortions. To address this, we present Spectral Motion Alignment (SMA), a\nnovel framework that refines and aligns motion vectors using Fourier and\nwavelet transforms. SMA learns motion patterns by incorporating\nfrequency-domain regularization, facilitating the learning of whole-frame\nglobal motion dynamics, and mitigating spatial artifacts. Extensive experiments\ndemonstrate SMA's efficacy in improving motion transfer while maintaining\ncomputational efficiency and compatibility across various video customization\nframeworks.\n","authors":["Geon Yeong Park","Hyeonho Jeong","Sang Wan Lee","Jong Chul Ye"],"pdf_url":"https://arxiv.org/pdf/2403.15249v1.pdf","comment":"Project page:\n  https://geonyeong-park.github.io/spectral-motion-alignment/"},{"id":"http://arxiv.org/abs/2403.05752v2","updated":"2024-03-22T14:44:17Z","published":"2024-03-09T01:17:26Z","title":"Task-Oriented GNNs Training on Large Knowledge Graphs for Accurate and\n  Efficient Modeling","summary":"  A Knowledge Graph (KG) is a heterogeneous graph encompassing a diverse range\nof node and edge types. Heterogeneous Graph Neural Networks (HGNNs) are popular\nfor training machine learning tasks like node classification and link\nprediction on KGs. However, HGNN methods exhibit excessive complexity\ninfluenced by the KG's size, density, and the number of node and edge types. AI\npractitioners handcraft a subgraph of a KG G relevant to a specific task. We\nrefer to this subgraph as a task-oriented subgraph (TOSG), which contains a\nsubset of task-related node and edge types in G. Training the task using TOSG\ninstead of G alleviates the excessive computation required for a large KG.\nCrafting the TOSG demands a deep understanding of the KG's structure and the\ntask's objectives. Hence, it is challenging and time-consuming. This paper\nproposes KG-TOSA, an approach to automate the TOSG extraction for task-oriented\nHGNN training on a large KG. In KG-TOSA, we define a generic graph pattern that\ncaptures the KG's local and global structure relevant to a specific task. We\nexplore different techniques to extract subgraphs matching our graph pattern:\nnamely (i) two techniques sampling around targeted nodes using biased random\nwalk or influence scores, and (ii) a SPARQL-based extraction method leveraging\nRDF engines' built-in indices. Hence, it achieves negligible preprocessing\noverhead compared to the sampling techniques. We develop a benchmark of real\nKGs of large sizes and various tasks for node classification and link\nprediction. Our experiments show that KG-TOSA helps state-of-the-art HGNN\nmethods reduce training time and memory usage by up to 70% while improving the\nmodel performance, e.g., accuracy and inference time.\n","authors":["Hussein Abdallah","Waleed Afandi","Panos Kalnis","Essam Mansour"],"pdf_url":"https://arxiv.org/pdf/2403.05752v2.pdf","comment":"12 pages,9 Figures, 3 Tables, ICDE:2024"},{"id":"http://arxiv.org/abs/2403.15246v1","updated":"2024-03-22T14:42:29Z","published":"2024-03-22T14:42:29Z","title":"FollowIR: Evaluating and Teaching Information Retrieval Models to Follow\n  Instructions","summary":"  Modern Large Language Models (LLMs) are capable of following long and complex\ninstructions that enable a diverse amount of user tasks. However, despite\nInformation Retrieval (IR) models using LLMs as the backbone of their\narchitectures, nearly all of them still only take queries as input, with no\ninstructions. For the handful of recent models that do take instructions, it's\nunclear how they use them. We introduce our dataset FollowIR, which contains a\nrigorous instruction evaluation benchmark as well as a training set for helping\nIR models learn to better follow real-world instructions. FollowIR builds off\nthe long history of the TREC conferences: as TREC provides human annotators\nwith instructions (also known as narratives) to determine document relevance,\nso should IR models be able to understand and decide relevance based on these\ndetailed instructions. Our evaluation benchmark starts with three deeply judged\nTREC collections and alters the annotator instructions, re-annotating relevant\ndocuments. Through this process, we can measure how well IR models follow\ninstructions, through a new pairwise evaluation framework. Our results indicate\nthat existing retrieval models fail to correctly use instructions, using them\nfor basic keywords and struggling to understand long-form information. However,\nwe show that it is possible for IR models to learn to follow complex\ninstructions: our new FollowIR-7B model has significant improvements (over 13%)\nafter fine-tuning on our training set.\n","authors":["Orion Weller","Benjamin Chang","Sean MacAvaney","Kyle Lo","Arman Cohan","Benjamin Van Durme","Dawn Lawrie","Luca Soldaini"],"pdf_url":"https://arxiv.org/pdf/2403.15246v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15245v1","updated":"2024-03-22T14:41:55Z","published":"2024-03-22T14:41:55Z","title":"Reasoning-Enhanced Object-Centric Learning for Videos","summary":"  Object-centric learning aims to break down complex visual scenes into more\nmanageable object representations, enhancing the understanding and reasoning\nabilities of machine learning systems toward the physical world. Recently,\nslot-based video models have demonstrated remarkable proficiency in segmenting\nand tracking objects, but they overlook the importance of the effective\nreasoning module. In the real world, reasoning and predictive abilities play a\ncrucial role in human perception and object tracking; in particular, these\nabilities are closely related to human intuitive physics. Inspired by this, we\ndesigned a novel reasoning module called the Slot-based Time-Space Transformer\nwith Memory buffer (STATM) to enhance the model's perception ability in complex\nscenes. The memory buffer primarily serves as storage for slot information from\nupstream modules, the Slot-based Time-Space Transformer makes predictions\nthrough slot-based spatiotemporal attention computations and fusion. Our\nexperiment results on various datasets show that STATM can significantly\nenhance object-centric learning capabilities of slot-based video models.\n","authors":["Jian Li","Pu Ren","Yang Liu","Hao Sun"],"pdf_url":"https://arxiv.org/pdf/2403.15245v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15244v1","updated":"2024-03-22T14:40:29Z","published":"2024-03-22T14:40:29Z","title":"A Stochastic Quasi-Newton Method for Non-convex Optimization with\n  Non-uniform Smoothness","summary":"  Classical convergence analyses for optimization algorithms rely on the\nwidely-adopted uniform smoothness assumption. However, recent experimental\nstudies have demonstrated that many machine learning problems exhibit\nnon-uniform smoothness, meaning the smoothness factor is a function of the\nmodel parameter instead of a universal constant. In particular, it has been\nobserved that the smoothness grows with respect to the gradient norm along the\ntraining trajectory. Motivated by this phenomenon, the recently introduced\n$(L_0, L_1)$-smoothness is a more general notion, compared to traditional\n$L$-smoothness, that captures such positive relationship between smoothness and\ngradient norm. Under this type of non-uniform smoothness, existing literature\nhas designed stochastic first-order algorithms by utilizing gradient clipping\ntechniques to obtain the optimal $\\mathcal{O}(\\epsilon^{-3})$ sample complexity\nfor finding an $\\epsilon$-approximate first-order stationary solution.\nNevertheless, the studies of quasi-Newton methods are still lacking.\nConsidering higher accuracy and more robustness for quasi-Newton methods, in\nthis paper we propose a fast stochastic quasi-Newton method when there exists\nnon-uniformity in smoothness. Leveraging gradient clipping and variance\nreduction, our algorithm can achieve the best-known\n$\\mathcal{O}(\\epsilon^{-3})$ sample complexity and enjoys convergence speedup\nwith simple hyperparameter tuning. Our numerical experiments show that our\nproposed algorithm outperforms the state-of-the-art approaches.\n","authors":["Zhenyu Sun","Ermin Wei"],"pdf_url":"https://arxiv.org/pdf/2403.15244v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15243v1","updated":"2024-03-22T14:36:39Z","published":"2024-03-22T14:36:39Z","title":"Robust Utility Optimization via a GAN Approach","summary":"  Robust utility optimization enables an investor to deal with market\nuncertainty in a structured way, with the goal of maximizing the worst-case\noutcome. In this work, we propose a generative adversarial network (GAN)\napproach to (approximately) solve robust utility optimization problems in\ngeneral and realistic settings. In particular, we model both the investor and\nthe market by neural networks (NN) and train them in a mini-max zero-sum game.\nThis approach is applicable for any continuous utility function and in\nrealistic market settings with trading costs, where only observable information\nof the market can be used. A large empirical study shows the versatile\nusability of our method. Whenever an optimal reference strategy is available,\nour method performs on par with it and in the (many) settings without known\noptimal strategy, our method outperforms all other reference strategies.\nMoreover, we can conclude from our study that the trained path-dependent\nstrategies do not outperform Markovian ones. Lastly, we uncover that our\ngenerative approach for learning optimal, (non-) robust investments under\ntrading costs generates universally applicable alternatives to well known\nasymptotic strategies of idealized settings.\n","authors":["Florian Krach","Josef Teichmann","Hanna Wutte"],"pdf_url":"https://arxiv.org/pdf/2403.15243v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.12333v2","updated":"2024-03-22T14:36:29Z","published":"2023-07-23T14:00:33Z","title":"An axiomatized PDE model of deep neural networks","summary":"  Inspired by the relation between deep neural network (DNN) and partial\ndifferential equations (PDEs), we study the general form of the PDE models of\ndeep neural networks. To achieve this goal, we formulate DNN as an evolution\noperator from a simple base model. Based on several reasonable assumptions, we\nprove that the evolution operator is actually determined by\nconvection-diffusion equation. This convection-diffusion equation model gives\nmathematical explanation for several effective networks. Moreover, we show that\nthe convection-diffusion model improves the robustness and reduces the\nRademacher complexity. Based on the convection-diffusion equation, we design a\nnew training method for ResNets. Experiments validate the performance of the\nproposed method.\n","authors":["Tangjun Wang","Wenqi Tao","Chenglong Bao","Zuoqiang Shi"],"pdf_url":"https://arxiv.org/pdf/2307.12333v2.pdf","comment":"The experiment design in the paper lacks careful thought and may be\n  misleading in demonstrating our contribution"},{"id":"http://arxiv.org/abs/2403.15239v1","updated":"2024-03-22T14:32:27Z","published":"2024-03-22T14:32:27Z","title":"Guided Decoding for Robot Motion Generation and Adaption","summary":"  We address motion generation for high-DoF robot arms in complex settings with\nobstacles, via points, etc. A significant advancement in this domain is\nachieved by integrating Learning from Demonstration (LfD) into the motion\ngeneration process. This integration facilitates rapid adaptation to new tasks\nand optimizes the utilization of accumulated expertise by allowing robots to\nlearn and generalize from demonstrated trajectories.\n  We train a transformer architecture on a large dataset of simulated\ntrajectories. This architecture, based on a conditional variational autoencoder\ntransformer, learns essential motion generation skills and adapts these to meet\nauxiliary tasks and constraints. Our auto-regressive approach enables real-time\nintegration of feedback from the physical system, enhancing the adaptability\nand efficiency of motion generation. We show that our model can generate motion\nfrom initial and target points, but also that it can adapt trajectories in\nnavigating complex tasks, including obstacle avoidance, via points, and meeting\nvelocity and acceleration constraints, across platforms.\n","authors":["Nutan Chen","Elie Aljalbout","Botond Cseke","Patrick van der Smagt"],"pdf_url":"https://arxiv.org/pdf/2403.15239v1.pdf","comment":"7 pages"},{"id":"http://arxiv.org/abs/2401.12764v3","updated":"2024-03-22T14:29:14Z","published":"2024-01-23T13:44:15Z","title":"Fast Nonlinear Two-Time-Scale Stochastic Approximation: Achieving\n  $O(1/k)$ Finite-Sample Complexity","summary":"  This paper proposes to develop a new variant of the two-time-scale stochastic\napproximation to find the roots of two coupled nonlinear operators, assuming\nonly noisy samples of these operators can be observed. Our key idea is to\nleverage the classic Ruppert-Polyak averaging technique to dynamically estimate\nthe operators through their samples. The estimated values of these averaging\nsteps will then be used in the two-time-scale stochastic approximation updates\nto find the desired solution. Our main theoretical result is to show that under\nthe strongly monotone condition of the underlying nonlinear operators the\nmean-squared errors of the iterates generated by the proposed method converge\nto zero at an optimal rate $O(1/k)$, where $k$ is the number of iterations. Our\nresult significantly improves the existing result of two-time-scale stochastic\napproximation, where the best known finite-time convergence rate is\n$O(1/k^{2/3})$. We illustrate this result by applying the proposed method to\ndevelop new reinforcement learning algorithms with improved performance.\n","authors":["Thinh T. Doan"],"pdf_url":"https://arxiv.org/pdf/2401.12764v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15230v1","updated":"2024-03-22T14:23:21Z","published":"2024-03-22T14:23:21Z","title":"An Exploratory Investigation into Code License Infringements in Large\n  Language Model Training Datasets","summary":"  Does the training of large language models potentially infringe upon code\nlicenses? Furthermore, are there any datasets available that can be safely used\nfor training these models without violating such licenses? In our study, we\nassess the current trends in the field and the importance of incorporating code\ninto the training of large language models. Additionally, we examine publicly\navailable datasets to see whether these models can be trained on them without\nthe risk of legal issues in the future. To accomplish this, we compiled a list\nof 53 large language models trained on file-level code. We then extracted their\ndatasets and analyzed how much they overlap with a dataset we created,\nconsisting exclusively of strong copyleft code.\n  Our analysis revealed that every dataset we examined contained license\ninconsistencies, despite being selected based on their associated repository\nlicenses. We analyzed a total of 514 million code files, discovering 38 million\nexact duplicates present in our strong copyleft dataset. Additionally, we\nexamined 171 million file-leading comments, identifying 16 million with strong\ncopyleft licenses and another 11 million comments that discouraged copying\nwithout explicitly mentioning a license. Based on the findings of our study,\nwhich highlights the pervasive issue of license inconsistencies in large\nlanguage models trained on code, our recommendation for both researchers and\nthe community is to prioritize the development and adoption of best practices\nfor dataset creation and management.\n","authors":["Jonathan Katzy","Răzvan-Mihai Popescu","Arie van Deursen","Maliheh Izadi"],"pdf_url":"https://arxiv.org/pdf/2403.15230v1.pdf","comment":"Accepted to FORGE 2024"},{"id":"http://arxiv.org/abs/2303.02204v3","updated":"2024-03-22T14:14:45Z","published":"2023-03-03T20:31:04Z","title":"KGLiDS: A Platform for Semantic Abstraction, Linking, and Automation of\n  Data Science","summary":"  In recent years, we have witnessed the growing interest from academia and\nindustry in applying data science technologies to analyze large amounts of\ndata. In this process, a myriad of artifacts (datasets, pipeline scripts, etc.)\nare created. However, there has been no systematic attempt to holistically\ncollect and exploit all the knowledge and experiences that are implicitly\ncontained in those artifacts. Instead, data scientists recover information and\nexpertise from colleagues or learn via trial and error. Hence, this paper\npresents a scalable platform, KGLiDS, that employs machine learning and\nknowledge graph technologies to abstract and capture the semantics of data\nscience artifacts and their connections. Based on this information, KGLiDS\nenables various downstream applications, such as data discovery and pipeline\nautomation. Our comprehensive evaluation covers use cases in data discovery,\ndata cleaning, transformation, and AutoML. It shows that KGLiDS is\nsignificantly faster with a lower memory footprint than the state-of-the-art\nsystems while achieving comparable or better accuracy.\n","authors":["Mossad Helali","Niki Monjazeb","Shubham Vashisth","Philippe Carrier","Ahmed Helal","Antonio Cavalcante","Khaled Ammar","Katja Hose","Essam Mansour"],"pdf_url":"https://arxiv.org/pdf/2303.02204v3.pdf","comment":"15 pages, 9 figures"},{"id":"http://arxiv.org/abs/2206.00759v3","updated":"2024-03-22T14:13:56Z","published":"2022-06-01T20:48:24Z","title":"Interpretability Guarantees with Merlin-Arthur Classifiers","summary":"  We propose an interactive multi-agent classifier that provides provable\ninterpretability guarantees even for complex agents such as neural networks.\nThese guarantees consist of lower bounds on the mutual information between\nselected features and the classification decision. Our results are inspired by\nthe Merlin-Arthur protocol from Interactive Proof Systems and express these\nbounds in terms of measurable metrics such as soundness and completeness.\nCompared to existing interactive setups, we rely neither on optimal agents nor\non the assumption that features are distributed independently. Instead, we use\nthe relative strength of the agents as well as the new concept of Asymmetric\nFeature Correlation which captures the precise kind of correlations that make\ninterpretability guarantees difficult. We evaluate our results on two\nsmall-scale datasets where high mutual information can be verified explicitly.\n","authors":["Stephan Wäldchen","Kartikey Sharma","Berkant Turan","Max Zimmer","Sebastian Pokutta"],"pdf_url":"https://arxiv.org/pdf/2206.00759v3.pdf","comment":"AISTATS24 Camera-Ready Version, 34 pages total (9 pages main part, 3\n  pages references, 22 pages appendix), 17 figures, 3 tables"},{"id":"http://arxiv.org/abs/2403.15218v1","updated":"2024-03-22T14:07:07Z","published":"2024-03-22T14:07:07Z","title":"Anytime, Anywhere, Anyone: Investigating the Feasibility of Segment\n  Anything Model for Crowd-Sourcing Medical Image Annotations","summary":"  Curating annotations for medical image segmentation is a labor-intensive and\ntime-consuming task that requires domain expertise, resulting in \"narrowly\"\nfocused deep learning (DL) models with limited translational utility. Recently,\nfoundation models like the Segment Anything Model (SAM) have revolutionized\nsemantic segmentation with exceptional zero-shot generalizability across\nvarious domains, including medical imaging, and hold a lot of promise for\nstreamlining the annotation process. However, SAM has yet to be evaluated in a\ncrowd-sourced setting to curate annotations for training 3D DL segmentation\nmodels. In this work, we explore the potential of SAM for crowd-sourcing\n\"sparse\" annotations from non-experts to generate \"dense\" segmentation masks\nfor training 3D nnU-Net models, a state-of-the-art DL segmentation model. Our\nresults indicate that while SAM-generated annotations exhibit high mean Dice\nscores compared to ground-truth annotations, nnU-Net models trained on\nSAM-generated annotations perform significantly worse than nnU-Net models\ntrained on ground-truth annotations ($p<0.001$, all).\n","authors":["Pranav Kulkarni","Adway Kanhere","Dharmam Savani","Andrew Chan","Devina Chatterjee","Paul H. Yi","Vishwa S. Parekh"],"pdf_url":"https://arxiv.org/pdf/2403.15218v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15210v1","updated":"2024-03-22T13:52:53Z","published":"2024-03-22T13:52:53Z","title":"Early Period of Training Impacts Out-of-Distribution Generalization","summary":"  Prior research has found that differences in the early period of neural\nnetwork training significantly impact the performance of in-distribution (ID)\ntasks. However, neural networks are often sensitive to out-of-distribution\n(OOD) data, making them less reliable in downstream applications. Yet, the\nimpact of the early training period on OOD generalization remains understudied\ndue to its complexity and lack of effective analytical methodologies. In this\nwork, we investigate the relationship between learning dynamics and OOD\ngeneralization during the early period of neural network training. We utilize\nthe trace of Fisher Information and sharpness, with a focus on gradual\nunfreezing (i.e. progressively unfreezing parameters during training) as the\nmethodology for investigation. Through a series of empirical experiments, we\nshow that 1) selecting the number of trainable parameters at different times\nduring training, i.e. realized by gradual unfreezing -- has a minuscule impact\non ID results, but greatly affects the generalization to OOD data; 2) the\nabsolute values of sharpness and trace of Fisher Information at the initial\nperiod of training are not indicative for OOD generalization, but the relative\nvalues could be; 3) the trace of Fisher Information and sharpness may be used\nas indicators for the removal of interventions during early period of training\nfor better OOD generalization.\n","authors":["Chen Cecilia Liu","Iryna Gurevych"],"pdf_url":"https://arxiv.org/pdf/2403.15210v1.pdf","comment":"WIP"}],"Computer Science and Game Theory":[{"id":"http://arxiv.org/abs/2403.15307v1","updated":"2024-03-22T15:58:39Z","published":"2024-03-22T15:58:39Z","title":"Strategic Network Creation for Enabling Greedy Routing","summary":"  In this paper, we present the first game-theoretic network creation model\nthat incorporates greedy routing, i.e., the agents in our model are embedded in\nsome metric space and strive for creating a network where all-pairs greedy\nrouting is enabled. In contrast to graph-theoretic shortest paths, our agents\nroute their traffic along greedy paths, which are sequences of nodes where the\ndistance in the metric space to the respective target node gets strictly\nsmaller by each hop. Besides enabling greedy routing, the agents also optimize\ntheir connection quality within the created network by constructing greedy\npaths with low stretch. This ensures that greedy routing is always possible in\nequilibrium networks, while realistically modeling the agents' incentives for\nlocal structural changes to the network. With this we augment the elegant\nnetwork creation model by Moscibroda, Schmidt, and Wattenhofer (PODC'06) with\nthe feature of greedy routing.\n  For our model, we analyze the existence of (approximate)-equilibria and the\ncomputational hardness in different underlying metric spaces. E.g., we\ncharacterize the set of equilibria in 1-2-metrics and tree metrics, we show\nthat in both metrics Nash equilibria always exist, and we prove that the\nwell-known $\\Theta$-graph construction yields constant-approximate Nash\nequilibria in Euclidean space. The latter justifies distributed network\nconstruction via $\\Theta$-graphs from a new point-of-view, since it shows that\nthis powerful technique not only guarantees networks having a low stretch but\nalso networks that are almost stable.\n","authors":["Julian Berger","Tobias Friedrich","Pascal Lenzner","Paraskevi Machaira","Janosch Ruff"],"pdf_url":"https://arxiv.org/pdf/2403.15307v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15293v1","updated":"2024-03-22T15:40:11Z","published":"2024-03-22T15:40:11Z","title":"Human behaviour through a LENS: How Linguistic content triggers Emotions\n  and Norms and determines Strategy choices","summary":"  Over the last two decades, a growing body of experimental research has\nprovided evidence that linguistic frames influence human behaviour in economic\ngames, beyond the economic consequences of the available actions. This article\nproposes a novel framework that transcends the traditional confines of\noutcome-based preference models. According to the LENS model, the Linguistic\ndescription of the decision problem triggers Emotional responses and suggests\npotential Norms of behaviour, which then interact to shape an individual's\nStrategic choice. The article reviews experimental evidence that supports each\npath of the LENS model. Furthermore, it identifies and discusses several\ncritical research questions that arise from this model, pointing towards\navenues for future inquiry.\n","authors":["Valerio Capraro"],"pdf_url":"https://arxiv.org/pdf/2403.15293v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15198v1","updated":"2024-03-22T13:34:41Z","published":"2024-03-22T13:34:41Z","title":"On the Weighted Top-Difference Distance: Axioms, Aggregation, and\n  Approximation","summary":"  We study a family of distance functions on rankings that allow for asymmetric\ntreatments of alternatives and consider the distinct relevance of the top and\nbottom positions for ordered lists. We provide a full axiomatic\ncharacterization of our distance. In doing so, we retrieve new\ncharacterizations of existing axioms and show how to effectively weaken them\nfor our purposes. This analysis highlights the generality of our distance as it\nembeds many (semi)metrics previously proposed in the literature. Subsequently,\nwe show that, notwithstanding its level of generality, our distance is still\nreadily applicable. We apply it to preference aggregation, studying the\nfeatures of the associated median voting rule. It is shown how the derived\npreference function satisfies many desirable features in the context of voting\nrules, ranging from fairness to majority and Pareto-related properties. We show\nhow to compute consensus rankings exactly, and provide generalized\nDiaconis-Graham inequalities that can be leveraged to obtain approximation\nalgorithms. Finally, we propose some truncation ideas for our distances\ninspired by Lu and Boutilier (2010). These can be leveraged to devise a\nPolynomial-Time-Approximation Scheme for the corresponding rank aggregation\nproblem.\n","authors":["Andrea Aveni","Ludovico Crippa","Giulio Principi"],"pdf_url":"https://arxiv.org/pdf/2403.15198v1.pdf","comment":"64 pages"},{"id":"http://arxiv.org/abs/2308.12520v2","updated":"2024-03-22T01:18:08Z","published":"2023-08-24T03:14:45Z","title":"A-PSRO: A Unified Strategy Learning Method with Advantage Function for\n  Normal-form Games","summary":"  Solving Nash equilibrium is the key challenge in normal-form games with large\nstrategy spaces, where open-ended learning frameworks offer an efficient\napproach. In this work, we propose an innovative unified open-ended learning\nframework A-PSRO, i.e., Advantage Policy Space Response Oracle, as a\ncomprehensive framework for both zero-sum and general-sum games. In particular,\nwe introduce the advantage function as an enhanced evaluation metric for\nstrategies, enabling a unified learning objective for agents engaged in\nnormal-form games. We prove that the advantage function exhibits favorable\nproperties and is connected with the Nash equilibrium, which can be used as an\nobjective to guide agents to learn strategies efficiently. Our experiments\nreveal that A-PSRO achieves a considerable decrease in exploitability in\nzero-sum games and an escalation in rewards in general-sum games, significantly\noutperforming previous PSRO algorithms.\n","authors":["Yudong Hu","Haoran Li","Congying Han","Tiande Guo","Mingqiang Li","Bonan Li"],"pdf_url":"https://arxiv.org/pdf/2308.12520v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15636v1","updated":"2024-03-22T22:27:28Z","published":"2024-03-22T22:27:28Z","title":"On the Variational Interpretation of Mirror Play in Monotone Games","summary":"  Mirror play (MP) is a well-accepted primal-dual multi-agent learning\nalgorithm where all agents simultaneously implement mirror descent in a\ndistributed fashion. The advantage of MP over vanilla gradient play lies in its\nusage of mirror maps that better exploit the geometry of decision domains.\nDespite extensive literature dedicated to the asymptotic convergence of MP to\nequilibrium, the understanding of the finite-time behavior of MP before\nreaching equilibrium is still rudimentary. To facilitate the study of MP's\nnon-equilibrium performance, this work establishes an equivalence between MP's\nfinite-time primal-dual path (mirror path) in monotone games and the\nclosed-loop Nash equilibrium path of a finite-horizon differential game,\nreferred to as mirror differential game (MDG). Our construction of MDG rests on\nthe Brezis-Ekeland variational principle, and the stage cost functional for MDG\nis Fenchel coupling between MP's iterates and associated gradient updates. The\nvariational interpretation of mirror path in static games as the equilibrium\npath in MDG holds in deterministic and stochastic cases. Such a variational\ninterpretation translates the non-equilibrium studies of learning dynamics into\na more tractable equilibrium analysis of dynamic games, as demonstrated in a\ncase study on the Cournot game, where MP dynamics corresponds to a linear\nquadratic game.\n","authors":["Yunian Pan","Tao Li","Quanyan Zhu"],"pdf_url":"https://arxiv.org/pdf/2403.15636v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15616v1","updated":"2024-03-22T21:06:48Z","published":"2024-03-22T21:06:48Z","title":"Balancing Fairness and Efficiency in Energy Resource Allocations","summary":"  Bringing fairness to energy resource allocation remains a challenge, due to\nthe complexity of system structures and economic interdependencies among users\nand system operators' decision-making. The rise of distributed energy resources\nhas introduced more diverse heterogeneous user groups, surpassing the\ncapabilities of traditional efficiency-oriented allocation schemes. Without\nexplicitly bringing fairness to user-system interaction, this disparity often\nleads to disproportionate payments for certain user groups due to their utility\nformats or group sizes.\n  Our paper addresses this challenge by formalizing the problem of fair energy\nresource allocation and introducing the framework for aggregators. This\nframework enables optimal fairness-efficiency trade-offs by selecting\nappropriate objectives in a principled way. By jointly optimizing over the\ntotal resources to allocate and individual allocations, our approach reveals\noptimized allocation schemes that lie on the Pareto front, balancing fairness\nand efficiency in resource allocation strategies.\n","authors":["Jiayi Li","Matthew Motoki","Baosen Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.15616v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15524v1","updated":"2024-03-22T14:13:11Z","published":"2024-03-22T14:13:11Z","title":"PPA-Game: Characterizing and Learning Competitive Dynamics Among Online\n  Content Creators","summary":"  We introduce the Proportional Payoff Allocation Game (PPA-Game) to model how\nagents, akin to content creators on platforms like YouTube and TikTok, compete\nfor divisible resources and consumers' attention. Payoffs are allocated to\nagents based on heterogeneous weights, reflecting the diversity in content\nquality among creators. Our analysis reveals that although a pure Nash\nequilibrium (PNE) is not guaranteed in every scenario, it is commonly observed,\nwith its absence being rare in our simulations. Beyond analyzing static\npayoffs, we further discuss the agents' online learning about resource payoffs\nby integrating a multi-player multi-armed bandit framework. We propose an\nonline algorithm facilitating each agent's maximization of cumulative payoffs\nover $T$ rounds. Theoretically, we establish that the regret of any agent is\nbounded by $O(\\log^{1 + \\eta} T)$ for any $\\eta > 0$. Empirical results further\nvalidate the effectiveness of our approach.\n","authors":["Renzhe Xu","Haotian Wang","Xingxuan Zhang","Bo Li","Peng Cui"],"pdf_url":"https://arxiv.org/pdf/2403.15524v1.pdf","comment":null}],"Information Theory":[{"id":"http://arxiv.org/abs/2403.15240v1","updated":"2024-03-22T14:32:37Z","published":"2024-03-22T14:32:37Z","title":"Information Rates of Successive Interference Cancellation for Optical\n  Fiber","summary":"  Successive interference cancellation (SIC) is used to approach the achievable\ninformation rates (AIRs) of joint detection and decoding for long-haul optical\nfiber links. The AIRs of memoryless ring constellations are compared to those\nof circularly symmetric complex Gaussian modulation for surrogate channel\nmodels with correlated phase noise. Simulations are performed for 1000 km of\nstandard single-mode fiber with ideal Raman amplification. In this setup, 32\nrings and 16 SIC-stages with Gaussian message-passing receivers achieve the AIR\npeaks of previous work. The computational complexity scales in proportion to\nthe number of SIC-stages, where one stage has the complexity of separate\ndetection and decoding.\n","authors":["Alex Jäger","Gerhard Kramer"],"pdf_url":"https://arxiv.org/pdf/2403.15240v1.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2403.15221v1","updated":"2024-03-22T14:11:36Z","published":"2024-03-22T14:11:36Z","title":"Mutual Information of a class of Poisson-type Channels using Markov\n  Renewal Theory","summary":"  The mutual information (MI) of Poisson-type channels has been linked to a\nfiltering problem since the 70s, but its evaluation for specific\ncontinuous-time, discrete-state systems remains a demanding task. As an\nadvantage, Markov renewal processes (MrP) retain their renewal property under\nstate space filtering. This offers a way to solve the filtering problem\nanalytically for small systems. We consider a class of communication systems $X\n\\to Y$ that can be derived from a MrP by a custom filtering procedure. For the\nsubclasses, where (i) $Y$ is a renewal process or (ii) $(X,Y)$ belongs to a\nclass of MrPs, we provide an evolution equation for finite transmission\nduration $T>0$ and limit theorems for $T \\to \\infty$ that facilitate\nsimulation-free evaluation of the MI $\\mathbb{I}(X_{[0,T]}; Y_{[0,T]})$ and its\nassociated mutual information rate (MIR). In other cases, simulation cost is\nsignificantly reduced. We show that systems with an additional $X$-modulating\nlevel $C$, which statically chooses between different processes $X_{[0,T]}(c)$,\ncan naturally be included in our framework thereby giving an expression for\n$\\mathbb{I}(C; Y_{[0,T]})$. The theoretical framework is showcased in an\napplication to bacterial gene expression, where filtering is analytically\ntractable.\n","authors":["Maximilian Gehri","Nicolai Engelmann","Heinz Koeppl"],"pdf_url":"https://arxiv.org/pdf/2403.15221v1.pdf","comment":"5 main pages, 1 main figure, 4 appendix pages, 2 appendix figures,\n  conference submission"},{"id":"http://arxiv.org/abs/2403.08343v2","updated":"2024-03-22T13:28:40Z","published":"2024-03-13T08:48:15Z","title":"Coverage and Rate Analysis for Integrated Sensing and Communication\n  Networks","summary":"  Integrated sensing and communication (ISAC) is increasingly recognized as a\npivotal technology for next-generation cellular networks, offering mutual\nbenefits in both sensing and communication capabilities. This advancement\nnecessitates a re-examination of the fundamental limits within networks where\nthese two functions coexist via shared spectrum and infrastructures. However,\ntraditional stochastic geometry-based performance analyses are confined to\neither communication or sensing networks separately. This paper bridges this\ngap by introducing a generalized stochastic geometry framework in ISAC\nnetworks. Based on this framework, we define and calculate the coverage and\nergodic rate of sensing and communication performance under resource\nconstraints. Then, we shed light on the fundamental limits of ISAC networks by\npresenting theoretical results for the coverage rate of the unified\nperformance, taking into account the coupling effects of dual functions in\ncoexistence networks. Further, we obtain the analytical formulations for\nevaluating the ergodic sensing rate constrained by the maximum communication\nrate, and the ergodic communication rate constrained by the maximum sensing\nrate. Extensive numerical results validate the accuracy of all theoretical\nderivations, and also indicate that denser networks significantly enhance ISAC\ncoverage. Specifically, increasing the base station density from $1$\n$\\text{km}^{-2}$ to $10$ $\\text{km}^{-2}$ can boost the ISAC coverage rate from\n$1.4\\%$ to $39.8\\%$. Further, results also reveal that with the increase of the\nconstrained sensing rate, the ergodic communication rate improves\nsignificantly, but the reverse is not obvious.\n","authors":["Xu Gan","Chongwen Huang","Zhaohui Yang","Xiaoming Chen","Jiguang He","Zhaoyang Zhang","Chau Yuen","Yong Liang Guan","Mérouane Debbah"],"pdf_url":"https://arxiv.org/pdf/2403.08343v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15165v1","updated":"2024-03-22T12:36:28Z","published":"2024-03-22T12:36:28Z","title":"Channel Orthogonalization with Reconfigurable Surfaces: General Models,\n  Theoretical Limits, and Effective Configuration","summary":"  We envision a future in which multi-antenna technology effectively exploits\nthe spatial domain as a set of non-interfering orthogonal resources, allowing\nfor flexible resource allocation and efficient modulation/demodulation.\nReconfigurable intelligent surface (RIS) has emerged as a promising technology\nwhich allows shaping the propagation environment for improved performance. This\npaper studies the ability of three extended types of reconfigurable surface\n(RS), including the recently proposed beyond diagonal RIS (BD-RIS), to achieve\nperfectly orthogonal channels in a general multi-user multiple-input\nmultiple-output (MU-MIMO) scenario. We propose practical implementations for\nthe three types of RS consisting of passive components, and obtain the\ncorresponding restrictions on their reconfigurability. We then use these\nrestrictions to derive closed-form conditions for achieving arbitrary\n(orthogonal) channels. We also study the problem of optimal orthogonal channel\nselection for achieving high channel gain without active amplification at the\nRS, and we propose some methods with satisfying performance. Finally, we\nprovide efficient channel estimation and RS configuration techniques such that\nall the computation, including the channel selection, may be performed at the\nbase station (BS). The numerical results showcase the potential and\npracticality of RS channel orthogonalization, thus taking a step towards\northogonal spatial domain multiplexing (OSDM).\n","authors":["Juan Vidal Alegría","Johan Thunberg","Ove Edfors"],"pdf_url":"https://arxiv.org/pdf/2403.15165v1.pdf","comment":"13 pages, 4 figures. This work has been submitted to the IEEE for\n  possible publication, copyright information may be affected upon publication"},{"id":"http://arxiv.org/abs/2403.15145v1","updated":"2024-03-22T11:56:41Z","published":"2024-03-22T11:56:41Z","title":"Robust Resource Allocation for STAR-RIS Assisted SWIPT Systems","summary":"  A simultaneously transmitting and reflecting reconfigurable intelligent\nsurface (STAR-RIS) assisted simultaneous wireless information and power\ntransfer (SWIPT) system is proposed. More particularly, an STAR-RIS is deployed\nto assist in the information/power transfer from a multi-antenna access point\n(AP) to multiple single-antenna information users (IUs) and energy users (EUs),\nwhere two practical STAR-RIS operating protocols, namely energy splitting (ES)\nand time switching (TS), are employed. Under the imperfect channel state\ninformation (CSI) condition, a multi-objective optimization problem (MOOP)\nframework, that simultaneously maximizes the minimum data rate and minimum\nharvested power, is employed to investigate the fundamental rate-energy\ntrade-off between IUs and EUs. To obtain the optimal robust resource allocation\nstrategy, the MOOP is first transformed into a single-objective optimization\nproblem (SOOP) via the {\\epsilon}-constraint method, which is then reformulated\nby approximating semi-infinite inequality constraints with the S-procedure. For\nES, an alternating optimization (AO)-based algorithm is proposed to jointly\ndesign AP active beamforming and STAR-RIS passive beamforming, where a penalty\nmethod is leveraged in STAR-RIS beamforming design. Furthermore, the developed\nalgorithm is extended to optimize the time allocation policy and beamforming\nvectors in a two-layer iterative manner for TS. Numerical results reveal that:\n1) deploying STAR-RISs achieves a significant performance gain over\nconventional RISs, especially in terms of harvested power for EUs; 2) the ES\nprotocol obtains a better user fairness performance when focusing only on IUs\nor EUs, while the TS protocol yields a better balance between IUs and EUs; 3)\nthe imperfect CSI affects IUs more significantly than EUs, whereas TS can\nconfer a more robust design to attenuate these effects.\n","authors":["Guangyu Zhu","Xidong Mu","Li Guo","Ao Huang","Shibiao Xu"],"pdf_url":"https://arxiv.org/pdf/2403.15145v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15131v1","updated":"2024-03-22T11:31:55Z","published":"2024-03-22T11:31:55Z","title":"Uplink soft handover for LEO constellations: how strong the\n  inter-satellite link should be","summary":"  We consider a constellation of low-earth-orbit (LEO) satellites connected to\na handheld device on the ground. Due to the very large orbital speed, an\neffective handover strategy becomes of paramount importance. In particular, we\nstudy the benefits of soft handover in the uplink from the physical-layer point\nof view. We give a realistic model for both the ground-to-satellite and the\ninter-satellite links, following the 3GPP channel model for the former. We\nsuppose that, during handover from a serving satellite to a target satellite,\none of the two satellites forwards the received signal from the ground user to\nthe other, thus acting as a relay. We quantify through simulations the loss of\nhard handover, compared to soft handover. For the latter, we test both\namplify-and-forward (AF) and decode-and-forward (DF) relaying techniques and\nverify that, at least in the simulated conditions, DF does not repay, in terms\nof block error rate (BLER), the increase of complexity with respect to AF.\nAlso, we study the effect of the LEO constellation size on the network BLER.\nFinally, we show that, with soft handover, the impact of misalignment on the\ninter-satellite link is severe, especially at optical frequencies.\n","authors":["Houcem Ben Salem","Alberto Tarable","Alessandro Nordio","Behrooz Makki"],"pdf_url":"https://arxiv.org/pdf/2403.15131v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15130v1","updated":"2024-03-22T11:31:22Z","published":"2024-03-22T11:31:22Z","title":"Coexisting Passive RIS and Active Relay Assisted NOMA Systems","summary":"  A novel coexisting passive reconfigurable intelligent surface (RIS) and\nactive decode-and-forward (DF) relay assisted non-orthogonal multiple access\n(NOMA) transmission framework is proposed. In particular, two communication\nprotocols are conceived, namely Hybrid NOMA (H-NOMA) and Full NOMA (F-NOMA).\nBased on the proposed two protocols, both the sum rate maximization and max-min\nrate fairness problems are formulated for jointly optimizing the power\nallocation at the access point and relay as well as the passive beamforming\ndesign at the RIS. To tackle the non-convex problems, an alternating\noptimization (AO) based algorithm is first developed, where the transmit power\nand the RIS phase-shift are alternatingly optimized by leveraging the\ntwo-dimensional search and rank-relaxed difference-of-convex (DC) programming,\nrespectively. Then, a two-layer penalty based joint optimization (JO) algorithm\nis developed to jointly optimize the resource allocation coefficients within\neach iteration. Finally, numerical results demonstrate that: i) the proposed\ncoexisting RIS and relay assisted transmission framework is capable of\nachieving a significant user performance improvement than conventional schemes\nwithout RIS or relay; ii) compared with the AO algorithm, the JO algorithm\nrequires less execution time at the cost of a slight performance loss; and iii)\nthe H-NOMA and F-NOMA protocols are generally preferable for ensuring user rate\nfairness and enhancing user sum rate, respectively.\n","authors":["Ao Huang","Li Guo","Xidong Mu","Chao Dong","Yuanwei Liu"],"pdf_url":"https://arxiv.org/pdf/2403.15130v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15120v1","updated":"2024-03-22T11:24:10Z","published":"2024-03-22T11:24:10Z","title":"STAR-RIS Assisted Downlink Active and Uplink Backscatter Communications\n  with NOMA","summary":"  A simultaneously transmitting and reflecting reconfigurable intelligent\nsurface (STAR-RIS) assisted downlink (DL) active and uplink (UL) backscatter\ncommunication (BackCom) framework is proposed. More particularly, a full-duplex\n(FD) base station (BS) communicates with the DL users via the STAR-RIS's\ntransmission link, while exciting and receiving the information from the UL\nBackCom devices with the aid of the STAR-RIS's reflection link. Non-orthogonal\nmultiple access (NOMA) is exploited in both DL and UL communications for\nimproving the spectrum efficiency. The system weighted sum rate maximization\nproblem is formulated for jointly optimizing the FD BS active receive and\ntransmit beamforming, the STAR- RIS passive beamforming, and the DL NOMA\ndecoding orders, subject to the DL user's individual rate constraint. To tackle\nthis challenging non-convex problem, we propose an alternating optimization\n(AO) based algorithm for the joint active and passive beamforming design with a\ngiven DL NOMA decoding order. To address the potential high computational\ncomplexity required for exhaustive searching all the NOMA decoding orders, an\nefficient NOMA user ordering scheme is further developed. Finally, numerical\nresults demonstrate that: i) compared with the baseline schemes employing\nconventional RISs or space division multiple access, the proposed scheme\nachieves higher performance gains; and ii) higher UL rate gain is obtained at a\ncost of DL performance degradation, as a remedy, a more flexible performance\ntradeoff can be achieved by introducing the STAR-RIS.\n","authors":["Ao Huang","Xidong Mu","Li Guo"],"pdf_url":"https://arxiv.org/pdf/2403.15120v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14978v1","updated":"2024-03-22T06:22:34Z","published":"2024-03-22T06:22:34Z","title":"Range-Angle Estimation for FDA-MIMO System With Frequency Offset","summary":"  Frequency diverse array multiple-input multiple-output (FDA-MIMO) radar\ndiffers from the traditional phased array (PA) radar, and can form\nrange-angle-dependent beampattern and differentiate between closely spaced\ntargets sharing the same angle but occupying distinct range cells. In the\nFDA-MIMO radar, target range estimation is achieved by employing a subtle\nfrequency variation between adjacent array antennas, so the estimation\nperformance is degraded severely in a practical scenario with frequency offset.\nIn this paper, the range-angle estimation problem for FDA-MIMO radar is\nconsidered with frequency offsets in both transmitting and receiving arrays.\nFirst, we build a system model for the FDA-MIMO radar with transmitting and\nreceiving frequency offsets. Then, the frequency offset is transferred into an\nequalized additional noise. The noise characteristics are analyzed in detail\ntheoretically, together with the influence on the range-angle estimation.\nMoreover, since the effect of the transmitting frequency offset is similar to\nadditional colored noise, denoising algorithms are introduced to mitigate the\nperformance deterioration caused by the frequency offset. Finally,\nCram\\'{e}r-Rao lower bounds (CRLB) for the range-angle estimation are derived\nin the scenario with the frequency offsets. Simulation results show the\nanalysis of frequency offset and the corresponding estimation performance using\ndifferent algorithms.\n","authors":["Mengjiang Sun","Peng Chen","Zhenxin Cao"],"pdf_url":"https://arxiv.org/pdf/2403.14978v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14943v1","updated":"2024-03-22T04:32:53Z","published":"2024-03-22T04:32:53Z","title":"Primary Rate Maximization in Movable Antennas Empowered Symbiotic Radio\n  Communications","summary":"  In this paper, we propose a movable antenna (MA) empowered scheme for\nsymbiotic radio (SR) communication systems. Specifically, multiple antennas at\nthe primary transmitter (PT) can be flexibly moved to favorable locations to\nboost the channel conditions of the primary and secondary transmissions. The\nprimary transmission is achieved by the active transmission from the PT to the\nprimary user (PU), while the backscatter device (BD) takes a ride over the\nincident signal from the PT to passively send the secondary signal to the PU.\nUnder this setup, we consider a primary rate maximization problem by jointly\noptimizing the transmit beamforming and the positions of MAs at the PT under a\npractical bit error rate constraint on the secondary transmission. Then, an\nalternating optimization framework with the utilization of the successive\nconvex approximation, semi-definite processing and simulated annealing (SA)\nmodified particle swarm optimization (SA-PSO) methods is proposed to find the\nsolution of the transmit beamforming and MAs' positions. Finally, numerical\nresults are provided to demonstrate the performance improvement provided by the\nproposed MA empowered scheme and the proposed algorithm.\n","authors":["Bin Lyu","Hao Liu","Wenqing Hong","Shimin Gong","Feng Tian"],"pdf_url":"https://arxiv.org/pdf/2403.14943v1.pdf","comment":"To appear in IEEE VTC-Spring 2024. 6 Pages,5 figures"}]},"2024-03-23T00:00:00Z":{"Distributed, Parallel, and Cluster Computing":[{"id":"http://arxiv.org/abs/2403.11259v2","updated":"2024-03-23T08:27:02Z","published":"2024-03-17T16:23:00Z","title":"A learning-based solution approach to the application placement problem\n  in mobile edge computing under uncertainty","summary":"  Placing applications in mobile edge computing servers presents a complex\nchallenge involving many servers, users, and their requests. Existing\nalgorithms take a long time to solve high-dimensional problems with significant\nuncertainty scenarios. Therefore, an efficient approach is required to maximize\nthe quality of service while considering all technical constraints. One of\nthese approaches is machine learning, which emulates optimal solutions for\napplication placement in edge servers. Machine learning models are expected to\nlearn how to allocate user requests to servers based on the spatial positions\nof users and servers. In this study, the problem is formulated as a two-stage\nstochastic programming. A sufficient amount of training records is generated by\nvarying parameters such as user locations, their request rates, and solving the\noptimization model. Then, based on the distance features of each user from the\navailable servers and their request rates, machine learning models generate\ndecision variables for the first stage of the stochastic optimization model,\nwhich is the user-to-server request allocation, and are employed as independent\ndecision agents that reliably mimic the optimization model. Support Vector\nMachines (SVM) and Multi-layer Perceptron (MLP) are used in this research to\nachieve practical decisions from the stochastic optimization models. The\nperformance of each model has shown an execution effectiveness of over 80%.\nThis research aims to provide a more efficient approach for tackling\nhigh-dimensional problems and scenarios with uncertainties in mobile edge\ncomputing by leveraging machine learning models for optimal decision-making in\nrequest allocation to edge servers. These results suggest that machine-learning\nmodels can significantly improve solution times compared to conventional\napproaches.\n","authors":["Taha-Hossein Hejazi","Zahra Ghadimkhani","Arezoo Borji"],"pdf_url":"https://arxiv.org/pdf/2403.11259v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15927v1","updated":"2024-03-23T20:21:46Z","published":"2024-03-23T20:21:46Z","title":"LOAM: Low-latency Communication, Caching, and Computation Placement in\n  Data-Intensive Computing Networks","summary":"  Deploying data- and computation-intensive applications such as large-scale AI\ninto heterogeneous dispersed computing networks can significantly enhance\napplication performance by mitigating bottlenecks caused by limited network\nresources, including bandwidth, storage, and computing power. However, current\nresource allocation methods in dispersed computing do not provide a\ncomprehensive solution that considers arbitrary topology, elastic resource\namount, reuse of computation results, and nonlinear congestion-dependent\noptimization objectives. In this paper, we propose LOAM, a low-latency joint\ncommunication, caching, and computation placement framework with a rigorous\nanalytical foundation that incorporates the above aspects. We tackle the\nNP-hard aggregated cost minimization problem with two methods: an offline\nmethod with a 1/2 approximation and an online adaptive method with a bounded\ngap from the optimum. Through extensive simulation, the proposed framework\noutperforms multiple baselines in both synthesis and real-world network\nscenarios.\n","authors":["Jinkun Zhang","Edmund Yeh"],"pdf_url":"https://arxiv.org/pdf/2403.15927v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.10681v4","updated":"2024-03-23T15:51:29Z","published":"2023-02-21T14:03:22Z","title":"FrankenSplit: Efficient Neural Feature Compression with Shallow\n  Variational Bottleneck Injection for Mobile Edge Computing","summary":"  The rise of mobile AI accelerators allows latency-sensitive applications to\nexecute lightweight Deep Neural Networks (DNNs) on the client side. However,\ncritical applications require powerful models that edge devices cannot host and\nmust therefore offload requests, where the high-dimensional data will compete\nfor limited bandwidth. This work proposes shifting away from focusing on\nexecuting shallow layers of partitioned DNNs. Instead, it advocates\nconcentrating the local resources on variational compression optimized for\nmachine interpretability. We introduce a novel framework for resource-conscious\ncompression models and extensively evaluate our method in an environment\nreflecting the asymmetric resource distribution between edge devices and\nservers. Our method achieves 60% lower bitrate than a state-of-the-art SC\nmethod without decreasing accuracy and is up to 16x faster than offloading with\nexisting codec standards.\n","authors":["Alireza Furutanpey","Philipp Raith","Schahram Dustdar"],"pdf_url":"https://arxiv.org/pdf/2302.10681v4.pdf","comment":"Submission to IEEE Transactions on Mobile Computing"},{"id":"http://arxiv.org/abs/2403.15855v1","updated":"2024-03-23T14:24:36Z","published":"2024-03-23T14:24:36Z","title":"Initialisation and Topology Effects in Decentralised Federated Learning","summary":"  Fully decentralised federated learning enables collaborative training of\nindividual machine learning models on distributed devices on a network while\nkeeping the training data localised. This approach enhances data privacy and\neliminates both the single point of failure and the necessity for central\ncoordination. Our research highlights that the effectiveness of decentralised\nfederated learning is significantly influenced by the network topology of\nconnected devices. A simplified numerical model for studying the early\nbehaviour of these systems leads us to an improved artificial neural network\ninitialisation strategy, which leverages the distribution of eigenvector\ncentralities of the nodes of the underlying network, leading to a radically\nimproved training efficiency. Additionally, our study explores the scaling\nbehaviour and choice of environmental parameters under our proposed\ninitialisation strategy. This work paves the way for more efficient and\nscalable artificial neural network training in a distributed and uncoordinated\nenvironment, offering a deeper understanding of the intertwining roles of\nnetwork structure and learning dynamics.\n","authors":["Arash Badie-Modiri","Chiara Boldrini","Lorenzo Valerio","János Kertész","Márton Karsai"],"pdf_url":"https://arxiv.org/pdf/2403.15855v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15815v1","updated":"2024-03-23T12:09:16Z","published":"2024-03-23T12:09:16Z","title":"Resource-efficient Parallel Split Learning in Heterogeneous Edge\n  Computing","summary":"  Edge AI has been recently proposed to facilitate the training and deployment\nof Deep Neural Network (DNN) models in proximity to the sources of data. To\nenable the training of large models on resource-constraint edge devices and\nprotect data privacy, parallel split learning is becoming a practical and\npopular approach. However, current parallel split learning neglects the\nresource heterogeneity of edge devices, which may lead to the straggler issue.\nIn this paper, we propose EdgeSplit, a novel parallel split learning framework\nto better accelerate distributed model training on heterogeneous and\nresource-constraint edge devices. EdgeSplit enhances the efficiency of model\ntraining on less powerful edge devices by adaptively segmenting the model into\nvarying depths. Our approach focuses on reducing total training time by\nformulating and solving a task scheduling problem, which determines the most\nefficient model partition points and bandwidth allocation for each device. We\nemploy a straightforward yet effective alternating algorithm for this purpose.\nComprehensive tests conducted with a range of DNN models and datasets\ndemonstrate that EdgeSplit not only facilitates the training of large models on\nresource-restricted edge devices but also surpasses existing baselines in\nperformance.\n","authors":["Mingjin Zhang","Jiannong Cao","Yuvraj Sahni","Xiangchun Chen","Shan Jiang"],"pdf_url":"https://arxiv.org/pdf/2403.15815v1.pdf","comment":"Accepted by International Conference on Computing, Networking and\n  Communications (ICNC 2024)"},{"id":"http://arxiv.org/abs/2308.11841v2","updated":"2024-03-23T08:45:03Z","published":"2023-08-23T00:17:51Z","title":"A Survey for Federated Learning Evaluations: Goals and Measures","summary":"  Evaluation is a systematic approach to assessing how well a system achieves\nits intended purpose. Federated learning (FL) is a novel paradigm for\nprivacy-preserving machine learning that allows multiple parties to\ncollaboratively train models without sharing sensitive data. However,\nevaluating FL is challenging due to its interdisciplinary nature and diverse\ngoals, such as utility, efficiency, and security. In this survey, we first\nreview the major evaluation goals adopted in the existing studies and then\nexplore the evaluation metrics used for each goal. We also introduce FedEval,\nan open-source platform that provides a standardized and comprehensive\nevaluation framework for FL algorithms in terms of their utility, efficiency,\nand security. Finally, we discuss several challenges and future research\ndirections for FL evaluation.\n","authors":["Di Chai","Leye Wang","Liu Yang","Junxue Zhang","Kai Chen","Qiang Yang"],"pdf_url":"https://arxiv.org/pdf/2308.11841v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15760v1","updated":"2024-03-23T08:24:09Z","published":"2024-03-23T08:24:09Z","title":"An Upload-Efficient Scheme for Transferring Knowledge From a Server-Side\n  Pre-trained Generator to Clients in Heterogeneous Federated Learning","summary":"  Heterogeneous Federated Learning (HtFL) enables collaborative learning on\nmultiple clients with different model architectures while preserving privacy.\nDespite recent research progress, knowledge sharing in HtFL is still difficult\ndue to data and model heterogeneity. To tackle this issue, we leverage the\nknowledge stored in pre-trained generators and propose a new upload-efficient\nknowledge transfer scheme called Federated Knowledge-Transfer Loop (FedKTL).\nOur FedKTL can produce client-task-related prototypical image-vector pairs via\nthe generator's inference on the server. With these pairs, each client can\ntransfer pre-existing knowledge from the generator to its local model through\nan additional supervised local task. We conduct extensive experiments on four\ndatasets under two types of data heterogeneity with 14 kinds of models\nincluding CNNs and ViTs. Results show that our upload-efficient FedKTL\nsurpasses seven state-of-the-art methods by up to 7.31% in accuracy. Moreover,\nour knowledge transfer scheme is applicable in scenarios with only one edge\nclient. Code: https://github.com/TsingZ0/FedKTL\n","authors":["Jianqing Zhang","Yang Liu","Yang Hua","Jian Cao"],"pdf_url":"https://arxiv.org/pdf/2403.15760v1.pdf","comment":"Accepted by CVPR2024"},{"id":"http://arxiv.org/abs/2403.15721v1","updated":"2024-03-23T05:01:26Z","published":"2024-03-23T05:01:26Z","title":"Radical-Cylon: A Heterogeneous Data Pipeline for Scientific Computing","summary":"  Managing and preparing complex data for deep learning, a prevalent approach\nin large-scale data science can be challenging. Data transfer for model\ntraining also presents difficulties, impacting scientific fields like genomics,\nclimate modeling, and astronomy. A large-scale solution like Google Pathways\nwith a distributed execution environment for deep learning models exists but is\nproprietary. Integrating existing open-source, scalable runtime tools and data\nframeworks on high-performance computing (HPC) platforms are crucial to address\nthese challenges. Our objective is to establish a smooth and unified method of\ncombining data engineering and deep learning frameworks with diverse execution\ncapabilities that can be deployed on various high-performance computing\nplatforms, including cloud and supercomputers. We aim to support heterogeneous\nsystems with accelerators, where Cylon and other data engineering and deep\nlearning frameworks can utilize heterogeneous execution. To achieve this, we\npropose Radical-Cylon, a heterogeneous runtime system with a parallel and\ndistributed data framework to execute Cylon as a task of Radical Pilot. We\nthoroughly explain Radical-Cylon's design and development and the execution\nprocess of Cylon tasks using Radical Pilot. This approach enables the use of\nheterogeneous MPI-communicators across multiple nodes. Radical-Cylon achieves\nbetter performance than Bare-Metal Cylon with minimal and constant overhead.\nRadical-Cylon achieves (4~15)% faster execution time than batch execution while\nperforming similar join and sort operations with 35 million and 3.5 billion\nrows with the same resources. The approach aims to excel in both scientific and\nengineering research HPC systems while demonstrating robust performance on\ncloud infrastructures. This dual capability fosters collaboration and\ninnovation within the open-source scientific research community.\n","authors":["Arup Kumar Sarker","Aymen Alsaadi","Niranda Perera","Mills Staylor","Gregor von Laszewski","Matteo Turilli","Ozgur Ozan Kilic","Mikhail Titov","Andre Merzky","Shantenu Jha","Geoffrey Fox"],"pdf_url":"https://arxiv.org/pdf/2403.15721v1.pdf","comment":"14 pages, 16 figures, 2 tables"},{"id":"http://arxiv.org/abs/2403.15701v1","updated":"2024-03-23T03:40:45Z","published":"2024-03-23T03:40:45Z","title":"Navigating the Landscape of Distributed File Systems: Architectures,\n  Implementations, and Considerations","summary":"  Distributed File Systems (DFS) have emerged as sophisticated solutions for\nefficient file storage and management across interconnected computer nodes. The\nmain objective of DFS is to achieve flexible, scalable, and resilient file\nstorage management by dispersing file data across multiple interconnected\ncomputer nodes, enabling users to seamlessly access and manipulate files\ndistributed across diverse nodes. This article provides an overview of DFS, its\narchitecture, classification methods, design considerations, challenges, and\ncommon implementations. Common DFS implementations discussed include NFS, AFS,\nGFS, HDFS, and CephFS, each tailored to specific use cases and design goals.\nUnderstanding the nuances of DFS architecture, classification, and design\nconsiderations is crucial for developing efficient, stable, and secure\ndistributed file systems to meet diverse user and application needs in modern\ncomputing environments.\n","authors":["Xueting Pan","Ziqian Luo","Lisang Zhou"],"pdf_url":"https://arxiv.org/pdf/2403.15701v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15665v1","updated":"2024-03-23T01:25:19Z","published":"2024-03-23T01:25:19Z","title":"Improved Methods of Task Assignment and Resource Allocation with\n  Preemption in Edge Computing Systems","summary":"  Edge computing has become a very popular service that enables mobile devices\nto run complex tasks with the help of network-based computing resources.\nHowever, edge clouds are often resource-constrained, which makes resource\nallocation a challenging issue. In addition, edge cloud servers must make\nallocation decisions with only limited information available, since the arrival\nof future client tasks might be impossible to predict, and the states and\nbehavior of neighboring servers might be obscured. We focus on a distributed\nresource allocation method in which servers operate independently and do not\ncommunicate with each other, but interact with clients (tasks) to make\nallocation decisions. We follow a two-round bidding approach to assign tasks to\nedge cloud servers, and servers are allowed to preempt previous tasks to\nallocate more useful ones. We evaluate the performance of our system using\nrealistic simulations and real-world trace data from a high-performance\ncomputing cluster. Results show that our heuristic improves system-wide\nperformance by $20-25\\%$ over previous work when accounting for the time taken\nby each approach. In this way, an ideal trade-off between performance and speed\nis achieved.\n","authors":["Caroline Rublein","Fidan Mehmeti","Mark Mahon","Thomas F. La Porta"],"pdf_url":"https://arxiv.org/pdf/2403.15665v1.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2403.15839v1","updated":"2024-03-23T13:28:37Z","published":"2024-03-23T13:28:37Z","title":"TablePuppet: A Generic Framework for Relational Federated Learning","summary":"  Current federated learning (FL) approaches view decentralized training data\nas a single table, divided among participants either horizontally (by rows) or\nvertically (by columns). However, these approaches are inadequate for handling\ndistributed relational tables across databases. This scenario requires\nintricate SQL operations like joins and unions to obtain the training data,\nwhich is either costly or restricted by privacy concerns. This raises the\nquestion: can we directly run FL on distributed relational tables?\n  In this paper, we formalize this problem as relational federated learning\n(RFL). We propose TablePuppet, a generic framework for RFL that decomposes the\nlearning process into two steps: (1) learning over join (LoJ) followed by (2)\nlearning over union (LoU). In a nutshell, LoJ pushes learning down onto the\nvertical tables being joined, and LoU further pushes learning down onto the\nhorizontal partitions of each vertical table. TablePuppet incorporates\ncomputation/communication optimizations to deal with the duplicate tuples\nintroduced by joins, as well as differential privacy (DP) to protect against\nboth feature and label leakages. We demonstrate the efficiency of TablePuppet\nin combination with two widely-used ML training algorithms, stochastic gradient\ndescent (SGD) and alternating direction method of multipliers (ADMM), and\ncompare their computation/communication complexity. We evaluate the SGD/ADMM\nalgorithms developed atop TablePuppet by training diverse ML models. Our\nexperimental results show that TablePuppet achieves model accuracy comparable\nto the centralized baselines running directly atop the SQL results. Moreover,\nADMM takes less communication time than SGD to converge to similar model\naccuracy.\n","authors":["Lijie Xu","Chulin Xie","Yiran Guo","Gustavo Alonso","Bo Li","Guoliang Li","Wei Wang","Wentao Wu","Ce Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.15839v1.pdf","comment":"14 pages, 8 figures"},{"id":"http://arxiv.org/abs/2403.15717v1","updated":"2024-03-23T04:44:55Z","published":"2024-03-23T04:44:55Z","title":"Ev-Edge: Efficient Execution of Event-based Vision Algorithms on\n  Commodity Edge Platforms","summary":"  Event cameras have emerged as a promising sensing modality for autonomous\nnavigation systems, owing to their high temporal resolution, high dynamic range\nand negligible motion blur. To process the asynchronous temporal event streams\nfrom such sensors, recent research has shown that a mix of Artificial Neural\nNetworks (ANNs), Spiking Neural Networks (SNNs) as well as hybrid SNN-ANN\nalgorithms are necessary to achieve high accuracies across a range of\nperception tasks. However, we observe that executing such workloads on\ncommodity edge platforms which feature heterogeneous processing elements such\nas CPUs, GPUs and neural accelerators results in inferior performance. This is\ndue to the mismatch between the irregular nature of event streams and diverse\ncharacteristics of algorithms on the one hand and the underlying hardware\nplatform on the other. We propose Ev-Edge, a framework that contains three key\noptimizations to boost the performance of event-based vision systems on edge\nplatforms: (1) An Event2Sparse Frame converter directly transforms raw event\nstreams into sparse frames, enabling the use of sparse libraries with minimal\nencoding overheads (2) A Dynamic Sparse Frame Aggregator merges sparse frames\nat runtime by trading off the temporal granularity of events and computational\ndemand thereby improving hardware utilization (3) A Network Mapper maps\nconcurrently executing tasks to different processing elements while also\nselecting layer precision by considering both compute and communication\noverheads. On several state-of-art networks for a range of autonomous\nnavigation tasks, Ev-Edge achieves 1.28x-2.05x improvements in latency and\n1.23x-2.15x in energy over an all-GPU implementation on the NVIDIA Jetson\nXavier AGX platform for single-task execution scenarios. Ev-Edge also achieves\n1.43x-1.81x latency improvements over round-robin scheduling methods in\nmulti-task execution scenarios.\n","authors":["Shrihari Sridharan","Surya Selvam","Kaushik Roy","Anand Raghunathan"],"pdf_url":"https://arxiv.org/pdf/2403.15717v1.pdf","comment":null}],"Databases":[{"id":"http://arxiv.org/abs/2403.13597v2","updated":"2024-03-23T17:05:15Z","published":"2024-03-20T13:44:30Z","title":"No more optimization rules: LLM-enabled policy-based multi-modal query\n  optimizer","summary":"  Large language model (LLM) has marked a pivotal moment in the field of\nmachine learning and deep learning. Recently its capability for query planning\nhas been investigated, including both single-modal and multi-modal queries.\nHowever, there is no work on the query optimization capability of LLM. As a\ncritical (or could even be the most important) step that significantly impacts\nthe execution performance of the query plan, such analysis and attempts should\nnot be missed. From another aspect, existing query optimizers are usually\nrule-based or rule-based + cost-based, i.e., they are dependent on manually\ncreated rules to complete the query plan rewrite/transformation. Given the fact\nthat modern optimizers include hundreds to thousands of rules, designing a\nmulti-modal query optimizer following a similar way is significantly\ntime-consuming since we will have to enumerate as many multi-modal optimization\nrules as possible, which has not been well addressed today. In this paper, we\ninvestigate the query optimization ability of LLM and use LLM to design LaPuda,\na novel LLM and Policy based multi-modal query optimizer. Instead of\nenumerating specific and detailed rules, LaPuda only needs a few abstract\npolicies to guide LLM in the optimization, by which much time and human effort\nare saved. Furthermore, to prevent LLM from making mistakes or negative\noptimization, we borrow the idea of gradient descent and propose a guided cost\ndescent (GCD) algorithm to perform the optimization, such that the optimization\ncan be kept in the correct direction. In our evaluation, our methods\nconsistently outperform the baselines in most cases. For example, the optimized\nplans generated by our methods result in 1~3x higher execution speed than those\nby the baselines.\n","authors":["Yifan Wang","Haodi Ma","Daisy Zhe Wang"],"pdf_url":"https://arxiv.org/pdf/2403.13597v2.pdf","comment":"Yifan and Haodi contribute equally to the work"},{"id":"http://arxiv.org/abs/2403.15839v1","updated":"2024-03-23T13:28:37Z","published":"2024-03-23T13:28:37Z","title":"TablePuppet: A Generic Framework for Relational Federated Learning","summary":"  Current federated learning (FL) approaches view decentralized training data\nas a single table, divided among participants either horizontally (by rows) or\nvertically (by columns). However, these approaches are inadequate for handling\ndistributed relational tables across databases. This scenario requires\nintricate SQL operations like joins and unions to obtain the training data,\nwhich is either costly or restricted by privacy concerns. This raises the\nquestion: can we directly run FL on distributed relational tables?\n  In this paper, we formalize this problem as relational federated learning\n(RFL). We propose TablePuppet, a generic framework for RFL that decomposes the\nlearning process into two steps: (1) learning over join (LoJ) followed by (2)\nlearning over union (LoU). In a nutshell, LoJ pushes learning down onto the\nvertical tables being joined, and LoU further pushes learning down onto the\nhorizontal partitions of each vertical table. TablePuppet incorporates\ncomputation/communication optimizations to deal with the duplicate tuples\nintroduced by joins, as well as differential privacy (DP) to protect against\nboth feature and label leakages. We demonstrate the efficiency of TablePuppet\nin combination with two widely-used ML training algorithms, stochastic gradient\ndescent (SGD) and alternating direction method of multipliers (ADMM), and\ncompare their computation/communication complexity. We evaluate the SGD/ADMM\nalgorithms developed atop TablePuppet by training diverse ML models. Our\nexperimental results show that TablePuppet achieves model accuracy comparable\nto the centralized baselines running directly atop the SQL results. Moreover,\nADMM takes less communication time than SGD to converge to similar model\naccuracy.\n","authors":["Lijie Xu","Chulin Xie","Yiran Guo","Gustavo Alonso","Bo Li","Guoliang Li","Wei Wang","Wentao Wu","Ce Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.15839v1.pdf","comment":"14 pages, 8 figures"},{"id":"http://arxiv.org/abs/2403.15807v1","updated":"2024-03-23T11:34:17Z","published":"2024-03-23T11:34:17Z","title":"Efficient Data Access Paths for Mixed Vector-Relational Search","summary":"  The rapid growth of machine learning capabilities and the adoption of data\nprocessing methods using vector embeddings sparked a great interest in creating\nsystems for vector data management. While the predominant approach of vector\ndata management is to use specialized index structures for fast search over the\nentirety of the vector embeddings, once combined with other (meta)data, the\nsearch queries can also become selective on relational attributes - typical for\nanalytical queries. As using vector indexes differs from traditional relational\ndata access, we revisit and analyze alternative access paths for efficient\nmixed vector-relational search.\n  We first evaluate the accurate but exhaustive scan-based search and propose\nhardware optimizations and alternative tensor-based formulation and batching to\noffset the cost. We outline the complex access-path design space, primarily\ndriven by relational selectivity, and the decisions to consider when selecting\nan exhaustive scan-based search against an approximate index-based approach.\nSince the vector index primarily avoids expensive computation across the entire\ndataset, contrary to the common relational knowledge, it is better to scan at\nlower selectivity and probe at higher, with a cross-point between the two\napproaches dictated by data dimensionality and the number of concurrent search\nqueries.\n","authors":["Viktor Sanca","Anastasia Ailamaki"],"pdf_url":"https://arxiv.org/pdf/2403.15807v1.pdf","comment":null}],"Performance":[{"id":"http://arxiv.org/abs/2312.06203v2","updated":"2024-03-23T06:38:37Z","published":"2023-12-11T08:36:27Z","title":"Offloading and Quality Control for AI Generated Content Services in 6G\n  Mobile Edge Computing Networks","summary":"  AI-Generated Content (AIGC), as a novel manner of providing Metaverse\nservices in the forthcoming Internet paradigm, can resolve the obstacles of\nimmersion requirements. Concurrently, edge computing, as an evolutionary\nparadigm of computing in communication systems, effectively augments real-time\ninteractive services. In pursuit of enhancing the accessibility of AIGC\nservices, the deployment of AIGC models (e.g., diffusion models) to edge\nservers and local devices has become a prevailing trend. Nevertheless, this\napproach faces constraints imposed by battery life and computational resources\nwhen tasks are offloaded to local devices, limiting the capacity to deliver\nhigh-quality content to users while adhering to stringent latency requirements.\nSo there will be a tradeoff between the utility of AIGC models and offloading\ndecisions in the edge computing paradigm. This paper proposes a joint\noptimization algorithm for offloading decisions, computation time, and\ndiffusion steps of the diffusion models in the reverse diffusion stage.\nMoreover, we take the average error into consideration as the metric for\nevaluating the quality of the generated results. Experimental results\nconclusively demonstrate that the proposed algorithm achieves superior joint\noptimization performance compared to the baselines.\n","authors":["Yitong Wang","Chang Liu","Jun Zhao"],"pdf_url":"https://arxiv.org/pdf/2312.06203v2.pdf","comment":"This paper appears in the 2024 IEEE 99th Vehicular Technology\n  Conference (VTC)"}],"Operating Systems":[{"id":"http://arxiv.org/abs/2403.15884v1","updated":"2024-03-23T16:35:37Z","published":"2024-03-23T16:35:37Z","title":"UPSS: a User-centric Private Storage System with its applications","summary":"  Strong confidentiality, integrity, user control, reliability and performance\nare critical requirements in privacy-sensitive applications. Such applications\nwould benefit from a data storage and sharing infrastructure that provides\nthese properties even in decentralized topologies with untrusted storage\nbackends, but users today are forced to choose between systemic security\nproperties and system reliability or performance. As an alternative to this\nstatus quo we present UPSS: the user-centric private sharing system, a\ncryptographic storage system that can be used as a conventional filesystem or\nas the foundation for security-sensitive applications such as redaction with\nintegrity and private revision control. We demonstrate that both the security\nand performance properties of UPSS exceed that of existing cryptographic\nfilesystems and that its performance is comparable to mature conventional\nfilesystems - in some cases, even superior. Whether used directly via its Rust\nAPI or as a conventional filesystem, UPSS provides strong security and\npractical performance on untrusted storage.\n","authors":["Arastoo Bozorgi","Mahya Soleimani Jadidi","Jonathan Anderson"],"pdf_url":"https://arxiv.org/pdf/2403.15884v1.pdf","comment":null}],"Computer Science and Game Theory":[{"id":"http://arxiv.org/abs/2403.15848v1","updated":"2024-03-23T13:51:31Z","published":"2024-03-23T13:51:31Z","title":"On the Stability of Learning in Network Games with Many Players","summary":"  Multi-agent learning algorithms have been shown to display complex, unstable\nbehaviours in a wide array of games. In fact, previous works indicate that\nconvergent behaviours are less likely to occur as the total number of agents\nincreases. This seemingly prohibits convergence to stable strategies, such as\nNash Equilibria, in games with many players.\n  To make progress towards addressing this challenge we study the Q-Learning\nDynamics, a classical model for exploration and exploitation in multi-agent\nlearning. In particular, we study the behaviour of Q-Learning on games where\ninteractions between agents are constrained by a network. We determine a number\nof sufficient conditions, depending on the game and network structure, which\nguarantee that agent strategies converge to a unique stable strategy, called\nthe Quantal Response Equilibrium (QRE). Crucially, these sufficient conditions\nare independent of the total number of agents, allowing for provable\nconvergence in arbitrarily large games.\n  Next, we compare the learned QRE to the underlying NE of the game, by showing\nthat any QRE is an $\\epsilon$-approximate Nash Equilibrium. We first provide\ntight bounds on $\\epsilon$ and show how these bounds lead naturally to a\ncentralised scheme for choosing exploration rates, which enables independent\nlearners to learn stable approximate Nash Equilibrium strategies. We validate\nthe method through experiments and demonstrate its effectiveness even in the\npresence of numerous agents and actions. Through these results, we show that\nindependent learning dynamics may converge to approximate Nash Equilibria, even\nin the presence of many agents.\n","authors":["Aamal Hussain","Dan Leonte","Francesco Belardinelli","Georgios Piliouras"],"pdf_url":"https://arxiv.org/pdf/2403.15848v1.pdf","comment":"AAMAS 2024. arXiv admin note: text overlap with arXiv:2307.13922"},{"id":"http://arxiv.org/abs/2311.03326v2","updated":"2024-03-23T11:05:53Z","published":"2023-11-06T18:22:18Z","title":"Non-convex potential games for finding global solutions to sensor\n  network localization","summary":"  Sensor network localization (SNL) problems require determining the physical\ncoordinates of all sensors in a network. This process relies on the global\ncoordinates of anchors and the available measurements between non-anchor and\nanchor nodes. Attributed to the intrinsic non-convexity, obtaining a globally\noptimal solution to SNL is challenging, as well as implementing corresponding\nalgorithms. In this paper, we formulate a non-convex multi-player potential\ngame for a generic SNL problem to investigate the identification condition of\nthe global Nash equilibrium (NE) therein, where the global NE represents the\nglobal solution of SNL. We employ canonical duality theory to transform the\nnon-convex game into a complementary dual problem. Then we develop a\nconjugation-based algorithm to compute the stationary points of the\ncomplementary dual problem. On this basis, we show an identification condition\nof the global NE: the stationary point of the proposed algorithm satisfies a\nduality relation. Finally, simulation results are provided to validate the\neffectiveness of the theoretical results.\n","authors":["Gehui Xu","Guanpu Chen","Yiguang Hong","Baris Fidan","Thomas Parisini","Karl H. Johansson"],"pdf_url":"https://arxiv.org/pdf/2311.03326v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.06860v6","updated":"2024-03-23T08:35:25Z","published":"2022-03-14T05:10:07Z","title":"Cooperative networks and f-Shapley value","summary":"  Lloyd Shapley's cooperative value allocation theory stands as a central\nconcept in game theory, extensively utilized across various domains to\ndistribute resources, evaluate individual contributions, and ensure fairness.\nThe Shapley value formula and his four axioms that characterize it form the\nfoundation of the theory.\n  Traditionally, the Shapley value is assigned under the assumption that all\nplayers in a cooperative game will ultimately form the grand coalition. In this\npaper, we reinterpret the Shapley value as an expectation of a certain\nstochastic path integral, with each path representing a general coalition\nformation process. As a result, the value allocation is naturally extended to\nall partial coalition states. In addition, we provide a set of five properties\nthat extend the Shapley axioms and characterize the stochastic path integral.\nFinally, by integrating Hodge calculus, stochastic processes, and path\nintegration of edge flows on graphs, we expand the cooperative value allocation\ntheory beyond the standard coalition game structure to encompass a broader\nrange of cooperative network configurations.\n","authors":["Tongseok Lim"],"pdf_url":"https://arxiv.org/pdf/2203.06860v6.pdf","comment":null}]},"2024-03-25T00:00:00Z":{"Distributed, Parallel, and Cluster Computing":[{"id":"http://arxiv.org/abs/2403.15324v2","updated":"2024-03-25T12:46:02Z","published":"2024-03-22T16:25:34Z","title":"ProvDeploy: Provenance-oriented Containerization of High Performance\n  Computing Scientific Workflows","summary":"  Many existing scientific workflows require High Performance Computing\nenvironments to produce results in a timely manner. These workflows have\nseveral software library components and use different environments, making the\ndeployment and execution of the software stack not trivial. This complexity\nincreases if the user needs to add provenance data capture services to the\nworkflow. This manuscript introduces ProvDeploy to assist the user in\nconfiguring containers for scientific workflows with integrated provenance data\ncapture. ProvDeploy was evaluated with a Scientific Machine Learning workflow,\nexploring containerization strategies focused on provenance in two distinct HPC\nenvironments\n","authors":["Liliane Kunstmann","Débora Pina","Daniel de Oliveira","Marta Mattoso"],"pdf_url":"https://arxiv.org/pdf/2403.15324v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16677v1","updated":"2024-03-25T12:14:48Z","published":"2024-03-25T12:14:48Z","title":"FOOL: Addressing the Downlink Bottleneck in Satellite Computing with\n  Neural Feature Compression","summary":"  Nanosatellite constellations equipped with sensors capturing large geographic\nregions provide unprecedented opportunities for Earth observation. As\nconstellation sizes increase, network contention poses a downlink bottleneck.\nOrbital Edge Computing (OEC) leverages limited onboard compute resources to\nreduce transfer costs by processing the raw captures at the source. However,\ncurrent solutions have limited practicability due to reliance on crude\nfiltering methods or over-prioritizing particular downstream tasks.\n  This work presents FOOL, an OEC-native and task-agnostic feature compression\nmethod that preserves prediction performance. FOOL partitions high-resolution\nsatellite imagery to maximize throughput. Further, it embeds context and\nleverages inter-tile dependencies to lower transfer costs with negligible\noverhead. While FOOL is a feature compressor, it can recover images with\ncompetitive scores on perceptual quality measures at lower bitrates. We\nextensively evaluate transfer cost reduction by including the peculiarity of\nintermittently available network connections in low earth orbit. Lastly, we\ntest the feasibility of our system for standardized nanosatellite form factors.\nWe demonstrate that FOOL permits downlinking over 100x the data volume without\nrelying on prior information on the downstream tasks.\n","authors":["Alireza Furutanpey","Qiyang Zhang","Philipp Raith","Tobias Pfandzelter","Shangguang Wang","Schahram Dustdar"],"pdf_url":"https://arxiv.org/pdf/2403.16677v1.pdf","comment":"18 pages, double column, 19 figures, 7 tables, Initial Submission to\n  IEEE Transactions on Mobile Computing"},{"id":"http://arxiv.org/abs/2310.07471v2","updated":"2024-03-25T11:07:13Z","published":"2023-10-11T13:18:23Z","title":"The Implications of Decentralization in Blockchained Federated Learning:\n  Evaluating the Impact of Model Staleness and Inconsistencies","summary":"  Blockchain promises to enhance distributed machine learning (ML) approaches\nsuch as federated learning (FL) by providing further decentralization,\nsecurity, immutability, and trust, which are key properties for enabling\ncollaborative intelligence in next-generation applications. Nonetheless, the\nintrinsic decentralized operation of peer-to-peer (P2P) blockchain nodes leads\nto an uncharted setting for FL, whereby the concepts of FL round and global\nmodel become meaningless, as devices' synchronization is lost without the\nfigure of a central orchestrating server. In this paper, we study the practical\nimplications of outsourcing the orchestration of FL to a democratic setting\nsuch as in a blockchain. In particular, we focus on the effects that model\nstaleness and inconsistencies, endorsed by blockchains' modus operandi, have on\nthe training procedure held by FL devices asynchronously. Using simulation, we\nevaluate the blockchained FL operation by applying two different ML models\n(ranging from low to high complexity) on the well-known MNIST and CIFAR-10\ndatasets, respectively, and focus on the accuracy and timeliness of the\nsolutions. Our results show the high impact of model inconsistencies on the\naccuracy of the models (up to a ~35% decrease in prediction accuracy), which\nunderscores the importance of properly designing blockchain systems based on\nthe characteristics of the underlying FL application.\n","authors":["Francesc Wilhelmi","Nima Afraz","Elia Guerra","Paolo Dini"],"pdf_url":"https://arxiv.org/pdf/2310.07471v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16557v1","updated":"2024-03-25T09:16:59Z","published":"2024-03-25T09:16:59Z","title":"Accelerating Federated Learning by Selecting Beneficial Herd of Local\n  Gradients","summary":"  Federated Learning (FL) is a distributed machine learning framework in\ncommunication network systems. However, the systems' Non-Independent and\nIdentically Distributed (Non-IID) data negatively affect the convergence\nefficiency of the global model, since only a subset of these data samples are\nbeneficial for model convergence. In pursuit of this subset, a reliable\napproach involves determining a measure of validity to rank the samples within\nthe dataset. In this paper, We propose the BHerd strategy which selects a\nbeneficial herd of local gradients to accelerate the convergence of the FL\nmodel. Specifically, we map the distribution of the local dataset to the local\ngradients and use the Herding strategy to obtain a permutation of the set of\ngradients, where the more advanced gradients in the permutation are closer to\nthe average of the set of gradients. These top portion of the gradients will be\nselected and sent to the server for global aggregation. We conduct experiments\non different datasets, models and scenarios by building a prototype system, and\nexperimental results demonstrate that our BHerd strategy is effective in\nselecting beneficial local gradients to mitigate the effects brought by the\nNon-IID dataset, thus accelerating model convergence.\n","authors":["Ping Luo","Xiaoge Deng","Ziqing Wen","Tao Sun","Dongsheng Li"],"pdf_url":"https://arxiv.org/pdf/2403.16557v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16542v1","updated":"2024-03-25T08:35:19Z","published":"2024-03-25T08:35:19Z","title":"Differentially Private Online Federated Learning with Correlated Noise","summary":"  We propose a novel differentially private algorithm for online federated\nlearning that employs temporally correlated noise to improve the utility while\nensuring the privacy of the continuously released models. To address challenges\nstemming from DP noise and local updates with streaming noniid data, we develop\na perturbed iterate analysis to control the impact of the DP noise on the\nutility. Moreover, we demonstrate how the drift errors from local updates can\nbe effectively managed under a quasi-strong convexity condition. Subject to an\n$(\\epsilon, \\delta)$-DP budget, we establish a dynamic regret bound over the\nentire time horizon that quantifies the impact of key parameters and the\nintensity of changes in dynamic environments. Numerical experiments validate\nthe efficacy of the proposed algorithm.\n","authors":["Jiaojiao Zhang","Linglingzhi Zhu","Mikael Johansson"],"pdf_url":"https://arxiv.org/pdf/2403.16542v1.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2402.13845v2","updated":"2024-03-25T08:16:29Z","published":"2024-02-21T14:43:34Z","title":"Multi-Agent Online Graph Exploration on Cycles and Tadpole Graphs","summary":"  We study the problem of multi-agent online graph exploration, in which a team\nof k agents has to explore a given graph, starting and ending on the same node.\nThe graph is initially unknown. Whenever a node is visited by an agent, its\nneighborhood and adjacent edges are revealed. The agents share a global view of\nthe explored parts of the graph. The cost of the exploration has to be\nminimized, where cost either describes the time needed for the entire\nexploration (time model), or the length of the longest path traversed by any\nagent (energy model). We investigate graph exploration on cycles and tadpole\ngraphs for 2-4 agents, providing optimal results on the competitive ratio in\nthe energy model (1-competitive with two agents on cycles and three agents on\ntadpole graphs), and for tadpole graphs in the time model (1.5-competitive with\nfour agents). We also show competitive upper bounds of 2 for the exploration of\ntadpole graphs with three agents, and 2.5 for the exploration of tadpole graphs\nwith two agents in the time model.\n","authors":["Erik van den Akker","Kevin Buchin","Klaus-Tycho Foerster"],"pdf_url":"https://arxiv.org/pdf/2402.13845v2.pdf","comment":"v2: Update Related Work, more detailed description of models in\n  Motivation"},{"id":"http://arxiv.org/abs/2403.16486v1","updated":"2024-03-25T07:15:06Z","published":"2024-03-25T07:15:06Z","title":"ColonyOS -- A Meta-Operating System for Distributed Computing Across\n  Heterogeneous Platform","summary":"  This paper presents ColonyOS, an open-source meta-operating system designed\nto improve integration and utilization of diverse computing platforms,\nincluding IoT, edge, cloud, and HPC. Operating as an overlay, ColonyOS can\ninterface with a wide range of computing environments, fostering creation of\nso-called compute continuums. This makes it possible to develop AI workflows\nand applications that can operate across platforms. At its core, ColonyOS\nconsists of distributed executors that integrate with various underlying\nplatforms based on a distributed microservice architecture. These executors\ncollectively form a colony, serving as a unified computing unit. To enable\nsecure integration of various platforms, each colony is provisioned with\nprecisely the resources needed, and all communication is confined within the\ncolony governed by a strict zero-trust security protocol. Interaction with\nColonyOS is done by submitting functional meta-descriptions of computational\ntasks, called function specifications. These are sent to a Colonies server,\nwhich acts as intermediary between applications and the executors. Upon\nassignment, an executor interprets the meta-description and translates it into\nan executable format, e.g. a Kubernetes deployment description, a Slurm script,\nor a direct function call within the executor. Furthermore, a built-in\nmeta-file system enables data synchronization directives to be included in\nmeta-descriptions, enabling seamless data management across platforms.\nUltimately, ColonyOS paves the way for development of hyper-distributed\napplications and workflows, which can seamlessly operate in a computing\ncontinuum. The paper describes design principles and implementation details of\nColonyOS.\n","authors":["Johan Kristiansson"],"pdf_url":"https://arxiv.org/pdf/2403.16486v1.pdf","comment":"See https://colonyos.io for more information"},{"id":"http://arxiv.org/abs/2403.16460v1","updated":"2024-03-25T06:43:28Z","published":"2024-03-25T06:43:28Z","title":"FedAC: A Adaptive Clustered Federated Learning Framework for\n  Heterogeneous Data","summary":"  Clustered federated learning (CFL) is proposed to mitigate the performance\ndeterioration stemming from data heterogeneity in federated learning (FL) by\ngrouping similar clients for cluster-wise model training. However, current CFL\nmethods struggle due to inadequate integration of global and intra-cluster\nknowledge and the absence of an efficient online model similarity metric, while\ntreating the cluster count as a fixed hyperparameter limits flexibility and\nrobustness. In this paper, we propose an adaptive CFL framework, named FedAC,\nwhich (1) efficiently integrates global knowledge into intra-cluster learning\nby decoupling neural networks and utilizing distinct aggregation methods for\neach submodule, significantly enhancing performance; (2) includes a\ncosteffective online model similarity metric based on dimensionality reduction;\n(3) incorporates a cluster number fine-tuning module for improved adaptability\nand scalability in complex, heterogeneous environments. Extensive experiments\nshow that FedAC achieves superior empirical performance, increasing the test\naccuracy by around 1.82% and 12.67% on CIFAR-10 and CIFAR-100 datasets,\nrespectively, under different non-IID settings compared to SOTA methods.\n","authors":["Yuxin Zhang","Haoyu Chen","Zheng Lin","Zhe Chen","Jin Zhao"],"pdf_url":"https://arxiv.org/pdf/2403.16460v1.pdf","comment":"14 pages, 4 figures"},{"id":"http://arxiv.org/abs/2403.16457v1","updated":"2024-03-25T06:38:46Z","published":"2024-03-25T06:38:46Z","title":"Raptor: Distributed Scheduling for Serverless Functions","summary":"  Serverless platforms that poorly schedule function requests inspire\ndevelopers to implement workarounds to issues like high cold start latencies,\npoor fault tolerance, and limited support for parallel processing. These\nsolutions litter environments with idle containers and add unnecessary pressure\nto the already underperforming scheduling services. An effective serverless\nscheduling policy should encourage developers to write small and reusable\nsnippets of code, and give operators the freedom to administer cluster\nworkloads however necessary in order to meet their operational demands. To this\nend, we have designed a distributed scheduling service that integrates with\nexisting serverless frameworks. Our service addresses three key issues that\naffect modern serverless platforms; high cold start latencies, poor fault\ntolerance, and limited native support for parallel processing patterns like\nfork-join and map-reduce. We have built a prototype that integrates with the\nexisting OpenWhisk services, and is fully backwards compatible with the\nexisting implementation. The updated architecture improves performance and adds\nnew scheduling and security features. Our empirical results demonstrate that\nour scheduler reduces cold start execution latencies by up to 80, steady state\nlatencies by up to 10, and does so with negligible time and memory overhead.\n","authors":["Kevin Exton","Maria Read"],"pdf_url":"https://arxiv.org/pdf/2403.16457v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08515v2","updated":"2024-03-25T02:52:27Z","published":"2024-03-13T13:24:24Z","title":"Plotinus: A Satellite Internet Digital Twin System","summary":"  The development of an integrated space-air-ground network (SAGIN) requires\nsophisticated satellite Internet emulation tools that can handle complex,\ndynamic topologies and offer in-depth analysis. Existing emulation platforms\nstruggle with challenges like the need for detailed implementation across all\nnetwork layers, real-time response, and scalability. This paper proposes a\ndigital twin system based on microservices for satellite Internet emulation,\nnamely Plotinus, which aims to solve these problems. Plotinus features a\nmodular design, allowing for easy replacement of the physical layer to emulate\ndifferent aerial vehicles and analyze channel interference. It also enables\nreplacing path computation methods to simplify testing and deploying\nalgorithms. In particular, Plotinus allows for real-time emulation with live\nnetwork traffic, enhancing practical network models. The evaluation result\nshows Plotinus's effective emulation of dynamic satellite networks with\nreal-world devices. Its adaptability for various communication models and\nalgorithm testing highlights Plotinus's role as a vital tool for developing and\nanalyzing SAGIN systems, offering a cross-layer, real-time and scalable digital\ntwin system.\n","authors":["Yue Gao","Kun Qiu","Zhe Chen","Wenjun Zhu","Qi Zhang","Handong Luo","Quanwei Lin","Ziheng Yang","Wenhao Liu"],"pdf_url":"https://arxiv.org/pdf/2403.08515v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17261v1","updated":"2024-03-25T23:09:22Z","published":"2024-03-25T23:09:22Z","title":"Distributed Simulation of Large Multi-body Systems","summary":"  We present a technique designed for parallelizing large rigid body\nsimulations, capable of exploiting multiple CPU cores within a computer and\nacross a network. Our approach can be applied to simulate both unilateral and\nbilateral constraints, requiring straightforward modifications to the\nunderlying physics engine. Starting from an approximate partitioning, we\nidentify interface bodies and add them to overlapping sets such that they are\nsimulated by multiple workers. At each timestep, we blend the states of overlap\nbodies using weights based on graph geodesic distances within the constraint\ngraph. The use of overlap simulation also allows us to perform load balancing\nusing efficient local evaluations of the constraint graph. We demonstrate our\ntechnique's scalability and load-balancing capabilities using several\nlarge-scale scenes.\n","authors":["Manas Kale","Paul G. Kry"],"pdf_url":"https://arxiv.org/pdf/2403.17261v1.pdf","comment":"For associated video, see https://www.youtube.com/watch?v=2gg-YnIGJ-w"},{"id":"http://arxiv.org/abs/2402.16731v2","updated":"2024-03-25T18:51:02Z","published":"2024-02-26T16:52:35Z","title":"Accelerating Graph Neural Networks on Real Processing-In-Memory Systems","summary":"  Graph Neural Networks (GNNs) are emerging ML models to analyze\ngraph-structure data. Graph Neural Network (GNN) execution involves both\ncompute-intensive and memory-intensive kernels, the latter dominates the total\ntime, being significantly bottlenecked by data movement between memory and\nprocessors. Processing-In-Memory (PIM) systems can alleviate this data movement\nbottleneck by placing simple processors near or inside to memory arrays. In\nthis work, we introduce PyGim, an efficient ML framework that accelerates GNNs\non real PIM systems. We propose intelligent parallelization techniques for\nmemory-intensive kernels of GNNs tailored for real PIM systems, and develop\nhandy Python API for them. We provide hybrid GNN execution, in which the\ncompute-intensive and memory-intensive kernels are executed in\nprocessor-centric and memory-centric computing systems, respectively, to match\ntheir algorithmic nature. We extensively evaluate PyGim on a real-world PIM\nsystem with 1992 PIM cores using emerging GNN models, and demonstrate that it\noutperforms its state-of-the-art CPU counterpart on Intel Xeon by on average\n3.04x, and achieves higher resource utilization than CPU and GPU systems. Our\nwork provides useful recommendations for software, system and hardware\ndesigners. PyGim will be open-sourced to enable the widespread use of PIM\nsystems in GNNs.\n","authors":["Christina Giannoula","Peiming Yang","Ivan Fernandez Vega","Jiacheng Yang","Yu Xin Li","Juan Gomez Luna","Mohammad Sadrosadati","Onur Mutlu","Gennady Pekhimenko"],"pdf_url":"https://arxiv.org/pdf/2402.16731v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17107v1","updated":"2024-03-25T18:45:31Z","published":"2024-03-25T18:45:31Z","title":"Design Principles of Dynamic Resource Management for High-Performance\n  Parallel Programming Models","summary":"  With Dynamic Resource Management (DRM) the resources assigned to a job can be\nchanged dynamically during its execution. From the system's perspective, DRM\nopens a new level of flexibility in resource allocation and job scheduling and\ntherefore has the potential to improve system efficiency metrics such as the\nutilization rate, job throughput, energy efficiency, and responsiveness. From\nthe application perspective, users can tailor the resources they request to\ntheir needs offering potential optimizations in queuing time or charged costs.\nDespite these obvious advantages and many attempts over the last decade to\nestablish DRM in HPC, it remains a concept discussed in academia rather than\nbeing successfully deployed on production systems. This stems from the fact\nthat support for DRM requires changes in all the layers of the HPC system\nsoftware stack including applications, programming models, process managers,\nand resource management software, as well as an extensive and holistic\nco-design process to establish new techniques and policies for scheduling and\nresource optimization. In this work, we therefore start with the assumption\nthat resources are accessible by processes executed either on them (e.g., on\nCPU) or controlling them (e.g., GPU-offloading). Then, the overall DRM problem\ncan be decomposed into dynamic process management (DPM) and dynamic resource\nmapping or allocation (DRA). The former determines which processes (or which\nchange in processes) must be managed and the latter identifies the resources\nwhere they will be executed. The interfaces for such \\mbox{DPM/DPA} in these\nlayers need to be standardized, which requires a careful design to be\ninteroperable while providing high flexibility. Based on a survey of existing\napproaches we propose design principles, that form the basis of a holistic\napproach to DMR in HPC and provide a prototype implementation using MPI.\n","authors":["Dominik Huber","Martin Schreiber","Martin Schulz","Howard Pritchard","Daniel Holmes"],"pdf_url":"https://arxiv.org/pdf/2403.17107v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17092v1","updated":"2024-03-25T18:28:48Z","published":"2024-03-25T18:28:48Z","title":"A Unified CPU-GPU Protocol for GNN Training","summary":"  Training a Graph Neural Network (GNN) model on large-scale graphs involves a\nhigh volume of data communication and compu- tations. While state-of-the-art\nCPUs and GPUs feature high computing power, the Standard GNN training protocol\nadopted in existing GNN frameworks cannot efficiently utilize the platform\nresources. To this end, we propose a novel Unified CPU-GPU protocol that can\nimprove the resource utilization of GNN training on a CPU-GPU platform. The\nUnified CPU-GPU protocol instantiates multiple GNN training processes in\nparallel on both the CPU and the GPU. By allocating training processes on the\nCPU to perform GNN training collaboratively with the GPU, the proposed protocol\nimproves the platform resource utilization and reduces the CPU-GPU data\ntransfer overhead. Since the performance of a CPU and a GPU varies, we develop\na novel load balancer that balances the workload dynamically between CPUs and\nGPUs during runtime. We evaluate our protocol using two representative GNN\nsampling algorithms, with two widely-used GNN models, on three datasets.\nCompared with the standard training protocol adopted in the state-of-the-art\nGNN frameworks, our protocol effectively improves resource utilization and\noverall training time. On a platform where the GPU moderately outperforms the\nCPU, our protocol speeds up GNN training by up to 1.41x. On a platform where\nthe GPU significantly outperforms the CPU, our protocol speeds up GNN training\nby up to 1.26x. Our protocol is open-sourced and can be seamlessly integrated\ninto state-of-the-art GNN frameworks and accelerate GNN training. Our protocol\nparticularly benefits those with limited GPU access due to its high demand.\n","authors":["Yi-Chien Lin","Gangda Deng","Viktor Prasanna"],"pdf_url":"https://arxiv.org/pdf/2403.17092v1.pdf","comment":"To appear in 21st ACM International Conference on Computing Frontiers\n  (CF' 24)"},{"id":"http://arxiv.org/abs/2206.08662v3","updated":"2024-03-25T16:23:38Z","published":"2022-06-17T09:51:03Z","title":"PICO: Pipeline Inference Framework for Versatile CNNs on Diverse Mobile\n  Devices","summary":"  Distributing the inference of convolutional neural network (CNN) to multiple\nmobile devices has been studied in recent years to achieve real-time inference\nwithout losing accuracy. However, how to map CNN to devices remains a\nchallenge. On the one hand, scheduling the workload of state-of-the-art CNNs\nwith multiple devices is NP-Hard because the structures of CNNs are directed\nacyclic graphs (DAG) rather than simple chains. On the other hand, distributing\nthe inference workload suffers from expensive communication and unbalanced\ncomputation due to the wireless environment and heterogeneous devices. This\npaper presents PICO, a pipeline cooperation framework to accelerate the\ninference of versatile CNNs on diverse mobile devices. At its core, PICO\nfeatures: (1) a generic graph partition algorithm that considers the\ncharacteristics of any given CNN and orchestrates it into a list of model\npieces with suitable granularity, and (2) a many-to-many mapping algorithm that\nproduces the best pipeline configuration for heterogeneous devices. In our\nexperiment with 2 ~ 8 Raspberry-Pi devices, the throughput can be improved by\n1.8 ~ 6.8x under different CPU frequencies.\n","authors":["Xiang Yang","Zikang Xu","Qi Qi","Jingyu Wang","Haifeng Sun","Jianxin Liao","Song Guo"],"pdf_url":"https://arxiv.org/pdf/2206.08662v3.pdf","comment":"Accepted by IEEE Transactions on Mobile Computing"},{"id":"http://arxiv.org/abs/2403.16903v1","updated":"2024-03-25T16:14:22Z","published":"2024-03-25T16:14:22Z","title":"Towards Secure and Trusted-by-Design Smart Contracts","summary":"  Distributed immutable ledgers, or blockchains, allow the secure digitization\nof evidential transactions without relying on a trusted third-party. Evidential\ntransactions involve the exchange of any form of physical evidence, such as\nmoney, birth certificate, visas, tickets, etc. Most of the time, evidential\ntransactions occur in the context of complex procedures, called evidential\nprotocols, among physical agents. The blockchain provides the mechanisms to\ntransfer evidence, while smart contracts - programs executing within the\nblockchain in a decentralized and replicated fashion - allow encoding\nevidential protocols on top of a blockchain.\n  As a smart contract foregoes trusted third-parties and runs on several\nmachines anonymously, it constitutes a highly critical program that has to be\nsecure and trusted-by-design. While most of the current smart contract\nlanguages focus on easy programmability, they do not directly address the need\nof guaranteeing trust and accountability, which becomes a significant issue\nwhen evidential protocols are encoded as smart contracts.\n","authors":["Zaynah Dargaye","Önder Gürcan","Florent Kirchner","Sara Tucci-Piergiovanni"],"pdf_url":"https://arxiv.org/pdf/2403.16903v1.pdf","comment":"17 pages, 1 algorithm, The 29th Francophone Days of Application\n  Languages - JFLA 2018"},{"id":"http://arxiv.org/abs/2403.16869v1","updated":"2024-03-25T15:35:31Z","published":"2024-03-25T15:35:31Z","title":"Lessons Learned from Building Edge Software System Testbeds","summary":"  Edge computing requires the complex software interaction of geo-distributed,\nheterogeneous components. The growing research and industry interest in edge\ncomputing software systems has necessitated exploring ways of testing and\nevaluating edge software at scale without relying on physical infrastructure.\nBeyond simulation, virtual testbeds that emulate edge infrastructure can\nprovide a cost-efficient yet realistic environment to evaluate edge software.\n  In this experience paper, we share lessons learned from building a total of\nfive edge software testbeds. We describe pitfalls in architecture and\ndevelopment as well as experiences from having students use our testbed tooling\nin distributed systems prototyping classes. While we remain confident that\nbuilding custom testbed tooling is the right approach for edge computing\nresearchers and practitioners alike, we hope this paper allows others to avoid\ncommon mistakes and benefit from our experience.\n","authors":["Tobias Pfandzelter","David Bermbach"],"pdf_url":"https://arxiv.org/pdf/2403.16869v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.12838v2","updated":"2024-03-25T15:01:58Z","published":"2023-10-05T08:51:59Z","title":"Toward parallel intelligence: an interdisciplinary solution for complex\n  systems","summary":"  The growing complexity of real-world systems necessitates interdisciplinary\nsolutions to confront myriad challenges in modeling, analysis, management, and\ncontrol. To meet these demands, the parallel systems method rooted in\nArtificial systems, Computational experiments, and Parallel execution (ACP)\napproach has been developed. The method cultivates a cycle, termed parallel\nintelligence, which iteratively creates data, acquires knowledge, and refines\nthe actual system. Over the past two decades, the parallel systems method has\ncontinuously woven advanced knowledge and technologies from various\ndisciplines, offering versatile interdisciplinary solutions for complex systems\nacross diverse fields. This review explores the origins and fundamental\nconcepts of the parallel systems method, showcasing its accomplishments as a\ndiverse array of parallel technologies and applications, while also\nprognosticating potential challenges. We posit that this method will\nconsiderably augment sustainable development while enhancing interdisciplinary\ncommunication and cooperation.\n","authors":["Yong Zhao","Zhengqiu Zhu","Bin Chen","Sihang Qiu","Jincai Huang","Xin Lu","Weiyi Yang","Chuan Ai","Kuihua Huang","Cheng He","Yucheng Jin","Zhong Liu","Fei-Yue Wang"],"pdf_url":"https://arxiv.org/pdf/2311.12838v2.pdf","comment":"41 pages, 6 figures. The Innovation (2023)"},{"id":"http://arxiv.org/abs/2403.17036v1","updated":"2024-03-25T03:46:50Z","published":"2024-03-25T03:46:50Z","title":"Union: An Automatic Workload Manager for Accelerating Network Simulation","summary":"  With the rapid growth of the machine learning applications, the workloads of\nfuture HPC systems are anticipated to be a mix of scientific simulation, big\ndata analytics, and machine learning applications. Simulation is a great\nresearch vehicle to understand the performance implications of co-running\nscientific applications with big data and machine learning workloads on\nlarge-scale systems. In this paper, we present Union, a workload manager that\nprovides an automatic framework to facilitate hybrid workload simulation in\nCODES. Furthermore, we use Union, along with CODES, to investigate various\nhybrid workloads composed of traditional simulation applications and emerging\nlearning applications on two dragonfly systems. The experiment results show\nthat both message latency and communication time are important performance\nmetrics to evaluate network interference. Network interference on HPC\napplications is more reflected by the message latency variation, whereas ML\napplication performance depends more on the communication time.\n","authors":["Xin Wang","Misbah Mubarak","Yao Kang","Robert B. Ross","Zhiling Lan"],"pdf_url":"https://arxiv.org/pdf/2403.17036v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16372v1","updated":"2024-03-25T02:32:43Z","published":"2024-03-25T02:32:43Z","title":"SignSGD with Federated Voting","summary":"  Distributed learning is commonly used for accelerating model training by\nharnessing the computational capabilities of multiple-edge devices. However, in\npractical applications, the communication delay emerges as a bottleneck due to\nthe substantial information exchange required between workers and a central\nparameter server. SignSGD with majority voting (signSGD-MV) is an effective\ndistributed learning algorithm that can significantly reduce communication\ncosts by one-bit quantization. However, due to heterogeneous computational\ncapabilities, it fails to converge when the mini-batch sizes differ among\nworkers. To overcome this, we propose a novel signSGD optimizer with\n\\textit{federated voting} (signSGD-FV). The idea of federated voting is to\nexploit learnable weights to perform weighted majority voting. The server\nlearns the weights assigned to the edge devices in an online fashion based on\ntheir computational capabilities. Subsequently, these weights are employed to\ndecode the signs of the aggregated local gradients in such a way to minimize\nthe sign decoding error probability. We provide a unified convergence rate\nanalysis framework applicable to scenarios where the estimated weights are\nknown to the parameter server either perfectly or imperfectly. We demonstrate\nthat the proposed signSGD-FV algorithm has a theoretical convergence guarantee\neven when edge devices use heterogeneous mini-batch sizes. Experimental results\nshow that signSGD-FV outperforms signSGD-MV, exhibiting a faster convergence\nrate, especially in heterogeneous mini-batch sizes.\n","authors":["Chanho Park","H. Vincent Poor","Namyoon Lee"],"pdf_url":"https://arxiv.org/pdf/2403.16372v1.pdf","comment":null}],"Performance":[{"id":"http://arxiv.org/abs/2403.16745v1","updated":"2024-03-25T13:19:09Z","published":"2024-03-25T13:19:09Z","title":"Multilevel Modeling as a Methodology for the Simulation of Human\n  Mobility","summary":"  Multilevel modeling is increasingly relevant in the context of modelling and\nsimulation since it leads to several potential benefits, such as software reuse\nand integration, the split of semantically separated levels into sub-models,\nthe possibility to employ different levels of detail, and the potential for\nparallel execution. The coupling that inevitably exists between the sub-models,\nhowever, implies the need for maintaining consistency between the various\ncomponents, more so when different simulation paradigms are employed (e.g.,\nsequential vs parallel, discrete vs continuous). In this paper we argue that\nmultilevel modelling is well suited for the simulation of human mobility, since\nit naturally leads to the decomposition of the model into two layers, the\n\"micro\" and \"macro\" layer, where individual entities (micro) and long-range\ninteractions (macro) are described. In this paper we investigate the challenges\nof multilevel modeling, and describe some preliminary results using prototype\nimplementations of multilayer simulators in the context of epidemic diffusion\nand vehicle pollution.\n","authors":["Luca Serena","Moreno Marzolla","Gabriele D'Angelo","Stefano Ferretti"],"pdf_url":"https://arxiv.org/pdf/2403.16745v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16404v1","updated":"2024-03-25T03:43:36Z","published":"2024-03-25T03:43:36Z","title":"Implementing and Evaluating E2LSH on Storage","summary":"  Locality sensitive hashing (LSH) is one of the widely-used approaches to\napproximate nearest neighbor search (ANNS) in high-dimensional spaces. The\nfirst work on LSH for the Euclidean distance, E2LSH, showed how ANNS can be\nsolved efficiently at a sublinear query time in the database size with\ntheoretically-guaranteed accuracy, although it required a large hash index\nsize. Since then, several LSH variants having much smaller index sizes have\nbeen proposed. Their query time is linear or superlinear, but they have been\nshown to run effectively faster because they require fewer I/Os when the index\nis stored on hard disk drives and because they also permit in-memory execution\nwith modern DRAM capacity.\n  In this paper, we show that E2LSH is regaining the advantage in query speed\nwith the advent of modern flash storage devices such as solid-state drives\n(SSDs). We evaluate E2LSH on a modern single-node computing environment and\nanalyze its computational cost and I/O cost, from which we derive storage\nperformance requirements for its external memory execution. Our analysis\nindicates that E2LSH on a single consumer-grade SSD can run faster than the\nstate-of-the-art small-index methods executed in-memory. It also indicates that\nE2LSH with emerging high-performance storage devices and interfaces can\napproach in-memory E2LSH speeds. We implement a simple adaptation of E2LSH to\nexternal memory, E2LSH-on-Storage (E2LSHoS), and evaluate it for practical\nlarge datasets of up to one billion objects using different combinations of\nmodern storage devices and interfaces. We demonstrate that our E2LSHoS\nimplementation runs much faster than small-index methods and can approach\nin-memory E2LSH speeds, and also that its query time scales sublinearly with\nthe database size beyond the index size limit of in-memory E2LSH.\n","authors":["Yu Nakanishi","Kazuhiro Hiwada","Yosuke Bando","Tomoya Suzuki","Hirotsugu Kajihara","Shintaro Sano","Tatsuro Endo","Tatsuo Shiozawa"],"pdf_url":"https://arxiv.org/pdf/2403.16404v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.16731v2","updated":"2024-03-25T18:51:02Z","published":"2024-02-26T16:52:35Z","title":"Accelerating Graph Neural Networks on Real Processing-In-Memory Systems","summary":"  Graph Neural Networks (GNNs) are emerging ML models to analyze\ngraph-structure data. Graph Neural Network (GNN) execution involves both\ncompute-intensive and memory-intensive kernels, the latter dominates the total\ntime, being significantly bottlenecked by data movement between memory and\nprocessors. Processing-In-Memory (PIM) systems can alleviate this data movement\nbottleneck by placing simple processors near or inside to memory arrays. In\nthis work, we introduce PyGim, an efficient ML framework that accelerates GNNs\non real PIM systems. We propose intelligent parallelization techniques for\nmemory-intensive kernels of GNNs tailored for real PIM systems, and develop\nhandy Python API for them. We provide hybrid GNN execution, in which the\ncompute-intensive and memory-intensive kernels are executed in\nprocessor-centric and memory-centric computing systems, respectively, to match\ntheir algorithmic nature. We extensively evaluate PyGim on a real-world PIM\nsystem with 1992 PIM cores using emerging GNN models, and demonstrate that it\noutperforms its state-of-the-art CPU counterpart on Intel Xeon by on average\n3.04x, and achieves higher resource utilization than CPU and GPU systems. Our\nwork provides useful recommendations for software, system and hardware\ndesigners. PyGim will be open-sourced to enable the widespread use of PIM\nsystems in GNNs.\n","authors":["Christina Giannoula","Peiming Yang","Ivan Fernandez Vega","Jiacheng Yang","Yu Xin Li","Juan Gomez Luna","Mohammad Sadrosadati","Onur Mutlu","Gennady Pekhimenko"],"pdf_url":"https://arxiv.org/pdf/2402.16731v2.pdf","comment":null}],"Artificial Intelligence":[{"id":"http://arxiv.org/abs/2403.16732v1","updated":"2024-03-25T13:06:31Z","published":"2024-03-25T13:06:31Z","title":"Enabling Uncertainty Estimation in Iterative Neural Networks","summary":"  Turning pass-through network architectures into iterative ones, which use\ntheir own output as input, is a well-known approach for boosting performance.\nIn this paper, we argue that such architectures offer an additional benefit:\nThe convergence rate of their successive outputs is highly correlated with the\naccuracy of the value to which they converge. Thus, we can use the convergence\nrate as a useful proxy for uncertainty. This results in an approach to\nuncertainty estimation that provides state-of-the-art estimates at a much lower\ncomputational cost than techniques like Ensembles, and without requiring any\nmodifications to the original iterative model. We demonstrate its practical\nvalue by embedding it in two application domains: road detection in aerial\nimages and the estimation of aerodynamic properties of 2D and 3D shapes.\n","authors":["Nikita Durasov","Doruk Oner","Jonathan Donier","Hieu Le","Pascal Fua"],"pdf_url":"https://arxiv.org/pdf/2403.16732v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16728v1","updated":"2024-03-25T13:02:43Z","published":"2024-03-25T13:02:43Z","title":"Improving Diffusion Models's Data-Corruption Resistance using Scheduled\n  Pseudo-Huber Loss","summary":"  Diffusion models are known to be vulnerable to outliers in training data. In\nthis paper we study an alternative diffusion loss function, which can preserve\nthe high quality of generated data like the original squared $L_{2}$ loss while\nat the same time being robust to outliers. We propose to use pseudo-Huber loss\nfunction with a time-dependent parameter to allow for the trade-off between\nrobustness on the most vulnerable early reverse-diffusion steps and fine\ndetails restoration on the final steps. We show that pseudo-Huber loss with the\ntime-dependent parameter exhibits better performance on corrupted datasets in\nboth image and audio domains. In addition, the loss function we propose can\npotentially help diffusion models to resist dataset corruption while not\nrequiring data filtering or purification compared to conventional training\nalgorithms.\n","authors":["Artem Khrapov","Vadim Popov","Tasnima Sadekova","Assel Yermekova","Mikhail Kudinov"],"pdf_url":"https://arxiv.org/pdf/2403.16728v1.pdf","comment":"13 pages, 16 figures"},{"id":"http://arxiv.org/abs/2403.16719v1","updated":"2024-03-25T12:56:48Z","published":"2024-03-25T12:56:48Z","title":"Towards a Formalisation of Value-based Actions and Consequentialist\n  Ethics","summary":"  Agents act to bring about a state of the world that is more compatible with\ntheir personal or institutional values. To formalise this intuition, the paper\nproposes an action framework based on the STRIPS formalisation. Technically,\nthe contribution expresses actions in terms of Value-based Formal Reasoning\n(VFR), which provides a set of propositions derived from an Agent's value\nprofile and the Agent's assessment of propositions with respect to the profile.\nConceptually, the contribution provides a computational framework for a form of\nconsequentialist ethics which is satisficing, luralistic, act-based, and\npreferential.\n","authors":["Adam Wyner","Tomasz Zurek","DOrota Stachura-Zurek"],"pdf_url":"https://arxiv.org/pdf/2403.16719v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16707v1","updated":"2024-03-25T12:44:52Z","published":"2024-03-25T12:44:52Z","title":"One-Shot Domain Incremental Learning","summary":"  Domain incremental learning (DIL) has been discussed in previous studies on\ndeep neural network models for classification. In DIL, we assume that samples\non new domains are observed over time. The models must classify inputs on all\ndomains. In practice, however, we may encounter a situation where we need to\nperform DIL under the constraint that the samples on the new domain are\nobserved only infrequently. Therefore, in this study, we consider the extreme\ncase where we have only one sample from the new domain, which we call one-shot\nDIL. We first empirically show that existing DIL methods do not work well in\none-shot DIL. We have analyzed the reason for this failure through various\ninvestigations. According to our analysis, we clarify that the difficulty of\none-shot DIL is caused by the statistics in the batch normalization layers.\nTherefore, we propose a technique regarding these statistics and demonstrate\nthe effectiveness of our technique through experiments on open datasets.\n","authors":["Yasushi Esaki","Satoshi Koide","Takuro Kutsuna"],"pdf_url":"https://arxiv.org/pdf/2403.16707v1.pdf","comment":"accepted at IEEE International Joint Conference on Neural Networks\n  (IJCNN) 2024"},{"id":"http://arxiv.org/abs/2403.16687v1","updated":"2024-03-25T12:23:12Z","published":"2024-03-25T12:23:12Z","title":"Investigation of the effectiveness of applying ChatGPT in Dialogic\n  Teaching Using Electroencephalography","summary":"  In recent years, the rapid development of artificial intelligence technology,\nespecially the emergence of large language models (LLMs) such as ChatGPT, has\npresented significant prospects for application in the field of education. LLMs\npossess the capability to interpret knowledge, answer questions, and consider\ncontext, thus providing support for dialogic teaching to students. Therefore,\nan examination of the capacity of LLMs to effectively fulfill instructional\nroles, thereby facilitating student learning akin to human educators within\ndialogic teaching scenarios, is an exceptionally valuable research topic. This\nresearch recruited 34 undergraduate students as participants, who were randomly\ndivided into two groups. The experimental group engaged in dialogic teaching\nusing ChatGPT, while the control group interacted with human teachers. Both\ngroups learned the histogram equalization unit in the information-related\ncourse \"Digital Image Processing\". The research findings show comparable scores\nbetween the two groups on the retention test. However, students who engaged in\ndialogue with ChatGPT exhibited lower performance on the transfer test.\nElectroencephalography data revealed that students who interacted with ChatGPT\nexhibited higher levels of cognitive activity, suggesting that ChatGPT could\nhelp students establish a knowledge foundation and stimulate cognitive\nactivity. However, its strengths on promoting students. knowledge application\nand creativity were insignificant. Based upon the research findings, it is\nevident that ChatGPT cannot fully excel in fulfilling teaching tasks in the\ndialogue teaching in information related courses. Combining ChatGPT with\ntraditional human teachers might be a more ideal approach. The synergistic use\nof both can provide students with more comprehensive learning support, thus\ncontributing to enhancing the quality of teaching.\n","authors":["Jiayue Zhang","Yiheng Liu","Wenqi Cai","Yali Peng","Senqing Qi","Taotao Long","Bao Ge"],"pdf_url":"https://arxiv.org/pdf/2403.16687v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16674v1","updated":"2024-03-25T12:13:20Z","published":"2024-03-25T12:13:20Z","title":"Understanding the Functional Roles of Modelling Components in Spiking\n  Neural Networks","summary":"  Spiking neural networks (SNNs), inspired by the neural circuits of the brain,\nare promising in achieving high computational efficiency with biological\nfidelity. Nevertheless, it is quite difficult to optimize SNNs because the\nfunctional roles of their modelling components remain unclear. By designing and\nevaluating several variants of the classic model, we systematically investigate\nthe functional roles of key modelling components, leakage, reset, and\nrecurrence, in leaky integrate-and-fire (LIF) based SNNs. Through extensive\nexperiments, we demonstrate how these components influence the accuracy,\ngeneralization, and robustness of SNNs. Specifically, we find that the leakage\nplays a crucial role in balancing memory retention and robustness, the reset\nmechanism is essential for uninterrupted temporal processing and computational\nefficiency, and the recurrence enriches the capability to model complex\ndynamics at a cost of robustness degradation. With these interesting\nobservations, we provide optimization suggestions for enhancing the performance\nof SNNs in different scenarios. This work deepens the understanding of how SNNs\nwork, which offers valuable guidance for the development of more effective and\nrobust neuromorphic models.\n","authors":["Huifeng Yin","Hanle Zheng","Jiayi Mao","Siyuan Ding","Xing Liu","Mingkun Xu","Yifan Hu","Jing Pei","Lei Deng"],"pdf_url":"https://arxiv.org/pdf/2403.16674v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16667v1","updated":"2024-03-25T12:04:03Z","published":"2024-03-25T12:04:03Z","title":"Deep Reinforcement Learning and Mean-Variance Strategies for Responsible\n  Portfolio Optimization","summary":"  Portfolio optimization involves determining the optimal allocation of\nportfolio assets in order to maximize a given investment objective.\nTraditionally, some form of mean-variance optimization is used with the aim of\nmaximizing returns while minimizing risk, however, more recently, deep\nreinforcement learning formulations have been explored. Increasingly, investors\nhave demonstrated an interest in incorporating ESG objectives when making\ninvestment decisions, and modifications to the classical mean-variance\noptimization framework have been developed. In this work, we study the use of\ndeep reinforcement learning for responsible portfolio optimization, by\nincorporating ESG states and objectives, and provide comparisons against\nmodified mean-variance approaches. Our results show that deep reinforcement\nlearning policies can provide competitive performance against mean-variance\napproaches for responsible portfolio allocation across additive and\nmultiplicative utility functions of financial and ESG responsibility\nobjectives.\n","authors":["Fernando Acero","Parisa Zehtabi","Nicolas Marchesotti","Michael Cashmore","Daniele Magazzeni","Manuela Veloso"],"pdf_url":"https://arxiv.org/pdf/2403.16667v1.pdf","comment":"Presented at the AAAI 2024 Workshop on AI in Finance for Social\n  Impact"},{"id":"http://arxiv.org/abs/2311.16515v2","updated":"2024-03-25T12:01:59Z","published":"2023-11-25T14:24:49Z","title":"Word4Per: Zero-shot Composed Person Retrieval","summary":"  Searching for specific person has great social benefits and security value,\nand it often involves a combination of visual and textual information.\nConventional person retrieval methods, whether image-based or text-based,\nusually fall short in effectively harnessing both types of information, leading\nto the loss of accuracy. In this paper, a whole new task called Composed Person\nRetrieval (CPR) is proposed to jointly utilize both image and text information\nfor target person retrieval. However, the supervised CPR requires very costly\nmanual annotation dataset, while there are currently no available resources. To\nmitigate this issue, we firstly introduce the Zero-shot Composed Person\nRetrieval (ZS-CPR), which leverages existing domain-related data to resolve the\nCPR problem without expensive annotations. Secondly, to learn ZS-CPR model, we\npropose a two-stage learning framework, Word4Per, where a lightweight Textual\nInversion Network (TINet) and a text-based person retrieval model based on\nfine-tuned Contrastive Language-Image Pre-training (CLIP) network are learned\nwithout utilizing any CPR data. Thirdly, a finely annotated Image-Text Composed\nPerson Retrieval (ITCPR) dataset is built as the benchmark to assess the\nperformance of the proposed Word4Per framework. Extensive experiments under\nboth Rank-1 and mAP demonstrate the effectiveness of Word4Per for the ZS-CPR\ntask, surpassing the comparative methods by over 10\\%. The code and ITCPR\ndataset will be publicly available at\nhttps://github.com/Delong-liu-bupt/Word4Per.\n","authors":["Delong Liu","Haiwen Li","Zhicheng Zhao","Fei Su","Yuan Dong"],"pdf_url":"https://arxiv.org/pdf/2311.16515v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16666v1","updated":"2024-03-25T12:01:27Z","published":"2024-03-25T12:01:27Z","title":"Revisiting the Sleeping Beauty problem","summary":"  The Sleeping Beauty problem is a probability riddle with no definite solution\nfor more than two decades and its solution is of great interest in many fields\nof knowledge. There are two main competing solutions to the problem: the halfer\napproach, and the thirder approach. The main reason for disagreement in the\nliterature is connected to the use of different probability spaces to represent\nthe same probabilistic riddle. In this work, we analyse the problem from a\nmathematical perspective, identifying probability distributions induced\ndirectly from the thought experiment's rules. The precise choices of\nprobability spaces provide both halfer and thirder solutions to the problem. To\ntry and decide on which approach to follow, a criterion involving the\ninformation available to Sleeping Beauty is proposed.\n","authors":["Paulo S. Piva","Gabriel Ruffolo"],"pdf_url":"https://arxiv.org/pdf/2403.16666v1.pdf","comment":"14 pages, 1 figure"},{"id":"http://arxiv.org/abs/2403.12061v2","updated":"2024-03-25T11:50:42Z","published":"2024-02-07T20:41:00Z","title":"Design-Space Exploration of SNN Models using Application-Specific\n  Multi-Core Architectures","summary":"  With the motivation and the difficulties that currently exist in\ncomprehending and utilizing the promising features of SNNs, we proposed a novel\nrun-time multi-core architecture-based simulator called \"RAVSim\" (Runtime\nAnalysis and Visualization Simulator), a cutting-edge SNN simulator, developed\nusing LabVIEW and it is publicly available on their website as an official\nmodule. RAVSim is a runtime virtual simulation environment tool that enables\nthe user to interact with the model, observe its behavior of output\nconcentration, and modify the set of parametric values at any time while the\nsimulation is in execution. Recently some popular tools have been presented,\nbut we believe that none of the tools allow users to interact with the model\nsimulation in run time.\n","authors":[" Sanaullah","Shamini Koravuna","Ulrich Rückert","Thorsten Jungeblut"],"pdf_url":"https://arxiv.org/pdf/2403.12061v2.pdf","comment":"Abstract Presentation in 2023 Neuro-Inspired Computing Elements\n  (NICE) Conference"},{"id":"http://arxiv.org/abs/2402.10685v2","updated":"2024-03-25T11:50:32Z","published":"2024-02-16T13:39:34Z","title":"LongHeads: Multi-Head Attention is Secretly a Long Context Processor","summary":"  Large language models (LLMs) have achieved impressive performance in numerous\ndomains but often struggle to process lengthy inputs effectively and\nefficiently due to limited length generalization and attention's quadratic\ncomputational demands. Many sought to mitigate this by restricting the\nattention window within the pre-trained length. However, these methods\nintroduce new issues such as ignoring the middle context and requiring\nadditional training. To address these problems, we propose LongHeads, a\ntraining-free framework that enhances LLM's long context ability by unlocking\nmulti-head attention's untapped potential. Instead of allowing each head to\nattend to the full sentence, which struggles with generalizing to longer\nsequences due to out-of-distribution (OOD) issues, we allow each head to\nprocess in-distribution length by selecting and attending to important context\nchunks. To this end, we propose a chunk selection strategy that relies on the\ninherent correlation between the query and the key representations, efficiently\ndistributing context chunks to different heads. In this way, each head ensures\nit can effectively process attended tokens within the trained length, while\ndifferent heads in different layers can collectively process longer contexts.\nLongHeads works efficiently in linear time, fits seamlessly with many LLMs that\nuse relative positional encoding. LongHeads achieves 100% accuracy at the 128k\nlength on passkey retrieval task, verifying LongHeads's efficacy in extending\nthe usable context window for existing models. We release our code at\nhttps://github.com/LuLuLuyi/LongHeads .\n","authors":["Yi Lu","Xin Zhou","Wei He","Jun Zhao","Tao Ji","Tao Gui","Qi Zhang","Xuanjing Huang"],"pdf_url":"https://arxiv.org/pdf/2402.10685v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.16476v4","updated":"2024-03-25T11:24:45Z","published":"2023-12-27T08:50:01Z","title":"SVGDreamer: Text Guided SVG Generation with Diffusion Model","summary":"  Recently, text-guided scalable vector graphics (SVGs) synthesis has shown\npromise in domains such as iconography and sketch. However, existing\ntext-to-SVG generation methods lack editability and struggle with visual\nquality and result diversity. To address these limitations, we propose a novel\ntext-guided vector graphics synthesis method called SVGDreamer. SVGDreamer\nincorporates a semantic-driven image vectorization (SIVE) process that enables\nthe decomposition of synthesis into foreground objects and background, thereby\nenhancing editability. Specifically, the SIVE process introduce attention-based\nprimitive control and an attention-mask loss function for effective control and\nmanipulation of individual elements. Additionally, we propose a Vectorized\nParticle-based Score Distillation (VPSD) approach to tackle the challenges of\nshape over-smoothing, color over-saturation, limited diversity in results, and\nslow convergence in existing text-to-SVG generation methods. VPSD models SVGs\nas distributions of control points and colors to counteract over-smoothing and\nover-saturation. Furthermore, VPSD leverages a reward model to reweight vector\nparticles, which improves aesthetic appeal and accelerates convergence.\nExtensive experiments have been conducted to validate the effectiveness of\nSVGDreamer, demonstrating its superiority over baseline methods in terms of\neditability, visual quality, and diversity. The code and demo of SVGDreamer can\nbe found at https://ximinng.github.io/SVGDreamer-project/\n","authors":["Ximing Xing","Haitao Zhou","Chuang Wang","Jing Zhang","Dong Xu","Qian Yu"],"pdf_url":"https://arxiv.org/pdf/2312.16476v4.pdf","comment":"Accepted by CVPR 2024. project link:\n  https://ximinng.github.io/SVGDreamer-project/"},{"id":"http://arxiv.org/abs/2310.07471v2","updated":"2024-03-25T11:07:13Z","published":"2023-10-11T13:18:23Z","title":"The Implications of Decentralization in Blockchained Federated Learning:\n  Evaluating the Impact of Model Staleness and Inconsistencies","summary":"  Blockchain promises to enhance distributed machine learning (ML) approaches\nsuch as federated learning (FL) by providing further decentralization,\nsecurity, immutability, and trust, which are key properties for enabling\ncollaborative intelligence in next-generation applications. Nonetheless, the\nintrinsic decentralized operation of peer-to-peer (P2P) blockchain nodes leads\nto an uncharted setting for FL, whereby the concepts of FL round and global\nmodel become meaningless, as devices' synchronization is lost without the\nfigure of a central orchestrating server. In this paper, we study the practical\nimplications of outsourcing the orchestration of FL to a democratic setting\nsuch as in a blockchain. In particular, we focus on the effects that model\nstaleness and inconsistencies, endorsed by blockchains' modus operandi, have on\nthe training procedure held by FL devices asynchronously. Using simulation, we\nevaluate the blockchained FL operation by applying two different ML models\n(ranging from low to high complexity) on the well-known MNIST and CIFAR-10\ndatasets, respectively, and focus on the accuracy and timeliness of the\nsolutions. Our results show the high impact of model inconsistencies on the\naccuracy of the models (up to a ~35% decrease in prediction accuracy), which\nunderscores the importance of properly designing blockchain systems based on\nthe characteristics of the underlying FL application.\n","authors":["Francesc Wilhelmi","Nima Afraz","Elia Guerra","Paolo Dini"],"pdf_url":"https://arxiv.org/pdf/2310.07471v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.00591v3","updated":"2024-03-25T10:52:20Z","published":"2024-02-01T13:37:53Z","title":"Sandra -- A Neuro-Symbolic Reasoner Based On Descriptions And Situations","summary":"  This paper presents sandra, a neuro-symbolic reasoner combining vectorial\nrepresentations with deductive reasoning. Sandra builds a vector space\nconstrained by an ontology and performs reasoning over it. The geometric nature\nof the reasoner allows its combination with neural networks, bridging the gap\nwith symbolic knowledge representations. Sandra is based on the Description and\nSituation (DnS) ontology design pattern, a formalization of frame semantics.\nGiven a set of facts (a situation) it allows to infer all possible perspectives\n(descriptions) that can provide a plausible interpretation for it, even in\npresence of incomplete information. We prove that our method is correct with\nrespect to the DnS model. We experiment with two different tasks and their\nstandard benchmarks, demonstrating that, without increasing complexity, sandra\n(i) outperforms all the baselines (ii) provides interpretability in the\nclassification process, and (iii) allows control over the vector space, which\nis designed a priori.\n","authors":["Nicolas Lazzari","Stefano De Giorgis","Aldo Gangemi","Valentina Presutti"],"pdf_url":"https://arxiv.org/pdf/2402.00591v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16591v1","updated":"2024-03-25T10:06:45Z","published":"2024-03-25T10:06:45Z","title":"Deciphering the Interplay between Local Differential Privacy, Average\n  Bayesian Privacy, and Maximum Bayesian Privacy","summary":"  The swift evolution of machine learning has led to emergence of various\ndefinitions of privacy due to the threats it poses to privacy, including the\nconcept of local differential privacy (LDP). Although widely embraced and\nutilized across numerous domains, this conventional approach to measure privacy\nstill exhibits certain limitations, spanning from failure to prevent\ninferential disclosure to lack of consideration for the adversary's background\nknowledge. In this comprehensive study, we introduce Bayesian privacy and delve\ninto the intricate relationship between local differential privacy and its\nBayesian counterparts, unveiling novel insights into utility-privacy\ntrade-offs. We introduce a framework that encapsulates both attack and defense\nstrategies, highlighting their interplay and effectiveness. Our theoretical\ncontributions are anchored in the rigorous definitions and relationships\nbetween Average Bayesian Privacy (ABP) and Maximum Bayesian Privacy (MBP),\nencapsulated by equations $\\epsilon_{p,a} \\leq\n\\frac{1}{\\sqrt{2}}\\sqrt{(\\epsilon_{p,m} + \\epsilon)\\cdot(e^{\\epsilon_{p,m} +\n\\epsilon} - 1)}$ and the equivalence between $\\xi$-MBP and $2\\xi$-LDP\nestablished under uniform prior distribution. These relationships fortify our\nunderstanding of the privacy guarantees provided by various mechanisms, leading\nto the realization that a mechanism satisfying $\\xi$-LDP also confers\n$\\xi$-MBP, and vice versa. Our work not only lays the groundwork for future\nempirical exploration but also promises to enhance the design of\nprivacy-preserving algorithms that do not compromise on utility, thereby\nfostering the development of trustworthy machine learning solutions.\n","authors":["Xiaojin Zhang","Yulin Fei","Wei Chen","Hai Jin"],"pdf_url":"https://arxiv.org/pdf/2403.16591v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16582v1","updated":"2024-03-25T09:49:42Z","published":"2024-03-25T09:49:42Z","title":"In the Search for Optimal Multi-view Learning Models for Crop\n  Classification with Global Remote Sensing Data","summary":"  Crop classification is of critical importance due to its role in studying\ncrop pattern changes, resource management, and carbon sequestration. When\nemploying data-driven techniques for its prediction, utilizing various temporal\ndata sources is necessary. Deep learning models have proven to be effective for\nthis task by mapping time series data to high-level representation for\nprediction. However, they face substantial challenges when dealing with\nmultiple input patterns. The literature offers limited guidance for Multi-View\nLearning (MVL) scenarios, as it has primarily focused on exploring fusion\nstrategies with specific encoders and validating them in local regions. In\ncontrast, we investigate the impact of simultaneous selection of the fusion\nstrategy and the encoder architecture evaluated on a global-scale cropland and\ncrop-type classifications. We use a range of five fusion strategies (Input,\nFeature, Decision, Ensemble, Hybrid) and five temporal encoder architectures\n(LSTM, GRU, TempCNN, TAE, L-TAE) as possible MVL model configurations. The\nvalidation is on the CropHarvest dataset that provides optical, radar, and\nweather time series, and topographic information as input data. We found that\nin scenarios with a limited number of labeled samples, a unique configuration\nis insufficient for all the cases. Instead, a specialized combination,\nincluding encoder and fusion strategy, should be meticulously sought. To\nstreamline this search process, we suggest initially identifying the optimal\nencoder architecture tailored for a particular fusion strategy, and then\ndetermining the most suitable fusion strategy for the classification task. We\nprovide a technical framework for researchers exploring crop classification or\nrelated tasks through a MVL approach.\n","authors":["Francisco Mena","Diego Arenas","Andreas Dengel"],"pdf_url":"https://arxiv.org/pdf/2403.16582v1.pdf","comment":"submitted to journal"},{"id":"http://arxiv.org/abs/2403.16578v1","updated":"2024-03-25T09:43:56Z","published":"2024-03-25T09:43:56Z","title":"SegICL: A Universal In-context Learning Framework for Enhanced\n  Segmentation in Medical Imaging","summary":"  Medical image segmentation models adapting to new tasks in a training-free\nmanner through in-context learning is an exciting advancement. Universal\nsegmentation models aim to generalize across the diverse modality of medical\nimages, yet their effectiveness often diminishes when applied to\nout-of-distribution (OOD) data modalities and tasks, requiring intricate\nfine-tuning of model for optimal performance. For addressing this challenge, we\nintroduce SegICL, a novel approach leveraging In-Context Learning (ICL) for\nimage segmentation. Unlike existing methods, SegICL has the capability to\nemploy text-guided segmentation and conduct in-context learning with a small\nset of image-mask pairs, eliminating the need for training the model from\nscratch or fine-tuning for OOD tasks (including OOD modality and dataset).\nExtensive experimental validation of SegICL demonstrates a positive correlation\nbetween the number of prompt samples and segmentation performance on OOD\nmodalities and tasks. This indicates that SegICL effectively address new\nsegmentation tasks based on contextual information. Additionally, SegICL also\nexhibits comparable segmentation performance to mainstream models on OOD and\nin-distribution tasks. Our code will be released soon.\n","authors":["Lingdong Shen","Fangxin Shang","Yehui Yang","Xiaoshuang Huang","Shining Xiang"],"pdf_url":"https://arxiv.org/pdf/2403.16578v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16571v1","updated":"2024-03-25T09:36:51Z","published":"2024-03-25T09:36:51Z","title":"NSINA: A News Corpus for Sinhala","summary":"  The introduction of large language models (LLMs) has advanced natural\nlanguage processing (NLP), but their effectiveness is largely dependent on\npre-training resources. This is especially evident in low-resource languages,\nsuch as Sinhala, which face two primary challenges: the lack of substantial\ntraining data and limited benchmarking datasets. In response, this study\nintroduces NSINA, a comprehensive news corpus of over 500,000 articles from\npopular Sinhala news websites, along with three NLP tasks: news media\nidentification, news category prediction, and news headline generation. The\nrelease of NSINA aims to provide a solution to challenges in adapting LLMs to\nSinhala, offering valuable resources and benchmarks for improving NLP in the\nSinhala language. NSINA is the largest news corpus for Sinhala, available up to\ndate.\n","authors":["Hansi Hettiarachchi","Damith Premasiri","Lasitha Uyangodage","Tharindu Ranasinghe"],"pdf_url":"https://arxiv.org/pdf/2403.16571v1.pdf","comment":"Accepted to LREC-COLING 2024 (The 2024 Joint International Conference\n  on Computational Linguistics, Language Resources and Evaluation)"},{"id":"http://arxiv.org/abs/2403.16561v1","updated":"2024-03-25T09:24:05Z","published":"2024-03-25T09:24:05Z","title":"FedFixer: Mitigating Heterogeneous Label Noise in Federated Learning","summary":"  Federated Learning (FL) heavily depends on label quality for its performance.\nHowever, the label distribution among individual clients is always both noisy\nand heterogeneous. The high loss incurred by client-specific samples in\nheterogeneous label noise poses challenges for distinguishing between\nclient-specific and noisy label samples, impacting the effectiveness of\nexisting label noise learning approaches. To tackle this issue, we propose\nFedFixer, where the personalized model is introduced to cooperate with the\nglobal model to effectively select clean client-specific samples. In the dual\nmodels, updating the personalized model solely at a local level can lead to\noverfitting on noisy data due to limited samples, consequently affecting both\nthe local and global models' performance. To mitigate overfitting, we address\nthis concern from two perspectives. Firstly, we employ a confidence regularizer\nto alleviate the impact of unconfident predictions caused by label noise.\nSecondly, a distance regularizer is implemented to constrain the disparity\nbetween the personalized and global models. We validate the effectiveness of\nFedFixer through extensive experiments on benchmark datasets. The results\ndemonstrate that FedFixer can perform well in filtering noisy label samples on\ndifferent clients, especially in highly heterogeneous label noise scenarios.\n","authors":["Xinyuan Ji","Zhaowei Zhu","Wei Xi","Olga Gadyatskaya","Zilong Song","Yong Cai","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2403.16561v1.pdf","comment":"accepted by AAA24"},{"id":"http://arxiv.org/abs/2403.16554v1","updated":"2024-03-25T09:04:14Z","published":"2024-03-25T09:04:14Z","title":"PE: A Poincare Explanation Method for Fast Text Hierarchy Generation","summary":"  The black-box nature of deep learning models in NLP hinders their widespread\napplication. The research focus has shifted to Hierarchical Attribution (HA)\nfor its ability to model feature interactions. Recent works model\nnon-contiguous combinations with a time-costly greedy search in Eculidean\nspaces, neglecting underlying linguistic information in feature\nrepresentations. In this work, we introduce a novel method, namely Poincar\\'e\nExplanation (PE), for modeling feature interactions using hyperbolic spaces in\nan $O(n^2logn)$ time complexity. Inspired by Poincar\\'e model, we propose a\nframework to project the embeddings into hyperbolic spaces, which exhibit\nbetter inductive biases for syntax and semantic hierarchical structures.\nEventually, we prove that the hierarchical clustering process in the projected\nspace could be viewed as building a minimum spanning tree and propose a time\nefficient algorithm. Experimental results demonstrate the effectiveness of our\napproach.\n","authors":["Qian Chen","Xiaofeng He","Hongzhao Li","Hongyu Yi"],"pdf_url":"https://arxiv.org/pdf/2403.16554v1.pdf","comment":"12 pages, 10 figures"},{"id":"http://arxiv.org/abs/2310.14714v4","updated":"2024-03-25T08:58:39Z","published":"2023-10-23T08:51:05Z","title":"BatteryML:An Open-source platform for Machine Learning on Battery\n  Degradation","summary":"  Battery degradation remains a pivotal concern in the energy storage domain,\nwith machine learning emerging as a potent tool to drive forward insights and\nsolutions. However, this intersection of electrochemical science and machine\nlearning poses complex challenges. Machine learning experts often grapple with\nthe intricacies of battery science, while battery researchers face hurdles in\nadapting intricate models tailored to specific datasets. Beyond this, a\ncohesive standard for battery degradation modeling, inclusive of data formats\nand evaluative benchmarks, is conspicuously absent. Recognizing these\nimpediments, we present BatteryML - a one-step, all-encompass, and open-source\nplatform designed to unify data preprocessing, feature extraction, and the\nimplementation of both traditional and state-of-the-art models. This\nstreamlined approach promises to enhance the practicality and efficiency of\nresearch applications. BatteryML seeks to fill this void, fostering an\nenvironment where experts from diverse specializations can collaboratively\ncontribute, thus elevating the collective understanding and advancement of\nbattery research.The code for our project is publicly available on GitHub at\nhttps://github.com/microsoft/BatteryML.\n","authors":["Han Zhang","Xiaofan Gui","Shun Zheng","Ziheng Lu","Yuqi Li","Jiang Bian"],"pdf_url":"https://arxiv.org/pdf/2310.14714v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.02760v2","updated":"2024-03-25T08:57:47Z","published":"2023-11-05T20:33:18Z","title":"Causal Question Answering with Reinforcement Learning","summary":"  Causal questions inquire about causal relationships between different events\nor phenomena. They are important for a variety of use cases, including virtual\nassistants and search engines. However, many current approaches to causal\nquestion answering cannot provide explanations or evidence for their answers.\nHence, in this paper, we aim to answer causal questions with a causality graph,\na large-scale dataset of causal relations between noun phrases along with the\nrelations' provenance data. Inspired by recent, successful applications of\nreinforcement learning to knowledge graph tasks, such as link prediction and\nfact-checking, we explore the application of reinforcement learning on a\ncausality graph for causal question answering. We introduce an\nActor-Critic-based agent which learns to search through the graph to answer\ncausal questions. We bootstrap the agent with a supervised learning procedure\nto deal with large action spaces and sparse rewards. Our evaluation shows that\nthe agent successfully prunes the search space to answer binary causal\nquestions by visiting less than 30 nodes per question compared to over 3,000\nnodes by a naive breadth-first search. Our ablation study indicates that our\nsupervised learning strategy provides a strong foundation upon which our\nreinforcement learning agent improves. The paths returned by our agent explain\nthe mechanisms by which a cause produces an effect. Moreover, for each edge on\na path, our causality graph provides its original source allowing for easy\nverification of paths.\n","authors":["Lukas Blübaum","Stefan Heindorf"],"pdf_url":"https://arxiv.org/pdf/2311.02760v2.pdf","comment":"Accepted at WWW 2024"},{"id":"http://arxiv.org/abs/2403.16552v1","updated":"2024-03-25T08:57:27Z","published":"2024-03-25T08:57:27Z","title":"QKFormer: Hierarchical Spiking Transformer using Q-K Attention","summary":"  Spiking Transformers, which integrate Spiking Neural Networks (SNNs) with\nTransformer architectures, have attracted significant attention due to their\npotential for energy efficiency and high performance. However, existing models\nin this domain still suffer from suboptimal performance. We introduce several\ninnovations to improve the performance: i) We propose a novel spike-form Q-K\nattention mechanism, tailored for SNNs, which efficiently models the importance\nof token or channel dimensions through binary vectors with linear complexity.\nii) We incorporate the hierarchical structure, which significantly benefits the\nperformance of both the brain and artificial neural networks, into spiking\ntransformers to obtain multi-scale spiking representation. iii) We design a\nversatile and powerful patch embedding module with a deformed shortcut\nspecifically for spiking transformers. Together, we develop QKFormer, a\nhierarchical spiking transformer based on Q-K attention with direct training.\nQKFormer shows significantly superior performance over existing\nstate-of-the-art SNN models on various mainstream datasets. Notably, with\ncomparable size to Spikformer (66.34 M, 74.81%), QKFormer (64.96 M) achieves a\ngroundbreaking top-1 accuracy of 85.65% on ImageNet-1k, substantially\noutperforming Spikformer by 10.84%. To our best knowledge, this is the first\ntime that directly training SNNs have exceeded 85% accuracy on ImageNet-1K. The\ncode and models are publicly available at\nhttps://github.com/zhouchenlin2096/QKFormer\n","authors":["Chenlin Zhou","Han Zhang","Zhaokun Zhou","Liutao Yu","Liwei Huang","Xiaopeng Fan","Li Yuan","Zhengyu Ma","Huihui Zhou","Yonghong Tian"],"pdf_url":"https://arxiv.org/pdf/2403.16552v1.pdf","comment":"10 pages, code: https://github.com/zhouchenlin2096/QKFormer"},{"id":"http://arxiv.org/abs/2403.01479v3","updated":"2024-03-25T08:46:15Z","published":"2024-03-03T11:13:44Z","title":"Align-to-Distill: Trainable Attention Alignment for Knowledge\n  Distillation in Neural Machine Translation","summary":"  The advent of scalable deep models and large datasets has improved the\nperformance of Neural Machine Translation. Knowledge Distillation (KD) enhances\nefficiency by transferring knowledge from a teacher model to a more compact\nstudent model. However, KD approaches to Transformer architecture often rely on\nheuristics, particularly when deciding which teacher layers to distill from. In\nthis paper, we introduce the 'Align-to-Distill' (A2D) strategy, designed to\naddress the feature mapping problem by adaptively aligning student attention\nheads with their teacher counterparts during training. The Attention Alignment\nModule in A2D performs a dense head-by-head comparison between student and\nteacher attention heads across layers, turning the combinatorial mapping\nheuristics into a learning problem. Our experiments show the efficacy of A2D,\ndemonstrating gains of up to +3.61 and +0.63 BLEU points for WMT-2022 De->Dsb\nand WMT-2014 En->De, respectively, compared to Transformer baselines.\n","authors":["Heegon Jin","Seonil Son","Jemin Park","Youngseok Kim","Hyungjong Noh","Yeonsoo Lee"],"pdf_url":"https://arxiv.org/pdf/2403.01479v3.pdf","comment":"Accepted to LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2402.09132v3","updated":"2024-03-25T08:46:02Z","published":"2024-02-14T12:28:38Z","title":"Exploring the Adversarial Capabilities of Large Language Models","summary":"  The proliferation of large language models (LLMs) has sparked widespread and\ngeneral interest due to their strong language generation capabilities, offering\ngreat potential for both industry and research. While previous research delved\ninto the security and privacy issues of LLMs, the extent to which these models\ncan exhibit adversarial behavior remains largely unexplored. Addressing this\ngap, we investigate whether common publicly available LLMs have inherent\ncapabilities to perturb text samples to fool safety measures, so-called\nadversarial examples resp.~attacks. More specifically, we investigate whether\nLLMs are inherently able to craft adversarial examples out of benign samples to\nfool existing safe rails. Our experiments, which focus on hate speech\ndetection, reveal that LLMs succeed in finding adversarial perturbations,\neffectively undermining hate speech detection systems. Our findings carry\nsignificant implications for (semi-)autonomous systems relying on LLMs,\nhighlighting potential challenges in their interaction with existing systems\nand safety measures.\n","authors":["Lukas Struppek","Minh Hieu Le","Dominik Hintersdorf","Kristian Kersting"],"pdf_url":"https://arxiv.org/pdf/2402.09132v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16543v1","updated":"2024-03-25T08:36:06Z","published":"2024-03-25T08:36:06Z","title":"Efficient Information Extraction in Few-Shot Relation Classification\n  through Contrastive Representation Learning","summary":"  Differentiating relationships between entity pairs with limited labeled\ninstances poses a significant challenge in few-shot relation classification.\nRepresentations of textual data extract rich information spanning the domain,\nentities, and relations. In this paper, we introduce a novel approach to\nenhance information extraction combining multiple sentence representations and\ncontrastive learning. While representations in relation classification are\ncommonly extracted using entity marker tokens, we argue that substantial\ninformation within the internal model representations remains untapped. To\naddress this, we propose aligning multiple sentence representations, such as\nthe [CLS] token, the [MASK] token used in prompting, and entity marker tokens.\nOur method employs contrastive learning to extract complementary discriminative\ninformation from these individual representations. This is particularly\nrelevant in low-resource settings where information is scarce. Leveraging\nmultiple sentence representations is especially effective in distilling\ndiscriminative information for relation classification when additional\ninformation, like relation descriptions, are not available. We validate the\nadaptability of our approach, maintaining robust performance in scenarios that\ninclude relation descriptions, and showcasing its flexibility to adapt to\ndifferent resource constraints.\n","authors":["Philipp Borchert","Jochen De Weerdt","Marie-Francine Moens"],"pdf_url":"https://arxiv.org/pdf/2403.16543v1.pdf","comment":"NAACL 2024"},{"id":"http://arxiv.org/abs/2309.08503v2","updated":"2024-03-25T08:33:37Z","published":"2023-09-15T16:05:48Z","title":"HealthFC: Verifying Health Claims with Evidence-Based Medical\n  Fact-Checking","summary":"  In the digital age, seeking health advice on the Internet has become a common\npractice. At the same time, determining the trustworthiness of online medical\ncontent is increasingly challenging. Fact-checking has emerged as an approach\nto assess the veracity of factual claims using evidence from credible knowledge\nsources. To help advance automated Natural Language Processing (NLP) solutions\nfor this task, in this paper we introduce a novel dataset HealthFC. It consists\nof 750 health-related claims in German and English, labeled for veracity by\nmedical experts and backed with evidence from systematic reviews and clinical\ntrials. We provide an analysis of the dataset, highlighting its characteristics\nand challenges. The dataset can be used for NLP tasks related to automated\nfact-checking, such as evidence retrieval, claim verification, or explanation\ngeneration. For testing purposes, we provide baseline systems based on\ndifferent approaches, examine their performance, and discuss the findings. We\nshow that the dataset is a challenging test bed with a high potential for\nfuture use.\n","authors":["Juraj Vladika","Phillip Schneider","Florian Matthes"],"pdf_url":"https://arxiv.org/pdf/2309.08503v2.pdf","comment":"Accepted to LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2401.11504v2","updated":"2024-03-25T08:16:06Z","published":"2024-01-21T14:28:41Z","title":"With Greater Text Comes Greater Necessity: Inference-Time Training Helps\n  Long Text Generation","summary":"  Long text generation, such as novel writing and discourse-level translation\nwith extremely long contexts, presents significant challenges to current\nlanguage models. Existing methods mainly focus on extending the model's context\nwindow through strategies like length extrapolation. However, these approaches\ndemand substantial hardware resources during the training and/or inference\nphases. Our proposed method, Temp-Lora, introduces an alternative concept.\nInstead of relying on the KV cache to store all context information, we embeds\nthis information directly into a temporary Lora module. In the process of long\ntext generation, this module is progressively trained with text generated\npreviously. This approach not only efficiently preserves contextual knowledge\nbut also prevents any permanent alteration to the model's parameters given that\nthe module is discarded post-generation. Extensive experiments on the PG19\nlanguage modeling benchmark and the GuoFeng discourse-level translation\nbenchmark validate the effectiveness of Temp-Lora. Our results show that: 1)\nTemp-Lora substantially enhances generation quality for long text, as indicated\nby a 13.2% decrease in perplexity (PPL) on a subset of PG19, and a 29.3%\ndecrease in PPL along with a 113.2% increase in BLEU score on a subset of\nGuoFeng, 2) Temp-Lora is compatible with and enhances most existing long text\ngeneration methods, and 3) Temp-Lora can greatly reduce computational costs by\nshortening the context window. For example, we can ensure a moderate\nimprovement in generation quality (a decrease of 3.8% in PPL) while enabling a\n51.5% memory usage reduction and a 60.0% decrease in latency for inference.\n","authors":["Y. Wang","D. Ma","D. Cai"],"pdf_url":"https://arxiv.org/pdf/2401.11504v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16530v1","updated":"2024-03-25T08:16:06Z","published":"2024-03-25T08:16:06Z","title":"An Intermediate Fusion ViT Enables Efficient Text-Image Alignment in\n  Diffusion Models","summary":"  Diffusion models have been widely used for conditional data cross-modal\ngeneration tasks such as text-to-image and text-to-video. However,\nstate-of-the-art models still fail to align the generated visual concepts with\nhigh-level semantics in a language such as object count, spatial relationship,\netc. We approach this problem from a multimodal data fusion perspective and\ninvestigate how different fusion strategies can affect vision-language\nalignment. We discover that compared to the widely used early fusion of\nconditioning text in a pretrained image feature space, a specially designed\nintermediate fusion can: (i) boost text-to-image alignment with improved\ngeneration quality and (ii) improve training and inference efficiency by\nreducing low-rank text-to-image attention calculations. We perform experiments\nusing a text-to-image generation task on the MS-COCO dataset. We compare our\nintermediate fusion mechanism with the classic early fusion mechanism on two\ncommon conditioning methods on a U-shaped ViT backbone. Our intermediate fusion\nmodel achieves a higher CLIP Score and lower FID, with 20% reduced FLOPs, and\n50% increased training speed compared to a strong U-ViT baseline with an early\nfusion.\n","authors":["Zizhao Hu","Shaochong Jia","Mohammad Rostami"],"pdf_url":"https://arxiv.org/pdf/2403.16530v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16527v1","updated":"2024-03-25T08:11:02Z","published":"2024-03-25T08:11:02Z","title":"Hallucination Detection in Foundation Models for Decision-Making: A\n  Flexible Definition and Review of the State of the Art","summary":"  Autonomous systems are soon to be ubiquitous, from manufacturing autonomy to\nagricultural field robots, and from health care assistants to the entertainment\nindustry. The majority of these systems are developed with modular\nsub-components for decision-making, planning, and control that may be\nhand-engineered or learning-based. While these existing approaches have been\nshown to perform well under the situations they were specifically designed for,\nthey can perform especially poorly in rare, out-of-distribution scenarios that\nwill undoubtedly arise at test-time. The rise of foundation models trained on\nmultiple tasks with impressively large datasets from a variety of fields has\nled researchers to believe that these models may provide common sense reasoning\nthat existing planners are missing. Researchers posit that this common sense\nreasoning will bridge the gap between algorithm development and deployment to\nout-of-distribution tasks, like how humans adapt to unexpected scenarios. Large\nlanguage models have already penetrated the robotics and autonomous systems\ndomains as researchers are scrambling to showcase their potential use cases in\ndeployment. While this application direction is very promising empirically,\nfoundation models are known to hallucinate and generate decisions that may\nsound reasonable, but are in fact poor. We argue there is a need to step back\nand simultaneously design systems that can quantify the certainty of a model's\ndecision, and detect when it may be hallucinating. In this work, we discuss the\ncurrent use cases of foundation models for decision-making tasks, provide a\ngeneral definition for hallucinations with examples, discuss existing\napproaches to hallucination detection and mitigation with a focus on decision\nproblems, and explore areas for further research in this exciting field.\n","authors":["Neeloy Chakraborty","Melkior Ornik","Katherine Driggs-Campbell"],"pdf_url":"https://arxiv.org/pdf/2403.16527v1.pdf","comment":"31 pages, 2 tables"},{"id":"http://arxiv.org/abs/2403.16524v1","updated":"2024-03-25T08:09:01Z","published":"2024-03-25T08:09:01Z","title":"Harnessing the power of LLMs for normative reasoning in MASs","summary":"  Software agents, both human and computational, do not exist in isolation and\noften need to collaborate or coordinate with others to achieve their goals. In\nhuman society, social mechanisms such as norms ensure efficient functioning,\nand these techniques have been adopted by researchers in multi-agent systems\n(MAS) to create socially aware agents. However, traditional techniques have\nlimitations, such as operating in limited environments often using brittle\nsymbolic reasoning. The advent of Large Language Models (LLMs) offers a\npromising solution, providing a rich and expressive vocabulary for norms and\nenabling norm-capable agents that can perform a range of tasks such as norm\ndiscovery, normative reasoning and decision-making. This paper examines the\npotential of LLM-based agents to acquire normative capabilities, drawing on\nrecent Natural Language Processing (NLP) and LLM research. We present our\nvision for creating normative LLM agents. In particular, we discuss how the\nrecently proposed \"LLM agent\" approaches can be extended to implement such\nnormative LLM agents. We also highlight challenges in this emerging field. This\npaper thus aims to foster collaboration between MAS, NLP and LLM researchers in\norder to advance the field of normative agents.\n","authors":["Bastin Tony Roy Savarimuthu","Surangika Ranathunga","Stephen Cranefield"],"pdf_url":"https://arxiv.org/pdf/2403.16524v1.pdf","comment":"12 pages, 1 figure, accepted to COINE 2024 workshop at AAMAS 2024\n  (https://coin-workshop.github.io/coine-2024-auckland/accepted_papers.html)"},{"id":"http://arxiv.org/abs/2403.16523v1","updated":"2024-03-25T08:06:08Z","published":"2024-03-25T08:06:08Z","title":"Causal Discovery from Poisson Branching Structural Causal Model Using\n  High-Order Cumulant with Path Analysis","summary":"  Count data naturally arise in many fields, such as finance, neuroscience, and\nepidemiology, and discovering causal structure among count data is a crucial\ntask in various scientific and industrial scenarios. One of the most common\ncharacteristics of count data is the inherent branching structure described by\na binomial thinning operator and an independent Poisson distribution that\ncaptures both branching and noise. For instance, in a population count\nscenario, mortality and immigration contribute to the count, where survival\nfollows a Bernoulli distribution, and immigration follows a Poisson\ndistribution. However, causal discovery from such data is challenging due to\nthe non-identifiability issue: a single causal pair is Markov equivalent, i.e.,\n$X\\rightarrow Y$ and $Y\\rightarrow X$ are distributed equivalent. Fortunately,\nin this work, we found that the causal order from $X$ to its child $Y$ is\nidentifiable if $X$ is a root vertex and has at least two directed paths to\n$Y$, or the ancestor of $X$ with the most directed path to $X$ has a directed\npath to $Y$ without passing $X$. Specifically, we propose a Poisson Branching\nStructure Causal Model (PB-SCM) and perform a path analysis on PB-SCM using\nhigh-order cumulants. Theoretical results establish the connection between the\npath and cumulant and demonstrate that the path information can be obtained\nfrom the cumulant. With the path information, causal order is identifiable\nunder some graphical conditions. A practical algorithm for learning causal\nstructure under PB-SCM is proposed and the experiments demonstrate and verify\nthe effectiveness of the proposed method.\n","authors":["Jie Qiao","Yu Xiang","Zhengming Chen","Ruichu Cai","Zhifeng Hao"],"pdf_url":"https://arxiv.org/pdf/2403.16523v1.pdf","comment":"Accepted by AAAI-2024"},{"id":"http://arxiv.org/abs/2403.16512v1","updated":"2024-03-25T07:55:29Z","published":"2024-03-25T07:55:29Z","title":"LLMs Are Few-Shot In-Context Low-Resource Language Learners","summary":"  In-context learning (ICL) empowers large language models (LLMs) to perform\ndiverse tasks in underrepresented languages using only short in-context\ninformation, offering a crucial avenue for narrowing the gap between\nhigh-resource and low-resource languages. Nonetheless, there is only a handful\nof works explored ICL for low-resource languages with most of them focusing on\nrelatively high-resource languages, such as French and Spanish. In this work,\nwe extensively study ICL and its cross-lingual variation (X-ICL) on 25\nlow-resource and 7 relatively higher-resource languages. Our study not only\nassesses the effectiveness of ICL with LLMs in low-resource languages but also\nidentifies the shortcomings of in-context label alignment, and introduces a\nmore effective alternative: query alignment. Moreover, we provide valuable\ninsights into various facets of ICL for low-resource languages. Our study\nconcludes the significance of few-shot in-context information on enhancing the\nlow-resource understanding quality of LLMs through semantically relevant\ninformation by closing the language gap in the target language and aligning the\nsemantics between the targeted low-resource and the high-resource language that\nthe model is proficient in. Our work highlights the importance of advancing ICL\nresearch, particularly for low-resource languages.\n","authors":["Samuel Cahyawijaya","Holy Lovenia","Pascale Fung"],"pdf_url":"https://arxiv.org/pdf/2403.16512v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16508v1","updated":"2024-03-25T07:47:52Z","published":"2024-03-25T07:47:52Z","title":"Return to Tradition: Learning Reliable Heuristics with Classical Machine\n  Learning","summary":"  Current approaches for learning for planning have yet to achieve competitive\nperformance against classical planners in several domains, and have poor\noverall performance. In this work, we construct novel graph representations of\nlifted planning tasks and use the WL algorithm to generate features from them.\nThese features are used with classical machine learning methods which have up\nto 2 orders of magnitude fewer parameters and train up to 3 orders of magnitude\nfaster than the state-of-the-art deep learning for planning models. Our novel\napproach, WL-GOOSE, reliably learns heuristics from scratch and outperforms the\n$h^{\\text{FF}}$ heuristic in a fair competition setting. It also outperforms or\nties with LAMA on 4 out of 10 domains on coverage and 7 out of 10 domains on\nplan quality. WL-GOOSE is the first learning for planning model which achieves\nthese feats. Furthermore, we study the connections between our novel WL feature\ngeneration method, previous theoretically flavoured learning architectures, and\nDescription Logic Features for planning.\n","authors":["Dillon Z. Chen","Felipe Trevizan","Sylvie Thiébaux"],"pdf_url":"https://arxiv.org/pdf/2403.16508v1.pdf","comment":"Extended version of ICAPS 2024 paper"},{"id":"http://arxiv.org/abs/2403.16501v1","updated":"2024-03-25T07:34:42Z","published":"2024-03-25T07:34:42Z","title":"Learning To Guide Human Decision Makers With Vision-Language Models","summary":"  There is increasing interest in developing AIs for assisting human decision\nmaking in \\textit{high-stakes} tasks, such as medical diagnosis, for the\npurpose of improving decision quality and reducing cognitive strain.\n  %\n  Mainstream approaches team up an expert with a machine learning model to\nwhich safer decisions are offloaded, thus letting the former focus on cases\nthat demand their attention.\n  %\n  This \\textit{separation of responsibilities} setup, however, is inadequate\nfor high-stakes scenarios. On the one hand, the expert may end up over-relying\non the machine's decisions due to \\textit{anchoring bias}, thus losing the\nhuman oversight that is increasingly being required by regulatory agencies to\nensure trustworthy AI. On the other hand, the expert is left entirely\nunassisted on the (typically hardest) decisions on which the model abstained.\n  %\n  As a remedy, we introduce \\textit{learning to guide} (LTG), an alternative\nframework in which -- rather than taking control from the human expert -- the\nmachine provides \\textit{guidance} useful for decision making, and the human is\nentirely responsible for coming up with a decision.\n  %\n  In order to ensure guidance is \\textit{interpretable} and\n\\textit{task-specific}, we develop \\method, an approach for turning\n\\textit{any} vision-language model into a capable generator of textual guidance\nby leveraging a modicum of human feedback.\n  %\n  Our empirical evaluation highlights the promise of \\method on a challenging,\nreal-world medical diagnosis task.\n","authors":["Debodeep Banerjee","Stefano Teso","Burcu Sayin Grunel","Andrea Passerini"],"pdf_url":"https://arxiv.org/pdf/2403.16501v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16495v1","updated":"2024-03-25T07:23:23Z","published":"2024-03-25T07:23:23Z","title":"LSTTN: A Long-Short Term Transformer-based Spatio-temporal Neural\n  Network for Traffic Flow Forecasting","summary":"  Accurate traffic forecasting is a fundamental problem in intelligent\ntransportation systems and learning long-range traffic representations with key\ninformation through spatiotemporal graph neural networks (STGNNs) is a basic\nassumption of current traffic flow prediction models. However, due to\nstructural limitations, existing STGNNs can only utilize short-range traffic\nflow data; therefore, the models cannot adequately learn the complex trends and\nperiodic features in traffic flow. Besides, it is challenging to extract the\nkey temporal information from the long historical traffic series and obtain a\ncompact representation. To solve the above problems, we propose a novel LSTTN\n(Long-Short Term Transformer-based Network) framework comprehensively\nconsidering the long- and short-term features in historical traffic flow.\nFirst, we employ a masked subseries Transformer to infer the content of masked\nsubseries from a small portion of unmasked subseries and their temporal context\nin a pretraining manner, forcing the model to efficiently learn compressed and\ncontextual subseries temporal representations from long historical series.\nThen, based on the learned representations, long-term trend is extracted by\nusing stacked 1D dilated convolution layers, and periodic features are\nextracted by dynamic graph convolution layers. For the difficulties in making\ntime-step level prediction, LSTTN adopts a short-term trend extractor to learn\nfine-grained short-term temporal features. Finally, LSTTN fuses the long-term\ntrend, periodic features and short-term features to obtain the prediction\nresults. Experiments on four real-world datasets show that in 60-minute-ahead\nlong-term forecasting, the LSTTN model achieves a minimum improvement of 5.63\\%\nand a maximum improvement of 16.78\\% over baseline models. The source code is\navailable at https://github.com/GeoX-Lab/LSTTN.\n","authors":["Qinyao Luo","Silu He","Xing Han","Yuhan Wang","Haifeng Li"],"pdf_url":"https://arxiv.org/pdf/2403.16495v1.pdf","comment":"15 pages, 10 figures, 6 tables"},{"id":"http://arxiv.org/abs/2312.11834v2","updated":"2024-03-25T07:00:29Z","published":"2023-12-19T04:02:50Z","title":"Multi-agent reinforcement learning using echo-state network and its\n  application to pedestrian dynamics","summary":"  In recent years, simulations of pedestrians using the multi-agent\nreinforcement learning (MARL) have been studied. This study considered the\nroads on a grid-world environment, and implemented pedestrians as MARL agents\nusing an echo-state network and the least squares policy iteration method.\nUnder this environment, the ability of these agents to learn to move forward by\navoiding other agents was investigated. Specifically, we considered two types\nof tasks: the choice between a narrow direct route and a broad detour, and the\nbidirectional pedestrian flow in a corridor. The simulations results indicated\nthat the learning was successful when the density of the agents was not that\nhigh.\n","authors":["Hisato Komatsu"],"pdf_url":"https://arxiv.org/pdf/2312.11834v2.pdf","comment":"26 pages, 17 figures"},{"id":"http://arxiv.org/abs/2306.04357v4","updated":"2024-03-25T06:54:10Z","published":"2023-06-07T11:40:07Z","title":"Dial-MAE: ConTextual Masked Auto-Encoder for Retrieval-based Dialogue\n  Systems","summary":"  Dialogue response selection aims to select an appropriate response from\nseveral candidates based on a given user and system utterance history. Most\nexisting works primarily focus on post-training and fine-tuning tailored for\ncross-encoders. However, there are no post-training methods tailored for dense\nencoders in dialogue response selection. We argue that when the current\nlanguage model, based on dense dialogue systems (such as BERT), is employed as\na dense encoder, it separately encodes dialogue context and response, leading\nto a struggle to achieve the alignment of both representations. Thus, we\npropose Dial-MAE (Dialogue Contextual Masking Auto-Encoder), a straightforward\nyet effective post-training technique tailored for dense encoders in dialogue\nresponse selection. Dial-MAE uses an asymmetric encoder-decoder architecture to\ncompress the dialogue semantics into dense vectors, which achieves better\nalignment between the features of the dialogue context and response. Our\nexperiments have demonstrated that Dial-MAE is highly effective, achieving\nstate-of-the-art performance on two commonly evaluated benchmarks.\n","authors":["Zhenpeng Su","Xing Wu","Wei Zhou","Guangyuan Ma","Songlin Hu"],"pdf_url":"https://arxiv.org/pdf/2306.04357v4.pdf","comment":"This paper has been accepted by NAACL 2024"},{"id":"http://arxiv.org/abs/2403.16460v1","updated":"2024-03-25T06:43:28Z","published":"2024-03-25T06:43:28Z","title":"FedAC: A Adaptive Clustered Federated Learning Framework for\n  Heterogeneous Data","summary":"  Clustered federated learning (CFL) is proposed to mitigate the performance\ndeterioration stemming from data heterogeneity in federated learning (FL) by\ngrouping similar clients for cluster-wise model training. However, current CFL\nmethods struggle due to inadequate integration of global and intra-cluster\nknowledge and the absence of an efficient online model similarity metric, while\ntreating the cluster count as a fixed hyperparameter limits flexibility and\nrobustness. In this paper, we propose an adaptive CFL framework, named FedAC,\nwhich (1) efficiently integrates global knowledge into intra-cluster learning\nby decoupling neural networks and utilizing distinct aggregation methods for\neach submodule, significantly enhancing performance; (2) includes a\ncosteffective online model similarity metric based on dimensionality reduction;\n(3) incorporates a cluster number fine-tuning module for improved adaptability\nand scalability in complex, heterogeneous environments. Extensive experiments\nshow that FedAC achieves superior empirical performance, increasing the test\naccuracy by around 1.82% and 12.67% on CIFAR-10 and CIFAR-100 datasets,\nrespectively, under different non-IID settings compared to SOTA methods.\n","authors":["Yuxin Zhang","Haoyu Chen","Zheng Lin","Zhe Chen","Jin Zhao"],"pdf_url":"https://arxiv.org/pdf/2403.16460v1.pdf","comment":"14 pages, 4 figures"},{"id":"http://arxiv.org/abs/2312.16430v5","updated":"2024-03-25T06:32:49Z","published":"2023-12-27T06:34:54Z","title":"Preference as Reward, Maximum Preference Optimization with Importance\n  Sampling","summary":"  Preference learning is a key technology for aligning language models with\nhuman values. Reinforcement Learning from Human Feedback (RLHF) is a\nmodel-based algorithm to optimize preference learning, which first fits a\nreward model for preference scores and then optimizes the generating policy\nwith an on-policy PPO algorithm to maximize the reward. The processing of RLHF\nis complex, time-consuming, and unstable. The Direct Preference Optimization\n(DPO) algorithm uses an off-policy algorithm to directly optimize the\ngenerating policy and eliminates the need for a reward model. DPO is more\ndata-efficient and stable. However, DPO has a drawback of overfitting to the\npreference data and ignoring the KL-regularization term when the preference is\ndeterministic. Identity mapping Preference Optimization(IPO) uses a\nroot-finding MSE loss to incorporate KL-regularization. However, both DPO and\nIPO fail to properly address the KL-regularization term because the support of\nthe preference distribution is not equal to the reference distribution. In this\npaper, we propose a simple and intuitive off-policy preference optimization\nalgorithm from an importance sampling view, which we call Maximum Preference\nOptimization (MPO). MPO incorporates the off-policy KL-regularization term,\nmaking regularization truly effective. MPO achieves the best of both worlds by\ncombining the objectives of RLHF and IPO while being an off-policy algorithm.\nFurthermore, MPO eliminates the need for a reward model and reference policy,\nsimplifying the learning process and reducing memory usage.\n","authors":["Zaifan Jiang","Xing Huang","Chao Wei"],"pdf_url":"https://arxiv.org/pdf/2312.16430v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16451v1","updated":"2024-03-25T06:30:54Z","published":"2024-03-25T06:30:54Z","title":"DeepMachining: Online Prediction of Machining Errors of Lathe Machines","summary":"  We describe DeepMachining, a deep learning-based AI system for online\nprediction of machining errors of lathe machine operations. We have built and\nevaluated DeepMachining based on manufacturing data from factories.\nSpecifically, we first pretrain a deep learning model for a given lathe\nmachine's operations to learn the salient features of machining states. Then,\nwe fine-tune the pretrained model to adapt to specific machining tasks. We\ndemonstrate that DeepMachining achieves high prediction accuracy for multiple\ntasks that involve different workpieces and cutting tools. To the best of our\nknowledge, this work is one of the first factory experiments using pre-trained\ndeep-learning models to predict machining errors of lathe machines.\n","authors":["Xiang-Li Lu","Hwai-Jung Hsu","Che-Wei Chou","H. T. Kung","Chen-Hsin Lee"],"pdf_url":"https://arxiv.org/pdf/2403.16451v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16443v1","updated":"2024-03-25T06:09:55Z","published":"2024-03-25T06:09:55Z","title":"CodeS: Natural Language to Code Repository via Multi-Layer Sketch","summary":"  The impressive performance of large language models (LLMs) on code-related\ntasks has shown the potential of fully automated software development. In light\nof this, we introduce a new software engineering task, namely Natural Language\nto code Repository (NL2Repo). This task aims to generate an entire code\nrepository from its natural language requirements. To address this task, we\npropose a simple yet effective framework CodeS, which decomposes NL2Repo into\nmultiple sub-tasks by a multi-layer sketch. Specifically, CodeS includes three\nmodules: RepoSketcher, FileSketcher, and SketchFiller. RepoSketcher first\ngenerates a repository's directory structure for given requirements;\nFileSketcher then generates a file sketch for each file in the generated\nstructure; SketchFiller finally fills in the details for each function in the\ngenerated file sketch. To rigorously assess CodeS on the NL2Repo task, we carry\nout evaluations through both automated benchmarking and manual feedback\nanalysis. For benchmark-based evaluation, we craft a repository-oriented\nbenchmark, SketchEval, and design an evaluation metric, SketchBLEU. For\nfeedback-based evaluation, we develop a VSCode plugin for CodeS and engage 30\nparticipants in conducting empirical studies. Extensive experiments prove the\neffectiveness and practicality of CodeS on the NL2Repo task.\n","authors":["Daoguang Zan","Ailun Yu","Wei Liu","Dong Chen","Bo Shen","Wei Li","Yafen Yao","Yongshun Gong","Xiaolin Chen","Bei Guan","Zhiguang Yang","Yongji Wang","Qianxiang Wang","Lizhen Cui"],"pdf_url":"https://arxiv.org/pdf/2403.16443v1.pdf","comment":"https://github.com/NL2Code/CodeS"},{"id":"http://arxiv.org/abs/2311.08298v2","updated":"2024-03-25T06:01:49Z","published":"2023-11-14T16:43:29Z","title":"A Survey of Confidence Estimation and Calibration in Large Language\n  Models","summary":"  Large language models (LLMs) have demonstrated remarkable capabilities across\na wide range of tasks in various domains. Despite their impressive performance,\nthey can be unreliable due to factual errors in their generations. Assessing\ntheir confidence and calibrating them across different tasks can help mitigate\nrisks and enable LLMs to produce better generations. There has been a lot of\nrecent research aiming to address this, but there has been no comprehensive\noverview to organize it and outline the main lessons learned. The present\nsurvey aims to bridge this gap. In particular, we outline the challenges and we\nsummarize recent technical advancements for LLM confidence estimation and\ncalibration. We further discuss their applications and suggest promising\ndirections for future work.\n","authors":["Jiahui Geng","Fengyu Cai","Yuxia Wang","Heinz Koeppl","Preslav Nakov","Iryna Gurevych"],"pdf_url":"https://arxiv.org/pdf/2311.08298v2.pdf","comment":"16 pages, 1 page, 1 table"},{"id":"http://arxiv.org/abs/2305.17196v2","updated":"2024-03-25T05:50:33Z","published":"2023-05-26T18:39:25Z","title":"A Knowledge Engineering Primer","summary":"  The aim of this primer is to introduce the subject of knowledge engineering\nin a concise but synthetic way to develop the reader's intuition about the\narea.\n","authors":["Agnieszka Ławrynowicz"],"pdf_url":"https://arxiv.org/pdf/2305.17196v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14643v2","updated":"2024-03-25T05:35:12Z","published":"2024-02-21T16:44:35Z","title":"Exploring ChatGPT and its Impact on Society","summary":"  Artificial intelligence has been around for a while, but suddenly it has\nreceived more attention than ever before. Thanks to innovations from companies\nlike Google, Microsoft, Meta, and other major brands in technology. OpenAI,\nthough, has triggered the button with its ground-breaking invention ChatGPT.\nChatGPT is a Large Language Model (LLM) based on Transformer architecture that\nhas the ability to generate human-like responses in a conversational context.\nIt uses deep learning algorithms to generate natural language responses to\ninput text. Its large number of parameters, contextual generation, and\nopen-domain training make it a versatile and effective tool for a wide range of\napplications, from chatbots to customer service to language translation. It has\nthe potential to revolutionize various industries and transform the way we\ninteract with technology. However, the use of ChatGPT has also raised several\nconcerns, including ethical, social, and employment challenges, which must be\ncarefully considered to ensure the responsible use of this technology. The\narticle provides an overview of ChatGPT, delving into its architecture and\ntraining process. It highlights the potential impacts of ChatGPT on the\nsociety. In this paper, we suggest some approaches involving technology,\nregulation, education, and ethics in an effort to maximize ChatGPT's benefits\nwhile minimizing its negative impacts. This study is expected to contribute to\na greater understanding of ChatGPT and aid in predicting the potential changes\nit may bring about.\n","authors":["Md. Asraful Haque","Shuai Li"],"pdf_url":"https://arxiv.org/pdf/2403.14643v2.pdf","comment":"13 Pages"},{"id":"http://arxiv.org/abs/2403.16432v1","updated":"2024-03-25T05:27:35Z","published":"2024-03-25T05:27:35Z","title":"$\\textit{LinkPrompt}$: Natural and Universal Adversarial Attacks on\n  Prompt-based Language Models","summary":"  Prompt-based learning is a new language model training paradigm that adapts\nthe Pre-trained Language Models (PLMs) to downstream tasks, which revitalizes\nthe performance benchmarks across various natural language processing (NLP)\ntasks. Instead of using a fixed prompt template to fine-tune the model, some\nresearch demonstrates the effectiveness of searching for the prompt via\noptimization. Such prompt optimization process of prompt-based learning on PLMs\nalso gives insight into generating adversarial prompts to mislead the model,\nraising concerns about the adversarial vulnerability of this paradigm. Recent\nstudies have shown that universal adversarial triggers (UATs) can be generated\nto alter not only the predictions of the target PLMs but also the prediction of\ncorresponding Prompt-based Fine-tuning Models (PFMs) under the prompt-based\nlearning paradigm. However, UATs found in previous works are often unreadable\ntokens or characters and can be easily distinguished from natural texts with\nadaptive defenses. In this work, we consider the naturalness of the UATs and\ndevelop $\\textit{LinkPrompt}$, an adversarial attack algorithm to generate UATs\nby a gradient-based beam search algorithm that not only effectively attacks the\ntarget PLMs and PFMs but also maintains the naturalness among the trigger\ntokens. Extensive results demonstrate the effectiveness of\n$\\textit{LinkPrompt}$, as well as the transferability of UATs generated by\n\\textit{LinkPrompt} to open-sourced Large Language Model (LLM) Llama2 and\nAPI-accessed LLM GPT-3.5-turbo.\n","authors":["Yue Xu","Wenjie Wang"],"pdf_url":"https://arxiv.org/pdf/2403.16432v1.pdf","comment":"Accepted to the main conference of NAACL2024"},{"id":"http://arxiv.org/abs/2403.16431v1","updated":"2024-03-25T05:22:34Z","published":"2024-03-25T05:22:34Z","title":"DOCTR: Disentangled Object-Centric Transformer for Point Scene\n  Understanding","summary":"  Point scene understanding is a challenging task to process real-world scene\npoint cloud, which aims at segmenting each object, estimating its pose, and\nreconstructing its mesh simultaneously. Recent state-of-the-art method first\nsegments each object and then processes them independently with multiple stages\nfor the different sub-tasks. This leads to a complex pipeline to optimize and\nmakes it hard to leverage the relationship constraints between multiple\nobjects. In this work, we propose a novel Disentangled Object-Centric\nTRansformer (DOCTR) that explores object-centric representation to facilitate\nlearning with multiple objects for the multiple sub-tasks in a unified manner.\nEach object is represented as a query, and a Transformer decoder is adapted to\niteratively optimize all the queries involving their relationship. In\nparticular, we introduce a semantic-geometry disentangled query (SGDQ) design\nthat enables the query features to attend separately to semantic information\nand geometric information relevant to the corresponding sub-tasks. A hybrid\nbipartite matching module is employed to well use the supervisions from all the\nsub-tasks during training. Qualitative and quantitative experimental results\ndemonstrate that our method achieves state-of-the-art performance on the\nchallenging ScanNet dataset. Code is available at\nhttps://github.com/SAITPublic/DOCTR.\n","authors":["Xiaoxuan Yu","Hao Wang","Weiming Li","Qiang Wang","Soonyong Cho","Younghun Sung"],"pdf_url":"https://arxiv.org/pdf/2403.16431v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.13964v3","updated":"2024-03-25T05:18:04Z","published":"2023-12-21T15:51:12Z","title":"PIA: Your Personalized Image Animator via Plug-and-Play Modules in\n  Text-to-Image Models","summary":"  Recent advancements in personalized text-to-image (T2I) models have\nrevolutionized content creation, empowering non-experts to generate stunning\nimages with unique styles. While promising, adding realistic motions into these\npersonalized images by text poses significant challenges in preserving distinct\nstyles, high-fidelity details, and achieving motion controllability by text. In\nthis paper, we present PIA, a Personalized Image Animator that excels in\naligning with condition images, achieving motion controllability by text, and\nthe compatibility with various personalized T2I models without specific tuning.\nTo achieve these goals, PIA builds upon a base T2I model with well-trained\ntemporal alignment layers, allowing for the seamless transformation of any\npersonalized T2I model into an image animation model. A key component of PIA is\nthe introduction of the condition module, which utilizes the condition frame\nand inter-frame affinity as input to transfer appearance information guided by\nthe affinity hint for individual frame synthesis in the latent space. This\ndesign mitigates the challenges of appearance-related image alignment within\nand allows for a stronger focus on aligning with motion-related guidance.\n","authors":["Yiming Zhang","Zhening Xing","Yanhong Zeng","Youqing Fang","Kai Chen"],"pdf_url":"https://arxiv.org/pdf/2312.13964v3.pdf","comment":"Project page: https://pi-animator.github.io/"}],"Databases":[{"id":"http://arxiv.org/abs/2403.16712v1","updated":"2024-03-25T12:49:40Z","published":"2024-03-25T12:49:40Z","title":"Chase Termination Beyond Polynomial Time","summary":"  The chase is a widely implemented approach to reason with tuple-generating\ndependencies (tgds), used in data exchange, data integration, and\nontology-based query answering. However, it is merely a semi-decision\nprocedure, which may fail to terminate. Many decidable conditions have been\nproposed for tgds to ensure chase termination, typically by forbidding some\nkind of \"cycle\" in the chase process. We propose a new criterion that\nexplicitly allows some such cycles, and yet ensures termination of the standard\nchase under reasonable conditions. This leads to new decidable fragments of\ntgds that are not only syntactically more general but also strictly more\nexpressive than the fragments defined by prior acyclicity conditions. Indeed,\nwhile known terminating fragments are restricted to PTime data complexity, our\nconditions yield decidable languages for any k-ExpTime. We further refine our\nsyntactic conditions to obtain fragments of tgds for which an optimised chase\nprocedure decides query entailment in PSpace or k-ExpSpace, respectively.\n","authors":["Philipp Hanisch","Markus Krötzsch"],"pdf_url":"https://arxiv.org/pdf/2403.16712v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15324v2","updated":"2024-03-25T12:46:02Z","published":"2024-03-22T16:25:34Z","title":"ProvDeploy: Provenance-oriented Containerization of High Performance\n  Computing Scientific Workflows","summary":"  Many existing scientific workflows require High Performance Computing\nenvironments to produce results in a timely manner. These workflows have\nseveral software library components and use different environments, making the\ndeployment and execution of the software stack not trivial. This complexity\nincreases if the user needs to add provenance data capture services to the\nworkflow. This manuscript introduces ProvDeploy to assist the user in\nconfiguring containers for scientific workflows with integrated provenance data\ncapture. ProvDeploy was evaluated with a Scientific Machine Learning workflow,\nexploring containerization strategies focused on provenance in two distinct HPC\nenvironments\n","authors":["Liliane Kunstmann","Débora Pina","Daniel de Oliveira","Marta Mattoso"],"pdf_url":"https://arxiv.org/pdf/2403.15324v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16404v1","updated":"2024-03-25T03:43:36Z","published":"2024-03-25T03:43:36Z","title":"Implementing and Evaluating E2LSH on Storage","summary":"  Locality sensitive hashing (LSH) is one of the widely-used approaches to\napproximate nearest neighbor search (ANNS) in high-dimensional spaces. The\nfirst work on LSH for the Euclidean distance, E2LSH, showed how ANNS can be\nsolved efficiently at a sublinear query time in the database size with\ntheoretically-guaranteed accuracy, although it required a large hash index\nsize. Since then, several LSH variants having much smaller index sizes have\nbeen proposed. Their query time is linear or superlinear, but they have been\nshown to run effectively faster because they require fewer I/Os when the index\nis stored on hard disk drives and because they also permit in-memory execution\nwith modern DRAM capacity.\n  In this paper, we show that E2LSH is regaining the advantage in query speed\nwith the advent of modern flash storage devices such as solid-state drives\n(SSDs). We evaluate E2LSH on a modern single-node computing environment and\nanalyze its computational cost and I/O cost, from which we derive storage\nperformance requirements for its external memory execution. Our analysis\nindicates that E2LSH on a single consumer-grade SSD can run faster than the\nstate-of-the-art small-index methods executed in-memory. It also indicates that\nE2LSH with emerging high-performance storage devices and interfaces can\napproach in-memory E2LSH speeds. We implement a simple adaptation of E2LSH to\nexternal memory, E2LSH-on-Storage (E2LSHoS), and evaluate it for practical\nlarge datasets of up to one billion objects using different combinations of\nmodern storage devices and interfaces. We demonstrate that our E2LSHoS\nimplementation runs much faster than small-index methods and can approach\nin-memory E2LSH speeds, and also that its query time scales sublinearly with\nthe database size beyond the index size limit of in-memory E2LSH.\n","authors":["Yu Nakanishi","Kazuhiro Hiwada","Yosuke Bando","Tomoya Suzuki","Hirotsugu Kajihara","Shintaro Sano","Tatsuro Endo","Tatsuo Shiozawa"],"pdf_url":"https://arxiv.org/pdf/2403.16404v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17229v1","updated":"2024-03-25T22:11:49Z","published":"2024-03-25T22:11:49Z","title":"Corra: Correlation-Aware Column Compression","summary":"  Column encoding schemes have witnessed a spark of interest lately. This is\nnot surprising -- as data volume increases, being able to keep one's dataset in\nmain memory for fast processing is a coveted desideratum. However, it also\nseems that single-column encoding schemes have reached a plateau in terms of\nthe compression size they can achieve.\n  We argue that this is because they do not exploit correlations in the data.\nConsider for instance the column pair ($\\texttt{city}$, $\\texttt{zip-code}$) of\nthe DMV dataset: a city has only a few dozen unique zip codes. Such\ninformation, if properly exploited, can significantly reduce the space\nconsumption of the latter column.\n  In this work, we depart from the established, well-trodden path of\ncompressing data using only single-column encoding schemes and introduce\n$\\textit{correlation-aware}$ encoding schemes. We demonstrate their advantages\ncompared to single-column encoding schemes on the well-known TPC-H's\n$\\texttt{lineitem}$, LDBC's $\\texttt{message}$, DMV, and Taxi. For example, we\nobtain a saving rate of 58.3% for $\\texttt{lineitem}$'s $\\texttt{shipdate}$,\nwhile the dropoff timestamps in Taxi witness a saving rate of 30.6%.\n","authors":["Hanwen Liu","Mihail Stoian","Alexander van Renen","Andreas Kipf"],"pdf_url":"https://arxiv.org/pdf/2403.17229v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17082v1","updated":"2024-03-25T18:16:29Z","published":"2024-03-25T18:16:29Z","title":"GGDMiner - Discovery of Graph Generating Dependencies for Graph Data\n  Profiling","summary":"  With the increasing use of graph-structured data, there is also increasing\ninterest in investigating graph data dependencies and their applications, e.g.,\nin graph data profiling. Graph Generating Dependencies (GGDs) are a class of\ndependencies for property graphs that can express the relation between\ndifferent graph patterns and constraints based on their attribute similarities.\nRich syntax and semantics of GGDs make them a good candidate for graph data\nprofiling. Nonetheless, GGDs are difficult to define manually, especially when\nthere are no data experts available. In this paper, we propose GGDMiner, a\nframework for discovering approximate GGDs from graph data automatically, with\nthe intention of profiling graph data through GGDs for the user. GGDMiner has\nthree main steps: (1) pre-processing, (2) candidate generation, and, (3) GGD\nextraction. To optimize memory consumption and execution time, GGDMiner uses a\nfactorized representation of each discovered graph pattern, called Answer\nGraph. Our results show that the discovered set of GGDs can give an overview\nabout the input graph, both schema level information and also correlations\nbetween the graph patterns and attributes.\n","authors":["Larissa C. Shimomura","Nikolay Yakovets","George Fletcher"],"pdf_url":"https://arxiv.org/pdf/2403.17082v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.14529v2","updated":"2024-03-25T17:05:17Z","published":"2023-12-22T08:53:28Z","title":"When is Shapley Value Computation a Matter of Counting?","summary":"  The Shapley value provides a natural means of quantifying the contributions\nof facts to database query answers. In this work, we seek to broaden our\nunderstanding of Shapley value computation (SVC) in the database setting by\nrevealing how it relates to Fixed-size Generalized Model Counting (FGMC), which\nis the problem of computing the number of sub-databases of a given size and\ncontaining a given set of assumed facts that satisfy a fixed query. Our focus\nwill be on explaining the difficulty of SVC via FGMC, and to this end, we\nidentify general conditions on queries which enable reductions from FGMC to\nSVC. As a byproduct, we not only obtain alternative explanations for most\nexisting results on SVC, but also new complexity results. In particular, we\nestablish FP-#P complexity dichotomies for constant-free connected UCQs and\nhomomorphism-closed connected graph queries. We further explore variants of\nSVC, either in the absence of assumed facts, or where we measure the\ncontribution of constants rather than facts.\n","authors":["Meghyn Bienvenu","Diego Figueira","Pierre Lafourcade"],"pdf_url":"https://arxiv.org/pdf/2312.14529v2.pdf","comment":"Published at PODS'24"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2402.07310v2","updated":"2024-03-25T12:58:45Z","published":"2024-02-11T21:16:42Z","title":"BioNeRF: Biologically Plausible Neural Radiance Fields for View\n  Synthesis","summary":"  This paper presents BioNeRF, a biologically plausible architecture that\nmodels scenes in a 3D representation and synthesizes new views through radiance\nfields. Since NeRF relies on the network weights to store the scene's\n3-dimensional representation, BioNeRF implements a cognitive-inspired mechanism\nthat fuses inputs from multiple sources into a memory-like structure, improving\nthe storing capacity and extracting more intrinsic and correlated information.\nBioNeRF also mimics a behavior observed in pyramidal cells concerning\ncontextual information, in which the memory is provided as the context and\ncombined with the inputs of two subsequent neural models, one responsible for\nproducing the volumetric densities and the other the colors used to render the\nscene. Experimental results show that BioNeRF outperforms state-of-the-art\nresults concerning a quality measure that encodes human perception in two\ndatasets: real-world images and synthetic data.\n","authors":["Leandro A. Passos","Douglas Rodrigues","Danilo Jodas","Kelton A. P. Costa","Ahsan Adeel","João Paulo Papa"],"pdf_url":"https://arxiv.org/pdf/2402.07310v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.17919v3","updated":"2024-03-25T12:52:42Z","published":"2024-01-31T15:33:37Z","title":"LOCOST: State-Space Models for Long Document Abstractive Summarization","summary":"  State-space models are a low-complexity alternative to transformers for\nencoding long sequences and capturing long-term dependencies. We propose\nLOCOST: an encoder-decoder architecture based on state-space models for\nconditional text generation with long context inputs. With a computational\ncomplexity of $O(L \\log L)$, this architecture can handle significantly longer\nsequences than state-of-the-art models that are based on sparse attention\npatterns. We evaluate our model on a series of long document abstractive\nsummarization tasks. The model reaches a performance level that is 93-96%\ncomparable to the top-performing sparse transformers of the same size while\nsaving up to 50% memory during training and up to 87% during inference.\nAdditionally, LOCOST effectively handles input texts exceeding 600K tokens at\ninference time, setting new state-of-the-art results on full-book summarization\nand opening new perspectives for long input processing.\n","authors":["Florian Le Bronnec","Song Duong","Mathieu Ravaut","Alexandre Allauzen","Nancy F. Chen","Vincent Guigue","Alberto Lumbreras","Laure Soulier","Patrick Gallinari"],"pdf_url":"https://arxiv.org/pdf/2401.17919v3.pdf","comment":"9 pages, 5 figures, 7 tables, EACL 2024 conference"},{"id":"http://arxiv.org/abs/2403.16707v1","updated":"2024-03-25T12:44:52Z","published":"2024-03-25T12:44:52Z","title":"One-Shot Domain Incremental Learning","summary":"  Domain incremental learning (DIL) has been discussed in previous studies on\ndeep neural network models for classification. In DIL, we assume that samples\non new domains are observed over time. The models must classify inputs on all\ndomains. In practice, however, we may encounter a situation where we need to\nperform DIL under the constraint that the samples on the new domain are\nobserved only infrequently. Therefore, in this study, we consider the extreme\ncase where we have only one sample from the new domain, which we call one-shot\nDIL. We first empirically show that existing DIL methods do not work well in\none-shot DIL. We have analyzed the reason for this failure through various\ninvestigations. According to our analysis, we clarify that the difficulty of\none-shot DIL is caused by the statistics in the batch normalization layers.\nTherefore, we propose a technique regarding these statistics and demonstrate\nthe effectiveness of our technique through experiments on open datasets.\n","authors":["Yasushi Esaki","Satoshi Koide","Takuro Kutsuna"],"pdf_url":"https://arxiv.org/pdf/2403.16707v1.pdf","comment":"accepted at IEEE International Joint Conference on Neural Networks\n  (IJCNN) 2024"},{"id":"http://arxiv.org/abs/2403.16695v1","updated":"2024-03-25T12:26:32Z","published":"2024-03-25T12:26:32Z","title":"Assessing the Performance of Deep Learning for Automated Gleason Grading\n  in Prostate Cancer","summary":"  Prostate cancer is a dominant health concern calling for advanced diagnostic\ntools. Utilizing digital pathology and artificial intelligence, this study\nexplores the potential of 11 deep neural network architectures for automated\nGleason grading in prostate carcinoma focusing on comparing traditional and\nrecent architectures. A standardized image classification pipeline, based on\nthe AUCMEDI framework, facilitated robust evaluation using an in-house dataset\nconsisting of 34,264 annotated tissue tiles. The results indicated varying\nsensitivity across architectures, with ConvNeXt demonstrating the strongest\nperformance. Notably, newer architectures achieved superior performance, even\nthough with challenges in differentiating closely related Gleason grades. The\nConvNeXt model was capable of learning a balance between complexity and\ngeneralizability. Overall, this study lays the groundwork for enhanced Gleason\ngrading systems, potentially improving diagnostic efficiency for prostate\ncancer.\n","authors":["Dominik Müller","Philip Meyer","Lukas Rentschler","Robin Manz","Daniel Hieber","Jonas Bäcker","Samantha Cramer","Christoph Wengenmayr","Bruno Märkl","Ralf Huss","Frank Kramer","Iñaki Soto-Rey","Johannes Raffler"],"pdf_url":"https://arxiv.org/pdf/2403.16695v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16689v1","updated":"2024-03-25T12:23:39Z","published":"2024-03-25T12:23:39Z","title":"Synapse: Learning Preferential Concepts from Visual Demonstrations","summary":"  This paper addresses the problem of preference learning, which aims to learn\nuser-specific preferences (e.g., \"good parking spot\", \"convenient drop-off\nlocation\") from visual input. Despite its similarity to learning factual\nconcepts (e.g., \"red cube\"), preference learning is a fundamentally harder\nproblem due to its subjective nature and the paucity of person-specific\ntraining data. We address this problem using a new framework called Synapse,\nwhich is a neuro-symbolic approach designed to efficiently learn preferential\nconcepts from limited demonstrations. Synapse represents preferences as\nneuro-symbolic programs in a domain-specific language (DSL) that operates over\nimages, and leverages a novel combination of visual parsing, large language\nmodels, and program synthesis to learn programs representing individual\npreferences. We evaluate Synapse through extensive experimentation including a\nuser case study focusing on mobility-related concepts in mobile robotics and\nautonomous driving. Our evaluation demonstrates that Synapse significantly\noutperforms existing baselines as well as its own ablations. The code and other\ndetails can be found on the project website https://amrl.cs.utexas.edu/synapse .\n","authors":["Sadanand Modak","Noah Patton","Isil Dillig","Joydeep Biswas"],"pdf_url":"https://arxiv.org/pdf/2403.16689v1.pdf","comment":"23 pages, 7 figures; Preprint"},{"id":"http://arxiv.org/abs/2403.16681v1","updated":"2024-03-25T12:15:55Z","published":"2024-03-25T12:15:55Z","title":"A note on generalization bounds for losses with finite moments","summary":"  This paper studies the truncation method from Alquier [1] to derive\nhigh-probability PAC-Bayes bounds for unbounded losses with heavy tails.\nAssuming that the $p$-th moment is bounded, the resulting bounds interpolate\nbetween a slow rate $1 / \\sqrt{n}$ when $p=2$, and a fast rate $1 / n$ when $p\n\\to \\infty$ and the loss is essentially bounded. Moreover, the paper derives a\nhigh-probability PAC-Bayes bound for losses with a bounded variance. This bound\nhas an exponentially better dependence on the confidence parameter and the\ndependency measure than previous bounds in the literature. Finally, the paper\nextends all results to guarantees in expectation and single-draw PAC-Bayes. In\norder to so, it obtains analogues of the PAC-Bayes fast rate bound for bounded\nlosses from [2] in these settings.\n","authors":["Borja Rodríguez-Gálvez","Omar Rivasplata","Ragnar Thobaben","Mikael Skoglund"],"pdf_url":"https://arxiv.org/pdf/2403.16681v1.pdf","comment":"9 pages: 5 of main text, 1 of references, and 3 of appendices"},{"id":"http://arxiv.org/abs/2403.16680v1","updated":"2024-03-25T12:15:47Z","published":"2024-03-25T12:15:47Z","title":"Symmetric Basis Convolutions for Learning Lagrangian Fluid Mechanics","summary":"  Learning physical simulations has been an essential and central aspect of\nmany recent research efforts in machine learning, particularly for\nNavier-Stokes-based fluid mechanics. Classic numerical solvers have\ntraditionally been computationally expensive and challenging to use in inverse\nproblems, whereas Neural solvers aim to address both concerns through machine\nlearning. We propose a general formulation for continuous convolutions using\nseparable basis functions as a superset of existing methods and evaluate a\nlarge set of basis functions in the context of (a) a compressible 1D SPH\nsimulation, (b) a weakly compressible 2D SPH simulation, and (c) an\nincompressible 2D SPH Simulation. We demonstrate that even and odd symmetries\nincluded in the basis functions are key aspects of stability and accuracy. Our\nbroad evaluation shows that Fourier-based continuous convolutions outperform\nall other architectures regarding accuracy and generalization. Finally, using\nthese Fourier-based networks, we show that prior inductive biases, such as\nwindow functions, are no longer necessary. An implementation of our approach,\nas well as complete datasets and solver implementations, is available at\nhttps://github.com/tum-pbs/SFBC.\n","authors":["Rene Winchenbach","Nils Thuerey"],"pdf_url":"https://arxiv.org/pdf/2403.16680v1.pdf","comment":"Published at International Conference on Learning Representation\n  (ICLR) 2024, 54 pages, 39 figures"},{"id":"http://arxiv.org/abs/2403.16678v1","updated":"2024-03-25T12:15:42Z","published":"2024-03-25T12:15:42Z","title":"DeepGleason: a System for Automated Gleason Grading of Prostate Cancer\n  using Deep Neural Networks","summary":"  Advances in digital pathology and artificial intelligence (AI) offer\npromising opportunities for clinical decision support and enhancing diagnostic\nworkflows. Previous studies already demonstrated AI's potential for automated\nGleason grading, but lack state-of-the-art methodology and model reusability.\nTo address this issue, we propose DeepGleason: an open-source deep neural\nnetwork based image classification system for automated Gleason grading using\nwhole-slide histopathology images from prostate tissue sections. Implemented\nwith the standardized AUCMEDI framework, our tool employs a tile-wise\nclassification approach utilizing fine-tuned image preprocessing techniques in\ncombination with a ConvNeXt architecture which was compared to various\nstate-of-the-art architectures. The neural network model was trained and\nvalidated on an in-house dataset of 34,264 annotated tiles from 369 prostate\ncarcinoma slides. We demonstrated that DeepGleason is capable of highly\naccurate and reliable Gleason grading with a macro-averaged F1-score of 0.806,\nAUC of 0.991, and Accuracy of 0.974. The internal architecture comparison\nrevealed that the ConvNeXt model was superior performance-wise on our dataset\nto established and other modern architectures like transformers. Furthermore,\nwe were able to outperform the current state-of-the-art in tile-wise\nfine-classification with a sensitivity and specificity of 0.94 and 0.98 for\nbenign vs malignant detection as well as of 0.91 and 0.75 for Gleason 3 vs\nGleason 4 & 5 classification, respectively. Our tool contributes to the wider\nadoption of AI-based Gleason grading within the research community and paves\nthe way for broader clinical application of deep learning models in digital\npathology. DeepGleason is open-source and publicly available for research\napplication in the following Git repository:\nhttps://github.com/frankkramer-lab/DeepGleason.\n","authors":["Dominik Müller","Philip Meyer","Lukas Rentschler","Robin Manz","Jonas Bäcker","Samantha Cramer","Christoph Wengenmayr","Bruno Märkl","Ralf Huss","Iñaki Soto-Rey","Johannes Raffler"],"pdf_url":"https://arxiv.org/pdf/2403.16678v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16677v1","updated":"2024-03-25T12:14:48Z","published":"2024-03-25T12:14:48Z","title":"FOOL: Addressing the Downlink Bottleneck in Satellite Computing with\n  Neural Feature Compression","summary":"  Nanosatellite constellations equipped with sensors capturing large geographic\nregions provide unprecedented opportunities for Earth observation. As\nconstellation sizes increase, network contention poses a downlink bottleneck.\nOrbital Edge Computing (OEC) leverages limited onboard compute resources to\nreduce transfer costs by processing the raw captures at the source. However,\ncurrent solutions have limited practicability due to reliance on crude\nfiltering methods or over-prioritizing particular downstream tasks.\n  This work presents FOOL, an OEC-native and task-agnostic feature compression\nmethod that preserves prediction performance. FOOL partitions high-resolution\nsatellite imagery to maximize throughput. Further, it embeds context and\nleverages inter-tile dependencies to lower transfer costs with negligible\noverhead. While FOOL is a feature compressor, it can recover images with\ncompetitive scores on perceptual quality measures at lower bitrates. We\nextensively evaluate transfer cost reduction by including the peculiarity of\nintermittently available network connections in low earth orbit. Lastly, we\ntest the feasibility of our system for standardized nanosatellite form factors.\nWe demonstrate that FOOL permits downlinking over 100x the data volume without\nrelying on prior information on the downstream tasks.\n","authors":["Alireza Furutanpey","Qiyang Zhang","Philipp Raith","Tobias Pfandzelter","Shangguang Wang","Schahram Dustdar"],"pdf_url":"https://arxiv.org/pdf/2403.16677v1.pdf","comment":"18 pages, double column, 19 figures, 7 tables, Initial Submission to\n  IEEE Transactions on Mobile Computing"},{"id":"http://arxiv.org/abs/2403.16674v1","updated":"2024-03-25T12:13:20Z","published":"2024-03-25T12:13:20Z","title":"Understanding the Functional Roles of Modelling Components in Spiking\n  Neural Networks","summary":"  Spiking neural networks (SNNs), inspired by the neural circuits of the brain,\nare promising in achieving high computational efficiency with biological\nfidelity. Nevertheless, it is quite difficult to optimize SNNs because the\nfunctional roles of their modelling components remain unclear. By designing and\nevaluating several variants of the classic model, we systematically investigate\nthe functional roles of key modelling components, leakage, reset, and\nrecurrence, in leaky integrate-and-fire (LIF) based SNNs. Through extensive\nexperiments, we demonstrate how these components influence the accuracy,\ngeneralization, and robustness of SNNs. Specifically, we find that the leakage\nplays a crucial role in balancing memory retention and robustness, the reset\nmechanism is essential for uninterrupted temporal processing and computational\nefficiency, and the recurrence enriches the capability to model complex\ndynamics at a cost of robustness degradation. With these interesting\nobservations, we provide optimization suggestions for enhancing the performance\nof SNNs in different scenarios. This work deepens the understanding of how SNNs\nwork, which offers valuable guidance for the development of more effective and\nrobust neuromorphic models.\n","authors":["Huifeng Yin","Hanle Zheng","Jiayi Mao","Siyuan Ding","Xing Liu","Mingkun Xu","Yifan Hu","Jing Pei","Lei Deng"],"pdf_url":"https://arxiv.org/pdf/2403.16674v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.02930v2","updated":"2024-03-25T12:07:13Z","published":"2024-03-05T12:48:29Z","title":"A Second Look on BASS -- Boosting Abstractive Summarization with Unified\n  Semantic Graphs -- A Replication Study","summary":"  We present a detailed replication study of the BASS framework, an abstractive\nsummarization system based on the notion of Unified Semantic Graphs. Our\ninvestigation includes challenges in replicating key components and an ablation\nstudy to systematically isolate error sources rooted in replicating novel\ncomponents. Our findings reveal discrepancies in performance compared to the\noriginal work. We highlight the significance of paying careful attention even\nto reasonably omitted details for replicating advanced frameworks like BASS,\nand emphasize key practices for writing replicable papers.\n","authors":["Osman Alperen Koraş","Jörg Schlötterer","Christin Seifert"],"pdf_url":"https://arxiv.org/pdf/2403.02930v2.pdf","comment":"This preprint has not undergone peer review or any post-submission\n  improvements or corrections. The Version of Record of this contribution is\n  published in Advances in Information Retrieval, 46th European Conference on\n  Information Retrieval, ECIR 2024. 16 pages, 4 figures"},{"id":"http://arxiv.org/abs/2403.14587v2","updated":"2024-03-25T12:00:19Z","published":"2024-03-21T17:42:45Z","title":"An Analysis of Linear Time Series Forecasting Models","summary":"  Despite their simplicity, linear models perform well at time series\nforecasting, even when pitted against deeper and more expensive models. A\nnumber of variations to the linear model have been proposed, often including\nsome form of feature normalisation that improves model generalisation. In this\npaper we analyse the sets of functions expressible using these linear model\narchitectures. In so doing we show that several popular variants of linear\nmodels for time series forecasting are equivalent and functionally\nindistinguishable from standard, unconstrained linear regression. We\ncharacterise the model classes for each linear variant. We demonstrate that\neach model can be reinterpreted as unconstrained linear regression over a\nsuitably augmented feature set, and therefore admit closed-form solutions when\nusing a mean-squared loss function. We provide experimental evidence that the\nmodels under inspection learn nearly identical solutions, and finally\ndemonstrate that the simpler closed form solutions are superior forecasters\nacross 72% of test settings.\n","authors":["William Toner","Luke Darlow"],"pdf_url":"https://arxiv.org/pdf/2403.14587v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16656v1","updated":"2024-03-25T11:47:53Z","published":"2024-03-25T11:47:53Z","title":"Graph Augmentation for Recommendation","summary":"  Graph augmentation with contrastive learning has gained significant attention\nin the field of recommendation systems due to its ability to learn expressive\nuser representations, even when labeled data is limited. However, directly\napplying existing GCL models to real-world recommendation environments poses\nchallenges. There are two primary issues to address. Firstly, the lack of\nconsideration for data noise in contrastive learning can result in noisy\nself-supervised signals, leading to degraded performance. Secondly, many\nexisting GCL approaches rely on graph neural network (GNN) architectures, which\ncan suffer from over-smoothing problems due to non-adaptive message passing. To\naddress these challenges, we propose a principled framework called GraphAug.\nThis framework introduces a robust data augmentor that generates denoised\nself-supervised signals, enhancing recommender systems. The GraphAug framework\nincorporates a graph information bottleneck (GIB)-regularized augmentation\nparadigm, which automatically distills informative self-supervision information\nand adaptively adjusts contrastive view generation. Through rigorous\nexperimentation on real-world datasets, we thoroughly assessed the performance\nof our novel GraphAug model. The outcomes consistently unveil its superiority\nover existing baseline methods. The source code for our model is publicly\navailable at: https://github.com/HKUDS/GraphAug.\n","authors":["Qianru Zhang","Lianghao Xia","Xuheng Cai","Siuming Yiu","Chao Huang","Christian S. Jensen"],"pdf_url":"https://arxiv.org/pdf/2403.16656v1.pdf","comment":"13 pages and accepted by ICDE 2024"},{"id":"http://arxiv.org/abs/2403.16654v1","updated":"2024-03-25T11:42:01Z","published":"2024-03-25T11:42:01Z","title":"A Novel Loss Function-based Support Vector Machine for Binary\n  Classification","summary":"  The previous support vector machine(SVM) including $0/1$ loss SVM, hinge loss\nSVM, ramp loss SVM, truncated pinball loss SVM, and others, overlooked the\ndegree of penalty for the correctly classified samples within the margin. This\noversight affects the generalization ability of the SVM classifier to some\nextent. To address this limitation, from the perspective of confidence margin,\nwe propose a novel Slide loss function ($\\ell_s$) to construct the support\nvector machine classifier($\\ell_s$-SVM). By introducing the concept of proximal\nstationary point, and utilizing the property of Lipschitz continuity, we derive\nthe first-order optimality conditions for $\\ell_s$-SVM. Based on this, we\ndefine the $\\ell_s$ support vectors and working set of $\\ell_s$-SVM. To\nefficiently handle $\\ell_s$-SVM, we devise a fast alternating direction method\nof multipliers with the working set ($\\ell_s$-ADMM), and provide the\nconvergence analysis. The numerical experiments on real world datasets confirm\nthe robustness and effectiveness of the proposed method.\n","authors":["Yan Li","Liping Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.16654v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.11922v3","updated":"2024-03-25T11:39:57Z","published":"2024-02-19T08:11:26Z","title":"Spatio-Temporal Few-Shot Learning via Diffusive Neural Network\n  Generation","summary":"  Spatio-temporal modeling is foundational for smart city applications, yet it\nis often hindered by data scarcity in many cities and regions. To bridge this\ngap, we propose a novel generative pre-training framework, GPD, for\nspatio-temporal few-shot learning with urban knowledge transfer. Unlike\nconventional approaches that heavily rely on common feature extraction or\nintricate few-shot learning designs, our solution takes a novel approach by\nperforming generative pre-training on a collection of neural network parameters\noptimized with data from source cities. We recast spatio-temporal few-shot\nlearning as pre-training a generative diffusion model, which generates tailored\nneural networks guided by prompts, allowing for adaptability to diverse data\ndistributions and city-specific characteristics. GPD employs a\nTransformer-based denoising diffusion model, which is model-agnostic to\nintegrate with powerful spatio-temporal neural networks. By addressing\nchallenges arising from data gaps and the complexity of generalizing knowledge\nacross cities, our framework consistently outperforms state-of-the-art\nbaselines on multiple real-world datasets for tasks such as traffic speed\nprediction and crowd flow prediction. The implementation of our approach is\navailable: https://github.com/tsinghua-fib-lab/GPD.\n","authors":["Yuan Yuan","Chenyang Shao","Jingtao Ding","Depeng Jin","Yong Li"],"pdf_url":"https://arxiv.org/pdf/2402.11922v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16644v1","updated":"2024-03-25T11:29:32Z","published":"2024-03-25T11:29:32Z","title":"Bridging the Sim-to-Real Gap with Bayesian Inference","summary":"  We present SIM-FSVGD for learning robot dynamics from data. As opposed to\ntraditional methods, SIM-FSVGD leverages low-fidelity physical priors, e.g., in\nthe form of simulators, to regularize the training of neural network models.\nWhile learning accurate dynamics already in the low data regime, SIM-FSVGD\nscales and excels also when more data is available. We empirically show that\nlearning with implicit physical priors results in accurate mean model\nestimation as well as precise uncertainty quantification. We demonstrate the\neffectiveness of SIM-FSVGD in bridging the sim-to-real gap on a\nhigh-performance RC racecar system. Using model-based RL, we demonstrate a\nhighly dynamic parking maneuver with drifting, using less than half the data\ncompared to the state of the art.\n","authors":["Jonas Rothfuss","Bhavya Sukhija","Lenart Treven","Florian Dörfler","Stelian Coros","Andreas Krause"],"pdf_url":"https://arxiv.org/pdf/2403.16644v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16640v1","updated":"2024-03-25T11:28:52Z","published":"2024-03-25T11:28:52Z","title":"Multi-Scale Texture Loss for CT denoising with GANs","summary":"  Generative Adversarial Networks (GANs) have proved as a powerful framework\nfor denoising applications in medical imaging. However, GAN-based denoising\nalgorithms still suffer from limitations in capturing complex relationships\nwithin the images. In this regard, the loss function plays a crucial role in\nguiding the image generation process, encompassing how much a synthetic image\ndiffers from a real image. To grasp highly complex and non-linear textural\nrelationships in the training process, this work presents a loss function that\nleverages the intrinsic multi-scale nature of the Gray-Level-Co-occurrence\nMatrix (GLCM). Although the recent advances in deep learning have demonstrated\nsuperior performance in classification and detection tasks, we hypothesize that\nits information content can be valuable when integrated into GANs' training. To\nthis end, we propose a differentiable implementation of the GLCM suited for\ngradient-based optimization. Our approach also introduces a self-attention\nlayer that dynamically aggregates the multi-scale texture information extracted\nfrom the images. We validate our approach by carrying out extensive experiments\nin the context of low-dose CT denoising, a challenging application that aims to\nenhance the quality of noisy CT scans. We utilize three publicly available\ndatasets, including one simulated and two real datasets. The results are\npromising as compared to other well-established loss functions, being also\nconsistent across three different GAN architectures. The code is available at:\nhttps://github.com/FrancescoDiFeola/DenoTextureLoss\n","authors":["Francesco Di Feola","Lorenzo Tronchin","Valerio Guarrasi","Paolo Soda"],"pdf_url":"https://arxiv.org/pdf/2403.16640v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16630v1","updated":"2024-03-25T11:20:23Z","published":"2024-03-25T11:20:23Z","title":"A comparative analysis of embedding models for patent similarity","summary":"  This paper makes two contributions to the field of text-based patent\nsimilarity. First, it compares the performance of different kinds of\npatent-specific pretrained embedding models, namely static word embeddings\n(such as word2vec and doc2vec models) and contextual word embeddings (such as\ntransformers based models), on the task of patent similarity calculation.\nSecond, it compares specifically the performance of Sentence Transformers\n(SBERT) architectures with different training phases on the patent similarity\ntask. To assess the models' performance, we use information about patent\ninterferences, a phenomenon in which two or more patent claims belonging to\ndifferent patent applications are proven to be overlapping by patent examiners.\nTherefore, we use these interferences cases as a proxy for maximum similarity\nbetween two patents, treating them as ground-truth to evaluate the performance\nof the different embedding models. Our results point out that, first, Patent\nSBERT-adapt-ub, the domain adaptation of the pretrained Sentence Transformer\narchitecture proposed in this research, outperforms the current\nstate-of-the-art in patent similarity. Second, they show that, in some cases,\nlarge static models performances are still comparable to contextual ones when\ntrained on extensive data; thus, we believe that the superiority in the\nperformance of contextual embeddings may not be related to the actual\narchitecture but rather to the way the training phase is performed.\n","authors":["Grazia Sveva Ascione","Valerio Sterzi"],"pdf_url":"https://arxiv.org/pdf/2403.16630v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.03472v2","updated":"2024-03-25T10:58:22Z","published":"2023-08-07T11:02:44Z","title":"Improving the forecast accuracy of wind power by leveraging multiple\n  hierarchical structure","summary":"  Renewable energy generation is of utmost importance for global\ndecarbonization. Forecasting renewable energies, particularly wind energy, is\nchallenging due to the inherent uncertainty in wind energy generation, which\ndepends on weather conditions. Recent advances in hierarchical forecasting\nthrough reconciliation have demonstrated a significant increase in the quality\nof wind energy forecasts for short-term periods. We leverage the\ncross-sectional and temporal hierarchical structure of turbines in wind farms\nand build cross-temporal hierarchies to further investigate how integrated\ncross-sectional and temporal dimensions can add value to forecast accuracy in\nwind farms. We found that cross-temporal reconciliation was superior to\nindividual cross-sectional reconciliation at multiple temporal aggregations.\nAdditionally, machine learning based forecasts that were cross-temporally\nreconciled demonstrated high accuracy at coarser temporal granularities, which\nmay encourage adoption for short-term wind forecasts. Empirically, we provide\ninsights for decision-makers on the best methods for forecasting high-frequency\nwind data across different forecasting horizons and levels.\n","authors":["Lucas English","Mahdi Abolghasemi"],"pdf_url":"https://arxiv.org/pdf/2308.03472v2.pdf","comment":"41 pages, 14 figures"},{"id":"http://arxiv.org/abs/2403.16612v1","updated":"2024-03-25T10:42:48Z","published":"2024-03-25T10:42:48Z","title":"Calibrating Bayesian UNet++ for Sub-Seasonal Forecasting","summary":"  Seasonal forecasting is a crucial task when it comes to detecting the extreme\nheat and colds that occur due to climate change. Confidence in the predictions\nshould be reliable since a small increase in the temperatures in a year has a\nbig impact on the world. Calibration of the neural networks provides a way to\nensure our confidence in the predictions. However, calibrating regression\nmodels is an under-researched topic, especially in forecasters. We calibrate a\nUNet++ based architecture, which was shown to outperform physics-based models\nin temperature anomalies. We show that with a slight trade-off between\nprediction error and calibration error, it is possible to get more reliable and\nsharper forecasts. We believe that calibration should be an important part of\nsafety-critical machine learning applications such as weather forecasters.\n","authors":["Busra Asan","Abdullah Akgul","Alper Unal","Melih Kandemir","Gozde Unal"],"pdf_url":"https://arxiv.org/pdf/2403.16612v1.pdf","comment":"Accepted as a workshop paper at \"ICLR 2024 Tackling Climate Change\n  with Machine Learning\""},{"id":"http://arxiv.org/abs/2403.16610v1","updated":"2024-03-25T10:40:04Z","published":"2024-03-25T10:40:04Z","title":"Distributed collaborative anomalous sound detection by embedding sharing","summary":"  To develop a machine sound monitoring system, a method for detecting\nanomalous sound is proposed. In this paper, we explore a method for multiple\nclients to collaboratively learn an anomalous sound detection model while\nkeeping their raw data private from each other. In the context of industrial\nmachine anomalous sound detection, each client possesses data from different\nmachines or different operational states, making it challenging to learn\nthrough federated learning or split learning. In our proposed method, each\nclient calculates embeddings using a common pre-trained model developed for\nsound data classification, and these calculated embeddings are aggregated on\nthe server to perform anomalous sound detection through outlier exposure.\nExperiments showed that our proposed method improves the AUC of anomalous sound\ndetection by an average of 6.8%.\n","authors":["Kota Dohi","Yohei Kawaguchi"],"pdf_url":"https://arxiv.org/pdf/2403.16610v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16607v1","updated":"2024-03-25T10:38:17Z","published":"2024-03-25T10:38:17Z","title":"Enhancing Industrial Transfer Learning with Style Filter: Cost Reduction\n  and Defect-Focus","summary":"  Addressing the challenge of data scarcity in industrial domains, transfer\nlearning emerges as a pivotal paradigm. This work introduces Style Filter, a\ntailored methodology for industrial contexts. By selectively filtering source\ndomain data before knowledge transfer, Style Filter reduces the quantity of\ndata while maintaining or even enhancing the performance of transfer learning\nstrategy. Offering label-free operation, minimal reliance on prior knowledge,\nindependence from specific models, and re-utilization, Style Filter is\nevaluated on authentic industrial datasets, highlighting its effectiveness when\nemployed before conventional transfer strategies in the deep learning domain.\nThe results underscore the effectiveness of Style Filter in real-world\nindustrial applications.\n","authors":["Chen Li","Ruijie Ma","Xiang Qian","Xiaohao Wang","Xinghui Li"],"pdf_url":"https://arxiv.org/pdf/2403.16607v1.pdf","comment":"17 pages, 11 figures,4 tables"},{"id":"http://arxiv.org/abs/2403.16594v1","updated":"2024-03-25T10:13:52Z","published":"2024-03-25T10:13:52Z","title":"EDUE: Expert Disagreement-Guided One-Pass Uncertainty Estimation for\n  Medical Image Segmentation","summary":"  Deploying deep learning (DL) models in medical applications relies on\npredictive performance and other critical factors, such as conveying\ntrustworthy predictive uncertainty. Uncertainty estimation (UE) methods provide\npotential solutions for evaluating prediction reliability and improving the\nmodel confidence calibration. Despite increasing interest in UE, challenges\npersist, such as the need for explicit methods to capture aleatoric uncertainty\nand align uncertainty estimates with real-life disagreements among domain\nexperts. This paper proposes an Expert Disagreement-Guided Uncertainty\nEstimation (EDUE) for medical image segmentation. By leveraging variability in\nground-truth annotations from multiple raters, we guide the model during\ntraining and incorporate random sampling-based strategies to enhance\ncalibration confidence. Our method achieves 55% and 23% improvement in\ncorrelation on average with expert disagreements at the image and pixel levels,\nrespectively, better calibration, and competitive segmentation performance\ncompared to the state-of-the-art deep ensembles, requiring only a single\nforward pass.\n","authors":["Kudaibergen Abutalip","Numan Saeed","Ikboljon Sobirov","Vincent Andrearczyk","Adrien Depeursinge","Mohammad Yaqub"],"pdf_url":"https://arxiv.org/pdf/2403.16594v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16591v1","updated":"2024-03-25T10:06:45Z","published":"2024-03-25T10:06:45Z","title":"Deciphering the Interplay between Local Differential Privacy, Average\n  Bayesian Privacy, and Maximum Bayesian Privacy","summary":"  The swift evolution of machine learning has led to emergence of various\ndefinitions of privacy due to the threats it poses to privacy, including the\nconcept of local differential privacy (LDP). Although widely embraced and\nutilized across numerous domains, this conventional approach to measure privacy\nstill exhibits certain limitations, spanning from failure to prevent\ninferential disclosure to lack of consideration for the adversary's background\nknowledge. In this comprehensive study, we introduce Bayesian privacy and delve\ninto the intricate relationship between local differential privacy and its\nBayesian counterparts, unveiling novel insights into utility-privacy\ntrade-offs. We introduce a framework that encapsulates both attack and defense\nstrategies, highlighting their interplay and effectiveness. Our theoretical\ncontributions are anchored in the rigorous definitions and relationships\nbetween Average Bayesian Privacy (ABP) and Maximum Bayesian Privacy (MBP),\nencapsulated by equations $\\epsilon_{p,a} \\leq\n\\frac{1}{\\sqrt{2}}\\sqrt{(\\epsilon_{p,m} + \\epsilon)\\cdot(e^{\\epsilon_{p,m} +\n\\epsilon} - 1)}$ and the equivalence between $\\xi$-MBP and $2\\xi$-LDP\nestablished under uniform prior distribution. These relationships fortify our\nunderstanding of the privacy guarantees provided by various mechanisms, leading\nto the realization that a mechanism satisfying $\\xi$-LDP also confers\n$\\xi$-MBP, and vice versa. Our work not only lays the groundwork for future\nempirical exploration but also promises to enhance the design of\nprivacy-preserving algorithms that do not compromise on utility, thereby\nfostering the development of trustworthy machine learning solutions.\n","authors":["Xiaojin Zhang","Yulin Fei","Wei Chen","Hai Jin"],"pdf_url":"https://arxiv.org/pdf/2403.16591v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16582v1","updated":"2024-03-25T09:49:42Z","published":"2024-03-25T09:49:42Z","title":"In the Search for Optimal Multi-view Learning Models for Crop\n  Classification with Global Remote Sensing Data","summary":"  Crop classification is of critical importance due to its role in studying\ncrop pattern changes, resource management, and carbon sequestration. When\nemploying data-driven techniques for its prediction, utilizing various temporal\ndata sources is necessary. Deep learning models have proven to be effective for\nthis task by mapping time series data to high-level representation for\nprediction. However, they face substantial challenges when dealing with\nmultiple input patterns. The literature offers limited guidance for Multi-View\nLearning (MVL) scenarios, as it has primarily focused on exploring fusion\nstrategies with specific encoders and validating them in local regions. In\ncontrast, we investigate the impact of simultaneous selection of the fusion\nstrategy and the encoder architecture evaluated on a global-scale cropland and\ncrop-type classifications. We use a range of five fusion strategies (Input,\nFeature, Decision, Ensemble, Hybrid) and five temporal encoder architectures\n(LSTM, GRU, TempCNN, TAE, L-TAE) as possible MVL model configurations. The\nvalidation is on the CropHarvest dataset that provides optical, radar, and\nweather time series, and topographic information as input data. We found that\nin scenarios with a limited number of labeled samples, a unique configuration\nis insufficient for all the cases. Instead, a specialized combination,\nincluding encoder and fusion strategy, should be meticulously sought. To\nstreamline this search process, we suggest initially identifying the optimal\nencoder architecture tailored for a particular fusion strategy, and then\ndetermining the most suitable fusion strategy for the classification task. We\nprovide a technical framework for researchers exploring crop classification or\nrelated tasks through a MVL approach.\n","authors":["Francisco Mena","Diego Arenas","Andreas Dengel"],"pdf_url":"https://arxiv.org/pdf/2403.16582v1.pdf","comment":"submitted to journal"},{"id":"http://arxiv.org/abs/2403.10615v2","updated":"2024-03-25T09:42:13Z","published":"2024-03-15T18:26:33Z","title":"LightIt: Illumination Modeling and Control for Diffusion Models","summary":"  We introduce LightIt, a method for explicit illumination control for image\ngeneration. Recent generative methods lack lighting control, which is crucial\nto numerous artistic aspects of image generation such as setting the overall\nmood or cinematic appearance. To overcome these limitations, we propose to\ncondition the generation on shading and normal maps. We model the lighting with\nsingle bounce shading, which includes cast shadows. We first train a shading\nestimation module to generate a dataset of real-world images and shading pairs.\nThen, we train a control network using the estimated shading and normals as\ninput. Our method demonstrates high-quality image generation and lighting\ncontrol in numerous scenes. Additionally, we use our generated dataset to train\nan identity-preserving relighting model, conditioned on an image and a target\nshading. Our method is the first that enables the generation of images with\ncontrollable, consistent lighting and performs on par with specialized\nrelighting state-of-the-art methods.\n","authors":["Peter Kocsis","Julien Philip","Kalyan Sunkavalli","Matthias Nießner","Yannick Hold-Geoffroy"],"pdf_url":"https://arxiv.org/pdf/2403.10615v2.pdf","comment":"Project page: https://peter-kocsis.github.io/LightIt/ Video:\n  https://youtu.be/cCfSBD5aPLI"},{"id":"http://arxiv.org/abs/2403.16576v1","updated":"2024-03-25T09:41:49Z","published":"2024-03-25T09:41:49Z","title":"Antigen-Specific Antibody Design via Direct Energy-based Preference\n  Optimization","summary":"  Antibody design, a crucial task with significant implications across various\ndisciplines such as therapeutics and biology, presents considerable challenges\ndue to its intricate nature. In this paper, we tackle antigen-specific antibody\ndesign as a protein sequence-structure co-design problem, considering both\nrationality and functionality. Leveraging a pre-trained conditional diffusion\nmodel that jointly models sequences and structures of\ncomplementarity-determining regions (CDR) in antibodies with equivariant neural\nnetworks, we propose direct energy-based preference optimization to guide the\ngeneration of antibodies with both rational structures and considerable binding\naffinities to given antigens. Our method involves fine-tuning the pre-trained\ndiffusion model using a residue-level decomposed energy preference.\nAdditionally, we employ gradient surgery to address conflicts between various\ntypes of energy, such as attraction and repulsion. Experiments on RAbD\nbenchmark show that our approach effectively optimizes the energy of generated\nantibodies and achieves state-of-the-art performance in designing high-quality\nantibodies with low total energy and high binding affinity, demonstrating the\nsuperiority of our approach.\n","authors":["Xiangxin Zhou","Dongyu Xue","Ruizhe Chen","Zaixiang Zheng","Liang Wang","Quanquan Gu"],"pdf_url":"https://arxiv.org/pdf/2403.16576v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16571v1","updated":"2024-03-25T09:36:51Z","published":"2024-03-25T09:36:51Z","title":"NSINA: A News Corpus for Sinhala","summary":"  The introduction of large language models (LLMs) has advanced natural\nlanguage processing (NLP), but their effectiveness is largely dependent on\npre-training resources. This is especially evident in low-resource languages,\nsuch as Sinhala, which face two primary challenges: the lack of substantial\ntraining data and limited benchmarking datasets. In response, this study\nintroduces NSINA, a comprehensive news corpus of over 500,000 articles from\npopular Sinhala news websites, along with three NLP tasks: news media\nidentification, news category prediction, and news headline generation. The\nrelease of NSINA aims to provide a solution to challenges in adapting LLMs to\nSinhala, offering valuable resources and benchmarks for improving NLP in the\nSinhala language. NSINA is the largest news corpus for Sinhala, available up to\ndate.\n","authors":["Hansi Hettiarachchi","Damith Premasiri","Lasitha Uyangodage","Tharindu Ranasinghe"],"pdf_url":"https://arxiv.org/pdf/2403.16571v1.pdf","comment":"Accepted to LREC-COLING 2024 (The 2024 Joint International Conference\n  on Computational Linguistics, Language Resources and Evaluation)"},{"id":"http://arxiv.org/abs/2403.16569v1","updated":"2024-03-25T09:36:10Z","published":"2024-03-25T09:36:10Z","title":"Revealing Vulnerabilities of Neural Networks in Parameter Learning and\n  Defense Against Explanation-Aware Backdoors","summary":"  Explainable Artificial Intelligence (XAI) strategies play a crucial part in\nincreasing the understanding and trustworthiness of neural networks.\nNonetheless, these techniques could potentially generate misleading\nexplanations. Blinding attacks can drastically alter a machine learning\nalgorithm's prediction and explanation, providing misleading information by\nadding visually unnoticeable artifacts into the input, while maintaining the\nmodel's accuracy. It poses a serious challenge in ensuring the reliability of\nXAI methods. To ensure the reliability of XAI methods poses a real challenge,\nwe leverage statistical analysis to highlight the changes in CNN weights within\na CNN following blinding attacks. We introduce a method specifically designed\nto limit the effectiveness of such attacks during the evaluation phase,\navoiding the need for extra training. The method we suggest defences against\nmost modern explanation-aware adversarial attacks, achieving an approximate\ndecrease of ~99\\% in the Attack Success Rate (ASR) and a ~91\\% reduction in the\nMean Square Error (MSE) between the original explanation and the defended\n(post-attack) explanation across three unique types of attacks.\n","authors":["Md Abdul Kadir","GowthamKrishna Addluri","Daniel Sonntag"],"pdf_url":"https://arxiv.org/pdf/2403.16569v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16561v1","updated":"2024-03-25T09:24:05Z","published":"2024-03-25T09:24:05Z","title":"FedFixer: Mitigating Heterogeneous Label Noise in Federated Learning","summary":"  Federated Learning (FL) heavily depends on label quality for its performance.\nHowever, the label distribution among individual clients is always both noisy\nand heterogeneous. The high loss incurred by client-specific samples in\nheterogeneous label noise poses challenges for distinguishing between\nclient-specific and noisy label samples, impacting the effectiveness of\nexisting label noise learning approaches. To tackle this issue, we propose\nFedFixer, where the personalized model is introduced to cooperate with the\nglobal model to effectively select clean client-specific samples. In the dual\nmodels, updating the personalized model solely at a local level can lead to\noverfitting on noisy data due to limited samples, consequently affecting both\nthe local and global models' performance. To mitigate overfitting, we address\nthis concern from two perspectives. Firstly, we employ a confidence regularizer\nto alleviate the impact of unconfident predictions caused by label noise.\nSecondly, a distance regularizer is implemented to constrain the disparity\nbetween the personalized and global models. We validate the effectiveness of\nFedFixer through extensive experiments on benchmark datasets. The results\ndemonstrate that FedFixer can perform well in filtering noisy label samples on\ndifferent clients, especially in highly heterogeneous label noise scenarios.\n","authors":["Xinyuan Ji","Zhaowei Zhu","Wei Xi","Olga Gadyatskaya","Zilong Song","Yong Cai","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2403.16561v1.pdf","comment":"accepted by AAA24"},{"id":"http://arxiv.org/abs/2403.16557v1","updated":"2024-03-25T09:16:59Z","published":"2024-03-25T09:16:59Z","title":"Accelerating Federated Learning by Selecting Beneficial Herd of Local\n  Gradients","summary":"  Federated Learning (FL) is a distributed machine learning framework in\ncommunication network systems. However, the systems' Non-Independent and\nIdentically Distributed (Non-IID) data negatively affect the convergence\nefficiency of the global model, since only a subset of these data samples are\nbeneficial for model convergence. In pursuit of this subset, a reliable\napproach involves determining a measure of validity to rank the samples within\nthe dataset. In this paper, We propose the BHerd strategy which selects a\nbeneficial herd of local gradients to accelerate the convergence of the FL\nmodel. Specifically, we map the distribution of the local dataset to the local\ngradients and use the Herding strategy to obtain a permutation of the set of\ngradients, where the more advanced gradients in the permutation are closer to\nthe average of the set of gradients. These top portion of the gradients will be\nselected and sent to the server for global aggregation. We conduct experiments\non different datasets, models and scenarios by building a prototype system, and\nexperimental results demonstrate that our BHerd strategy is effective in\nselecting beneficial local gradients to mitigate the effects brought by the\nNon-IID dataset, thus accelerating model convergence.\n","authors":["Ping Luo","Xiaoge Deng","Ziqing Wen","Tao Sun","Dongsheng Li"],"pdf_url":"https://arxiv.org/pdf/2403.16557v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.14714v4","updated":"2024-03-25T08:58:39Z","published":"2023-10-23T08:51:05Z","title":"BatteryML:An Open-source platform for Machine Learning on Battery\n  Degradation","summary":"  Battery degradation remains a pivotal concern in the energy storage domain,\nwith machine learning emerging as a potent tool to drive forward insights and\nsolutions. However, this intersection of electrochemical science and machine\nlearning poses complex challenges. Machine learning experts often grapple with\nthe intricacies of battery science, while battery researchers face hurdles in\nadapting intricate models tailored to specific datasets. Beyond this, a\ncohesive standard for battery degradation modeling, inclusive of data formats\nand evaluative benchmarks, is conspicuously absent. Recognizing these\nimpediments, we present BatteryML - a one-step, all-encompass, and open-source\nplatform designed to unify data preprocessing, feature extraction, and the\nimplementation of both traditional and state-of-the-art models. This\nstreamlined approach promises to enhance the practicality and efficiency of\nresearch applications. BatteryML seeks to fill this void, fostering an\nenvironment where experts from diverse specializations can collaboratively\ncontribute, thus elevating the collective understanding and advancement of\nbattery research.The code for our project is publicly available on GitHub at\nhttps://github.com/microsoft/BatteryML.\n","authors":["Han Zhang","Xiaofan Gui","Shun Zheng","Ziheng Lu","Yuqi Li","Jiang Bian"],"pdf_url":"https://arxiv.org/pdf/2310.14714v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.02760v2","updated":"2024-03-25T08:57:47Z","published":"2023-11-05T20:33:18Z","title":"Causal Question Answering with Reinforcement Learning","summary":"  Causal questions inquire about causal relationships between different events\nor phenomena. They are important for a variety of use cases, including virtual\nassistants and search engines. However, many current approaches to causal\nquestion answering cannot provide explanations or evidence for their answers.\nHence, in this paper, we aim to answer causal questions with a causality graph,\na large-scale dataset of causal relations between noun phrases along with the\nrelations' provenance data. Inspired by recent, successful applications of\nreinforcement learning to knowledge graph tasks, such as link prediction and\nfact-checking, we explore the application of reinforcement learning on a\ncausality graph for causal question answering. We introduce an\nActor-Critic-based agent which learns to search through the graph to answer\ncausal questions. We bootstrap the agent with a supervised learning procedure\nto deal with large action spaces and sparse rewards. Our evaluation shows that\nthe agent successfully prunes the search space to answer binary causal\nquestions by visiting less than 30 nodes per question compared to over 3,000\nnodes by a naive breadth-first search. Our ablation study indicates that our\nsupervised learning strategy provides a strong foundation upon which our\nreinforcement learning agent improves. The paths returned by our agent explain\nthe mechanisms by which a cause produces an effect. Moreover, for each edge on\na path, our causality graph provides its original source allowing for easy\nverification of paths.\n","authors":["Lukas Blübaum","Stefan Heindorf"],"pdf_url":"https://arxiv.org/pdf/2311.02760v2.pdf","comment":"Accepted at WWW 2024"},{"id":"http://arxiv.org/abs/2402.09132v3","updated":"2024-03-25T08:46:02Z","published":"2024-02-14T12:28:38Z","title":"Exploring the Adversarial Capabilities of Large Language Models","summary":"  The proliferation of large language models (LLMs) has sparked widespread and\ngeneral interest due to their strong language generation capabilities, offering\ngreat potential for both industry and research. While previous research delved\ninto the security and privacy issues of LLMs, the extent to which these models\ncan exhibit adversarial behavior remains largely unexplored. Addressing this\ngap, we investigate whether common publicly available LLMs have inherent\ncapabilities to perturb text samples to fool safety measures, so-called\nadversarial examples resp.~attacks. More specifically, we investigate whether\nLLMs are inherently able to craft adversarial examples out of benign samples to\nfool existing safe rails. Our experiments, which focus on hate speech\ndetection, reveal that LLMs succeed in finding adversarial perturbations,\neffectively undermining hate speech detection systems. Our findings carry\nsignificant implications for (semi-)autonomous systems relying on LLMs,\nhighlighting potential challenges in their interaction with existing systems\nand safety measures.\n","authors":["Lukas Struppek","Minh Hieu Le","Dominik Hintersdorf","Kristian Kersting"],"pdf_url":"https://arxiv.org/pdf/2402.09132v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16542v1","updated":"2024-03-25T08:35:19Z","published":"2024-03-25T08:35:19Z","title":"Differentially Private Online Federated Learning with Correlated Noise","summary":"  We propose a novel differentially private algorithm for online federated\nlearning that employs temporally correlated noise to improve the utility while\nensuring the privacy of the continuously released models. To address challenges\nstemming from DP noise and local updates with streaming noniid data, we develop\na perturbed iterate analysis to control the impact of the DP noise on the\nutility. Moreover, we demonstrate how the drift errors from local updates can\nbe effectively managed under a quasi-strong convexity condition. Subject to an\n$(\\epsilon, \\delta)$-DP budget, we establish a dynamic regret bound over the\nentire time horizon that quantifies the impact of key parameters and the\nintensity of changes in dynamic environments. Numerical experiments validate\nthe efficacy of the proposed algorithm.\n","authors":["Jiaojiao Zhang","Linglingzhi Zhu","Mikael Johansson"],"pdf_url":"https://arxiv.org/pdf/2403.16542v1.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2403.16523v1","updated":"2024-03-25T08:06:08Z","published":"2024-03-25T08:06:08Z","title":"Causal Discovery from Poisson Branching Structural Causal Model Using\n  High-Order Cumulant with Path Analysis","summary":"  Count data naturally arise in many fields, such as finance, neuroscience, and\nepidemiology, and discovering causal structure among count data is a crucial\ntask in various scientific and industrial scenarios. One of the most common\ncharacteristics of count data is the inherent branching structure described by\na binomial thinning operator and an independent Poisson distribution that\ncaptures both branching and noise. For instance, in a population count\nscenario, mortality and immigration contribute to the count, where survival\nfollows a Bernoulli distribution, and immigration follows a Poisson\ndistribution. However, causal discovery from such data is challenging due to\nthe non-identifiability issue: a single causal pair is Markov equivalent, i.e.,\n$X\\rightarrow Y$ and $Y\\rightarrow X$ are distributed equivalent. Fortunately,\nin this work, we found that the causal order from $X$ to its child $Y$ is\nidentifiable if $X$ is a root vertex and has at least two directed paths to\n$Y$, or the ancestor of $X$ with the most directed path to $X$ has a directed\npath to $Y$ without passing $X$. Specifically, we propose a Poisson Branching\nStructure Causal Model (PB-SCM) and perform a path analysis on PB-SCM using\nhigh-order cumulants. Theoretical results establish the connection between the\npath and cumulant and demonstrate that the path information can be obtained\nfrom the cumulant. With the path information, causal order is identifiable\nunder some graphical conditions. A practical algorithm for learning causal\nstructure under PB-SCM is proposed and the experiments demonstrate and verify\nthe effectiveness of the proposed method.\n","authors":["Jie Qiao","Yu Xiang","Zhengming Chen","Ruichu Cai","Zhifeng Hao"],"pdf_url":"https://arxiv.org/pdf/2403.16523v1.pdf","comment":"Accepted by AAAI-2024"},{"id":"http://arxiv.org/abs/2306.08929v2","updated":"2024-03-25T08:00:38Z","published":"2023-06-15T08:02:07Z","title":"On the resilience of Collaborative Learning-based Recommender Systems\n  Against Community Detection Attack","summary":"  Collaborative-learning-based recommender systems emerged following the\nsuccess of collaborative learning techniques such as Federated Learning (FL)\nand Gossip Learning (GL). In these systems, users participate in the training\nof a recommender system while maintaining their history of consumed items on\ntheir devices. While these solutions seemed appealing for preserving the\nprivacy of the participants at first glance, recent studies have revealed that\ncollaborative learning can be vulnerable to various privacy attacks. In this\npaper, we study the resilience of collaborative learning-based recommender\nsystems against a novel privacy attack called Community Detection Attack (CDA).\nThis attack enables an adversary to identify community members based on a\nchosen set of items (eg., identifying users interested in specific\npoints-of-interest). Through experiments on three real recommendation datasets\nusing two state-of-the-art recommendation models, we evaluate the sensitivity\nof an FL-based recommender system as well as two flavors of Gossip\nLearning-based recommender systems to CDA. The results show that across all\nmodels and datasets, the FL setting is more vulnerable to CDA compared to\nGossip settings. Furthermore, we assess two off-the-shelf mitigation\nstrategies, namely differential privacy (DP) and a \\emph{Share less} policy,\nwhich consists of sharing a subset of less sensitive model parameters. The\nfindings indicate a more favorable privacy-utility trade-off for the\n\\emph{Share less} strategy, particularly in FedRecs.\n","authors":["Yacine Belal","Sonia Ben Mokhtar","Mohamed Maouche","Anthony Simonet-Boulogne"],"pdf_url":"https://arxiv.org/pdf/2306.08929v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16509v1","updated":"2024-03-25T07:48:34Z","published":"2024-03-25T07:48:34Z","title":"Human Understanding AI Paper Challenge 2024 -- Dataset Design","summary":"  In 2024, we will hold a research paper competition (the third Human\nUnderstanding AI Paper Challenge) for the research and development of\nartificial intelligence technologies to understand human daily life. This\ndocument introduces the datasets that will be provided to participants in the\ncompetition, and summarizes the issues to consider in data processing and\nlearning model development.\n","authors":["Se Won Oh","Hyuntae Jeong","Jeong Mook Lim","Seungeun Chung","Kyoung Ju Noh"],"pdf_url":"https://arxiv.org/pdf/2403.16509v1.pdf","comment":"7 pages, 3 figures"},{"id":"http://arxiv.org/abs/2403.16497v1","updated":"2024-03-25T07:29:18Z","published":"2024-03-25T07:29:18Z","title":"PathoTune: Adapting Visual Foundation Model to Pathological Specialists","summary":"  As natural image understanding moves towards the pretrain-finetune era,\nresearch in pathology imaging is concurrently evolving. Despite the predominant\nfocus on pretraining pathological foundation models, how to adapt foundation\nmodels to downstream tasks is little explored. For downstream adaptation, we\npropose the existence of two domain gaps, i.e., the Foundation-Task Gap and the\nTask-Instance Gap. To mitigate these gaps, we introduce PathoTune, a framework\ndesigned to efficiently adapt pathological or even visual foundation models to\npathology-specific tasks via multi-modal prompt tuning. The proposed framework\nleverages Task-specific Visual Prompts and Task-specific Textual Prompts to\nidentify task-relevant features, along with Instance-specific Visual Prompts\nfor encoding single pathological image features. Results across multiple\ndatasets at both patch-level and WSI-level demonstrate its superior performance\nover single-modality prompt tuning approaches. Significantly, PathoTune\nfacilitates the direct adaptation of natural visual foundation models to\npathological tasks, drastically outperforming pathological foundation models\nwith simple linear probing. The code will be available upon acceptance.\n","authors":["Jiaxuan Lu","Fang Yan","Xiaofan Zhang","Yue Gao","Shaoting Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.16497v1.pdf","comment":"Submitted to MICCAI 2024"},{"id":"http://arxiv.org/abs/2403.16495v1","updated":"2024-03-25T07:23:23Z","published":"2024-03-25T07:23:23Z","title":"LSTTN: A Long-Short Term Transformer-based Spatio-temporal Neural\n  Network for Traffic Flow Forecasting","summary":"  Accurate traffic forecasting is a fundamental problem in intelligent\ntransportation systems and learning long-range traffic representations with key\ninformation through spatiotemporal graph neural networks (STGNNs) is a basic\nassumption of current traffic flow prediction models. However, due to\nstructural limitations, existing STGNNs can only utilize short-range traffic\nflow data; therefore, the models cannot adequately learn the complex trends and\nperiodic features in traffic flow. Besides, it is challenging to extract the\nkey temporal information from the long historical traffic series and obtain a\ncompact representation. To solve the above problems, we propose a novel LSTTN\n(Long-Short Term Transformer-based Network) framework comprehensively\nconsidering the long- and short-term features in historical traffic flow.\nFirst, we employ a masked subseries Transformer to infer the content of masked\nsubseries from a small portion of unmasked subseries and their temporal context\nin a pretraining manner, forcing the model to efficiently learn compressed and\ncontextual subseries temporal representations from long historical series.\nThen, based on the learned representations, long-term trend is extracted by\nusing stacked 1D dilated convolution layers, and periodic features are\nextracted by dynamic graph convolution layers. For the difficulties in making\ntime-step level prediction, LSTTN adopts a short-term trend extractor to learn\nfine-grained short-term temporal features. Finally, LSTTN fuses the long-term\ntrend, periodic features and short-term features to obtain the prediction\nresults. Experiments on four real-world datasets show that in 60-minute-ahead\nlong-term forecasting, the LSTTN model achieves a minimum improvement of 5.63\\%\nand a maximum improvement of 16.78\\% over baseline models. The source code is\navailable at https://github.com/GeoX-Lab/LSTTN.\n","authors":["Qinyao Luo","Silu He","Xing Han","Yuhan Wang","Haifeng Li"],"pdf_url":"https://arxiv.org/pdf/2403.16495v1.pdf","comment":"15 pages, 10 figures, 6 tables"},{"id":"http://arxiv.org/abs/2403.16482v1","updated":"2024-03-25T07:08:01Z","published":"2024-03-25T07:08:01Z","title":"Determined Multi-Label Learning via Similarity-Based Prompt","summary":"  In multi-label classification, each training instance is associated with\nmultiple class labels simultaneously. Unfortunately, collecting the fully\nprecise class labels for each training instance is time- and labor-consuming\nfor real-world applications. To alleviate this problem, a novel labeling\nsetting termed \\textit{Determined Multi-Label Learning} (DMLL) is proposed,\naiming to effectively alleviate the labeling cost inherent in multi-label\ntasks. In this novel labeling setting, each training instance is associated\nwith a \\textit{determined label} (either \"Yes\" or \"No\"), which indicates\nwhether the training instance contains the provided class label. The provided\nclass label is randomly and uniformly selected from the whole candidate labels\nset. Besides, each training instance only need to be determined once, which\nsignificantly reduce the annotation cost of the labeling task for multi-label\ndatasets. In this paper, we theoretically derive an risk-consistent estimator\nto learn a multi-label classifier from these determined-labeled training data.\nAdditionally, we introduce a similarity-based prompt learning method for the\nfirst time, which minimizes the risk-consistent loss of large-scale pre-trained\nmodels to learn a supplemental prompt with richer semantic information.\nExtensive experimental validation underscores the efficacy of our approach,\ndemonstrating superior performance compared to existing state-of-the-art\nmethods.\n","authors":["Meng Wei","Zhongnian Li","Peng Ying","Yong Zhou","Xinzheng Xu"],"pdf_url":"https://arxiv.org/pdf/2403.16482v1.pdf","comment":"10 pages, 4 figures"},{"id":"http://arxiv.org/abs/2312.11834v2","updated":"2024-03-25T07:00:29Z","published":"2023-12-19T04:02:50Z","title":"Multi-agent reinforcement learning using echo-state network and its\n  application to pedestrian dynamics","summary":"  In recent years, simulations of pedestrians using the multi-agent\nreinforcement learning (MARL) have been studied. This study considered the\nroads on a grid-world environment, and implemented pedestrians as MARL agents\nusing an echo-state network and the least squares policy iteration method.\nUnder this environment, the ability of these agents to learn to move forward by\navoiding other agents was investigated. Specifically, we considered two types\nof tasks: the choice between a narrow direct route and a broad detour, and the\nbidirectional pedestrian flow in a corridor. The simulations results indicated\nthat the learning was successful when the density of the agents was not that\nhigh.\n","authors":["Hisato Komatsu"],"pdf_url":"https://arxiv.org/pdf/2312.11834v2.pdf","comment":"26 pages, 17 figures"},{"id":"http://arxiv.org/abs/2403.06606v2","updated":"2024-03-25T06:57:57Z","published":"2024-03-11T10:50:53Z","title":"Distributionally Generative Augmentation for Fair Facial Attribute\n  Classification","summary":"  Facial Attribute Classification (FAC) holds substantial promise in widespread\napplications. However, FAC models trained by traditional methodologies can be\nunfair by exhibiting accuracy inconsistencies across varied data\nsubpopulations. This unfairness is largely attributed to bias in data, where\nsome spurious attributes (e.g., Male) statistically correlate with the target\nattribute (e.g., Smiling). Most of existing fairness-aware methods rely on the\nlabels of spurious attributes, which may be unavailable in practice. This work\nproposes a novel, generation-based two-stage framework to train a fair FAC\nmodel on biased data without additional annotation. Initially, we identify the\npotential spurious attributes based on generative models. Notably, it enhances\ninterpretability by explicitly showing the spurious attributes in image space.\nFollowing this, for each image, we first edit the spurious attributes with a\nrandom degree sampled from a uniform distribution, while keeping target\nattribute unchanged. Then we train a fair FAC model by fostering model\ninvariance to these augmentation. Extensive experiments on three common\ndatasets demonstrate the effectiveness of our method in promoting fairness in\nFAC without compromising accuracy. Codes are in\nhttps://github.com/heqianpei/DiGA.\n","authors":["Fengda Zhang","Qianpei He","Kun Kuang","Jiashuo Liu","Long Chen","Chao Wu","Jun Xiao","Hanwang Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.06606v2.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2403.16469v1","updated":"2024-03-25T06:50:25Z","published":"2024-03-25T06:50:25Z","title":"Learning from Reduced Labels for Long-Tailed Data","summary":"  Long-tailed data is prevalent in real-world classification tasks and heavily\nrelies on supervised information, which makes the annotation process\nexceptionally labor-intensive and time-consuming. Unfortunately, despite being\na common approach to mitigate labeling costs, existing weakly supervised\nlearning methods struggle to adequately preserve supervised information for\ntail samples, resulting in a decline in accuracy for the tail classes. To\nalleviate this problem, we introduce a novel weakly supervised labeling setting\ncalled Reduced Label. The proposed labeling setting not only avoids the decline\nof supervised information for the tail samples, but also decreases the labeling\ncosts associated with long-tailed data. Additionally, we propose an\nstraightforward and highly efficient unbiased framework with strong theoretical\nguarantees to learn from these Reduced Labels. Extensive experiments conducted\non benchmark datasets including ImageNet validate the effectiveness of our\napproach, surpassing the performance of state-of-the-art weakly supervised\nmethods.\n","authors":["Meng Wei","Zhongnian Li","Yong Zhou","Xinzheng Xu"],"pdf_url":"https://arxiv.org/pdf/2403.16469v1.pdf","comment":"12 pages, 3 figures"},{"id":"http://arxiv.org/abs/2403.16464v1","updated":"2024-03-25T06:46:27Z","published":"2024-03-25T06:46:27Z","title":"Training Generative Adversarial Network-Based Vocoder with Limited Data\n  Using Augmentation-Conditional Discriminator","summary":"  A generative adversarial network (GAN)-based vocoder trained with an\nadversarial discriminator is commonly used for speech synthesis because of its\nfast, lightweight, and high-quality characteristics. However, this data-driven\nmodel requires a large amount of training data incurring high data-collection\ncosts. This fact motivates us to train a GAN-based vocoder on limited data. A\npromising solution is to augment the training data to avoid overfitting.\nHowever, a standard discriminator is unconditional and insensitive to\ndistributional changes caused by data augmentation. Thus, augmented speech\n(which can be extraordinary) may be considered real speech. To address this\nissue, we propose an augmentation-conditional discriminator (AugCondD) that\nreceives the augmentation state as input in addition to speech, thereby\nassessing the input speech according to the augmentation state, without\ninhibiting the learning of the original non-augmented distribution.\nExperimental results indicate that AugCondD improves speech quality under\nlimited data conditions while achieving comparable speech quality under\nsufficient data conditions. Audio samples are available at\nhttps://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/augcondd/.\n","authors":["Takuhiro Kaneko","Hirokazu Kameoka","Kou Tanaka"],"pdf_url":"https://arxiv.org/pdf/2403.16464v1.pdf","comment":"Accepted to ICASSP 2024. Project page:\n  https://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/augcondd/"},{"id":"http://arxiv.org/abs/2403.16460v1","updated":"2024-03-25T06:43:28Z","published":"2024-03-25T06:43:28Z","title":"FedAC: A Adaptive Clustered Federated Learning Framework for\n  Heterogeneous Data","summary":"  Clustered federated learning (CFL) is proposed to mitigate the performance\ndeterioration stemming from data heterogeneity in federated learning (FL) by\ngrouping similar clients for cluster-wise model training. However, current CFL\nmethods struggle due to inadequate integration of global and intra-cluster\nknowledge and the absence of an efficient online model similarity metric, while\ntreating the cluster count as a fixed hyperparameter limits flexibility and\nrobustness. In this paper, we propose an adaptive CFL framework, named FedAC,\nwhich (1) efficiently integrates global knowledge into intra-cluster learning\nby decoupling neural networks and utilizing distinct aggregation methods for\neach submodule, significantly enhancing performance; (2) includes a\ncosteffective online model similarity metric based on dimensionality reduction;\n(3) incorporates a cluster number fine-tuning module for improved adaptability\nand scalability in complex, heterogeneous environments. Extensive experiments\nshow that FedAC achieves superior empirical performance, increasing the test\naccuracy by around 1.82% and 12.67% on CIFAR-10 and CIFAR-100 datasets,\nrespectively, under different non-IID settings compared to SOTA methods.\n","authors":["Yuxin Zhang","Haoyu Chen","Zheng Lin","Zhe Chen","Jin Zhao"],"pdf_url":"https://arxiv.org/pdf/2403.16460v1.pdf","comment":"14 pages, 4 figures"},{"id":"http://arxiv.org/abs/2403.16459v1","updated":"2024-03-25T06:42:02Z","published":"2024-03-25T06:42:02Z","title":"On the rates of convergence for learning with convolutional neural\n  networks","summary":"  We study the approximation and learning capacities of convolutional neural\nnetworks (CNNs). Our first result proves a new approximation bound for CNNs\nwith certain constraint on the weights. Our second result gives a new analysis\non the covering number of feed-forward neural networks, which include CNNs as\nspecial cases. The analysis carefully takes into account the size of the\nweights and hence gives better bounds than existing literature in some\nsituations. Using these two results, we are able to derive rates of convergence\nfor estimators based on CNNs in many learning problems. In particular, we\nestablish minimax optimal convergence rates of the least squares based on CNNs\nfor learning smooth functions in the nonparametric regression setting. For\nbinary classification, we derive convergence rates for CNN classifiers with\nhinge loss and logistic loss. It is also shown that the obtained rates are\nminimax optimal in several settings.\n","authors":["Yunfei Yang","Han Feng","Ding-Xuan Zhou"],"pdf_url":"https://arxiv.org/pdf/2403.16459v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.16430v5","updated":"2024-03-25T06:32:49Z","published":"2023-12-27T06:34:54Z","title":"Preference as Reward, Maximum Preference Optimization with Importance\n  Sampling","summary":"  Preference learning is a key technology for aligning language models with\nhuman values. Reinforcement Learning from Human Feedback (RLHF) is a\nmodel-based algorithm to optimize preference learning, which first fits a\nreward model for preference scores and then optimizes the generating policy\nwith an on-policy PPO algorithm to maximize the reward. The processing of RLHF\nis complex, time-consuming, and unstable. The Direct Preference Optimization\n(DPO) algorithm uses an off-policy algorithm to directly optimize the\ngenerating policy and eliminates the need for a reward model. DPO is more\ndata-efficient and stable. However, DPO has a drawback of overfitting to the\npreference data and ignoring the KL-regularization term when the preference is\ndeterministic. Identity mapping Preference Optimization(IPO) uses a\nroot-finding MSE loss to incorporate KL-regularization. However, both DPO and\nIPO fail to properly address the KL-regularization term because the support of\nthe preference distribution is not equal to the reference distribution. In this\npaper, we propose a simple and intuitive off-policy preference optimization\nalgorithm from an importance sampling view, which we call Maximum Preference\nOptimization (MPO). MPO incorporates the off-policy KL-regularization term,\nmaking regularization truly effective. MPO achieves the best of both worlds by\ncombining the objectives of RLHF and IPO while being an off-policy algorithm.\nFurthermore, MPO eliminates the need for a reward model and reference policy,\nsimplifying the learning process and reducing memory usage.\n","authors":["Zaifan Jiang","Xing Huang","Chao Wei"],"pdf_url":"https://arxiv.org/pdf/2312.16430v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16451v1","updated":"2024-03-25T06:30:54Z","published":"2024-03-25T06:30:54Z","title":"DeepMachining: Online Prediction of Machining Errors of Lathe Machines","summary":"  We describe DeepMachining, a deep learning-based AI system for online\nprediction of machining errors of lathe machine operations. We have built and\nevaluated DeepMachining based on manufacturing data from factories.\nSpecifically, we first pretrain a deep learning model for a given lathe\nmachine's operations to learn the salient features of machining states. Then,\nwe fine-tune the pretrained model to adapt to specific machining tasks. We\ndemonstrate that DeepMachining achieves high prediction accuracy for multiple\ntasks that involve different workpieces and cutting tools. To the best of our\nknowledge, this work is one of the first factory experiments using pre-trained\ndeep-learning models to predict machining errors of lathe machines.\n","authors":["Xiang-Li Lu","Hwai-Jung Hsu","Che-Wei Chou","H. T. Kung","Chen-Hsin Lee"],"pdf_url":"https://arxiv.org/pdf/2403.16451v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08282v3","updated":"2024-03-25T06:21:37Z","published":"2023-10-12T12:39:08Z","title":"Data driven modeling for self-similar dynamics","summary":"  Multiscale modeling of complex systems is crucial for understanding their\nintricacies. Data-driven multiscale modeling has emerged as a promising\napproach to tackle challenges associated with complex systems. On the other\nhand, self-similarity is prevalent in complex systems, hinting that large-scale\ncomplex systems can be modeled at a reduced cost. In this paper, we introduce a\nmultiscale neural network framework that incorporates self-similarity as prior\nknowledge, facilitating the modeling of self-similar dynamical systems. For\ndeterministic dynamics, our framework can discern whether the dynamics are\nself-similar. For uncertain dynamics, it can compare and determine which\nparameter set is closer to self-similarity. The framework allows us to extract\nscale-invariant kernels from the dynamics for modeling at any scale. Moreover,\nour method can identify the power law exponents in self-similar systems.\nPreliminary tests on the Ising model yielded critical exponents consistent with\ntheoretical expectations, providing valuable insights for addressing critical\nphase transitions in non-equilibrium systems.\n","authors":["Ruyi Tao","Ningning Tao","Yi-zhuang You","Jiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.08282v3.pdf","comment":"10 pages,7 figures,1 table"}],"Computer Science and Game Theory":[{"id":"http://arxiv.org/abs/2403.16373v1","updated":"2024-03-25T02:37:18Z","published":"2024-03-25T02:37:18Z","title":"A new social welfare function with a number of desirable properties","summary":"  By relaxing the dominating set in three ways (e.g., from \"each member beats\nevery non-member\" to \"each member beats or ties every non-member, with an\nadditional requirement that at least one member beat every non-member\"), we\npropose a new social welfare function, which satisfies a number of desirable\nproperties including Condorcet winner principle, Condorcet loser principle,\nstrong Gehrlein-stability (hence Smith set principle), anonymity, neutrality,\nweak Pareto, strong Pareto, non-dictatorship, and [independence of irrelevant\nalternatives (IIA) when the pairwise majority relation is an ordering on the\nalternative set]. If the pairwise majority relation is complete and transitive,\nthe proposed method yields a collective preference relation that coincides with\nthe input majority relation. It thus shares the same collective preference\nfunction on the dichotomous domain with the approval voting and the majority\nvoting. It runs in polynomial time and thus possesses a competitive advantage\nover a number of computationally intractable voting rules such as the Dodgson's\nrule, the Kemeny's rule, the Slater's rule, the Banks rule, and the Schwartz's\ntournament equilibrium set (TEQ) rule. When it is used in tournaments, its\nwinner belongs to the uncovered set, the top cycle set, the Smith set, and the\nSchwartz set. In addition, in a tournament where the number of alternatives is\nnot more than 4, its winner set is a subset, sometimes proper, of the Copeland\nwinner set. Whether this attractive argument is still valid in\nfour-more-alternative tournaments remains an open question.\n","authors":["Fujun Hou"],"pdf_url":"https://arxiv.org/pdf/2403.16373v1.pdf","comment":"A new social choice function (and a corresponding social welfare\n  function) is proposed. It has a number of desirable properties. An open\n  question is also posed"},{"id":"http://arxiv.org/abs/2208.03406v2","updated":"2024-03-25T20:32:00Z","published":"2022-08-05T23:32:33Z","title":"Multilinear formulations for computing Nash equilibrium of multi-player\n  matrix games","summary":"  We present multilinear and mixed-integer multilinear programs to find a Nash\nequilibrium in multi-player noncooperative games. We compare the formulations\nto common algorithms in Gambit, and conclude that a multilinear feasibility\nprogram finds a Nash equilibrium faster than any of the methods we compare it\nto, including the quantal response equilibrium method, which is recommended for\nlarge games. Hence, the multilinear feasibility program is an alternative\nmethod to find a Nash equilibrium in multi-player games, and outperforms many\ncommon algorithms. The mixed-integer formulations are generalisations of known\nmixed-integer programs for two-player games, however unlike two-player games,\nthese mixed-integer programs do not give better performance than existing\nalgorithms.\n","authors":["Miriam Fischer","Akshay Gupte"],"pdf_url":"https://arxiv.org/pdf/2208.03406v2.pdf","comment":"15 page conference paper accepted"},{"id":"http://arxiv.org/abs/2403.17139v1","updated":"2024-03-25T19:23:44Z","published":"2024-03-25T19:23:44Z","title":"An Equilibrium Analysis of the Arad-Rubinstein Game","summary":"  Colonel Blotto games with discrete strategy spaces effectively illus- trate\nthe intricate nature of multidimensional strategic reasoning. This paper\nstudies the equilibrium set of such games where, in line with prior\nexperimental work, the tie-breaking rule is allowed to be flexible. We begin by\npointing out that equilibrium constructions known from the literature extend to\nour class of games. However, we also note that, irrespective of the\ntie-breaking rule, the equilibrium set is excessively large. Specifically, any\npure strategy that allocates at most twice the fair share to each battlefield\nis used with positive probability in some equilibrium. Furthermore, refinements\nbased on the elimination of weakly dominated strategies prove ineffective. To\nderive specific predictions amid this multiplicity, we compute strategies\nresulting from long-run adaptive learning.\n","authors":["Christian Ewerhart","Stanisław Kaźmierowski"],"pdf_url":"https://arxiv.org/pdf/2403.17139v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16980v1","updated":"2024-03-25T17:42:27Z","published":"2024-03-25T17:42:27Z","title":"Economic DAO Governance: A Contestable Control Approach","summary":"  In this article, we propose a new form of DAO governance that uses a\nsequential auction mechanism to overcome entrenched control issues that have\nemerged for DAOs by creating a regime of temporary contestable control. The\nmechanism avoids potential public choice problems inherent in voting approaches\nbut at the same time provides a vehicle that can enhance and secure value than\ninheres to DAO voting and other DAO non-market governance procedures. It is\nrobust to empty voting and is code feasible. It facilitates the ability of DAOs\nto meet their normative and operational goals in the face of diverse regulatory\napproaches. Designed to shift control to the party with the most promising\nbusiness plan, at the same time it distributes surplus in a way that tends to\npromote investment by other parties.\n","authors":["Jeff Strnad"],"pdf_url":"https://arxiv.org/pdf/2403.16980v1.pdf","comment":"84 pages, 2 figures"},{"id":"http://arxiv.org/abs/2309.02613v2","updated":"2024-03-25T16:32:26Z","published":"2023-09-05T23:13:02Z","title":"Project-Fair and Truthful Mechanisms for Budget Aggregation","summary":"  We study the budget aggregation problem in which a set of strategic voters\nmust split a finite divisible resource (such as money or time) among a set of\ncompeting projects. Our goal is twofold: We seek truthful mechanisms that\nprovide fairness guarantees to the projects. For the first objective, we focus\non the class of moving phantom mechanisms [Freeman et al., 2021], which are --\nto this day -- essentially the only known truthful mechanisms in this setting.\nFor project fairness, we consider the mean division as a fair baseline, and\nbound the maximum difference between the funding received by any project and\nthis baseline. We propose a novel and simple moving phantom mechanism that\nprovides optimal project fairness guarantees. As a corollary of our results, we\nshow that our new mechanism minimizes the $\\ell_1$ distance to the mean (a\nmeasure suggested by Caragiannis et al. [2022]) for three projects and gives\nthe first non-trivial bounds on this quantity for more than three projects.\n","authors":["Rupert Freeman","Ulrike Schmidt-Kraepelin"],"pdf_url":"https://arxiv.org/pdf/2309.02613v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16843v1","updated":"2024-03-25T15:04:11Z","published":"2024-03-25T15:04:11Z","title":"Do LLM Agents Have Regret? A Case Study in Online Learning and Games","summary":"  Large language models (LLMs) have been increasingly employed for\n(interactive) decision-making, via the development of LLM-based autonomous\nagents. Despite their emerging successes, the performance of LLM agents in\ndecision-making has not been fully investigated through quantitative metrics,\nespecially in the multi-agent setting when they interact with each other, a\ntypical scenario in real-world LLM-agent applications. To better understand the\nlimits of LLM agents in these interactive environments, we propose to study\ntheir interactions in benchmark decision-making settings in online learning and\ngame theory, through the performance metric of \\emph{regret}. We first\nempirically study the {no-regret} behaviors of LLMs in canonical\n(non-stationary) online learning problems, as well as the emergence of\nequilibria when LLM agents interact through playing repeated games. We then\nprovide some theoretical insights into the no-regret behaviors of LLM agents,\nunder certain assumptions on the supervised pre-training and the rationality\nmodel of human decision-makers who generate the data. Notably, we also identify\n(simple) cases where advanced LLMs such as GPT-4 fail to be no-regret. To\npromote the no-regret behaviors, we propose a novel \\emph{unsupervised}\ntraining loss of \\emph{regret-loss}, which, in contrast to the supervised\npre-training loss, does not require the labels of (optimal) actions. We then\nestablish the statistical guarantee of generalization bound for regret-loss\nminimization, followed by the optimization guarantee that minimizing such a\nloss may automatically lead to known no-regret learning algorithms. Our further\nexperiments demonstrate the effectiveness of our regret-loss, especially in\naddressing the above ``regrettable'' cases.\n","authors":["Chanwoo Park","Xiangyu Liu","Asuman Ozdaglar","Kaiqing Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.16843v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16799v1","updated":"2024-03-25T14:19:48Z","published":"2024-03-25T14:19:48Z","title":"Efficient Method for Finding Optimal Strategies in Chopstick Auctions\n  with Uniform Objects Values","summary":"  We propose an algorithm for computing Nash equilibria (NE) in a class of\nconflicts with multiple battlefields with uniform battlefield values and a\nnon-linear aggregation function. By expanding the symmetrization idea of Hart\n[9], proposed for the Colonel Blotto game, to the wider class of symmetric\nconflicts with multiple battlefields, we reduce the number of strategies of the\nplayers by an exponential factor. We propose a clash matrix algorithm which\nallows for computing the payoffs in the symmetrized model in polynomial time.\nCombining symmetrization and clash matrix algorithm with the double oracle\nalgorithm we obtain an algorithm for computing NE in the models in question\nthat achieves a significant speed-up as compared to the standard, LP-based,\napproach. We also introduce a heuristic to further speed up the process.\nOverall, our approach offers an efficient and novel method for computing NE in\na specific class of conflicts, with potential practical applications in various\nfields.\n","authors":["Stanisław Kaźmierowski","Marcin Dziubiński"],"pdf_url":"https://arxiv.org/pdf/2403.16799v1.pdf","comment":"Accepted for AAMAS-24 conference"}],"Information Theory":[{"id":"http://arxiv.org/abs/2403.16699v1","updated":"2024-03-25T12:33:10Z","published":"2024-03-25T12:33:10Z","title":"Resonant Beam Communications: A New Design Paradigm and Challenges","summary":"  Resonant beam communications (RBCom), which adopt oscillating photons between\ntwo separate retroreflectors for information transmission, exhibit potential\nadvantages over other types of wireless optical communications (WOC). However,\necho interference generated by the modulated beam reflected from the receiver\naffects the transmission of the desired information. To tackle this challenge,\na synchronization-based point-to-point RBCom system is proposed to eliminate\nthe echo interference, and the design for the transmitter and receiver is\ndiscussed. Subsequently, the performance of the proposed RBCom is evaluated and\ncompared with that of visible light communications (VLC) and free space optical\ncommunications (FOC). Finally, future research directions are outlined and\nseveral implementation challenges of RBCom systems are highlighted.\n","authors":["Yuanming Tian","Dongxu Li","Chuan Huang","Qingwen Liu","Shengli Zhou"],"pdf_url":"https://arxiv.org/pdf/2403.16699v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16694v1","updated":"2024-03-25T12:26:18Z","published":"2024-03-25T12:26:18Z","title":"Design and Performance of Resonant Beam Communications -- Part II:\n  Mobile Scenario","summary":"  This two-part paper focuses on the system design and performance analysis for\na point-to-point resonant beam communication (RBCom) system under both the\nquasi-static and mobile scenarios. Part I of this paper proposes a\nsynchronization-based information transmission scheme and derives the capacity\nupper and lower bounds for the quasi-static channel case. In Part II, we\naddress the mobile scenario, where the receiver is in relative motion to the\ntransmitter, and derive a mobile RBCom channel model that jointly considers the\nDoppler effect, channel variation, and echo interference. With the obtained\nchannel model, we prove that the channel gain of the mobile RBCom decreases as\nthe number of transmitted frames increases, and thus show that the considered\nmobile RBCom terminates after the transmitter sends a certain number of frames\nwithout frequency compensation. By deriving an upper bound on the number of\nsuccessfully transmitted frames, we formulate the throughput maximization\nproblem for the considered mobile RBCom system, and solve it via a sequential\nparametric convex approximation (SPCA) method. Finally, simulation results\nvalidate the analysis of our proposed method in some typical scenarios.\n","authors":["Dongxu Li","Yuanming Tian","Chuan Huang","Qingwen Liu","Shengli Zhou"],"pdf_url":"https://arxiv.org/pdf/2403.16694v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16676v1","updated":"2024-03-25T12:14:34Z","published":"2024-03-25T12:14:34Z","title":"Design and Performance of Resonant Beam Communications -- Part I:\n  Quasi-Static Scenario","summary":"  This two-part paper studies a point-to-point resonant beam communication\n(RBCom) system, where two separately deployed retroreflectors are adopted to\ngenerate the resonant beam between the transmitter and the receiver, and\nanalyzes the transmission rate of the considered system under both the\nquasi-static and mobile scenarios. Part I of this paper focuses on the\nquasi-static scenario where the locations of the transmitter and the receiver\nare relatively fixed. Specifically, we propose a new information-bearing scheme\nwhich adopts a synchronization-based amplitude modulation method to mitigate\nthe echo interference caused by the reflected resonant beam. With this scheme,\nwe show that the quasi-static RBCom channel is equivalent to a Markov channel\nand can be further simplified as an amplitude-constrained additive white\nGaussian noise channel. Moreover, we develop an algorithm that jointly employs\nthe bisection and exhaustive search to maximize its capacity upper and lower\nbounds. Finally, numerical results validate our analysis. Part II of this paper\ndiscusses the performance of the RBCom system under the mobile scenario.\n","authors":["Dongxu Li","Yuanming Tian","Chuan Huang","Qingwen Liu","Shengli Zhou"],"pdf_url":"https://arxiv.org/pdf/2403.16676v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16575v1","updated":"2024-03-25T09:40:54Z","published":"2024-03-25T09:40:54Z","title":"A Measure of Synergy based on Union Information","summary":"  The partial information decomposition (PID) framework is concerned with\ndecomposing the information that a set of (two or more) random variables (the\nsources) has about another variable (the target) into three types of\ninformation: unique, redundant, and synergistic. Classical information theory\nalone does not provide a unique way to decompose information in this manner and\nadditional assumptions have to be made. One often overlooked way to achieve\nthis decomposition is using a so-called measure of union information - which\nquantifies the information that is present in at least one of the sources -\nfrom which a synergy measure stems. In this paper, we introduce a new measure\nof union information based on adopting a communication channel perspective,\ncompare it with existing measures, and study some of its properties. We also\ninclude a comprehensive critical review of characterizations of union\ninformation and synergy measures that have been proposed in the literature.\n","authors":["André F. C. Gomes","Mário A. T. Figueiredo"],"pdf_url":"https://arxiv.org/pdf/2403.16575v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05794v3","updated":"2024-03-25T09:40:15Z","published":"2023-10-09T15:29:32Z","title":"Computation-Limited Signals: A Channel Capacity Regime Constrained by\n  Computational Complexity","summary":"  In this letter, we introduce the computational-limited (comp-limited)\nsignals, a communication capacity regime in which the signal time computational\ncomplexity overhead is the key constraint -- rather than power or bandwidth --\nto the overall communication capacity. We present the Spectro-Computational\n(SC) analysis, a novel mathematical framework that enhances classic concepts of\ninformation theory -- such as throughput, spectral efficiency and capacity --\nto account for the signal processing computational complexity overhead. We\nconsider a specific Shannon regime under which capacity is expected to get\narbitrarily large as channel resources grow. Under that regime, we identify the\nconditions under which the time complexity overhead causes capacity to decrease\nrather than increasing, thereby creating the case for the comp-limited regime.\nWe also provide examples of the SC analysis and show the OFDM waveform is\ncomp-limited unless the lower-bound computational complexity of the $N$-point\nDFT problem verifies as $\\Omega(N)$, which remains an open challenge.\n","authors":["Saulo Queiroz","João P. Vilela","Edmundo Monteiro"],"pdf_url":"https://arxiv.org/pdf/2310.05794v3.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2305.04615v2","updated":"2024-03-25T08:20:56Z","published":"2023-05-08T10:47:32Z","title":"Performance Analysis of In-Band-Full-Duplex Multi-Cell Wideband IAB\n  Networks","summary":"  This paper analyzes the performance of the 3rd Generation Partnership Project\n(3GPP)-inspired multi-cell wideband single-hop backhaul\nmillimeter-wave-in-band-full-duplex (IBFD)-integrated access and backhaul (IAB)\nnetworks by using stochastic geometry. We model the wired-connected Next\nGeneration NodeBs (gNBs) as the Mat\\'ern hard-core point process (MHCPP) to\nmeet the real-world deployment requirement and reduce the cost caused by wired\nconnection in the network. We first derive association probabilities that\nreflect how likely the typical user-equipment is served by a gNB or an IAB-node\nbased on the maximum long-term averaged biased-received-desired-signal power\ncriteria. Further, by leveraging the composite Gamma-Lognormal distribution, we\nderive the closed-form signal to interference plus noise ratio coverage,\ncapacity with outage, and ergodic capacity of the network. In order to avoid\nunderestimating the noise, we consider the sidelobe gain on inter-cell\ninterference links and the analog to digital converter quantization noise.\nCompared with the half-duplex transmission, numerical results show an enhanced\ncapacity with outage and ergodic capacity provided by IBFD under successful\nself-interference cancellation. We also study how the power bias and density\nratio of the IAB-node to gNB, and the hard-core distance can affect system\nperformances.\n","authors":["Junkai Zhang","Tharmalingam Ratnarajah"],"pdf_url":"https://arxiv.org/pdf/2305.04615v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.00611v2","updated":"2024-03-25T08:11:49Z","published":"2024-03-01T15:35:30Z","title":"Probabilistic positioning via ray tracing with noisy angle of arrival\n  measurements","summary":"  This paper investigates the problems of interference prediction and sensing\nfor efficient spectrum access and link adaptation. The considered approach for\ninterference prediction relies on a parametric model. However, we assume that\nthe number of observations available to learn theses parameters is limited.\nThis implies that they should be treated as random variables rather than fixed\nvalues. We show how this can impact the spectrum access and link adaptation\nstrategies. We also introduce the notion of \"interferer-coherence time\" to\nestablish the number of independent interferer state realizations experienced\nby a codeword. We explain how it can be computed taking into account the model\nuncertainty and how this impacts the link adaptation.\n","authors":["Vincent Corlay","Viet-Hoa Nguyen","Nicolas Gresset"],"pdf_url":"https://arxiv.org/pdf/2403.00611v2.pdf","comment":"Submitted to IEEE Communications Letters"},{"id":"http://arxiv.org/abs/2403.16498v1","updated":"2024-03-25T07:29:39Z","published":"2024-03-25T07:29:39Z","title":"BackCom Assisted Hybrid NOMA Uplink Transmission for Ambient IoT","summary":"  Hybrid non-orthogonal multiple access (H-NOMA) has recently received\nsignificant attention as a general framework of multiple access, where both\nconventional orthogonal multiple access (OMA) and pure NOMA are its special\ncases. This paper focuses on the application of H-NOMA to ambient Internet of\nThings (IoT) with energy-constrained devices, where a new backscatter\ncommunication (BackCom) assisted H-NOMA uplink scheme is developed. Resource\nallocation for H-NOMA uplink transmission is also considered, where an overall\npower minimization problem is formulated. Insightful understandings for the key\nfeatures of BackCom assisted H-NOMA and its difference from conventional H-NOMA\nare illustrated by developing analytical results for the two-user special case.\nFor the general multi-user scenario, two algorithms, one based on the\nbranch-bound (BB) principle and the other based on successive convex\napproximation (SCA), are developed to realize different tradeoffs between the\nsystem performance and complexity. The numerical results are also provided to\nverify the accuracy of the developed analytical results and demonstrate the\nperformance gain of H-NOMA over OMA.\n","authors":["Zhiguo Ding"],"pdf_url":"https://arxiv.org/pdf/2403.16498v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16477v1","updated":"2024-03-25T07:04:16Z","published":"2024-03-25T07:04:16Z","title":"Safeguarding Next Generation Multiple Access Using Physical Layer\n  Security Techniques: A Tutorial","summary":"  Driven by the ever-increasing requirements of ultra-high spectral efficiency,\nultra-low latency, and massive connectivity, the forefront of wireless research\ncalls for the design of advanced next generation multiple access schemes to\nfacilitate provisioning of these stringent demands. This inspires the embrace\nof non-orthogonal multiple access (NOMA) in future wireless communication\nnetworks. Nevertheless, the support of massive access via NOMA leads to\nadditional security threats, due to the open nature of the air interface, the\nbroadcast characteristic of radio propagation as well as intertwined\nrelationship among paired NOMA users. To address this specific challenge, the\nsuperimposed transmission of NOMA can be explored as new opportunities for\nsecurity aware design, for example, multiuser interference inherent in NOMA can\nbe constructively engineered to benefit communication secrecy and privacy. The\npurpose of this tutorial is to provide a comprehensive overview on the\nstate-of-the-art physical layer security techniques that guarantee wireless\nsecurity and privacy for NOMA networks, along with the opportunities, technical\nchallenges, and future research trends.\n","authors":["Lu Lv","Dongyang Xu","Rose Qingyang Hu","Yinghui Ye","Long Yang","Xianfu Lei","Xianbin Wang","Dong In Kim","Arumugam Nallanathan"],"pdf_url":"https://arxiv.org/pdf/2403.16477v1.pdf","comment":"Invited paper by Proceedings of the IEEE"},{"id":"http://arxiv.org/abs/2403.16472v1","updated":"2024-03-25T06:56:27Z","published":"2024-03-25T06:56:27Z","title":"Power-Aware Sparse Reflect Beamforming in Active RIS-aided Interference\n  Channels","summary":"  Active reconfigurable intelligent surface (RIS) has attracted significant\nattention in wireless communications, due to its reflecting elements (REs)\ncapable of reflecting incident signals with not only phase shifts but also\namplitude amplifications. In this paper, we are interested in active RIS-aided\ninterference channels in which $K$ user pairs share the same time and frequency\nresources with the aid of active RIS. Thanks to the promising amplitude\namplification capability, activating a moderate number of REs, rather than all\nof them, is sufficient for the active RIS to mitigate cross-channel\ninterferences. Motivated by this, we propose a power-aware sparse reflect\nbeamforming design for the active RIS-aided interference channels, which allows\nthe active RIS to flexibly adjust the number of activated REs for the sake of\nreducing hardware and power costs. Specifically, we establish the power\nconsumption model in which only those activated REs consume the biasing and\noperation power that supports the amplitude amplification, yielding an\n$\\ell_0$-norm power consumption function. Based on the proposed model, we\ninvestigate a sum-rate maximization problem and an active RIS power\nminimization problem by carefully designing the sparse reflect beamforming\nvector. To solve these problems, we first replace the nonconvex $\\ell_0$-norm\nfunction with an iterative reweighted $\\ell_1$-norm function. Then, fractional\nprogramming is used to solve the sum-rate maximization, while semidefinite\nprogramming together with the difference-of-convex algorithm (DCA) is used to\nsolve the active RIS power minimization. Numerical results show that the\nproposed sparse designs can notably increase the sum rate of user pairs and\ndecrease the power consumption of active RIS in interference channels.\n","authors":["Ruizhe Long","Hu Zhou","Ying-Chang Liang"],"pdf_url":"https://arxiv.org/pdf/2403.16472v1.pdf","comment":null}]},"2024-03-24T00:00:00Z":{"Distributed, Parallel, and Cluster Computing":[{"id":"http://arxiv.org/abs/2403.16301v1","updated":"2024-03-24T21:22:52Z","published":"2024-03-24T21:22:52Z","title":"Q-adaptive: A Multi-Agent Reinforcement Learning Based Routing on\n  Dragonfly Network","summary":"  on adaptive routing to balance network traffic for optimum performance.\nIdeally, adaptive routing attempts to forward packets between minimal and\nnon-minimal paths with the least congestion. In practice, current adaptive\nrouting algorithms estimate routing path congestion based on local information\nsuch as output queue occupancy. Using local information to estimate global path\ncongestion is inevitably inaccurate because a router has no precise knowledge\nof link states a few hops away. This inaccuracy could lead to interconnect\ncongestion. In this study, we present Q-adaptive routing, a multi-agent\nreinforcement learning routing scheme for Dragonfly systems. Q-adaptive routing\nenables routers to learn to route autonomously by leveraging advanced\nreinforcement learning technology. The proposed Q-adaptive routing is highly\nscalable thanks to its fully distributed nature without using any shared\ninformation between routers. Furthermore, a new two-level Q-table is designed\nfor Q-adaptive to make it computational lightly and saves 50% of router memory\nusage compared with the previous Q-routing. We implement the proposed\nQ-adaptive routing in SST/Merlin simulator. Our evaluation results show that\nQ-adaptive routing achieves up to 10.5% system throughput improvement and 5.2x\naverage packet latency reduction compared with adaptive routing algorithms.\nRemarkably, Q-adaptive can even outperform the optimal VALn non-minimal routing\nunder the ADV+1 adversarial traffic pattern with up to 3% system throughput\nimprovement and 75% average packet latency reduction.\n","authors":["Yao Kang","Xin Wang","Zhiling Lan"],"pdf_url":"https://arxiv.org/pdf/2403.16301v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16298v1","updated":"2024-03-24T21:05:36Z","published":"2024-03-24T21:05:36Z","title":"MRSch: Multi-Resource Scheduling for HPC","summary":"  Emerging workloads in high-performance computing (HPC) are embracing\nsignificant changes, such as having diverse resource requirements instead of\nbeing CPU-centric. This advancement forces cluster schedulers to consider\nmultiple schedulable resources during decision-making. Existing scheduling\nstudies rely on heuristic or optimization methods, which are limited by an\ninability to adapt to new scenarios for ensuring long-term scheduling\nperformance. We present an intelligent scheduling agent named MRSch for\nmulti-resource scheduling in HPC that leverages direct future prediction (DFP),\nan advanced multi-objective reinforcement learning algorithm. While DFP\ndemonstrated outstanding performance in a gaming competition, it has not been\npreviously explored in the context of HPC scheduling. Several key techniques\nare developed in this study to tackle the challenges involved in multi-resource\nscheduling. These techniques enable MRSch to learn an appropriate scheduling\npolicy automatically and dynamically adapt its policy in response to workload\nchanges via dynamic resource prioritizing. We compare MRSch with existing\nscheduling methods through extensive tracebase simulations. Our results\ndemonstrate that MRSch improves scheduling performance by up to 48% compared to\nthe existing scheduling methods.\n","authors":["Boyang Li","Yuping Fan","Matthew Dearing","Zhiling Lan","Paul Richy","William Allcocky","Michael Papka"],"pdf_url":"https://arxiv.org/pdf/2403.16298v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13247v2","updated":"2024-03-24T20:58:50Z","published":"2024-03-20T02:17:47Z","title":"FedNMUT -- Federated Noisy Model Update Tracking Convergence Analysis","summary":"  A novel Decentralized Noisy Model Update Tracking Federated Learning\nalgorithm (FedNMUT) is proposed that is tailored to function efficiently in the\npresence of noisy communication channels that reflect imperfect information\nexchange. This algorithm uses gradient tracking to minimize the impact of data\nheterogeneity while minimizing communication overhead. The proposed algorithm\nincorporates noise into its parameters to mimic the conditions of noisy\ncommunication channels, thereby enabling consensus among clients through a\ncommunication graph topology in such challenging environments. FedNMUT\nprioritizes parameter sharing and noise incorporation to increase the\nresilience of decentralized learning systems against noisy communications.\nTheoretical results for the smooth non-convex objective function are provided\nby us, and it is shown that the $\\epsilon-$stationary solution is achieved by\nour algorithm at the rate of $\\mathcal{O}\\left(\\frac{1}{\\sqrt{T}}\\right)$,\nwhere $T$ is the total number of communication rounds. Additionally, via\nempirical validation, we demonstrated that the performance of FedNMUT is\nsuperior to the existing state-of-the-art methods and conventional\nparameter-mixing approaches in dealing with imperfect information sharing. This\nproves the capability of the proposed algorithm to counteract the negative\neffects of communication noise in a decentralized learning framework.\n","authors":["Vishnu Pandi Chellapandi","Antesh Upadhyay","Abolfazl Hashemi","Stanislaw H. Żak"],"pdf_url":"https://arxiv.org/pdf/2403.13247v2.pdf","comment":"arXiv admin note: text overlap with arXiv:2303.10695"},{"id":"http://arxiv.org/abs/2403.16293v1","updated":"2024-03-24T20:56:16Z","published":"2024-03-24T20:56:16Z","title":"Interpretable Modeling of Deep Reinforcement Learning Driven Scheduling","summary":"  In the field of high-performance computing (HPC), there has been recent\nexploration into the use of deep reinforcement learning for cluster scheduling\n(DRL scheduling), which has demonstrated promising outcomes. However, a\nsignificant challenge arises from the lack of interpretability in deep neural\nnetworks (DNN), rendering them as black-box models to system managers. This\nlack of model interpretability hinders the practical deployment of DRL\nscheduling. In this work, we present a framework called IRL (Interpretable\nReinforcement Learning) to address the issue of interpretability of DRL\nscheduling. The core idea is to interpret DNN (i.e., the DRL policy) as a\ndecision tree by utilizing imitation learning. Unlike DNN, decision tree models\nare non-parametric and easily comprehensible to humans. To extract an effective\nand efficient decision tree, IRL incorporates the Dataset Aggregation (DAgger)\nalgorithm and introduces the notion of critical state to prune the derived\ndecision tree. Through trace-based experiments, we demonstrate that IRL is\ncapable of converting a black-box DNN policy into an interpretable rulebased\ndecision tree while maintaining comparable scheduling performance.\nAdditionally, IRL can contribute to the setting of rewards in DRL scheduling.\n","authors":["Boyang Li","Zhiling Lan","Michael E. Papka"],"pdf_url":"https://arxiv.org/pdf/2403.16293v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16288v1","updated":"2024-03-24T20:37:33Z","published":"2024-03-24T20:37:33Z","title":"Study of Workload Interference with Intelligent Routing on Dragonfly","summary":"  Dragonfly interconnect is a crucial network technology for supercomputers. To\nsupport exascale systems, network resources are shared such that links and\nrouters are not dedicated to any node pair. While link utilization is\nincreased, workload performance is often offset by network contention.\nRecently, intelligent routing built on reinforcement learning demonstrates\nhigher network throughput with lower packet latency. However, its effectiveness\nin reducing workload interference is unknown. In this work, we present\nextensive network simulations to study multi-workload contention under\ndifferent routing mechanisms, intelligent routing and adaptive routing, on a\nlarge-scale Dragonfly system. We develop an enhanced network simulation\ntoolkit, along with a suite of workloads with distinctive communication\npatterns. We also present two metrics to characterize application communication\nintensity. Our analysis focuses on examining how different workloads interfere\nwith each other under different routing mechanisms by inspecting both\napplication-level and network-level metrics. Several key insights are made from\nthe analysis.\n","authors":["Yao Kang","Xin Wang","Zhiling Lan"],"pdf_url":"https://arxiv.org/pdf/2403.16288v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16125v1","updated":"2024-03-24T12:43:04Z","published":"2024-03-24T12:43:04Z","title":"A Codesign of Scheduling and Parallelization for Large Model Training in\n  Heterogeneous Clusters","summary":"  Joint consideration of scheduling and adaptive parallelism offers great\nopportunities for improving the training efficiency of large models on\nheterogeneous GPU clusters. However, integrating adaptive parallelism into a\ncluster scheduler expands the cluster scheduling space. The new space is the\nproduct of the original scheduling space and the parallelism exploration space\nof adaptive parallelism (also a product of pipeline, data, and tensor\nparallelism). The exponentially enlarged scheduling space and ever-changing\noptimal parallelism plan from adaptive parallelism together result in the\ncontradiction between low-overhead and accurate performance data acquisition\nfor efficient cluster scheduling. This paper presents Crius, a training system\nfor efficiently scheduling multiple large models with adaptive parallelism in a\nheterogeneous cluster. Crius proposes a novel scheduling granularity called\nCell. It represents a job with deterministic resources and pipeline stages. The\nexploration space of Cell is shrunk to the product of only data and tensor\nparallelism, thus exposing the potential for accurate and low-overhead\nperformance estimation. Crius then accurately estimates Cells and efficiently\nschedules training jobs. When a Cell is selected as a scheduling choice, its\nrepresented job runs with the optimal parallelism plan explored. Experimental\nresults show that Crius reduces job completion time by up to 48.9% and\nschedules large models with up to 1.49x cluster throughput improvement.\n","authors":["Chunyu Xue","Weihao Cui","Han Zhao","Quan Chen","Shulai Zhang","Pengyu Yang","Jing Yang","Shaobo Li","Minyi Guo"],"pdf_url":"https://arxiv.org/pdf/2403.16125v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.07537v3","updated":"2024-03-24T09:42:39Z","published":"2023-04-15T11:48:18Z","title":"Gradient-less Federated Gradient Boosting Trees with Learnable Learning\n  Rates","summary":"  The privacy-sensitive nature of decentralized datasets and the robustness of\neXtreme Gradient Boosting (XGBoost) on tabular data raise the needs to train\nXGBoost in the context of federated learning (FL). Existing works on federated\nXGBoost in the horizontal setting rely on the sharing of gradients, which\ninduce per-node level communication frequency and serious privacy concerns. To\nalleviate these problems, we develop an innovative framework for horizontal\nfederated XGBoost which does not depend on the sharing of gradients and\nsimultaneously boosts privacy and communication efficiency by making the\nlearning rates of the aggregated tree ensembles learnable. We conduct extensive\nevaluations on various classification and regression datasets, showing our\napproach achieves performance comparable to the state-of-the-art method and\neffectively improves communication efficiency by lowering both communication\nrounds and communication overhead by factors ranging from 25x to 700x. Project\nPage: https://flower.ai/blog/2023-04-19-xgboost-with-flower/\n","authors":["Chenyang Ma","Xinchi Qiu","Daniel J. Beutel","Nicholas D. Lane"],"pdf_url":"https://arxiv.org/pdf/2304.07537v3.pdf","comment":"Accepted at the 3rd ACM Workshop on Machine Learning and Systems\n  (EuroMLSys), May 8th 2023, Rome, Italy"},{"id":"http://arxiv.org/abs/2312.13938v2","updated":"2024-03-24T00:25:26Z","published":"2023-12-21T15:32:20Z","title":"How Does Stake Distribution Influence Consensus? Analyzing Blockchain\n  Decentralization","summary":"  In the PoS blockchain landscape, the challenge of achieving full\ndecentralization is often hindered by a disproportionate concentration of\nstaked tokens among a few validators. This study analyses this challenge by\nfirst formalizing decentralization metrics for weighted consensus mechanisms.\nAn empirical analysis across ten permissionless blockchains uncovers\nsignificant weight concentration among validators, underscoring the need for an\nequitable approach. To counter this, we introduce the Square Root Stake Weight\n(SRSW) model, which effectively recalibrates staking weight distribution. Our\nexamination of the SRSW model demonstrates notable improvements in the\ndecentralization metrics: the Gini index improves by 37.16% on average, while\nNakamoto coefficients for liveness and safety see mean enhancements of 101.04%\nand 80.09%, respectively. This research is a pivotal step toward a more fair\nand equitable distribution of staking weight, advancing the decentralization in\nblockchain consensus mechanisms.\n","authors":["Shashank Motepalli","Hans-Arno Jacobsen"],"pdf_url":"https://arxiv.org/pdf/2312.13938v2.pdf","comment":"To appear in ICBC 2024"}],"Performance":[{"id":"http://arxiv.org/abs/2403.16063v1","updated":"2024-03-24T08:25:42Z","published":"2024-03-24T08:25:42Z","title":"Explainable Port Mapping Inference with Sparse Performance Counters for\n  AMD's Zen Architectures","summary":"  Performance models are instrumental for optimizing performance-sensitive\ncode. When modeling the use of functional units of out-of-order x86-64 CPUs,\ndata availability varies by the manufacturer: Instruction-to-port mappings for\nIntel's processors are available, whereas information for AMD's designs are\nlacking. The reason for this disparity is that standard techniques to infer\nexact port mappings require hardware performance counters that AMD does not\nprovide.\n  In this work, we modify the port mapping inference algorithm of the widely\nused uops.info project to not rely on Intel's performance counters. The\nmodifications are based on a formal port mapping model with a\ncounter-example-guided algorithm powered by an SMT solver. We investigate in\nhow far AMD's processors comply with this model and where unexpected\nperformance characteristics prevent an accurate port mapping. Our results\nprovide valuable insights for creators of CPU performance models as well as for\nsoftware developers who want to achieve peak performance on recent AMD CPUs.\n","authors":["Fabian Ritter","Sebastian Hack"],"pdf_url":"https://arxiv.org/pdf/2403.16063v1.pdf","comment":"Accepted at ASPLOS 2024"},{"id":"http://arxiv.org/abs/2403.16013v1","updated":"2024-03-24T04:58:49Z","published":"2024-03-24T04:58:49Z","title":"Performance evaluation of accelerated complex multiple-precision LU\n  decomposition","summary":"  The direct method is one of the most important algorithms for solving linear\nsystems of equations, with LU decomposition comprising a significant portion of\nits computation time. This study explores strategies to accelerate complex LU\ndecomposition using multiple-precision floating-point arithmetic of the\nmultiple-component type. Specifically, we explore the potential efficiency\ngains using a combination of SIMDization and the 3M method for complex matrix\nmultiplication. Our benchmark tests compare this approach with the direct\nmethod implementation in MPLAPACK, focusing on computation time and numerical\nerrors.\n","authors":["Tomonori Kouya"],"pdf_url":"https://arxiv.org/pdf/2403.16013v1.pdf","comment":null}],"Databases":[{"id":"http://arxiv.org/abs/2312.09331v2","updated":"2024-03-24T23:33:00Z","published":"2023-12-14T20:40:57Z","title":"Insert-Only versus Insert-Delete in Dynamic Query Evaluation","summary":"  We study the dynamic query evaluation problem: Given a join query Q and a\nstream of updates, we would like to construct a data structure that supports\nconstant-delay enumeration of the query output after each update.\n  We show that a stream of N insert-only updates (to an initially empty\ndatabase) can be executed in total time O(N^{w(Q)}), where w(Q) is the\nfractional hypertree width of Q. This matches the complexity of the static\nquery evaluation problem for Q and a database of size N. One corollary is that\nthe average time per single-tuple insert is constant for acyclic joins.\n  In contrast, we show that a stream of N insert-and-delete updates to Q can be\nexecuted in total time O(N^{w(Q')}), where Q' is obtained from Q by extending\nevery relational atom with extra variables that represent the \"lifespans\" of\ntuples in Q. We show that this reduction is optimal in the sense that the\nstatic evaluation runtime of Q' provides a lower bound on the total update time\nof Q. Our approach recovers the optimal single-tuple update time for known\nqueries such as the hierarchical and Loomis-Whitney join queries.\n","authors":["Mahmoud Abo Khamis","Ahmet Kara","Dan Olteanu","Dan Suciu"],"pdf_url":"https://arxiv.org/pdf/2312.09331v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16312v1","updated":"2024-03-24T22:35:05Z","published":"2024-03-24T22:35:05Z","title":"On Reporting Durable Patterns in Temporal Proximity Graphs","summary":"  Finding patterns in graphs is a fundamental problem in databases and data\nmining. In many applications, graphs are temporal and evolve over time, so we\nare interested in finding durable patterns, such as triangles and paths, which\npersist over a long time. While there has been work on finding durable simple\npatterns, existing algorithms do not have provable guarantees and run in\nstrictly super-linear time. The paper leverages the observation that many\ngraphs arising in practice are naturally proximity graphs or can be\napproximated as such, where nodes are embedded as points in some\nhigh-dimensional space, and two nodes are connected by an edge if they are\nclose to each other. We work with an implicit representation of the proximity\ngraph, where nodes are additionally annotated by time intervals, and design\nnear-linear-time algorithms for finding (approximately) durable patterns above\na given durability threshold. We also consider an interactive setting where a\nclient experiments with different durability thresholds in a sequence of\nqueries; we show how to compute incremental changes to result patterns\nefficiently in time near-linear to the size of the changes.\n","authors":["Pankaj K. Agarwal","Xiao Hu","Stavros Sintos","Jun Yang"],"pdf_url":"https://arxiv.org/pdf/2403.16312v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.00082v4","updated":"2024-03-24T22:32:53Z","published":"2024-01-31T04:55:06Z","title":"Enhancing Grover's Search Algorithm: A Modified Approach to Increase the\n  Probability of Good States","summary":"  This article introduces an enhancement to the Grover search algorithm to\nspeed up computing the probability of finding good states. It suggests\nincorporating a rotation phase angle determined mathematically from the\nderivative of the model during the initial iteration. At each iteration, a new\nphase angle is computed and used in a rotation gate around y+z axis in the\ndiffusion operator. The computed phase angles are optimized through an adaptive\nadjustment based on the estimated increasing ratio of the consecutive\namplitudes. The findings indicate an average decrease of 28% in the required\nnumber of iterations resulting in a faster overall process and fewer number of\nquantum gates. For large search space, this improvement rises to 29.58%. Given\nthe computational capabilities of the computer utilized for the simulation, the\napproach is applied to instances with up to 12 qubits or 4096 possible\ncombination of search entries.\n","authors":["Ismael Abdulrahman"],"pdf_url":"https://arxiv.org/pdf/2402.00082v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16204v1","updated":"2024-03-24T15:57:24Z","published":"2024-03-24T15:57:24Z","title":"SQL-Encoder: Improving NL2SQL In-Context Learning Through a\n  Context-Aware Encoder","summary":"  Detecting structural similarity between queries is essential for selecting\nexamples in in-context learning models. However, assessing structural\nsimilarity based solely on the natural language expressions of queries, without\nconsidering SQL queries, presents a significant challenge. This paper explores\nthe significance of this similarity metric and proposes a model for accurately\nestimating it. To achieve this, we leverage a dataset comprising 170k question\npairs, meticulously curated to train a similarity prediction model. Our\ncomprehensive evaluation demonstrates that the proposed model adeptly captures\nthe structural similarity between questions, as evidenced by improvements in\nKendall-Tau distance and precision@k metrics. Notably, our model outperforms\nstrong competitive embedding models from OpenAI and Cohere. Furthermore,\ncompared to these competitive models, our proposed encoder enhances the\ndownstream performance of NL2SQL models in 1-shot in-context learning scenarios\nby 1-2\\% for GPT-3.5-turbo, 4-8\\% for CodeLlama-7B, and 2-3\\% for\nCodeLlama-13B.\n","authors":["Mohammadreza Pourreza","Davood Rafiei","Yuxi Feng","Raymond Li","Zhenan Fan","Weiwei Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.16204v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16110v1","updated":"2024-03-24T12:01:27Z","published":"2024-03-24T12:01:27Z","title":"ByteCard: Enhancing Data Warehousing with Learned Cardinality Estimation","summary":"  Cardinality estimation is a critical component and a longstanding challenge\nin modern data warehouses. ByteHouse, ByteDance's cloud-native engine for big\ndata analysis in exabyte-scale environments, serves numerous internal\ndecision-making business scenarios. With the increasing demand of ByteHouse,\ncardinality estimation becomes the bottleneck for efficiently processing\nqueries. Specifically, the existing query optimizer of ByteHouse uses the\ntraditional Selinger-like cardinality estimator, which can produce huge\nestimation errors, resulting in sub-optimal query plans. To improve cardinality\nestimation accuracy while maintaining a practical inference overhead, we\ndevelop ByteCard framework that enables efficient training/updating and\nintegration of cardinality estimators. Furthermore, ByteCard adapts recent\nadvances in cardinality estimation to build models that can balance accuracy\nand practicality (e.g., inference latency, model size, training/updating\noverhead). We observe significant query processing speed-up in ByteHouse after\nreplacing the system's existing cardinality estimation with ByteCard's\nestimations for several optimization strategies. Evaluations on real-world\ndatasets show the integration of ByteCard leads to an improvement of up to 30%\nin the 99th quantile of latency. At last, we share our valuable experience in\nengineering advanced cardinality estimators. We believe this experience can\nhelp other data warehouses integrate more accurate and sophisticated solutions\non the critical path of query execution.\n","authors":["Yuxing Han","Haoyu Wang","Lixiang Chen","Yifeng Dong","Xing Chen","Benquan Yu","Chengcheng Yang","Weining Qian"],"pdf_url":"https://arxiv.org/pdf/2403.16110v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.18157v3","updated":"2024-03-24T06:03:04Z","published":"2023-11-30T00:06:58Z","title":"Finding Smallest Witnesses for Conjunctive Queries","summary":"  A witness is a sub-database that preserves the query results of the original\ndatabase but of much smaller size. It has wide applications in query rewriting\nand debugging, query explanation, IoT analytics, multi-layer network routing,\netc. In this paper, we study the smallest witness problem (SWP) for the class\nof conjunctive queries (CQs) without self-joins.\n  We first establish the dichotomy that SWP for a CQ can be computed in\npolynomial time if and only if it has {\\em head-cluster property}, unless\n$\\texttt{P} = \\texttt{NP}$. We next turn to the approximated version by\nrelaxing the size of a witness from being minimum. We surprisingly find that\nthe {\\em head-domination} property - that has been identified for the deletion\npropagation problem \\cite{kimelfeld2012maximizing} - can also precisely capture\nthe hardness of the approximated smallest witness problem. In polynomial time,\nSWP for any CQ with head-domination property can be approximated within a\nconstant factor, while SWP for any CQ without such a property cannot be\napproximated within a logarithmic factor, unless $\\texttt{P} = \\texttt{NP}$.\n  We further explore efficient approximation algorithms for CQs without\nhead-domination property: (1) we show a trivial algorithm which achieves a\npolynomially large approximation ratio for general CQs; (2) for any CQ with\nonly one non-output attribute, such as star CQs, we show a greedy algorithm\nwith a logarithmic approximation ratio; (3) for line CQs, which contain at\nleast two non-output attributes, we relate SWP problem to the directed steiner\nforest problem, whose algorithms can be applied to line CQs directly.\nMeanwhile, we establish a much higher lower bound, exponentially larger than\nthe logarithmic lower bound obtained above. It remains open to close the gap\nbetween the lower and upper bound of the approximated SWP for CQs without\nhead-domination property.\n","authors":["Xiao Hu","Stavros Sintos"],"pdf_url":"https://arxiv.org/pdf/2311.18157v3.pdf","comment":null}],"Computer Science and Game Theory":[{"id":"http://arxiv.org/abs/2403.16223v1","updated":"2024-03-24T16:33:01Z","published":"2024-03-24T16:33:01Z","title":"A Coupled Optimization Framework for Correlated Equilibria in\n  Normal-Form Game","summary":"  In competitive multi-player interactions, simultaneous optimality is a key\nrequirement for establishing strategic equilibria. This property is explicit\nwhen the game-theoretic equilibrium is the simultaneously optimal solution of\ncoupled optimization problems. However, no such optimization problems exist for\nthe correlated equilibrium, a strategic equilibrium where the players can\ncorrelate their actions. We address the lack of a coupled optimization\nframework for the correlated equilibrium by introducing an {unnormalized game}\n-- an extension of normal-form games in which the player strategies are lifted\nto unnormalized measures over the joint actions. We show that the set of fully\nmixed generalized Nash equilibria of this unnormalized game is a subset of the\ncorrelated equilibrium of the normal-form game. Furthermore, we introduce an\nentropy regularization to the unnormalized game and prove that the\nentropy-regularized generalized Nash equilibrium is a sub-optimal correlated\nequilibrium of the normal form game where the degree of sub-optimality depends\non the magnitude of regularization. We prove that the entropy-regularized\nunnormalized game has a closed-form solution, and empirically verify its\ncomputational efficacy at approximating the correlated equilibrium of\nnormal-form games.\n","authors":["Sarah H. Q. Li","Yue Yu","Florian Dörfler","John Lygeros"],"pdf_url":"https://arxiv.org/pdf/2403.16223v1.pdf","comment":"8 pages, 2 figures"}]},"2024-03-26T00:00:00Z":{"Artificial Intelligence":[{"id":"http://arxiv.org/abs/2403.16649v2","updated":"2024-03-26T06:08:20Z","published":"2024-03-25T11:37:15Z","title":"CLHA: A Simple yet Effective Contrastive Learning Framework for Human\n  Alignment","summary":"  Reinforcement learning from human feedback (RLHF) is a crucial technique in\naligning large language models (LLMs) with human preferences, ensuring these\nLLMs behave in beneficial and comprehensible ways to users. However, a\nlongstanding challenge in human alignment techniques based on reinforcement\nlearning lies in their inherent complexity and difficulty in training. To\naddress this challenge, we present a simple yet effective Contrastive Learning\nFramework for Human Alignment (CLHA) to align LLMs with human preferences\ndirectly. CLHA employs a novel rescoring strategy to evaluate the noise within\nthe data by considering its inherent quality and dynamically adjusting the\ntraining process. Simultaneously, CLHA utilizes pairwise contrastive loss and\nadaptive supervised fine-tuning loss to adaptively modify the likelihood of\ngenerating responses, ensuring enhanced alignment with human preferences. Using\nadvanced methods, CLHA surpasses other algorithms, showcasing superior\nperformance in terms of reward model scores, automatic evaluations, and human\nassessments on the widely used ``Helpful and Harmless'' dataset.\n","authors":["Feiteng Fang","Liang Zhu","Min Yang","Xi Feng","Jinchang Hou","Qixuan Zhao","Chengming Li","Xiping Hu","Ruifeng Xu"],"pdf_url":"https://arxiv.org/pdf/2403.16649v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16427v2","updated":"2024-03-26T07:21:01Z","published":"2024-03-25T05:12:18Z","title":"Re2LLM: Reflective Reinforcement Large Language Model for Session-based\n  Recommendation","summary":"  Large Language Models (LLMs) are emerging as promising approaches to enhance\nsession-based recommendation (SBR), where both prompt-based and\nfine-tuning-based methods have been widely investigated to align LLMs with SBR.\nHowever, the former methods struggle with optimal prompts to elicit the correct\nreasoning of LLMs due to the lack of task-specific feedback, leading to\nunsatisfactory recommendations. Although the latter methods attempt to\nfine-tune LLMs with domain-specific knowledge, they face limitations such as\nhigh computational costs and reliance on open-source backbones. To address such\nissues, we propose a \\underline{Re}flective \\underline{Re}inforcement\n\\underline{L}arge \\underline{L}anguage \\underline{M}odel (Re2LLM) for SBR,\nguiding LLMs to focus on specialized knowledge essential for more accurate\nrecommendations effectively and efficiently. In particular, we first design the\nReflective Exploration Module to effectively extract knowledge that is readily\nunderstandable and digestible by LLMs. To be specific, we direct LLMs to\nexamine recommendation errors through self-reflection and construct a knowledge\nbase (KB) comprising hints capable of rectifying these errors. To efficiently\nelicit the correct reasoning of LLMs, we further devise the Reinforcement\nUtilization Module to train a lightweight retrieval agent. It learns to select\nhints from the constructed KB based on the task-specific feedback, where the\nhints can serve as guidance to help correct LLMs reasoning for better\nrecommendations. Extensive experiments on multiple real-world datasets\ndemonstrate that our method consistently outperforms state-of-the-art methods.\n","authors":["Ziyan Wang","Yingpeng Du","Zhu Sun","Haoyan Chua","Kaidong Feng","Wenya Wang","Jie Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.16427v2.pdf","comment":"11 pages, 4 figures"},{"id":"http://arxiv.org/abs/2403.08763v3","updated":"2024-03-26T17:58:48Z","published":"2024-03-13T17:58:57Z","title":"Simple and Scalable Strategies to Continually Pre-train Large Language\n  Models","summary":"  Large language models (LLMs) are routinely pre-trained on billions of tokens,\nonly to start the process over again once new data becomes available. A much\nmore efficient solution is to continually pre-train these models, saving\nsignificant compute compared to re-training. However, the distribution shift\ninduced by new data typically results in degraded performance on previous data\nor poor adaptation to the new data. In this work, we show that a simple and\nscalable combination of learning rate (LR) re-warming, LR re-decaying, and\nreplay of previous data is sufficient to match the performance of fully\nre-training from scratch on all available data, as measured by the final loss\nand the average score on several language model (LM) evaluation benchmarks.\nSpecifically, we show this for a weak but realistic distribution shift between\ntwo commonly used LLM pre-training datasets (English$\\rightarrow$English) and a\nstronger distribution shift (English$\\rightarrow$German) at the $405$M\nparameter model scale with large dataset sizes (hundreds of billions of\ntokens). Selecting the weak but realistic shift for larger-scale experiments,\nwe also find that our continual learning strategies match the re-training\nbaseline for a 10B parameter LLM. Our results demonstrate that LLMs can be\nsuccessfully updated via simple and scalable continual learning strategies,\nmatching the re-training baseline using only a fraction of the compute.\nFinally, inspired by previous work, we propose alternatives to the cosine\nlearning rate schedule that help circumvent forgetting induced by LR re-warming\nand that are not bound to a fixed token budget.\n","authors":["Adam Ibrahim","Benjamin Thérien","Kshitij Gupta","Mats L. Richter","Quentin Anthony","Timothée Lesort","Eugene Belilovsky","Irina Rish"],"pdf_url":"https://arxiv.org/pdf/2403.08763v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17933v1","updated":"2024-03-26T17:58:29Z","published":"2024-03-26T17:58:29Z","title":"SLEDGE: Synthesizing Simulation Environments for Driving Agents with\n  Generative Models","summary":"  SLEDGE is the first generative simulator for vehicle motion planning trained\non real-world driving logs. Its core component is a learned model that is able\nto generate agent bounding boxes and lane graphs. The model's outputs serve as\nan initial state for traffic simulation. The unique properties of the entities\nto be generated for SLEDGE, such as their connectivity and variable count per\nscene, render the naive application of most modern generative models to this\ntask non-trivial. Therefore, together with a systematic study of existing lane\ngraph representations, we introduce a novel raster-to-vector autoencoder\n(RVAE). It encodes agents and the lane graph into distinct channels in a\nrasterized latent map. This facilitates both lane-conditioned agent generation\nand combined generation of lanes and agents with a Diffusion Transformer. Using\ngenerated entities in SLEDGE enables greater control over the simulation, e.g.\nupsampling turns or increasing traffic density. Further, SLEDGE can support\n500m long routes, a capability not found in existing data-driven simulators\nlike nuPlan. It presents new challenges for planning algorithms, evidenced by\nfailure rates of over 40% for PDM, the winner of the 2023 nuPlan challenge,\nwhen tested on hard routes and dense traffic generated by our model. Compared\nto nuPlan, SLEDGE requires 500$\\times$ less storage to set up (<4GB), making it\na more accessible option and helping with democratizing future research in this\nfield.\n","authors":["Kashyap Chitta","Daniel Dauner","Andreas Geiger"],"pdf_url":"https://arxiv.org/pdf/2403.17933v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17927v1","updated":"2024-03-26T17:57:57Z","published":"2024-03-26T17:57:57Z","title":"MAGIS: LLM-Based Multi-Agent Framework for GitHub Issue Resolution","summary":"  In software evolution, resolving the emergent issues within GitHub\nrepositories is a complex challenge that involves not only the incorporation of\nnew code but also the maintenance of existing functionalities. Large Language\nModels (LLMs) have shown promise in code generation and understanding but face\ndifficulties in code change, particularly at the repository level. To overcome\nthese challenges, we empirically study the reason why LLMs mostly fail to\nresolve GitHub issues and analyze some impact factors. Motivated by the\nempirical findings, we propose a novel LLM-based Multi-Agent framework for\nGitHub Issue reSolution, MAGIS, consisting of four kinds of agents customized\nfor the software evolution: Manager, Repository Custodian, Developer, and\nQuality Assurance Engineer agents. This framework leverages the collaboration\nof various agents in the planning and coding process to unlock the potential of\nLLMs to resolve GitHub issues. In experiments, we employ the SWE-bench\nbenchmark to compare MAGIS with popular LLMs, including GPT-3.5, GPT-4, and\nClaude-2. MAGIS can resolve 13.94% GitHub issues, which significantly\noutperforms the baselines. Specifically, MAGIS achieves an eight-fold increase\nin resolved ratio over the direct application of GPT-4, the based LLM of our\nmethod. We also analyze the factors for improving GitHub issue resolution\nrates, such as line location, task allocation, etc.\n","authors":["Wei Tao","Yucheng Zhou","Wenqiang Zhang","Yu Cheng"],"pdf_url":"https://arxiv.org/pdf/2403.17927v1.pdf","comment":"work in progress"},{"id":"http://arxiv.org/abs/2403.17924v1","updated":"2024-03-26T17:57:05Z","published":"2024-03-26T17:57:05Z","title":"AID: Attention Interpolation of Text-to-Image Diffusion","summary":"  Conditional diffusion models can create unseen images in various settings,\naiding image interpolation. Interpolation in latent spaces is well-studied, but\ninterpolation with specific conditions like text or poses is less understood.\nSimple approaches, such as linear interpolation in the space of conditions,\noften result in images that lack consistency, smoothness, and fidelity. To that\nend, we introduce a novel training-free technique named Attention Interpolation\nvia Diffusion (AID). Our key contributions include 1) proposing an inner/outer\ninterpolated attention layer; 2) fusing the interpolated attention with\nself-attention to boost fidelity; and 3) applying beta distribution to\nselection to increase smoothness. We also present a variant, Prompt-guided\nAttention Interpolation via Diffusion (PAID), that considers interpolation as a\ncondition-dependent generative process. This method enables the creation of new\nimages with greater consistency, smoothness, and efficiency, and offers control\nover the exact path of interpolation. Our approach demonstrates effectiveness\nfor conceptual and spatial interpolation. Code and demo are available at\nhttps://github.com/QY-H00/attention-interpolation-diffusion.\n","authors":["Qiyuan He","Jinghao Wang","Ziwei Liu","Angela Yao"],"pdf_url":"https://arxiv.org/pdf/2403.17924v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17919v1","updated":"2024-03-26T17:55:02Z","published":"2024-03-26T17:55:02Z","title":"LISA: Layerwise Importance Sampling for Memory-Efficient Large Language\n  Model Fine-Tuning","summary":"  The machine learning community has witnessed impressive advancements since\nthe first appearance of large language models (LLMs), yet their huge memory\nconsumption has become a major roadblock to large-scale training. Parameter\nEfficient Fine-Tuning techniques such as Low-Rank Adaptation (LoRA) have been\nproposed to alleviate this problem, but their performance still fails to match\nfull parameter training in most large-scale fine-tuning settings. Attempting to\ncomplement this deficiency, we investigate layerwise properties of LoRA on\nfine-tuning tasks and observe an uncommon skewness of weight norms across\ndifferent layers. Utilizing this key observation, a surprisingly simple\ntraining strategy is discovered, which outperforms both LoRA and full parameter\ntraining in a wide range of settings with memory costs as low as LoRA. We name\nit Layerwise Importance Sampled AdamW (LISA), a promising alternative for LoRA,\nwhich applies the idea of importance sampling to different layers in LLMs and\nrandomly freeze most middle layers during optimization. Experimental results\nshow that with similar or less GPU memory consumption, LISA surpasses LoRA or\neven full parameter tuning in downstream fine-tuning tasks, where LISA\nconsistently outperforms LoRA by over $11\\%$-$37\\%$ in terms of MT-Bench\nscores. On large models, specifically LLaMA-2-70B, LISA achieves on-par or\nbetter performance than LoRA on MT-Bench, GSM8K, and PubMedQA, demonstrating\nits effectiveness across different domains.\n","authors":["Rui Pan","Xiang Liu","Shizhe Diao","Renjie Pi","Jipeng Zhang","Chi Han","Tong Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.17919v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17918v1","updated":"2024-03-26T17:54:15Z","published":"2024-03-26T17:54:15Z","title":"AgentStudio: A Toolkit for Building General Virtual Agents","summary":"  Creating autonomous virtual agents capable of using arbitrary software on any\ndigital device remains a major challenge for artificial intelligence. Two key\nobstacles hinder progress: insufficient infrastructure for building virtual\nagents in real-world environments, and the need for in-the-wild evaluation of\nfundamental agent abilities. To address this, we introduce AgentStudio, an\nonline, realistic, and multimodal toolkit that covers the entire lifecycle of\nagent development. This includes environment setups, data collection, agent\nevaluation, and visualization. The observation and action spaces are highly\ngeneric, supporting both function calling and human-computer interfaces. This\nversatility is further enhanced by AgentStudio's graphical user interfaces,\nwhich allow efficient development of datasets and benchmarks in real-world\nsettings. To illustrate, we introduce a visual grounding dataset and a\nreal-world benchmark suite, both created with our graphical interfaces.\nFurthermore, we present several actionable insights derived from AgentStudio,\ne.g., general visual grounding, open-ended tool creation, learning from videos,\netc. We have open-sourced the environments, datasets, benchmarks, and\ninterfaces to promote research towards developing general virtual agents for\nthe future.\n","authors":["Longtao Zheng","Zhiyuan Huang","Zhenghai Xue","Xinrun Wang","Bo An","Shuicheng Yan"],"pdf_url":"https://arxiv.org/pdf/2403.17918v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17916v1","updated":"2024-03-26T17:53:27Z","published":"2024-03-26T17:53:27Z","title":"CMP: Cooperative Motion Prediction with Multi-Agent Communication","summary":"  The confluence of the advancement of Autonomous Vehicles (AVs) and the\nmaturity of Vehicle-to-Everything (V2X) communication has enabled the\ncapability of cooperative connected and automated vehicles (CAVs). Building on\ntop of cooperative perception, this paper explores the feasibility and\neffectiveness of cooperative motion prediction. Our method, CMP, takes LiDAR\nsignals as input to enhance tracking and prediction capabilities. Unlike\nprevious work that focuses separately on either cooperative perception or\nmotion prediction, our framework, to the best of our knowledge, is the first to\naddress the unified problem where CAVs share information in both perception and\nprediction modules. Incorporated into our design is the unique capability to\ntolerate realistic V2X bandwidth limitations and transmission delays, while\ndealing with bulky perception representations. We also propose a prediction\naggregation module, which unifies the predictions obtained by different CAVs\nand generates the final prediction. Through extensive experiments and ablation\nstudies, we demonstrate the effectiveness of our method in cooperative\nperception, tracking, and motion prediction tasks. In particular, CMP reduces\nthe average prediction error by 17.2\\% with fewer missing detections compared\nwith the no cooperation setting. Our work marks a significant step forward in\nthe cooperative capabilities of CAVs, showcasing enhanced performance in\ncomplex scenarios.\n","authors":["Zhuoyuan Wu","Yuping Wang","Hengbo Ma","Zhaowei Li","Hang Qiu","Jiachen Li"],"pdf_url":"https://arxiv.org/pdf/2403.17916v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17914v1","updated":"2024-03-26T17:51:06Z","published":"2024-03-26T17:51:06Z","title":"Hierarchical Multi-label Classification for Fine-level Event Extraction\n  from Aviation Accident Reports","summary":"  A large volume of accident reports is recorded in the aviation domain, which\ngreatly values improving aviation safety. To better use those reports, we need\nto understand the most important events or impact factors according to the\naccident reports. However, the increasing number of accident reports requires\nlarge efforts from domain experts to label those reports. In order to make the\nlabeling process more efficient, many researchers have started developing\nalgorithms to identify the underlying events from accident reports\nautomatically. This article argues that we can identify the events more\naccurately by leveraging the event taxonomy. More specifically, we consider the\nproblem a hierarchical classification task where we first identify the\ncoarse-level information and then predict the fine-level information. We\nachieve this hierarchical classification process by incorporating a novel\nhierarchical attention module into BERT. To further utilize the information\nfrom event taxonomy, we regularize the proposed model according to the\nrelationship and distribution among labels. The effectiveness of our framework\nis evaluated with the data collected by National Transportation Safety Board\n(NTSB). It has been shown that fine-level prediction accuracy is highly\nimproved, and the regularization term can be beneficial to the rare event\nidentification problem.\n","authors":["Xinyu Zhao","Hao Yan","Yongming Liu"],"pdf_url":"https://arxiv.org/pdf/2403.17914v1.pdf","comment":"Accepted in INFORMS Journal of Data Science"},{"id":"http://arxiv.org/abs/2307.16897v2","updated":"2024-03-26T17:40:47Z","published":"2023-07-31T17:59:48Z","title":"DiVa-360: The Dynamic Visual Dataset for Immersive Neural Fields","summary":"  Advances in neural fields are enabling high-fidelity capture of the shape and\nappearance of dynamic 3D scenes. However, their capabilities lag behind those\noffered by conventional representations such as 2D videos because of\nalgorithmic challenges and the lack of large-scale multi-view real-world\ndatasets. We address the dataset limitation with DiVa-360, a real-world 360\ndynamic visual dataset that contains synchronized high-resolution and\nlong-duration multi-view video sequences of table-scale scenes captured using a\ncustomized low-cost system with 53 cameras. It contains 21 object-centric\nsequences categorized by different motion types, 25 intricate hand-object\ninteraction sequences, and 8 long-duration sequences for a total of 17.4 M\nimage frames. In addition, we provide foreground-background segmentation masks,\nsynchronized audio, and text descriptions. We benchmark the state-of-the-art\ndynamic neural field methods on DiVa-360 and provide insights about existing\nmethods and future challenges on long-duration neural field capture.\n","authors":["Cheng-You Lu","Peisen Zhou","Angela Xing","Chandradeep Pokhariya","Arnab Dey","Ishaan Shah","Rugved Mavidipalli","Dylan Hu","Andrew Comport","Kefan Chen","Srinath Sridhar"],"pdf_url":"https://arxiv.org/pdf/2307.16897v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17891v1","updated":"2024-03-26T17:22:29Z","published":"2024-03-26T17:22:29Z","title":"Image-based Novel Fault Detection with Deep Learning Classifiers using\n  Hierarchical Labels","summary":"  One important characteristic of modern fault classification systems is the\nability to flag the system when faced with previously unseen fault types. This\nwork considers the unknown fault detection capabilities of deep neural\nnetwork-based fault classifiers. Specifically, we propose a methodology on how,\nwhen available, labels regarding the fault taxonomy can be used to increase\nunknown fault detection performance without sacrificing model performance. To\nachieve this, we propose to utilize soft label techniques to improve the\nstate-of-the-art deep novel fault detection techniques during the training\nprocess and novel hierarchically consistent detection statistics for online\nnovel fault detection. Finally, we demonstrated increased detection performance\non novel fault detection in inspection images from the hot steel rolling\nprocess, with results well replicated across multiple scenarios and baseline\ndetection methods.\n","authors":["Nurettin Sergin","Jiayu Huang","Tzyy-Shuh Chang","Hao Yan"],"pdf_url":"https://arxiv.org/pdf/2403.17891v1.pdf","comment":"Accepted in IISE Transaction"},{"id":"http://arxiv.org/abs/2403.04202v3","updated":"2024-03-26T17:18:33Z","published":"2024-03-07T04:12:24Z","title":"Dynamics of Moral Behavior in Heterogeneous Populations of Learning\n  Agents","summary":"  Growing concerns about safety and alignment of AI systems highlight the\nimportance of embedding moral capabilities in artificial agents. A promising\nsolution is the use of learning from experience, i.e., Reinforcement Learning.\nIn multi-agent (social) environments, complex population-level phenomena may\nemerge from interactions between individual learning agents. Many of the\nexisting studies rely on simulated social dilemma environments to study the\ninteractions of independent learning agents. However, they tend to ignore the\nmoral heterogeneity that is likely to be present in societies of agents in\npractice. For example, at different points in time a single learning agent may\nface opponents who are consequentialist (i.e., caring about maximizing some\noutcome over time) or norm-based (i.e., focusing on conforming to a specific\nnorm here and now). The extent to which agents' co-development may be impacted\nby such moral heterogeneity in populations is not well understood. In this\npaper, we present a study of the learning dynamics of morally heterogeneous\npopulations interacting in a social dilemma setting. Using a Prisoner's Dilemma\nenvironment with a partner selection mechanism, we investigate the extent to\nwhich the prevalence of diverse moral agents in populations affects individual\nagents' learning behaviors and emergent population-level outcomes. We observe\nseveral types of non-trivial interactions between pro-social and anti-social\nagents, and find that certain classes of moral agents are able to steer selfish\nagents towards more cooperative behavior.\n","authors":["Elizaveta Tennant","Stephen Hailes","Mirco Musolesi"],"pdf_url":"https://arxiv.org/pdf/2403.04202v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.03773v3","updated":"2024-03-26T17:17:39Z","published":"2023-04-04T21:49:02Z","title":"Safe Explicable Planning","summary":"  Human expectations arise from their understanding of others and the world. In\nthe context of human-AI interaction, this understanding may not align with\nreality, leading to the AI agent failing to meet expectations and compromising\nteam performance. Explicable planning, introduced as a method to bridge this\ngap, aims to reconcile human expectations with the agent's optimal behavior,\nfacilitating interpretable decision-making. However, an unresolved critical\nissue is ensuring safety in explicable planning, as it could result in\nexplicable behaviors that are unsafe. To address this, we propose Safe\nExplicable Planning (SEP), which extends the prior work to support the\nspecification of a safety bound. The goal of SEP is to find behaviors that\nalign with human expectations while adhering to the specified safety criterion.\nOur approach generalizes the consideration of multiple objectives stemming from\nmultiple models rather than a single model, yielding a Pareto set of safe\nexplicable policies. We present both an exact method, guaranteeing finding the\nPareto set, and a more efficient greedy method that finds one of the policies\nin the Pareto set. Additionally, we offer approximate solutions based on state\naggregation to improve scalability. We provide formal proofs that validate the\ndesired theoretical properties of these methods. Evaluation through simulations\nand physical robot experiments confirms the effectiveness of our approach for\nsafe explicable planning.\n","authors":["Akkamahadevi Hanni","Andrew Boateng","Yu Zhang"],"pdf_url":"https://arxiv.org/pdf/2304.03773v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17873v1","updated":"2024-03-26T17:02:42Z","published":"2024-03-26T17:02:42Z","title":"Addressing Social Misattributions of Large Language Models: An\n  HCXAI-based Approach","summary":"  Human-centered explainable AI (HCXAI) advocates for the integration of social\naspects into AI explanations. Central to the HCXAI discourse is the Social\nTransparency (ST) framework, which aims to make the socio-organizational\ncontext of AI systems accessible to their users. In this work, we suggest\nextending the ST framework to address the risks of social misattributions in\nLarge Language Models (LLMs), particularly in sensitive areas like mental\nhealth. In fact LLMs, which are remarkably capable of simulating roles and\npersonas, may lead to mismatches between designers' intentions and users'\nperceptions of social attributes, risking to promote emotional manipulation and\ndangerous behaviors, cases of epistemic injustice, and unwarranted trust. To\naddress these issues, we propose enhancing the ST framework with a fifth\n'W-question' to clarify the specific social attributions assigned to LLMs by\nits designers and users. This addition aims to bridge the gap between LLM\ncapabilities and user perceptions, promoting the ethically responsible\ndevelopment and use of LLM-based technology.\n","authors":["Andrea Ferrario","Alberto Termine","Alessandro Facchini"],"pdf_url":"https://arxiv.org/pdf/2403.17873v1.pdf","comment":"Extended version of the manuscript accepted for the ACM CHI Workshop\n  on Human-Centered Explainable AI 2024 (HCXAI24)"},{"id":"http://arxiv.org/abs/2401.06795v2","updated":"2024-03-26T16:44:34Z","published":"2024-01-08T18:42:55Z","title":"AI and Generative AI for Research Discovery and Summarization","summary":"  AI and generative AI tools, including chatbots like ChatGPT that rely on\nlarge language models (LLMs), have burst onto the scene this year, creating\nincredible opportunities to increase work productivity and improve our lives.\nStatisticians and data scientists have begun experiencing the benefits from the\navailability of these tools in numerous ways, such as the generation of\nprogramming code from text prompts to analyze data or fit statistical models.\nOne area that these tools can make a substantial impact is in research\ndiscovery and summarization. Standalone tools and plugins to chatbots are being\ndeveloped that allow researchers to more quickly find relevant literature than\npre-2023 search tools. Furthermore, generative AI tools have improved to the\npoint where they can summarize and extract the key points from research\narticles in succinct language. Finally, chatbots based on highly parameterized\nLLMs can be used to simulate abductive reasoning, which provides researchers\nthe ability to make connections among related technical topics, which can also\nbe used for research discovery. We review the developments in AI and generative\nAI for research discovery and summarization, and propose directions where these\ntypes of tools are likely to head in the future that may be of interest to\nstatistician and data scientists.\n","authors":["Mark Glickman","Yi Zhang"],"pdf_url":"https://arxiv.org/pdf/2401.06795v2.pdf","comment":"29 pages, 9 figures"},{"id":"http://arxiv.org/abs/2403.17847v1","updated":"2024-03-26T16:36:50Z","published":"2024-03-26T16:36:50Z","title":"Climate Downscaling: A Deep-Learning Based Super-resolution Model of\n  Precipitation Data with Attention Block and Skip Connections","summary":"  Human activities accelerate consumption of fossil fuels and produce\ngreenhouse gases, resulting in urgent issues today: global warming and the\nclimate change. These indirectly cause severe natural disasters, plenty of\nlives suffering and huge losses of agricultural properties. To mitigate impacts\non our lands, scientists are developing renewable, reusable, and clean energies\nand climatologists are trying to predict the extremes. Meanwhile, governments\nare publicizing resource-saving policies for a more eco-friendly society and\narousing environment awareness. One of the most influencing factors is the\nprecipitation, bringing condensed water vapor onto lands. Water resources are\nthe most significant but basic needs in society, not only supporting our\nlivings, but also economics. In Taiwan, although the average annual\nprecipitation is up to 2,500 millimeter (mm), the water allocation for each\nperson is lower than the global average due to drastically geographical\nelevation changes and uneven distribution through the year. Thus, it is crucial\nto track and predict the rainfall to make the most use of it and to prevent the\nfloods. However, climate models have limited resolution and require intensive\ncomputational power for local-scale use. Therefore, we proposed a deep\nconvolutional neural network with skip connections, attention blocks, and\nauxiliary data concatenation, in order to downscale the low-resolution\nprecipitation data into high-resolution one. Eventually, we compare with other\nclimate downscaling methods and show better performance in metrics of Mean\nAbsolute Error (MAE), Root Mean Square Error (RMSE), Pearson Correlation,\nstructural similarity index (SSIM), and forecast indicators.\n","authors":["Chia-Hao Chiang","Zheng-Han Huang","Liwen Liu","Hsin-Chien Liang","Yi-Chi Wang","Wan-Ling Tseng","Chao Wang","Che-Ta Chen","Ko-Chih Wang"],"pdf_url":"https://arxiv.org/pdf/2403.17847v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17846v1","updated":"2024-03-26T16:36:43Z","published":"2024-03-26T16:36:43Z","title":"Hierarchical Open-Vocabulary 3D Scene Graphs for Language-Grounded Robot\n  Navigation","summary":"  Recent open-vocabulary robot mapping methods enrich dense geometric maps with\npre-trained visual-language features. While these maps allow for the prediction\nof point-wise saliency maps when queried for a certain language concept,\nlarge-scale environments and abstract queries beyond the object level still\npose a considerable hurdle, ultimately limiting language-grounded robotic\nnavigation. In this work, we present HOV-SG, a hierarchical open-vocabulary 3D\nscene graph mapping approach for language-grounded robot navigation. Leveraging\nopen-vocabulary vision foundation models, we first obtain state-of-the-art\nopen-vocabulary segment-level maps in 3D and subsequently construct a 3D scene\ngraph hierarchy consisting of floor, room, and object concepts, each enriched\nwith open-vocabulary features. Our approach is able to represent multi-story\nbuildings and allows robotic traversal of those using a cross-floor Voronoi\ngraph. HOV-SG is evaluated on three distinct datasets and surpasses previous\nbaselines in open-vocabulary semantic accuracy on the object, room, and floor\nlevel while producing a 75% reduction in representation size compared to dense\nopen-vocabulary maps. In order to prove the efficacy and generalization\ncapabilities of HOV-SG, we showcase successful long-horizon\nlanguage-conditioned robot navigation within real-world multi-storage\nenvironments. We provide code and trial video data at http://hovsg.github.io/.\n","authors":["Abdelrhman Werby","Chenguang Huang","Martin Büchner","Abhinav Valada","Wolfram Burgard"],"pdf_url":"https://arxiv.org/pdf/2403.17846v1.pdf","comment":"Code and video are available at http://hovsg.github.io/"},{"id":"http://arxiv.org/abs/2403.17839v1","updated":"2024-03-26T16:27:37Z","published":"2024-03-26T16:27:37Z","title":"ReMamber: Referring Image Segmentation with Mamba Twister","summary":"  Referring Image Segmentation (RIS) leveraging transformers has achieved great\nsuccess on the interpretation of complex visual-language tasks. However, the\nquadratic computation cost makes it resource-consuming in capturing long-range\nvisual-language dependencies. Fortunately, Mamba addresses this with efficient\nlinear complexity in processing. However, directly applying Mamba to\nmulti-modal interactions presents challenges, primarily due to inadequate\nchannel interactions for the effective fusion of multi-modal data. In this\npaper, we propose ReMamber, a novel RIS architecture that integrates the power\nof Mamba with a multi-modal Mamba Twister block. The Mamba Twister explicitly\nmodels image-text interaction, and fuses textual and visual features through\nits unique channel and spatial twisting mechanism. We achieve the\nstate-of-the-art on three challenging benchmarks. Moreover, we conduct thorough\nanalyses of ReMamber and discuss other fusion designs using Mamba. These\nprovide valuable perspectives for future research.\n","authors":["Yuhuan Yang","Chaofan Ma","Jiangchao Yao","Zhun Zhong","Ya Zhang","Yanfeng Wang"],"pdf_url":"https://arxiv.org/pdf/2403.17839v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.03123v2","updated":"2024-03-26T16:22:54Z","published":"2023-04-13T16:01:28Z","title":"ChatGPT Needs SPADE (Sustainability, PrivAcy, Digital divide, and\n  Ethics) Evaluation: A Review","summary":"  ChatGPT is another large language model (LLM) vastly available for the\nconsumers on their devices but due to its performance and ability to converse\neffectively, it has gained a huge popularity amongst research as well as\nindustrial community. Recently, many studies have been published to show the\neffectiveness, efficiency, integration, and sentiments of chatGPT and other\nLLMs. In contrast, this study focuses on the important aspects that are mostly\noverlooked, i.e. sustainability, privacy, digital divide, and ethics and\nsuggests that not only chatGPT but every subsequent entry in the category of\nconversational bots should undergo Sustainability, PrivAcy, Digital divide, and\nEthics (SPADE) evaluation. This paper discusses in detail the issues and\nconcerns raised over chatGPT in line with aforementioned characteristics. We\nalso discuss the recent EU AI Act briefly in accordance with the SPADE\nevaluation. We support our hypothesis by some preliminary data collection and\nvisualizations along with hypothesized facts. We also suggest mitigations and\nrecommendations for each of the concerns. Furthermore, we also suggest some\npolicies and recommendations for AI policy act, if designed by the governments.\n","authors":["Sunder Ali Khowaja","Parus Khuwaja","Kapal Dev","Weizheng Wang","Lewis Nkenyereye"],"pdf_url":"https://arxiv.org/pdf/2305.03123v2.pdf","comment":"29 pages, 8 figures, 4 tables"},{"id":"http://arxiv.org/abs/2403.17827v1","updated":"2024-03-26T16:06:42Z","published":"2024-03-26T16:06:42Z","title":"DiffH2O: Diffusion-Based Synthesis of Hand-Object Interactions from\n  Textual Descriptions","summary":"  Generating natural hand-object interactions in 3D is challenging as the\nresulting hand and object motions are expected to be physically plausible and\nsemantically meaningful. Furthermore, generalization to unseen objects is\nhindered by the limited scale of available hand-object interaction datasets. We\npropose DiffH2O, a novel method to synthesize realistic, one or two-handed\nobject interactions from provided text prompts and geometry of the object. The\nmethod introduces three techniques that enable effective learning from limited\ndata. First, we decompose the task into a grasping stage and a text-based\ninteraction stage and use separate diffusion models for each. In the grasping\nstage, the model only generates hand motions, whereas in the interaction phase\nboth hand and object poses are synthesized. Second, we propose a compact\nrepresentation that tightly couples hand and object poses. Third, we propose\ntwo different guidance schemes to allow more control of the generated motions:\ngrasp guidance and detailed textual guidance. Grasp guidance takes a single\ntarget grasping pose and guides the diffusion model to reach this grasp at the\nend of the grasping stage, which provides control over the grasping pose. Given\na grasping motion from this stage, multiple different actions can be prompted\nin the interaction phase. For textual guidance, we contribute comprehensive\ntext descriptions to the GRAB dataset and show that they enable our method to\nhave more fine-grained control over hand-object interactions. Our quantitative\nand qualitative evaluation demonstrates that the proposed method outperforms\nbaseline methods and leads to natural hand-object motions. Moreover, we\ndemonstrate the practicality of our framework by utilizing a hand pose estimate\nfrom an off-the-shelf pose estimator for guidance, and then sampling multiple\ndifferent actions in the interaction stage.\n","authors":["Sammy Christen","Shreyas Hampali","Fadime Sener","Edoardo Remelli","Tomas Hodan","Eric Sauser","Shugao Ma","Bugra Tekin"],"pdf_url":"https://arxiv.org/pdf/2403.17827v1.pdf","comment":"Project Page: https://diffh2o.github.io/"},{"id":"http://arxiv.org/abs/2403.17826v1","updated":"2024-03-26T16:06:33Z","published":"2024-03-26T16:06:33Z","title":"On the Computational Complexity of Stackelberg Planning and\n  Meta-Operator Verification: Technical Report","summary":"  Stackelberg planning is a recently introduced single-turn two-player\nadversarial planning model, where two players are acting in a joint classical\nplanning task, the objective of the first player being hampering the second\nplayer from achieving its goal. This places the Stackelberg planning problem\nsomewhere between classical planning and general combinatorial two-player\ngames. But, where exactly? All investigations of Stackelberg planning so far\nfocused on practical aspects. We close this gap by conducting the first\ntheoretical complexity analysis of Stackelberg planning. We show that in\ngeneral Stackelberg planning is actually no harder than classical planning.\nUnder a polynomial plan-length restriction, however, Stackelberg planning is a\nlevel higher up in the polynomial complexity hierarchy, suggesting that\ncompilations into classical planning come with a worst-case exponential\nplan-length increase. In attempts to identify tractable fragments, we further\nstudy its complexity under various planning task restrictions, showing that\nStackelberg planning remains intractable where classical planning is not. We\nfinally inspect the complexity of meta-operator verification, a problem that\nhas been recently connected to Stackelberg planning.\n","authors":["Gregor Behnke","Marcel Steinmetz"],"pdf_url":"https://arxiv.org/pdf/2403.17826v1.pdf","comment":"Presented at ICAPS24"},{"id":"http://arxiv.org/abs/2403.14077v2","updated":"2024-03-26T16:02:36Z","published":"2024-03-21T01:57:30Z","title":"Can ChatGPT Detect DeepFakes? A Study of Using Multimodal Large Language\n  Models for Media Forensics","summary":"  DeepFakes, which refer to AI-generated media content, have become an\nincreasing concern due to their use as a means for disinformation. Detecting\nDeepFakes is currently solved with programmed machine learning algorithms. In\nthis work, we investigate the capabilities of multimodal large language models\n(LLMs) in DeepFake detection. We conducted qualitative and quantitative\nexperiments to demonstrate multimodal LLMs and show that they can expose\nAI-generated images through careful experimental design and prompt engineering.\nThis is interesting, considering that LLMs are not inherently tailored for\nmedia forensic tasks, and the process does not require programming. We discuss\nthe limitations of multimodal LLMs for these tasks and suggest possible\nimprovements.\n","authors":["Shan Jia","Reilin Lyu","Kangran Zhao","Yize Chen","Zhiyuan Yan","Yan Ju","Chuanbo Hu","Xin Li","Baoyuan Wu","Siwei Lyu"],"pdf_url":"https://arxiv.org/pdf/2403.14077v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.15964v2","updated":"2024-03-26T15:58:26Z","published":"2023-11-27T16:07:37Z","title":"Efficient Pre-training for Localized Instruction Generation of Videos","summary":"  Procedural videos show step-by-step demonstrations of tasks like recipe\npreparation. Understanding such videos is challenging, involving the precise\nlocalization of steps and the generation of textual instructions. Manually\nannotating steps and writing instructions is costly, which limits the size of\ncurrent datasets and hinders effective learning. Leveraging large but noisy\nvideo-transcript datasets for pre-training can boost performance, but demands\nsignificant computational resources. Furthermore, transcripts contain\nirrelevant content and exhibit style variation compared to instructions written\nby human annotators. To mitigate both issues, we propose a technique,\nSieve-&-Swap, to automatically curate a smaller dataset: (i) Sieve filters\nirrelevant transcripts and (ii) Swap enhances the quality of the text\ninstruction by automatically replacing the transcripts with human-written\ninstructions from a text-only recipe dataset. The curated dataset, three orders\nof magnitude smaller than current web-scale datasets, enables efficient\ntraining of large-scale models with competitive performance. We complement our\nSieve-\\&-Swap approach with a Procedure Transformer (ProcX) for end-to-end step\nlocalization and instruction generation for procedural videos. When this model\nis pre-trained on our curated dataset, it achieves state-of-the-art performance\nin zero-shot and finetuning settings on YouCook2 and Tasty, while using a\nfraction of the computational resources.\n","authors":["Anil Batra","Davide Moltisanti","Laura Sevilla-Lara","Marcus Rohrbach","Frank Keller"],"pdf_url":"https://arxiv.org/pdf/2311.15964v2.pdf","comment":"This version has some missing experiments and elaborative technical\n  details"},{"id":"http://arxiv.org/abs/2403.17819v1","updated":"2024-03-26T15:54:48Z","published":"2024-03-26T15:54:48Z","title":"Accelerating Radio Spectrum Regulation Workflows with Large Language\n  Models (LLMs)","summary":"  Wireless spectrum regulation is a complex and demanding process due to the\nrapid pace of technological progress, increasing demand for spectrum, and a\nmultitude of stakeholders with potentially conflicting interests, alongside\nsignificant economic implications. To navigate this, regulators must engage\neffectively with all parties, keep pace with global technology trends, conduct\ntechnical evaluations, issue licenses in a timely manner, and comply with\nvarious legal and policy frameworks.\n  In light of these challenges, this paper demonstrates example applications of\nLarge Language Models (LLMs) to expedite spectrum regulatory processes. We\nexplore various roles that LLMs can play in this context while identifying some\nof the challenges to address. The paper also offers practical case studies and\ninsights, with appropriate experiments, highlighting the transformative\npotential of LLMs in spectrum management.\n","authors":["Amir Ghasemi","Paul Guinand"],"pdf_url":"https://arxiv.org/pdf/2403.17819v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17814v1","updated":"2024-03-26T15:52:36Z","published":"2024-03-26T15:52:36Z","title":"D-PAD: Deep-Shallow Multi-Frequency Patterns Disentangling for Time\n  Series Forecasting","summary":"  In time series forecasting, effectively disentangling intricate temporal\npatterns is crucial. While recent works endeavor to combine decomposition\ntechniques with deep learning, multiple frequencies may still be mixed in the\ndecomposed components, e.g., trend and seasonal. Furthermore, frequency domain\nanalysis methods, e.g., Fourier and wavelet transforms, have limitations in\nresolution in the time domain and adaptability. In this paper, we propose\nD-PAD, a deep-shallow multi-frequency patterns disentangling neural network for\ntime series forecasting. Specifically, a multi-component decomposing (MCD)\nblock is introduced to decompose the series into components with different\nfrequency ranges, corresponding to the \"shallow\" aspect. A\ndecomposition-reconstruction-decomposition (D-R-D) module is proposed to\nprogressively extract the information of frequencies mixed in the components,\ncorresponding to the \"deep\" aspect. After that, an interaction and fusion (IF)\nmodule is used to further analyze the components. Extensive experiments on\nseven real-world datasets demonstrate that D-PAD achieves the state-of-the-art\nperformance, outperforming the best baseline by an average of 9.48% and 7.15%\nin MSE and MAE, respectively.\n","authors":["Xiaobing Yuan","Ling Chen"],"pdf_url":"https://arxiv.org/pdf/2403.17814v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.12261v2","updated":"2024-03-26T15:52:06Z","published":"2024-01-22T00:37:01Z","title":"Analyzing the Quality Attributes of AI Vision Models in Open\n  Repositories Under Adversarial Attacks","summary":"  As AI models rapidly evolve, they are frequently released to open\nrepositories, such as HuggingFace. It is essential to perform quality assurance\nvalidation on these models before integrating them into the production\ndevelopment lifecycle. In addition to evaluating efficiency in terms of\nbalanced accuracy and computing costs, adversarial attacks are potential\nthreats to the robustness and explainability of AI models. Meanwhile, XAI\napplies algorithms that approximate inputs to outputs post-hoc to identify the\ncontributing features. Adversarial perturbations may also degrade the utility\nof XAI explanations that require further investigation. In this paper, we\npresent an integrated process designed for downstream evaluation tasks,\nincluding validating AI model accuracy, evaluating robustness with benchmark\nperturbations, comparing explanation utility, and assessing overhead. We\ndemonstrate an evaluation scenario involving six computer vision models, which\ninclude CNN-based, Transformer-based, and hybrid architectures, three types of\nperturbations, and five XAI methods, resulting in ninety unique combinations.\nThe process reveals the explanation utility among the XAI methods in terms of\nthe identified key areas responding to the adversarial perturbation. The\nprocess produces aggregated results that illustrate multiple attributes of each\nAI model.\n","authors":["Zerui Wang","Yan Liu"],"pdf_url":"https://arxiv.org/pdf/2401.12261v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.11911v4","updated":"2024-03-26T15:47:14Z","published":"2024-01-22T12:54:04Z","title":"Blinded by Generated Contexts: How Language Models Merge Generated and\n  Retrieved Contexts for Open-Domain QA?","summary":"  While auxiliary information has become a key to enhancing Large Language\nModels (LLMs), relatively little is known about how LLMs merge these contexts,\nspecifically contexts generated by LLMs and those retrieved from external\nsources. To investigate this, we formulate a systematic framework to identify\nwhether LLMs' responses, derived from the integration of generated and\nretrieved contexts, are attributed to either generated or retrieved contexts.\nTo easily trace the origin of the response, we construct datasets with\nconflicting contexts, i.e., each question is paired with both generated and\nretrieved contexts, yet only one of them contains the correct answer. Our\nexperiments reveal a significant bias in several LLMs (GPT-4/3.5 and Llama2) to\nfavor generated contexts, even when they provide incorrect information. We\nfurther identify two key factors contributing to this bias: i) contexts\ngenerated by LLMs typically show greater similarity to the questions,\nincreasing their likelihood of being selected; ii) the segmentation process\nused in retrieved contexts disrupts their completeness, thereby hindering their\nfull utilization in LLMs. Our analysis enhances the understanding of how LLMs\nmerge diverse contexts, offering valuable insights for advancing current\naugmentation methods for LLMs.\n","authors":["Hexiang Tan","Fei Sun","Wanli Yang","Yuanzhuo Wang","Qi Cao","Xueqi Cheng"],"pdf_url":"https://arxiv.org/pdf/2401.11911v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.03788v4","updated":"2024-03-26T15:41:28Z","published":"2023-02-07T22:56:58Z","title":"Toward a Theory of Causation for Interpreting Neural Code Models","summary":"  Neural Language Models of Code, or Neural Code Models (NCMs), are rapidly\nprogressing from research prototypes to commercial developer tools. As such,\nunderstanding the capabilities and limitations of such models is becoming\ncritical. However, the abilities of these models are typically measured using\nautomated metrics that often only reveal a portion of their real-world\nperformance. While, in general, the performance of NCMs appears promising,\ncurrently much is unknown about how such models arrive at decisions. To this\nend, this paper introduces $do_{code}$, a post hoc interpretability method\nspecific to NCMs that is capable of explaining model predictions. $do_{code}$\nis based upon causal inference to enable programming language-oriented\nexplanations. While the theoretical underpinnings of $do_{code}$ are extensible\nto exploring different model properties, we provide a concrete instantiation\nthat aims to mitigate the impact of spurious correlations by grounding\nexplanations of model behavior in properties of programming languages. To\ndemonstrate the practical benefit of $do_{code}$, we illustrate the insights\nthat our framework can provide by performing a case study on two popular deep\nlearning architectures and ten NCMs. The results of this case study illustrate\nthat our studied NCMs are sensitive to changes in code syntax. All our NCMs,\nexcept for the BERT-like model, statistically learn to predict tokens related\nto blocks of code (\\eg brackets, parenthesis, semicolon) with less confounding\nbias as compared to other programming language constructs. These insights\ndemonstrate the potential of $do_{code}$ as a useful method to detect and\nfacilitate the elimination of confounding bias in NCMs.\n","authors":["David N. Palacio","Alejandro Velasco","Nathan Cooper","Alvaro Rodriguez","Kevin Moran","Denys Poshyvanyk"],"pdf_url":"https://arxiv.org/pdf/2302.03788v4.pdf","comment":"Accepted to appear in IEEE Transactions on Software Engineering"},{"id":"http://arxiv.org/abs/2403.16222v2","updated":"2024-03-26T15:28:27Z","published":"2024-03-24T16:30:05Z","title":"Cyber-Security Knowledge Graph Generation by Hierarchical Nonnegative\n  Matrix Factorization","summary":"  Much of human knowledge in cybersecurity is encapsulated within the\never-growing volume of scientific papers. As this textual data continues to\nexpand, the importance of document organization methods becomes increasingly\ncrucial for extracting actionable insights hidden within large text datasets.\nKnowledge Graphs (KGs) serve as a means to store factual information in a\nstructured manner, providing explicit, interpretable knowledge that includes\ndomain-specific information from the cybersecurity scientific literature. One\nof the challenges in constructing a KG from scientific literature is the\nextraction of ontology from unstructured text. In this paper, we address this\ntopic and introduce a method for building a multi-modal KG by extracting\nstructured ontology from scientific papers. We demonstrate this concept in the\ncybersecurity domain. One modality of the KG represents observable information\nfrom the papers, such as the categories in which they were published or the\nauthors. The second modality uncovers latent (hidden) patterns of text\nextracted through hierarchical and semantic non-negative matrix factorization\n(NMF), such as named entities, topics or clusters, and keywords. We illustrate\nthis concept by consolidating more than two million scientific papers uploaded\nto arXiv into the cyber-domain, using hierarchical and semantic NMF, and by\nbuilding a cyber-domain-specific KG.\n","authors":["Ryan Barron","Maksim E. Eren","Manish Bhattarai","Selma Wanna","Nicholas Solovyev","Kim Rasmussen","Boian S. Alexandrov","Charles Nicholas","Cynthia Matuszek"],"pdf_url":"https://arxiv.org/pdf/2403.16222v2.pdf","comment":"Accepted at IEEE ISDFS"},{"id":"http://arxiv.org/abs/2403.01748v2","updated":"2024-03-26T15:26:21Z","published":"2024-03-04T05:55:01Z","title":"Decode Neural signal as Speech","summary":"  Decoding language from brain dynamics is an important open direction in the\nrealm of brain-computer interface (BCI), especially considering the rapid\ngrowth of large language models. Compared to invasive-based signals which\nrequire electrode implantation surgery, non-invasive neural signals (e.g. EEG,\nMEG) have attracted increasing attention considering their safety and\ngenerality. However, the exploration is not adequate in three aspects: 1)\nprevious methods mainly focus on EEG but none of the previous works address\nthis problem on MEG with better signal quality; 2) prior works have\npredominantly used ``teacher-forcing\" during generative decoding, which is\nimpractical; 3) prior works are mostly ``BART-based\" not fully auto-regressive,\nwhich performs better in other sequence tasks. In this paper, we explore the\nbrain-to-text translation of MEG signals in a speech-decoding formation. Here\nwe are the first to investigate a cross-attention-based ``whisper\" model for\ngenerating text directly from MEG signals without teacher forcing. Our model\nachieves impressive BLEU-1 scores of 60.30 and 52.89 without pretraining \\&\nteacher-forcing on two major datasets (\\textit{GWilliams} and\n\\textit{Schoffelen}). This paper conducts a comprehensive review to understand\nhow speech decoding formation performs on the neural decoding tasks, including\npretraining initialization, training \\& evaluation set splitting, augmentation,\nand scaling law.\n","authors":["Yiqian Yang","Yiqun Duan","Qiang Zhang","Renjing Xu","Hui Xiong"],"pdf_url":"https://arxiv.org/pdf/2403.01748v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17787v1","updated":"2024-03-26T15:20:49Z","published":"2024-03-26T15:20:49Z","title":"Evaluating the Efficacy of Prompt-Engineered Large Multimodal Models\n  Versus Fine-Tuned Vision Transformers in Image-Based Security Applications","summary":"  The success of Large Language Models (LLMs) has led to a parallel rise in the\ndevelopment of Large Multimodal Models (LMMs), such as Gemini-pro, which have\nbegun to transform a variety of applications. These sophisticated multimodal\nmodels are designed to interpret and analyze complex data, integrating both\ntextual and visual information on a scale previously unattainable, opening new\navenues for a range of applications. This paper investigates the applicability\nand effectiveness of prompt-engineered Gemini-pro LMMs versus fine-tuned Vision\nTransformer (ViT) models in addressing critical security challenges. We focus\non two distinct tasks: a visually evident task of detecting simple triggers,\nsuch as small squares in images, indicative of potential backdoors, and a\nnon-visually evident task of malware classification through visual\nrepresentations. Our results highlight a significant divergence in performance,\nwith Gemini-pro falling short in accuracy and reliability when compared to\nfine-tuned ViT models. The ViT models, on the other hand, demonstrate\nexceptional accuracy, achieving near-perfect performance on both tasks. This\nstudy not only showcases the strengths and limitations of prompt-engineered\nLMMs in cybersecurity applications but also emphasizes the unmatched efficacy\nof fine-tuned ViT models for precise and dependable tasks.\n","authors":["Fouad Trad","Ali Chehab"],"pdf_url":"https://arxiv.org/pdf/2403.17787v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16760v2","updated":"2024-03-26T15:17:51Z","published":"2024-03-25T13:39:33Z","title":"As Good As A Coin Toss: Human detection of AI-generated images, videos,\n  audio, and audiovisual stimuli","summary":"  As synthetic media becomes progressively more realistic and barriers to using\nit continue to lower, the technology has been increasingly utilized for\nmalicious purposes, from financial fraud to nonconsensual pornography. Today,\nthe principal defense against being misled by synthetic media relies on the\nability of the human observer to visually and auditorily discern between real\nand fake. However, it remains unclear just how vulnerable people actually are\nto deceptive synthetic media in the course of their day to day lives. We\nconducted a perceptual study with 1276 participants to assess how accurate\npeople were at distinguishing synthetic images, audio only, video only, and\naudiovisual stimuli from authentic. To reflect the circumstances under which\npeople would likely encounter synthetic media in the wild, testing conditions\nand stimuli emulated a typical online platform, while all synthetic media used\nin the survey was sourced from publicly accessible generative AI technology.\n  We find that overall, participants struggled to meaningfully discern between\nsynthetic and authentic content. We also find that detection performance\nworsens when the stimuli contains synthetic content as compared to authentic\ncontent, images featuring human faces as compared to non face objects, a single\nmodality as compared to multimodal stimuli, mixed authenticity as compared to\nbeing fully synthetic for audiovisual stimuli, and features foreign languages\nas compared to languages the observer is fluent in. Finally, we also find that\nprior knowledge of synthetic media does not meaningfully impact their detection\nperformance. Collectively, these results indicate that people are highly\nsusceptible to being tricked by synthetic media in their daily lives and that\nhuman perceptual detection capabilities can no longer be relied upon as an\neffective counterdefense.\n","authors":["Di Cooke","Abigail Edwards","Sophia Barkoff","Kathryn Kelly"],"pdf_url":"https://arxiv.org/pdf/2403.16760v2.pdf","comment":"For study pre-registration, see https://osf.io/fnhr3"},{"id":"http://arxiv.org/abs/2403.17784v1","updated":"2024-03-26T15:16:14Z","published":"2024-03-26T15:16:14Z","title":"SciCapenter: Supporting Caption Composition for Scientific Figures with\n  Machine-Generated Captions and Ratings","summary":"  Crafting effective captions for figures is important. Readers heavily depend\non these captions to grasp the figure's message. However, despite a\nwell-developed set of AI technologies for figures and captions, these have\nrarely been tested for usefulness in aiding caption writing. This paper\nintroduces SciCapenter, an interactive system that puts together cutting-edge\nAI technologies for scientific figure captions to aid caption composition.\nSciCapenter generates a variety of captions for each figure in a scholarly\narticle, providing scores and a comprehensive checklist to assess caption\nquality across multiple critical aspects, such as helpfulness, OCR mention, key\ntakeaways, and visual properties reference. Users can directly edit captions in\nSciCapenter, resubmit for revised evaluations, and iteratively refine them. A\nuser study with Ph.D. students indicates that SciCapenter significantly lowers\nthe cognitive load of caption writing. Participants' feedback further offers\nvaluable design insights for future systems aiming to enhance caption writing.\n","authors":["Ting-Yao Hsu","Chieh-Yang Huang","Shih-Hong Huang","Ryan Rossi","Sungchul Kim","Tong Yu","C. Lee Giles","Ting-Hao K. Huang"],"pdf_url":"https://arxiv.org/pdf/2403.17784v1.pdf","comment":"CHI EA '24: Extended Abstracts of the 2024 CHI Conference on Human\n  Factors in Computing Systems"},{"id":"http://arxiv.org/abs/2306.15909v4","updated":"2024-03-26T15:13:20Z","published":"2023-06-28T04:16:16Z","title":"RL$^3$: Boosting Meta Reinforcement Learning via RL inside RL$^2$","summary":"  Meta reinforcement learning (meta-RL) methods such as RL$^2$ have emerged as\npromising approaches for learning data-efficient RL algorithms tailored to a\ngiven task distribution. However, they show poor asymptotic performance and\nstruggle with out-of-distribution tasks because they rely on sequence models,\nsuch as recurrent neural networks or transformers, to process experiences\nrather than summarize them using general-purpose RL components such as value\nfunctions. In contrast, traditional RL algorithms are data-inefficient as they\ndo not use domain knowledge, but they do converge to an optimal policy in the\nlimit. We propose RL$^3$, a principled hybrid approach that incorporates\naction-values, learned per task through traditional RL, in the inputs to\nmeta-RL. We show that RL$^3$ earns greater cumulative reward in the long term,\ncompared to RL$^2$, while maintaining data-efficiency in the short term, and\ngeneralizes better to out-of-distribution tasks. Experiments are conducted on\nboth custom and benchmark discrete domains from the meta-RL literature that\nexhibit a range of short-term, long-term, and complex dependencies.\n","authors":["Abhinav Bhatia","Samer B. Nashed","Shlomo Zilberstein"],"pdf_url":"https://arxiv.org/pdf/2306.15909v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.12243v4","updated":"2024-03-26T15:12:19Z","published":"2023-08-23T16:42:27Z","title":"Multi-Objective Optimization for Sparse Deep Multi-Task Learning","summary":"  Different conflicting optimization criteria arise naturally in various Deep\nLearning scenarios. These can address different main tasks (i.e., in the\nsetting of Multi-Task Learning), but also main and secondary tasks such as loss\nminimization versus sparsity. The usual approach is a simple weighting of the\ncriteria, which formally only works in the convex setting. In this paper, we\npresent a Multi-Objective Optimization algorithm using a modified Weighted\nChebyshev scalarization for training Deep Neural Networks (DNNs) with respect\nto several tasks. By employing this scalarization technique, the algorithm can\nidentify all optimal solutions of the original problem while reducing its\ncomplexity to a sequence of single-objective problems. The simplified problems\nare then solved using an Augmented Lagrangian method, enabling the use of\npopular optimization techniques such as Adam and Stochastic Gradient Descent,\nwhile efficaciously handling constraints. Our work aims to address the\n(economical and also ecological) sustainability issue of DNN models, with a\nparticular focus on Deep Multi-Task models, which are typically designed with a\nvery large number of weights to perform equally well on multiple tasks. Through\nexperiments conducted on two Machine Learning datasets, we demonstrate the\npossibility of adaptively sparsifying the model during training without\nsignificantly impacting its performance, if we are willing to apply\ntask-specific adaptations to the network weights. Code is available at\nhttps://github.com/salomonhotegni/MDMTN\n","authors":["S. S. Hotegni","M. Berkemeier","S. Peitz"],"pdf_url":"https://arxiv.org/pdf/2308.12243v4.pdf","comment":"12 pages, 7 figures"},{"id":"http://arxiv.org/abs/2403.17778v1","updated":"2024-03-26T15:11:18Z","published":"2024-03-26T15:11:18Z","title":"Towards a FAIR Documentation of Workflows and Models in Applied\n  Mathematics","summary":"  Modeling-Simulation-Optimization workflows play a fundamental role in applied\nmathematics. The Mathematical Research Data Initiative, MaRDI, responded to\nthis by developing a FAIR and machine-interpretable template for a\ncomprehensive documentation of such workflows. MaRDMO, a Plugin for the\nResearch Data Management Organiser, enables scientists from diverse fields to\ndocument and publish their workflows on the MaRDI Portal seamlessly using the\nMaRDI template. Central to these workflows are mathematical models. MaRDI\naddresses them with the MathModDB ontology, offering a structured formal model\ndescription. Here, we showcase the interaction between MaRDMO and the MathModDB\nKnowledge Graph through an algebraic modeling workflow from the Digital\nHumanities. This demonstration underscores the versatility of both services\nbeyond their original numerical domain.\n","authors":["Marco Reidelbach","Björn Schembera","Marcus Weber"],"pdf_url":"https://arxiv.org/pdf/2403.17778v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17768v1","updated":"2024-03-26T14:54:48Z","published":"2024-03-26T14:54:48Z","title":"SciNews: From Scholarly Complexities to Public Narratives -- A Dataset\n  for Scientific News Report Generation","summary":"  Scientific news reports serve as a bridge, adeptly translating complex\nresearch articles into reports that resonate with the broader public. The\nautomated generation of such narratives enhances the accessibility of scholarly\ninsights. In this paper, we present a new corpus to facilitate this paradigm\ndevelopment. Our corpus comprises a parallel compilation of academic\npublications and their corresponding scientific news reports across nine\ndisciplines. To demonstrate the utility and reliability of our dataset, we\nconduct an extensive analysis, highlighting the divergences in readability and\nbrevity between scientific news narratives and academic manuscripts. We\nbenchmark our dataset employing state-of-the-art text generation models. The\nevaluation process involves both automatic and human evaluation, which lays the\ngroundwork for future explorations into the automated generation of scientific\nnews reports. The dataset and code related to this work are available at\nhttps://dongqi.me/projects/SciNews.\n","authors":["Dongqi Pu","Yifan Wang","Jia Loy","Vera Demberg"],"pdf_url":"https://arxiv.org/pdf/2403.17768v1.pdf","comment":"LREC-COLING 2024 Main Conference Paper"},{"id":"http://arxiv.org/abs/2403.15585v2","updated":"2024-03-26T14:51:57Z","published":"2024-03-22T19:19:51Z","title":"MedPromptX: Grounded Multimodal Prompting for Chest X-ray Diagnosis","summary":"  Chest X-ray images are commonly used for predicting acute and chronic\ncardiopulmonary conditions, but efforts to integrate them with structured\nclinical data face challenges due to incomplete electronic health records\n(EHR). This paper introduces \\textbf{MedPromptX}, the first model to integrate\nmultimodal large language models (MLLMs), few-shot prompting (FP) and visual\ngrounding (VG) to combine imagery with EHR data for chest X-ray diagnosis. A\npre-trained MLLM is utilized to complement the missing EHR information,\nproviding a comprehensive understanding of patients' medical history.\nAdditionally, FP reduces the necessity for extensive training of MLLMs while\neffectively tackling the issue of hallucination. Nevertheless, the process of\ndetermining the optimal number of few-shot examples and selecting high-quality\ncandidates can be burdensome, yet it profoundly influences model performance.\nHence, we propose a new technique that dynamically refines few-shot data for\nreal-time adjustment to new patient scenarios. Moreover, VG aids in focusing\nthe model's attention on relevant regions of interest in X-ray images,\nenhancing the identification of abnormalities. We release MedPromptX-VQA, a new\nin-context visual question answering dataset encompassing interleaved image and\nEHR data derived from MIMIC-IV and MIMIC-CXR databases. Results demonstrate the\nSOTA performance of MedPromptX, achieving an 11% improvement in F1-score\ncompared to the baselines. Code and data are available at\nhttps://github.com/BioMedIA-MBZUAI/MedPromptX\n","authors":["Mai A. Shaaban","Adnan Khan","Mohammad Yaqub"],"pdf_url":"https://arxiv.org/pdf/2403.15585v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11996v2","updated":"2024-03-26T14:46:04Z","published":"2024-03-18T17:30:27Z","title":"Accelerating Scientific Discovery with Generative Knowledge Extraction,\n  Graph-Based Representation, and Multimodal Intelligent Graph Reasoning","summary":"  Leveraging generative Artificial Intelligence (AI), we have transformed a\ndataset comprising 1,000 scientific papers into an ontological knowledge graph.\nThrough an in-depth structural analysis, we have calculated node degrees,\nidentified communities and connectivities, and evaluated clustering\ncoefficients and betweenness centrality of pivotal nodes, uncovering\nfascinating knowledge architectures. The graph has an inherently scale-free\nnature, is highly connected, and can be used for graph reasoning by taking\nadvantage of transitive and isomorphic properties that reveal unprecedented\ninterdisciplinary relationships that can be used to answer queries, identify\ngaps in knowledge, propose never-before-seen material designs, and predict\nmaterial behaviors. We compute deep node embeddings for combinatorial node\nsimilarity ranking for use in a path sampling strategy links dissimilar\nconcepts that have previously not been related. One comparison revealed\nstructural parallels between biological materials and Beethoven's 9th Symphony,\nhighlighting shared patterns of complexity through isomorphic mapping. In\nanother example, the algorithm proposed a hierarchical mycelium-based composite\nbased on integrating path sampling with principles extracted from Kandinsky's\n'Composition VII' painting. The resulting material integrates an innovative set\nof concepts that include a balance of chaos/order, adjustable porosity,\nmechanical strength, and complex patterned chemical functionalization. We\nuncover other isomorphisms across science, technology and art, revealing a\nnuanced ontology of immanence that reveal a context-dependent heterarchical\ninterplay of constituents. Graph-based generative AI achieves a far higher\ndegree of novelty, explorative capacity, and technical detail, than\nconventional approaches and establishes a widely useful framework for\ninnovation by revealing hidden connections.\n","authors":["Markus J. Buehler"],"pdf_url":"https://arxiv.org/pdf/2403.11996v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17755v1","updated":"2024-03-26T14:44:51Z","published":"2024-03-26T14:44:51Z","title":"DataCook: Crafting Anti-Adversarial Examples for Healthcare Data\n  Copyright Protection","summary":"  In the realm of healthcare, the challenges of copyright protection and\nunauthorized third-party misuse are increasingly significant. Traditional\nmethods for data copyright protection are applied prior to data distribution,\nimplying that models trained on these data become uncontrollable. This paper\nintroduces a novel approach, named DataCook, designed to safeguard the\ncopyright of healthcare data during the deployment phase. DataCook operates by\n\"cooking\" the raw data before distribution, enabling the development of models\nthat perform normally on this processed data. However, during the deployment\nphase, the original test data must be also \"cooked\" through DataCook to ensure\nnormal model performance. This process grants copyright holders control over\nauthorization during the deployment phase. The mechanism behind DataCook is by\ncrafting anti-adversarial examples (AntiAdv), which are designed to enhance\nmodel confidence, as opposed to standard adversarial examples (Adv) that aim to\nconfuse models. Similar to Adv, AntiAdv introduces imperceptible perturbations,\nensuring that the data processed by DataCook remains easily understandable. We\nconducted extensive experiments on MedMNIST datasets, encompassing both 2D/3D\ndata and the high-resolution variants. The outcomes indicate that DataCook\neffectively meets its objectives, preventing models trained on AntiAdv from\nanalyzing unauthorized data effectively, without compromising the validity and\naccuracy of the data in legitimate scenarios. Code and data are available at\nhttps://github.com/MedMNIST/DataCook.\n","authors":["Sihan Shang","Jiancheng Yang","Zhenglong Sun","Pascal Fua"],"pdf_url":"https://arxiv.org/pdf/2403.17755v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06247v2","updated":"2024-03-26T14:42:21Z","published":"2024-03-10T16:11:17Z","title":"Text-Guided Variational Image Generation for Industrial Anomaly\n  Detection and Segmentation","summary":"  We propose a text-guided variational image generation method to address the\nchallenge of getting clean data for anomaly detection in industrial\nmanufacturing. Our method utilizes text information about the target object,\nlearned from extensive text library documents, to generate non-defective data\nimages resembling the input image. The proposed framework ensures that the\ngenerated non-defective images align with anticipated distributions derived\nfrom textual and image-based knowledge, ensuring stability and generality.\nExperimental results demonstrate the effectiveness of our approach, surpassing\nprevious methods even with limited non-defective data. Our approach is\nvalidated through generalization tests across four baseline models and three\ndistinct datasets. We present an additional analysis to enhance the\neffectiveness of anomaly detection models by utilizing the generated images.\n","authors":["Mingyu Lee","Jongwon Choi"],"pdf_url":"https://arxiv.org/pdf/2403.06247v2.pdf","comment":"18 pages, Accepted to CVPR 2024"},{"id":"http://arxiv.org/abs/2310.02129v4","updated":"2024-03-26T14:38:23Z","published":"2023-10-03T15:10:46Z","title":"Unveiling the Pitfalls of Knowledge Editing for Large Language Models","summary":"  As the cost associated with fine-tuning Large Language Models (LLMs)\ncontinues to rise, recent research efforts have pivoted towards developing\nmethodologies to edit implicit knowledge embedded within LLMs. Yet, there's\nstill a dark cloud lingering overhead -- will knowledge editing trigger\nbutterfly effect? since it is still unclear whether knowledge editing might\nintroduce side effects that pose potential risks or not. This paper pioneers\nthe investigation into the potential pitfalls associated with knowledge editing\nfor LLMs. To achieve this, we introduce new benchmark datasets and propose\ninnovative evaluation metrics. Our results underline two pivotal concerns: (1)\nKnowledge Conflict: Editing groups of facts that logically clash can magnify\nthe inherent inconsistencies in LLMs-a facet neglected by previous methods. (2)\nKnowledge Distortion: Altering parameters with the aim of editing factual\nknowledge can irrevocably warp the innate knowledge structure of LLMs.\nExperimental results vividly demonstrate that knowledge editing might\ninadvertently cast a shadow of unintended consequences on LLMs, which warrant\nattention and efforts for future works. Code and data are available at\nhttps://github.com/zjunlp/PitfallsKnowledgeEditing.\n","authors":["Zhoubo Li","Ningyu Zhang","Yunzhi Yao","Mengru Wang","Xi Chen","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2310.02129v4.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2307.05300v4","updated":"2024-03-26T14:32:33Z","published":"2023-07-11T14:45:19Z","title":"Unleashing the Emergent Cognitive Synergy in Large Language Models: A\n  Task-Solving Agent through Multi-Persona Self-Collaboration","summary":"  Human intelligence thrives on cognitive synergy, where collaboration among\ndifferent minds yield superior outcomes compared to isolated individuals. In\nthis work, we propose Solo Performance Prompting (SPP), which transforms a\nsingle LLM into a cognitive synergist by engaging in multi-turn\nself-collaboration with multiple personas. A cognitive synergist is an\nintelligent agent that collaboratively combines multiple minds' strengths and\nknowledge to enhance problem-solving in complex tasks. By dynamically\nidentifying and simulating different personas based on task inputs, SPP\nunleashes the potential of cognitive synergy in LLMs. Our in-depth analysis\nshows that assigning multiple fine-grained personas in LLMs improves\nproblem-solving abilities compared to using a single or fixed number of\npersonas. We evaluate SPP on three challenging tasks: Trivia Creative Writing,\nCodenames Collaborative, and Logic Grid Puzzle, encompassing both\nknowledge-intensive and reasoning-intensive types. Unlike previous works, such\nas Chain-of-Thought, that solely enhance the reasoning abilities in LLMs,\nexperimental results demonstrate that SPP effectively reduces factual\nhallucination, and maintains strong reasoning capabilities. Additionally,\ncomparative experiments show that cognitive synergy only emerges in GPT-4 and\ndoes not appear in less capable models, such as GPT-3.5-turbo and\nLlama2-13b-chat, which draws an interesting analogy to human development. Code,\ndata, and prompts can be found at:\nhttps://github.com/MikeWangWZHL/Solo-Performance-Prompting.git.\n","authors":["Zhenhailong Wang","Shaoguang Mao","Wenshan Wu","Tao Ge","Furu Wei","Heng Ji"],"pdf_url":"https://arxiv.org/pdf/2307.05300v4.pdf","comment":"Accepted as a main conference paper at NAACL 2024"},{"id":"http://arxiv.org/abs/2403.17742v1","updated":"2024-03-26T14:30:23Z","published":"2024-03-26T14:30:23Z","title":"Using Stratified Sampling to Improve LIME Image Explanations","summary":"  We investigate the use of a stratified sampling approach for LIME Image, a\npopular model-agnostic explainable AI method for computer vision tasks, in\norder to reduce the artifacts generated by typical Monte Carlo sampling. Such\nartifacts are due to the undersampling of the dependent variable in the\nsynthetic neighborhood around the image being explained, which may result in\ninadequate explanations due to the impossibility of fitting a linear regressor\non the sampled data. We then highlight a connection with the Shapley theory,\nwhere similar arguments about undersampling and sample relevance were suggested\nin the past. We derive all the formulas and adjustment factors required for an\nunbiased stratified sampling estimator. Experiments show the efficacy of the\nproposed approach.\n","authors":["Muhammad Rashid","Elvio G. Amparore","Enrico Ferrari","Damiano Verda"],"pdf_url":"https://arxiv.org/pdf/2403.17742v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17740v1","updated":"2024-03-26T14:29:34Z","published":"2024-03-26T14:29:34Z","title":"All-in-One: Heterogeneous Interaction Modeling for Cold-Start Rating\n  Prediction","summary":"  Cold-start rating prediction is a fundamental problem in recommender systems\nthat has been extensively studied. Many methods have been proposed that exploit\nexplicit relations among existing data, such as collaborative filtering, social\nrecommendations and heterogeneous information network, to alleviate the data\ninsufficiency issue for cold-start users and items. However, the explicit\nrelations constructed based on data between different roles may be unreliable\nand irrelevant, which limits the performance ceiling of the specific\nrecommendation task. Motivated by this, in this paper, we propose a flexible\nframework dubbed heterogeneous interaction rating network (HIRE). HIRE dose not\nsolely rely on the pre-defined interaction pattern or the manually constructed\nheterogeneous information network. Instead, we devise a Heterogeneous\nInteraction Module (HIM) to jointly model the heterogeneous interactions and\ndirectly infer the important interactions via the observed data. In the\nexperiments, we evaluate our model under three cold-start settings on three\nreal-world datasets. The experimental results show that HIRE outperforms other\nbaselines by a large margin. Furthermore, we visualize the inferred\ninteractions of HIRE to confirm the contribution of our model.\n","authors":["Shuheng Fang","Kangfei Zhao","Yu Rong","Zhixun Li","Jeffrey Xu Yu"],"pdf_url":"https://arxiv.org/pdf/2403.17740v1.pdf","comment":"14 pages, 9 figures"},{"id":"http://arxiv.org/abs/2311.02099v4","updated":"2024-03-26T14:25:52Z","published":"2023-10-30T21:52:37Z","title":"A Safe Preference Learning Approach for Personalization with\n  Applications to Autonomous Vehicles","summary":"  This work introduces a preference learning method that ensures adherence to\ngiven specifications, with an application to autonomous vehicles. Our approach\nincorporates the priority ordering of Signal Temporal Logic (STL) formulas\ndescribing traffic rules into a learning framework. By leveraging Parametric\nWeighted Signal Temporal Logic (PWSTL), we formulate the problem of\nsafety-guaranteed preference learning based on pairwise comparisons and propose\nan approach to solve this learning problem. Our approach finds a feasible\nvaluation for the weights of the given PWSTL formula such that, with these\nweights, preferred signals have weighted quantitative satisfaction measures\ngreater than their non-preferred counterparts. The feasible valuation of\nweights given by our approach leads to a weighted STL formula that can be used\nin correct-and-custom-by-construction controller synthesis. We demonstrate the\nperformance of our method with a pilot human subject study in two different\nsimulated driving scenarios involving a stop sign and a pedestrian crossing.\nOur approach yields competitive results compared to existing preference\nlearning methods in terms of capturing preferences and notably outperforms them\nwhen safety is considered.\n","authors":["Ruya Karagulle","Nikos Arechiga","Andrew Best","Jonathan DeCastro","Necmiye Ozay"],"pdf_url":"https://arxiv.org/pdf/2311.02099v4.pdf","comment":"9 pages, 3 figures, 2 tables. This work has been published at IEEE\n  Robotics and Automation Letters. Copyright may be transferred without notice,\n  after which this version may no longer be accessible"},{"id":"http://arxiv.org/abs/2403.17735v1","updated":"2024-03-26T14:24:01Z","published":"2024-03-26T14:24:01Z","title":"Out-of-distribution Rumor Detection via Test-Time Adaptation","summary":"  Due to the rapid spread of rumors on social media, rumor detection has become\nan extremely important challenge. Existing methods for rumor detection have\nachieved good performance, as they have collected enough corpus from the same\ndata distribution for model training. However, significant distribution shifts\nbetween the training data and real-world test data occur due to differences in\nnews topics, social media platforms, languages and the variance in propagation\nscale caused by news popularity. This leads to a substantial decline in the\nperformance of these existing methods in Out-Of-Distribution (OOD) situations.\nTo address this problem, we propose a simple and efficient method named\nTest-time Adaptation for Rumor Detection under distribution shifts (TARD). This\nmethod models the propagation of news in the form of a propagation graph, and\nbuilds propagation graph test-time adaptation framework, enhancing the model's\nadaptability and robustness when facing OOD problems. Extensive experiments\nconducted on two group datasets collected from real-world social platforms\ndemonstrate that our framework outperforms the state-of-the-art methods in\nperformance.\n","authors":["Xiang Tao","Mingqing Zhang","Qiang Liu","Shu Wu","Liang Wang"],"pdf_url":"https://arxiv.org/pdf/2403.17735v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17726v1","updated":"2024-03-26T14:14:30Z","published":"2024-03-26T14:14:30Z","title":"Tiny Models are the Computational Saver for Large Models","summary":"  This paper introduces TinySaver, an early-exit-like dynamic model compression\napproach which employs tiny models to substitute large models adaptively.\nDistinct from traditional compression techniques, dynamic methods like\nTinySaver can leverage the difficulty differences to allow certain inputs to\ncomplete their inference processes early, thereby conserving computational\nresources. Most existing early exit designs are implemented by attaching\nadditional network branches to the model's backbone. Our study, however,\nreveals that completely independent tiny models can replace a substantial\nportion of the larger models' job with minimal impact on performance. Employing\nthem as the first exit can remarkably enhance computational efficiency. By\nsearching and employing the most appropriate tiny model as the computational\nsaver for a given large model, the proposed approaches work as a novel and\ngeneric method to model compression. This finding will help the research\ncommunity in exploring new compression methods to address the escalating\ncomputational demands posed by rapidly evolving AI models. Our evaluation of\nthis approach in ImageNet-1k classification demonstrates its potential to\nreduce the number of compute operations by up to 90%, with only negligible\nlosses in performance, across various modern vision models. The code of this\nwork will be available.\n","authors":["Qingyuan Wang","Barry Cardiff","Antoine Frappé","Benoit Larras","Deepu John"],"pdf_url":"https://arxiv.org/pdf/2403.17726v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.05337v2","updated":"2024-03-26T14:09:56Z","published":"2023-12-08T19:52:48Z","title":"Artificial Neural Nets and the Representation of Human Concepts","summary":"  What do artificial neural networks (ANNs) learn? The machine learning (ML)\ncommunity shares the narrative that ANNs must develop abstract human concepts\nto perform complex tasks. Some go even further and believe that these concepts\nare stored in individual units of the network. Based on current research, I\nsystematically investigate the assumptions underlying this narrative. I\nconclude that ANNs are indeed capable of performing complex prediction tasks,\nand that they may learn human and non-human concepts to do so. However,\nevidence indicates that ANNs do not represent these concepts in individual\nunits.\n","authors":["Timo Freiesleben"],"pdf_url":"https://arxiv.org/pdf/2312.05337v2.pdf","comment":"For: Philosophy of Science for Machine Learning: Core Issues and New\n  Perspectives, edited by Juan Duran and Giorgia Pozzi"},{"id":"http://arxiv.org/abs/2403.17710v1","updated":"2024-03-26T13:58:00Z","published":"2024-03-26T13:58:00Z","title":"Optimization-based Prompt Injection Attack to LLM-as-a-Judge","summary":"  LLM-as-a-Judge is a novel solution that can assess textual information with\nlarge language models (LLMs). Based on existing research studies, LLMs\ndemonstrate remarkable performance in providing a compelling alternative to\ntraditional human assessment. However, the robustness of these systems against\nprompt injection attacks remains an open question. In this work, we introduce\nJudgeDeceiver, a novel optimization-based prompt injection attack tailored to\nLLM-as-a-Judge. Our method formulates a precise optimization objective for\nattacking the decision-making process of LLM-as-a-Judge and utilizes an\noptimization algorithm to efficiently automate the generation of adversarial\nsequences, achieving targeted and effective manipulation of model evaluations.\nCompared to handcraft prompt injection attacks, our method demonstrates\nsuperior efficacy, posing a significant challenge to the current security\nparadigms of LLM-based judgment systems. Through extensive experiments, we\nshowcase the capability of JudgeDeceiver in altering decision outcomes across\nvarious cases, highlighting the vulnerability of LLM-as-a-Judge systems to the\noptimization-based prompt injection attack.\n","authors":["Jiawen Shi","Zenghui Yuan","Yinuo Liu","Yue Huang","Pan Zhou","Lichao Sun","Neil Zhenqiang Gong"],"pdf_url":"https://arxiv.org/pdf/2403.17710v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17706v1","updated":"2024-03-26T13:50:34Z","published":"2024-03-26T13:50:34Z","title":"Enhanced Short Text Modeling: Leveraging Large Language Models for Topic\n  Refinement","summary":"  Crafting effective topic models for brief texts, like tweets and news\nheadlines, is essential for capturing the swift shifts in social dynamics.\nTraditional topic models, however, often fall short in accurately representing\nthe semantic intricacies of short texts due to their brevity and lack of\ncontextual data. In our study, we harness the advanced capabilities of Large\nLanguage Models (LLMs) to introduce a novel approach termed \"Topic Refinement\".\nThis approach does not directly involve itself in the initial modeling of\ntopics but focuses on improving topics after they have been mined. By employing\nprompt engineering, we direct LLMs to eliminate off-topic words within a given\ntopic, ensuring that only contextually relevant words are preserved or\nsubstituted with ones that fit better semantically. This method emulates\nhuman-like scrutiny and improvement of topics, thereby elevating the semantic\nquality of the topics generated by various models. Our comprehensive evaluation\nacross three unique datasets has shown that our topic refinement approach\nsignificantly enhances the semantic coherence of topics.\n","authors":["Shuyu Chang","Rui Wang","Peng Ren","Haiping Huang"],"pdf_url":"https://arxiv.org/pdf/2403.17706v1.pdf","comment":"6 pages, 4 figures"}],"Distributed, Parallel, and Cluster Computing":[{"id":"http://arxiv.org/abs/2403.17878v1","updated":"2024-03-26T17:10:15Z","published":"2024-03-26T17:10:15Z","title":"Empowering Data Mesh with Federated Learning","summary":"  The evolution of data architecture has seen the rise of data lakes, aiming to\nsolve the bottlenecks of data management and promote intelligent\ndecision-making. However, this centralized architecture is limited by the\nproliferation of data sources and the growing demand for timely analysis and\nprocessing. A new data paradigm, Data Mesh, is proposed to overcome these\nchallenges. Data Mesh treats domains as a first-class concern by distributing\nthe data ownership from the central team to each data domain, while keeping the\nfederated governance to monitor domains and their data products. Many\nmulti-million dollar organizations like Paypal, Netflix, and Zalando have\nalready transformed their data analysis pipelines based on this new\narchitecture. In this decentralized architecture where data is locally\npreserved by each domain team, traditional centralized machine learning is\nincapable of conducting effective analysis across multiple domains, especially\nfor security-sensitive organizations. To this end, we introduce a pioneering\napproach that incorporates Federated Learning into Data Mesh. To the best of\nour knowledge, this is the first open-source applied work that represents a\ncritical advancement toward the integration of federated learning methods into\nthe Data Mesh paradigm, underscoring the promising prospects for\nprivacy-preserving and decentralized data analysis strategies within Data Mesh\narchitecture.\n","authors":["Haoyuan Li","Salman Toor"],"pdf_url":"https://arxiv.org/pdf/2403.17878v1.pdf","comment":"In Proceedings of ACM Knowledge Discovery and Data Mining, Barcelona,\n  Spain, 25th - 29th August, 2024 (Conference acronym KDD), 9 pages"},{"id":"http://arxiv.org/abs/2403.17863v1","updated":"2024-03-26T16:50:44Z","published":"2024-03-26T16:50:44Z","title":"An AI-Native Runtime for Multi-Wearable Environments","summary":"  The miniaturization of AI accelerators is paving the way for next-generation\nwearable applications within wearable technologies. We introduce Mojito, an\nAI-native runtime with advanced MLOps designed to facilitate the development\nand deployment of these applications on wearable devices. It emphasizes the\nnecessity of dynamic orchestration of distributed resources equipped with\nultra-low-power AI accelerators to overcome challenges associated with\nunpredictable runtime environments. Through its innovative approaches, Mojito\ndemonstrates how future wearable technologies can evolve to be more autonomous.\n","authors":["Chulhong Min","Utku Günay Acer","SiYoung Jang","Sangwon Choi","Diana A. Vasile","Taesik Gong","Juheon Yi","Fahim Kawsar"],"pdf_url":"https://arxiv.org/pdf/2403.17863v1.pdf","comment":"7 pages, 4 figures"},{"id":"http://arxiv.org/abs/2401.07788v2","updated":"2024-03-26T16:49:44Z","published":"2024-01-15T15:54:54Z","title":"Activations and Gradients Compression for Model-Parallel Training","summary":"  Large neural networks require enormous computational clusters of machines.\nModel-parallel training, when the model architecture is partitioned\nsequentially between workers, is a popular approach for training modern models.\nInformation compression can be applied to decrease workers communication time,\nas it is often a bottleneck in such systems. This work explores how\nsimultaneous compression of activations and gradients in model-parallel\ndistributed training setup affects convergence. We analyze compression methods\nsuch as quantization and TopK compression, and also experiment with error\ncompensation techniques. Moreover, we employ TopK with AQ-SGD per-batch error\nfeedback approach. We conduct experiments on image classification and language\nmodel fine-tuning tasks. Our findings demonstrate that gradients require milder\ncompression rates than activations. We observe that $K=10\\%$ is the lowest TopK\ncompression level, which does not harm model convergence severely. Experiments\nalso show that models trained with TopK perform well only when compression is\nalso applied during inference. We find that error feedback techniques do not\nimprove model-parallel training compared to plain compression, but allow model\ninference without compression with almost no quality drop. Finally, when\napplied with the AQ-SGD approach, TopK stronger than with $ K=30\\%$ worsens\nmodel performance significantly.\n","authors":["Mikhail Rudakov","Aleksandr Beznosikov","Yaroslav Kholodov","Alexander Gasnikov"],"pdf_url":"https://arxiv.org/pdf/2401.07788v2.pdf","comment":"17 pages, 6 figures, 5 tables"},{"id":"http://arxiv.org/abs/2403.17833v1","updated":"2024-03-26T16:14:43Z","published":"2024-03-26T16:14:43Z","title":"GPFL: A Gradient Projection-Based Client Selection Framework for\n  Efficient Federated Learning","summary":"  Federated learning client selection is crucial for determining participant\nclients while balancing model accuracy and communication efficiency. Existing\nmethods have limitations in handling data heterogeneity, computational burdens,\nand independent client treatment. To address these challenges, we propose GPFL,\nwhich measures client value by comparing local and global descent directions.\nWe also employ an Exploit-Explore mechanism to enhance performance.\nExperimental results on FEMINST and CIFAR-10 datasets demonstrate that GPFL\noutperforms baselines in Non-IID scenarios, achieving over 9\\% improvement in\nFEMINST test accuracy. Moreover, GPFL exhibits shorter computation times\nthrough pre-selection and parameter reuse in federated learning.\n","authors":["Shijie Na","Yuzhi Liang","Siu-Ming Yiu"],"pdf_url":"https://arxiv.org/pdf/2403.17833v1.pdf","comment":"8 pages, 5 figures"},{"id":"http://arxiv.org/abs/2310.00226v2","updated":"2024-03-26T16:00:34Z","published":"2023-09-30T02:10:04Z","title":"A simple GPU implementation of spectral-element methods for solving 3D\n  Poisson type equations on rectangular domains and its applications","summary":"  It is well known since 1960s that by exploring the tensor product structure\nof the discrete Laplacian on Cartesian meshes, one can develop a simple direct\nPoisson solver with an $\\mathcal O(N^{\\frac{d+1}d})$ complexity in d-dimension,\nwhere N is the number of the total unknowns. The GPU acceleration of\nnumerically solving PDEs has been explored successfully around fifteen years\nago and become more and more popular in the past decade, driven by significant\nadvancement in both hardware and software technologies, especially in the\nrecent few years. We present in this paper a simple but extremely fast MATLAB\nimplementation on a modern GPU, which can be easily reproduced, for solving 3D\nPoisson type equations using a spectral-element method. In particular, it costs\nless than one second on a Nvidia A100 for solving a Poisson equation with one\nbillion degree of freedoms. We also present applications of this fast solver to\nsolve a linear (time-independent) Schr\\\"odinger equation and a nonlinear\n(time-dependent) Cahn-Hilliard equation.\n","authors":["Xinyu Liu","Jie Shen","Xiangxiong Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.00226v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17574v1","updated":"2024-03-26T10:28:41Z","published":"2024-03-26T10:28:41Z","title":"SPES: Towards Optimizing Performance-Resource Trade-Off for Serverless\n  Functions","summary":"  As an emerging cloud computing deployment paradigm, serverless computing is\ngaining traction due to its efficiency and ability to harness on-demand cloud\nresources. However, a significant hurdle remains in the form of the cold start\nproblem, causing latency when launching new function instances from scratch.\nExisting solutions tend to use over-simplistic strategies for function\npre-loading/unloading without full invocation pattern exploitation, rendering\nunsatisfactory optimization of the trade-off between cold start latency and\nresource waste. To bridge this gap, we propose SPES, the first differentiated\nscheduler for runtime cold start mitigation by optimizing serverless function\nprovision. Our insight is that the common architecture of serverless systems\nprompts the con- centration of certain invocation patterns, leading to\npredictable invocation behaviors. This allows us to categorize functions and\npre-load/unload proper function instances with finer-grained strategies based\non accurate invocation prediction. Experiments demonstrate the success of SPES\nin optimizing serverless function provision on both sides: reducing the\n75th-percentile cold start rates by 49.77% and the wasted memory time by\n56.43%, compared to the state-of-the-art. By mitigating the cold start issue,\nSPES is a promising advancement in facilitating cloud services deployed on\nserverless architectures.\n","authors":["Cheryl Lee","Zhouruixin Zhu","Tianyi Yang","Yintong Huo","Yuxin Su","Pinjia He","Michael R. Lyu"],"pdf_url":"https://arxiv.org/pdf/2403.17574v1.pdf","comment":"12 pages, accepted by ICDE 2024 (40th IEEE International Conference\n  on Data Engineering)"},{"id":"http://arxiv.org/abs/2210.15962v2","updated":"2024-03-26T07:56:26Z","published":"2022-10-28T07:49:58Z","title":"Parallel Self-Avoiding Walks for a Low-Autocorrelation Binary Sequences\n  Problem","summary":"  A low-autocorrelation binary sequences problem with a high figure of merit\nfactor represents a formidable computational challenge. An efficient parallel\ncomputing algorithm is required to reach the new best-known solutions for this\nproblem. Therefore, we developed the $\\mathit{sokol}_{\\mathit{skew}}$ solver\nfor the skew-symmetric search space. The developed solver takes the advantage\nof parallel computing on graphics processing units. The solver organized the\nsearch process as a sequence of parallel and contiguous self-avoiding walks and\nachieved a speedup factor of 387 compared with $\\mathit{lssOrel}$, its\npredecessor. The $\\mathit{sokol}_{\\mathit{skew}}$ solver belongs to stochastic\nsolvers and can not guarantee the optimality of solutions. To mitigate this\nproblem, we established the predictive model of stopping conditions according\nto the small instances for which the optimal skew-symmetric solutions are\nknown. With its help and 99% probability, the $\\mathit{sokol}_{\\mathit{skew}}$\nsolver found all the known and seven new best-known skew-symmetric sequences\nfor odd instances from $L=121$ to $L=223$. For larger instances, the solver can\nnot reach 99% probability within our limitations, but it still found several\nnew best-known binary sequences. We also analyzed the trend of the best merit\nfactor values, and it shows that as sequence size increases, the value of the\nmerit factor also increases, and this trend is flatter for larger instances.\n","authors":["Borko Bošković","Jana Herzog","Janez Brest"],"pdf_url":"https://arxiv.org/pdf/2210.15962v2.pdf","comment":"17 pages, 5 figures"},{"id":"http://arxiv.org/abs/2403.16861v2","updated":"2024-03-26T07:56:21Z","published":"2024-03-25T15:26:10Z","title":"DISL: Fueling Research with A Large Dataset of Solidity Smart Contracts","summary":"  The DISL dataset features a collection of $514,506$ unique Solidity files\nthat have been deployed to Ethereum mainnet. It caters to the need for a large\nand diverse dataset of real-world smart contracts. DISL serves as a resource\nfor developing machine learning systems and for benchmarking software\nengineering tools designed for smart contracts. By aggregating every verified\nsmart contract from Etherscan up to January 15, 2024, DISL surpasses existing\ndatasets in size and recency.\n","authors":["Gabriele Morello","Mojtaba Eshghie","Sofia Bobadilla","Martin Monperrus"],"pdf_url":"https://arxiv.org/pdf/2403.16861v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17400v1","updated":"2024-03-26T05:40:15Z","published":"2024-03-26T05:40:15Z","title":"A Survey on Resource Management in Joint Communication and\n  Computing-Embedded SAGIN","summary":"  The advent of the 6G era aims for ubiquitous connectivity, with the\nintegration of non-terrestrial networks (NTN) offering extensive coverage and\nenhanced capacity. As manufacturing advances and user demands evolve,\nspace-air-ground integrated networks (SAGIN) with computational capabilities\nemerge as a viable solution for services requiring low latency and high\ncomputational power. Resource management within joint communication and\ncomputing-embedded SAGIN (JCC-SAGIN) presents greater complexity than\ntraditional terrestrial networks. This complexity arises from the\nspatiotemporal dynamics of network topology and service demand, the\ninterdependency of large-scale resource variables, and intricate tradeoffs\namong various performance metrics. Thus, a thorough examination of resource\nmanagement strategies in JCC-SAGIN is crucial, emphasizing the role of\nnon-terrestrial platforms with processing capabilities in 6G. This paper begins\nby reviewing the architecture, enabling technologies, and applications in\nJCC-SAGIN. Then, we offer a detailed overview of resource management modeling\nand optimization methods, encompassing both traditional optimization approaches\nand learning-based intelligent decision-making frameworks. Finally, we outline\nthe prospective research directions in JCC-SAGIN.\n","authors":["Qian Chen","Zheng Guo","Weixiao Meng","Shuai Han","Cheng Li","Tony Q. S. Quek"],"pdf_url":"https://arxiv.org/pdf/2403.17400v1.pdf","comment":"43 pages, 17 figures"},{"id":"http://arxiv.org/abs/2401.01412v2","updated":"2024-03-26T05:37:13Z","published":"2024-01-02T19:28:53Z","title":"P-TimeSync: A Precise Time Synchronization Simulation with Network\n  Propagation Delays","summary":"  Time serves as the foundation of modern society and will continue to grow in\nvalue in the future world. Unlike previous research papers, authors delve into\nvarious time sources, ranging from atomic time and GPS time to quartz time.\nSpecifically, we explore the time uncertainty associated with the four major\nGlobal Navigation Satellite Systems. In existing time synchronization\nsimulations provide partial usages. However, our research introduces a\ncomprehensive and precise time synchronization simulation named P-TimeSync,\nleading to a better understanding of time synchronization in distributed\nenvironments. It is a state-of-the-art simulation tool for time because (1) it\ncan simulate atomic clocks and quartz clocks with user-defined software clock\nalgorithms, (2) the simulation provides nanosecond-level precision time across\ndifferent network propagation paths and distances, (3) the tool offers a\nvisualization platform with classic algorithms for distributed time\nsynchronization, such as Cristian's algorithm and Berkeley algorithm. The\nsimulation easily allows for the redefinition of configurations and functions,\nsupporting advanced research and development. The simulation tool could be\ndownloaded via the website: https://github.com/rui5097/purdue_timesync\n","authors":["Wei Dai","Rui Zhang","Jinwei Liu"],"pdf_url":"https://arxiv.org/pdf/2401.01412v2.pdf","comment":"6 pages, 5 figures, conference paper"},{"id":"http://arxiv.org/abs/2308.15804v2","updated":"2024-03-26T04:59:17Z","published":"2023-08-30T07:17:20Z","title":"Securing Blockchain Systems: A Novel Collaborative Learning Framework to\n  Detect Attacks in Transactions and Smart Contracts","summary":"  With the escalating prevalence of malicious activities exploiting\nvulnerabilities in blockchain systems, there is an urgent requirement for\nrobust attack detection mechanisms. To address this challenge, this paper\npresents a novel collaborative learning framework designed to detect attacks in\nblockchain transactions and smart contracts by analyzing transaction features.\nOur framework exhibits the capability to classify various types of blockchain\nattacks, including intricate attacks at the machine code level (e.g., injecting\nmalicious codes to withdraw coins from users unlawfully), which typically\nnecessitate significant time and security expertise to detect. To achieve that,\nthe proposed framework incorporates a unique tool that transforms transaction\nfeatures into visual representations, facilitating efficient analysis and\nclassification of low-level machine codes. Furthermore, we propose a customized\ncollaborative learning model to enable real-time detection of diverse attack\ntypes at distributed mining nodes. In order to create a comprehensive dataset,\nwe deploy a pilot system based on a private Ethereum network and conduct\nmultiple attack scenarios. To the best of our knowledge, our dataset is the\nmost comprehensive and diverse collection of transactions and smart contracts\nsynthesized in a laboratory for cyberattack detection in blockchain systems.\nOur framework achieves a detection accuracy of approximately 94\\% through\nextensive simulations and real-time experiments with a throughput of over 2,150\ntransactions per second. These compelling results validate the efficacy of our\nframework and showcase its adaptability in addressing real-world cyberattack\nscenarios.\n","authors":["Tran Viet Khoa","Do Hai Son","Chi-Hieu Nguyen","Dinh Thai Hoang","Diep N. Nguyen","Nguyen Linh Trung","Tran Thi Thuy Quynh","Trong-Minh Hoang","Nguyen Viet Ha","Eryk Dutkiewicz","Mohammad Abu Alsheikh"],"pdf_url":"https://arxiv.org/pdf/2308.15804v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17331v1","updated":"2024-03-26T02:30:50Z","published":"2024-03-26T02:30:50Z","title":"FedMIL: Federated-Multiple Instance Learning for Video Analysis with\n  Optimized DPP Scheduling","summary":"  Many AI platforms, including traffic monitoring systems, use Federated\nLearning (FL) for decentralized sensor data processing for learning-based\napplications while preserving privacy and ensuring secured information\ntransfer. On the other hand, applying supervised learning to large data\nsamples, like high-resolution images requires intensive human labor to label\ndifferent parts of a data sample. Multiple Instance Learning (MIL) alleviates\nthis challenge by operating over labels assigned to the 'bag' of instances. In\nthis paper, we introduce Federated Multiple-Instance Learning (FedMIL). This\nframework applies federated learning to boost the training performance in\nvideo-based MIL tasks such as vehicle accident detection using distributed CCTV\nnetworks. However, data sources in decentralized settings are not typically\nIndependently and Identically Distributed (IID), making client selection\nimperative to collectively represent the entire dataset with minimal clients.\nTo address this challenge, we propose DPPQ, a framework based on the\nDeterminantal Point Process (DPP) with a quality-based kernel to select clients\nwith the most diverse datasets that achieve better performance compared to both\nrandom selection and current DPP-based client selection methods even with less\ndata utilization in the majority of non-IID cases. This offers a significant\nadvantage for deployment on edge devices with limited computational resources,\nproviding a reliable solution for training AI models in massive smart sensor\nnetworks.\n","authors":["Ashish Bastola","Hao Wang","Xiwen Chen","Abolfazl Razi"],"pdf_url":"https://arxiv.org/pdf/2403.17331v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17287v1","updated":"2024-03-26T00:33:49Z","published":"2024-03-26T00:33:49Z","title":"Not All Federated Learning Algorithms Are Created Equal: A Performance\n  Evaluation Study","summary":"  Federated Learning (FL) emerged as a practical approach to training a model\nfrom decentralized data. The proliferation of FL led to the development of\nnumerous FL algorithms and mechanisms. Many prior efforts have given their\nprimary focus on accuracy of those approaches, but there exists little\nunderstanding of other aspects such as computational overheads, performance and\ntraining stability, etc. To bridge this gap, we conduct extensive performance\nevaluation on several canonical FL algorithms (FedAvg, FedProx, FedYogi,\nFedAdam, SCAFFOLD, and FedDyn) by leveraging an open-source federated learning\nframework called Flame. Our comprehensive measurement study reveals that no\nsingle algorithm works best across different performance metrics. A few key\nobservations are: (1) While some state-of-the-art algorithms achieve higher\naccuracy than others, they incur either higher computation overheads (FedDyn)\nor communication overheads (SCAFFOLD). (2) Recent algorithms present smaller\nstandard deviation in accuracy across clients than FedAvg, indicating that the\nadvanced algorithms' performances are stable. (3) However, algorithms such as\nFedDyn and SCAFFOLD are more prone to catastrophic failures without the support\nof additional techniques such as gradient clipping. We hope that our empirical\nstudy can help the community to build best practices in evaluating FL\nalgorithms.\n","authors":["Gustav A. Baumgart","Jaemin Shin","Ali Payani","Myungjin Lee","Ramana Rao Kompella"],"pdf_url":"https://arxiv.org/pdf/2403.17287v1.pdf","comment":null}],"Performance":[{"id":"http://arxiv.org/abs/2403.17312v1","updated":"2024-03-26T01:46:34Z","published":"2024-03-26T01:46:34Z","title":"ALISA: Accelerating Large Language Model Inference via Sparsity-Aware KV\n  Caching","summary":"  The Transformer architecture has significantly advanced natural language\nprocessing (NLP) and has been foundational in developing large language models\n(LLMs) such as LLaMA and OPT, which have come to dominate a broad range of NLP\ntasks. Despite their superior accuracy, LLMs present unique challenges in\npractical inference, concerning the compute and memory-intensive nature. Thanks\nto the autoregressive characteristic of LLM inference, KV caching for the\nattention layers in Transformers can effectively accelerate LLM inference by\nsubstituting quadratic-complexity computation with linear-complexity memory\naccesses. Yet, this approach requires increasing memory as demand grows for\nprocessing longer sequences. The overhead leads to reduced throughput due to\nI/O bottlenecks and even out-of-memory errors, particularly on\nresource-constrained systems like a single commodity GPU. In this paper, we\npropose ALISA, a novel algorithm-system co-design solution to address the\nchallenges imposed by KV caching. On the algorithm level, ALISA prioritizes\ntokens that are most important in generating a new token via a Sparse Window\nAttention (SWA) algorithm. SWA introduces high sparsity in attention layers and\nreduces the memory footprint of KV caching at negligible accuracy loss. On the\nsystem level, ALISA employs three-phase token-level dynamical scheduling and\noptimizes the trade-off between caching and recomputation, thus maximizing the\noverall performance in resource-constrained systems. In a single GPU-CPU\nsystem, we demonstrate that under varying workloads, ALISA improves the\nthroughput of baseline systems such as FlexGen and vLLM by up to 3X and 1.9X,\nrespectively.\n","authors":["Youpeng Zhao","Di Wu","Jun Wang"],"pdf_url":"https://arxiv.org/pdf/2403.17312v1.pdf","comment":"ISCA 2024"}],"Operating Systems":[{"id":"http://arxiv.org/abs/2403.16971v2","updated":"2024-03-26T02:35:07Z","published":"2024-03-25T17:32:23Z","title":"AIOS: LLM Agent Operating System","summary":"  The integration and deployment of large language model (LLM)-based\nintelligent agents have been fraught with challenges that compromise their\nefficiency and efficacy. Among these issues are sub-optimal scheduling and\nresource allocation of agent requests over the LLM, the difficulties in\nmaintaining context during interactions between agent and LLM, and the\ncomplexities inherent in integrating heterogeneous agents with different\ncapabilities and specializations. The rapid increase of agent quantity and\ncomplexity further exacerbates these issues, often leading to bottlenecks and\nsub-optimal utilization of resources. Inspired by these challenges, this paper\npresents AIOS, an LLM agent operating system, which embeds large language model\ninto operating systems (OS) as the brain of the OS, enabling an operating\nsystem \"with soul\" -- an important step towards AGI. Specifically, AIOS is\ndesigned to optimize resource allocation, facilitate context switch across\nagents, enable concurrent execution of agents, provide tool service for agents,\nand maintain access control for agents. We present the architecture of such an\noperating system, outline the core challenges it aims to resolve, and provide\nthe basic design and implementation of the AIOS. Our experiments on concurrent\nexecution of multiple agents demonstrate the reliability and efficiency of our\nAIOS modules. Through this, we aim to not only improve the performance and\nefficiency of LLM agents but also to pioneer for better development and\ndeployment of the AIOS ecosystem in the future. The project is open-source at\nhttps://github.com/agiresearch/AIOS.\n","authors":["Kai Mei","Zelong Li","Shuyuan Xu","Ruosong Ye","Yingqiang Ge","Yongfeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.16971v2.pdf","comment":"14 pages, 5 figures, 5 tables; comments and suggestions are\n  appreciated"}],"Databases":[{"id":"http://arxiv.org/abs/2403.17885v1","updated":"2024-03-26T17:17:10Z","published":"2024-03-26T17:17:10Z","title":"Empirical Analysis of EIP-3675: Miner Dynamics, Transaction Fees, and\n  Transaction Time","summary":"  The Ethereum Improvement Proposal 3675 (EIP-3675) marks a significant shift,\ntransitioning from a Proof of Work (PoW) to a Proof of Stake (PoS) consensus\nmechanism. This transition resulted in a staggering 99.95% decrease in energy\nconsumption. However, the transition prompts two critical questions: (1). How\ndoes EIP-3675 affect miners' dynamics? and (2). How do users determine priority\nfees, considering that paying too little may cause delays or non-inclusion, yet\npaying too much wastes money with little to no benefits? To address the first\nquestion, we present a comprehensive empirical study examining EIP-3675's\neffect on miner dynamics (i.e., miner participation, distribution, and the\ndegree of randomness in miner selection). Our findings reveal that the\ntransition has encouraged broader participation of miners in block append\noperation, resulting in a larger pool of unique miners ($\\approx50\\times$ PoW),\nand the change in miner distribution with the increased number of unique small\ncategory miners ($\\approx60\\times$ PoW). However, there is an unintended\nconsequence: a reduction in the miner selection randomness, which signifies the\nnegative impact of the transition to PoS-Ethereum on network decentralization.\nRegarding the second question, we employed regression-based machine learning\nmodels; the Gradient Boosting Regressor performed best in predicting priority\nfees, while the K-Neighbours Regressor was worst.\n","authors":["Umesh Bhatt","Sarvesh Pandey"],"pdf_url":"https://arxiv.org/pdf/2403.17885v1.pdf","comment":"This paper has been accepted as a full paper at the IEEE\n  International Conference on Blockchain and Cryptocurrency (ICBC) 2024"},{"id":"http://arxiv.org/abs/2306.13186v3","updated":"2024-03-26T16:29:17Z","published":"2023-06-22T20:03:09Z","title":"A Decade of Scholarly Research on Open Knowledge Graphs","summary":"  The proliferation of open knowledge graphs has led to a surge in scholarly\nresearch on the topic over the past decade. This paper presents a bibliometric\nanalysis of the scholarly literature on open knowledge graphs published between\n2013 and 2023. The study aims to identify the trends, patterns, and impact of\nresearch in this field, as well as the key topics and research questions that\nhave emerged. The work uses bibliometric techniques to analyze a sample of 4445\nscholarly articles retrieved from Scopus. The findings reveal an\never-increasing number of publications on open knowledge graphs published every\nyear, particularly in developed countries (+50 per year). These outputs are\npublished in highly-referred scholarly journals and conferences. The study\nidentifies three main research themes: (1) knowledge graph construction and\nenrichment, (2) evaluation and reuse, and (3) fusion of knowledge graphs into\nNLP systems. Within these themes, the study identifies specific tasks that have\nreceived considerable attention, including entity linking, knowledge graph\nembedding, and graph neural networks.\n","authors":["Houcemeddine Turki","Abraham Toluwase Owodunni","Mohamed Ali Hadj Taieb","René Fabrice Bile","Mohamed Ben Aouicha"],"pdf_url":"https://arxiv.org/pdf/2306.13186v3.pdf","comment":"Camera-ready edition for LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.17786v1","updated":"2024-03-26T15:18:59Z","published":"2024-03-26T15:18:59Z","title":"Query Refinement for Diverse Top-$k$ Selection","summary":"  Database queries are often used to select and rank items as decision support\nfor many applications. As automated decision-making tools become more\nprevalent, there is a growing recognition of the need to diversify their\noutcomes. In this paper, we define and study the problem of modifying the\nselection conditions of an ORDER BY query so that the result of the modified\nquery closely fits some user-defined notion of diversity while simultaneously\nmaintaining the intent of the original query. We show the hardness of this\nproblem and propose a Mixed Integer Linear Programming (MILP) based solution.\nWe further present optimizations designed to enhance the scalability and\napplicability of the solution in real-life scenarios. We investigate the\nperformance characteristics of our algorithm and show its efficiency and the\nusefulness of our optimizations.\n","authors":["Felix S. Campbell","Alon Silberstein","Yuval Moskovitch","Julia Stoyanovich"],"pdf_url":"https://arxiv.org/pdf/2403.17786v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17778v1","updated":"2024-03-26T15:11:18Z","published":"2024-03-26T15:11:18Z","title":"Towards a FAIR Documentation of Workflows and Models in Applied\n  Mathematics","summary":"  Modeling-Simulation-Optimization workflows play a fundamental role in applied\nmathematics. The Mathematical Research Data Initiative, MaRDI, responded to\nthis by developing a FAIR and machine-interpretable template for a\ncomprehensive documentation of such workflows. MaRDMO, a Plugin for the\nResearch Data Management Organiser, enables scientists from diverse fields to\ndocument and publish their workflows on the MaRDI Portal seamlessly using the\nMaRDI template. Central to these workflows are mathematical models. MaRDI\naddresses them with the MathModDB ontology, offering a structured formal model\ndescription. Here, we showcase the interaction between MaRDMO and the MathModDB\nKnowledge Graph through an algebraic modeling workflow from the Digital\nHumanities. This demonstration underscores the versatility of both services\nbeyond their original numerical domain.\n","authors":["Marco Reidelbach","Björn Schembera","Marcus Weber"],"pdf_url":"https://arxiv.org/pdf/2403.17778v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.02129v4","updated":"2024-03-26T14:38:23Z","published":"2023-10-03T15:10:46Z","title":"Unveiling the Pitfalls of Knowledge Editing for Large Language Models","summary":"  As the cost associated with fine-tuning Large Language Models (LLMs)\ncontinues to rise, recent research efforts have pivoted towards developing\nmethodologies to edit implicit knowledge embedded within LLMs. Yet, there's\nstill a dark cloud lingering overhead -- will knowledge editing trigger\nbutterfly effect? since it is still unclear whether knowledge editing might\nintroduce side effects that pose potential risks or not. This paper pioneers\nthe investigation into the potential pitfalls associated with knowledge editing\nfor LLMs. To achieve this, we introduce new benchmark datasets and propose\ninnovative evaluation metrics. Our results underline two pivotal concerns: (1)\nKnowledge Conflict: Editing groups of facts that logically clash can magnify\nthe inherent inconsistencies in LLMs-a facet neglected by previous methods. (2)\nKnowledge Distortion: Altering parameters with the aim of editing factual\nknowledge can irrevocably warp the innate knowledge structure of LLMs.\nExperimental results vividly demonstrate that knowledge editing might\ninadvertently cast a shadow of unintended consequences on LLMs, which warrant\nattention and efforts for future works. Code and data are available at\nhttps://github.com/zjunlp/PitfallsKnowledgeEditing.\n","authors":["Zhoubo Li","Ningyu Zhang","Yunzhi Yao","Mengru Wang","Xi Chen","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2310.02129v4.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2403.16110v2","updated":"2024-03-26T13:02:42Z","published":"2024-03-24T12:01:27Z","title":"ByteCard: Enhancing ByteDance's Data Warehouse with Learned Cardinality\n  Estimation","summary":"  Cardinality estimation is a critical component and a longstanding challenge\nin modern data warehouses. ByteHouse, ByteDance's cloud-native engine for big\ndata analysis in exabyte-scale environments, serves numerous internal\ndecision-making business scenarios. With the increasing demand of ByteHouse,\ncardinality estimation becomes the bottleneck for efficiently processing\nqueries. Specifically, the existing query optimizer of ByteHouse uses the\ntraditional Selinger-like cardinality estimator, which can produce huge\nestimation errors, resulting in sub-optimal query plans. To improve cardinality\nestimation accuracy while maintaining a practical inference overhead, we\ndevelop ByteCard framework that enables efficient training/updating and\nintegration of cardinality estimators. Furthermore, ByteCard adapts recent\nadvances in cardinality estimation to build models that can balance accuracy\nand practicality (e.g., inference latency, model size, training/updating\noverhead). We observe significant query processing speed-up in ByteHouse after\nreplacing the system's existing cardinality estimation with ByteCard's\nestimations for several optimization strategies. Evaluations on real-world\ndatasets show the integration of ByteCard leads to an improvement of up to 30%\nin the 99th quantile of latency. At last, we share our valuable experience in\nengineering advanced cardinality estimators. We believe this experience can\nhelp other data warehouses integrate more accurate and sophisticated solutions\non the critical path of query execution.\n","authors":["Yuxing Han","Haoyu Wang","Lixiang Chen","Yifeng Dong","Xing Chen","Benquan Yu","Chengcheng Yang","Weining Qian"],"pdf_url":"https://arxiv.org/pdf/2403.16110v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17665v1","updated":"2024-03-26T12:55:12Z","published":"2024-03-26T12:55:12Z","title":"When View- and Conflict-Robustness Coincide for Multiversion Concurrency\n  Control","summary":"  A DBMS allows trading consistency for efficiency through the allocation of\nisolation levels that are strictly weaker than serializability. The robustness\nproblem asks whether, for a given set of transactions and a given allocation of\nisolation levels, every possible interleaved execution of those transactions\nthat is allowed under the provided allocation, is always safe. In the\nliterature, safe is interpreted as conflict-serializable (to which we refer\nhere as conflict-robustness). In this paper, we study the view-robustness\nproblem, interpreting safe as view-serializable. View-serializability is a more\npermissive notion that allows for a greater number of schedules to be\nserializable and aligns more closely with the intuitive understanding of what\nit means for a database to be consistent. However, view-serializability is more\ncomplex to analyze (e.g., conflict-serializability can be decided in polynomial\ntime whereas deciding view-serializability is NP-complete). While\nconflict-robustness implies view-robustness, the converse does not hold in\ngeneral. In this paper, we provide a sufficient condition for isolation levels\nguaranteeing that conflict- and view-robustness coincide and show that this\ncondition is satisfied by the isolation levels occurring in Postgres and\nOracle: read committed (RC), snapshot isolation (SI) and serializable snapshot\nisolation (SSI). It hence follows that for these systems, widening from\nconflict- to view-serializability does not allow for more sets of transactions\nto become robust. Interestingly, the complexity of deciding serializability\nwithin these isolation levels is still quite different. Indeed, deciding\nconflict-serializability for schedules allowed under RC and SI remains in\npolynomial time, while we show that deciding view-serializability within these\nisolation levels remains NP-complete.\n","authors":["Brecht Vandevoort","Bas Ketsman","Frank Neven"],"pdf_url":"https://arxiv.org/pdf/2403.17665v1.pdf","comment":"22 pages"},{"id":"http://arxiv.org/abs/2403.17469v1","updated":"2024-03-26T07:57:04Z","published":"2024-03-26T07:57:04Z","title":"Geometric planted matchings beyond the Gaussian model","summary":"  We consider the problem of recovering an unknown matching between a set of\n$n$ randomly placed points in $\\mathbb{R}^d$ and random perturbations of these\npoints. This can be seen as a model for particle tracking and more generally,\nentity resolution. We use matchings in random geometric graphs to derive\nminimax lower bounds for this problem that hold under great generality. Using\nthese results we show that for a broad class of distributions, the order of the\nnumber of mistakes made by an estimator that minimizes the sum of squared\nEuclidean distances is minimax optimal when $d$ is fixed and is optimal up to\n$n^{o(1)}$ factors when $d = o(\\log n)$. In the high-dimensional regime we\nconsider a setup where both initial positions and perturbations have\nindependent sub-Gaussian coordinates. In this setup we give sufficient\nconditions under which the same estimator makes no mistakes with high\nprobability. We prove an analogous result for an adapted version of this\nestimator that incorporates information on the covariance matrix of the\nperturbations.\n","authors":["Lucas da Rocha Schwengber","Roberto Imbuzeiro Oliveira"],"pdf_url":"https://arxiv.org/pdf/2403.17469v1.pdf","comment":"36 pages, 2 figures"},{"id":"http://arxiv.org/abs/2403.17344v1","updated":"2024-03-26T03:07:32Z","published":"2024-03-26T03:07:32Z","title":"Disambiguate Entity Matching through Relation Discovery with Large\n  Language Models","summary":"  Entity matching is a critical challenge in data integration and cleaning,\ncentral to tasks like fuzzy joins and deduplication. Traditional approaches\nhave focused on overcoming fuzzy term representations through methods such as\nedit distance, Jaccard similarity, and more recently, embeddings and deep\nneural networks, including advancements from large language models (LLMs) like\nGPT. However, the core challenge in entity matching extends beyond term\nfuzziness to the ambiguity in defining what constitutes a \"match,\" especially\nwhen integrating with external databases. This ambiguity arises due to varying\nlevels of detail and granularity among entities, complicating exact matches. We\npropose a novel approach that shifts focus from purely identifying semantic\nsimilarities to understanding and defining the \"relations\" between entities as\ncrucial for resolving ambiguities in matching. By predefining a set of\nrelations relevant to the task at hand, our method allows analysts to navigate\nthe spectrum of similarity more effectively, from exact matches to conceptually\nrelated entities.\n","authors":["Zezhou Huang"],"pdf_url":"https://arxiv.org/pdf/2403.17344v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08770v2","updated":"2024-03-26T02:10:46Z","published":"2023-11-15T08:32:15Z","title":"A proof-of-concept online metadata catalogue service of Earth\n  observation datasets for human health research in exposomics","summary":"  This article describes research carried out during 2023 under an\nInternational Society for Photogrammetry and Remote Sensing (ISPRS)-funded\nproject to develop and disseminate a metadata catalogue of Earth observation\ndata sources/products and types that are relevant to human health research in\nexposomics, as a free service to interested researchers worldwide. The\nproof-of-concept catalogue was informed by input from existing research\nliterature on the subject (desk research), as well as online communications\nwith, and relevant research publications collected from, a small panel (n = 5)\nof select experts from the academia in three countries (China, UK and USA). It\nhas 90 metadata records of relevant Earth observation datasets (n = 40) and\nassociated health-focused research publications (n = 50). The project's online\nportal offers a searchable version of the catalogue featuring a number of\nsearch modes and filtering options. It is hoped future, more comprehensive\nversions of this service will enable more researchers and studies to discover\nand use remote sensing data about population-level exposures to disease\ndeterminants (exposomic determinants of disease) in combination with other\nrelevant data to reveal fresh insights that could improve our understanding of\nrelevant diseases, and hence contribute to the development of better-optimized\nprevention and management plans to tackle them.\n","authors":["Keumseok Koh","Maged N. Kamel Boulos","Gang Zheng","Hongsheng Zhang","Muralikrishna V. Iyyanki","Bosco Bwambale","Ashraf Dewan"],"pdf_url":"https://arxiv.org/pdf/2311.08770v2.pdf","comment":"5 figures"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2402.13452v2","updated":"2024-03-26T17:59:14Z","published":"2024-02-21T01:11:28Z","title":"LocalTweets to LocalHealth: A Mental Health Surveillance Framework Based\n  on Twitter Data","summary":"  Prior research on Twitter (now X) data has provided positive evidence of its\nutility in developing supplementary health surveillance systems. In this study,\nwe present a new framework to surveil public health, focusing on mental health\n(MH) outcomes. We hypothesize that locally posted tweets are indicative of\nlocal MH outcomes and collect tweets posted from 765 neighborhoods (census\nblock groups) in the USA. We pair these tweets from each neighborhood with the\ncorresponding MH outcome reported by the Center for Disease Control (CDC) to\ncreate a benchmark dataset, LocalTweets. With LocalTweets, we present the first\npopulation-level evaluation task for Twitter-based MH surveillance systems. We\nthen develop an efficient and effective method, LocalHealth, for predicting MH\noutcomes based on LocalTweets. When used with GPT3.5, LocalHealth achieves the\nhighest F1-score and accuracy of 0.7429 and 79.78\\%, respectively, a 59\\%\nimprovement in F1-score over the GPT3.5 in zero-shot setting. We also utilize\nLocalHealth to extrapolate CDC's estimates to proxy unreported neighborhoods,\nachieving an F1-score of 0.7291. Our work suggests that Twitter data can be\neffectively leveraged to simulate neighborhood-level MH outcomes.\n","authors":["Vijeta Deshpande","Minhwa Lee","Zonghai Yao","Zihao Zhang","Jason Brian Gibbons","Hong Yu"],"pdf_url":"https://arxiv.org/pdf/2402.13452v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08763v3","updated":"2024-03-26T17:58:48Z","published":"2024-03-13T17:58:57Z","title":"Simple and Scalable Strategies to Continually Pre-train Large Language\n  Models","summary":"  Large language models (LLMs) are routinely pre-trained on billions of tokens,\nonly to start the process over again once new data becomes available. A much\nmore efficient solution is to continually pre-train these models, saving\nsignificant compute compared to re-training. However, the distribution shift\ninduced by new data typically results in degraded performance on previous data\nor poor adaptation to the new data. In this work, we show that a simple and\nscalable combination of learning rate (LR) re-warming, LR re-decaying, and\nreplay of previous data is sufficient to match the performance of fully\nre-training from scratch on all available data, as measured by the final loss\nand the average score on several language model (LM) evaluation benchmarks.\nSpecifically, we show this for a weak but realistic distribution shift between\ntwo commonly used LLM pre-training datasets (English$\\rightarrow$English) and a\nstronger distribution shift (English$\\rightarrow$German) at the $405$M\nparameter model scale with large dataset sizes (hundreds of billions of\ntokens). Selecting the weak but realistic shift for larger-scale experiments,\nwe also find that our continual learning strategies match the re-training\nbaseline for a 10B parameter LLM. Our results demonstrate that LLMs can be\nsuccessfully updated via simple and scalable continual learning strategies,\nmatching the re-training baseline using only a fraction of the compute.\nFinally, inspired by previous work, we propose alternatives to the cosine\nlearning rate schedule that help circumvent forgetting induced by LR re-warming\nand that are not bound to a fixed token budget.\n","authors":["Adam Ibrahim","Benjamin Thérien","Kshitij Gupta","Mats L. Richter","Quentin Anthony","Timothée Lesort","Eugene Belilovsky","Irina Rish"],"pdf_url":"https://arxiv.org/pdf/2403.08763v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17933v1","updated":"2024-03-26T17:58:29Z","published":"2024-03-26T17:58:29Z","title":"SLEDGE: Synthesizing Simulation Environments for Driving Agents with\n  Generative Models","summary":"  SLEDGE is the first generative simulator for vehicle motion planning trained\non real-world driving logs. Its core component is a learned model that is able\nto generate agent bounding boxes and lane graphs. The model's outputs serve as\nan initial state for traffic simulation. The unique properties of the entities\nto be generated for SLEDGE, such as their connectivity and variable count per\nscene, render the naive application of most modern generative models to this\ntask non-trivial. Therefore, together with a systematic study of existing lane\ngraph representations, we introduce a novel raster-to-vector autoencoder\n(RVAE). It encodes agents and the lane graph into distinct channels in a\nrasterized latent map. This facilitates both lane-conditioned agent generation\nand combined generation of lanes and agents with a Diffusion Transformer. Using\ngenerated entities in SLEDGE enables greater control over the simulation, e.g.\nupsampling turns or increasing traffic density. Further, SLEDGE can support\n500m long routes, a capability not found in existing data-driven simulators\nlike nuPlan. It presents new challenges for planning algorithms, evidenced by\nfailure rates of over 40% for PDM, the winner of the 2023 nuPlan challenge,\nwhen tested on hard routes and dense traffic generated by our model. Compared\nto nuPlan, SLEDGE requires 500$\\times$ less storage to set up (<4GB), making it\na more accessible option and helping with democratizing future research in this\nfield.\n","authors":["Kashyap Chitta","Daniel Dauner","Andreas Geiger"],"pdf_url":"https://arxiv.org/pdf/2403.17933v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17921v1","updated":"2024-03-26T17:55:58Z","published":"2024-03-26T17:55:58Z","title":"The Need for Speed: Pruning Transformers with One Recipe","summary":"  We introduce the $\\textbf{O}$ne-shot $\\textbf{P}$runing $\\textbf{T}$echnique\nfor $\\textbf{I}$nterchangeable $\\textbf{N}$etworks ($\\textbf{OPTIN}$) framework\nas a tool to increase the efficiency of pre-trained transformer architectures\n$\\textit{without requiring re-training}$. Recent works have explored improving\ntransformer efficiency, however often incur computationally expensive\nre-training procedures or depend on architecture-specific characteristics, thus\nimpeding practical wide-scale adoption. To address these shortcomings, the\nOPTIN framework leverages intermediate feature distillation, capturing the\nlong-range dependencies of model parameters (coined $\\textit{trajectory}$), to\nproduce state-of-the-art results on natural language, image classification,\ntransfer learning, and semantic segmentation tasks $\\textit{without\nre-training}$. Given a FLOP constraint, the OPTIN framework will compress the\nnetwork while maintaining competitive accuracy performance and improved\nthroughput. Particularly, we show a $\\leq 2$% accuracy degradation from NLP\nbaselines and a $0.5$% improvement from state-of-the-art methods on image\nclassification at competitive FLOPs reductions. We further demonstrate the\ngeneralization of tasks and architecture with comparative performance using\nMask2Former for semantic segmentation and cnn-style networks. OPTIN presents\none of the first one-shot efficient frameworks for compressing transformer\narchitectures that generalizes well across different class domains, in\nparticular: natural language and image-related tasks, without\n$\\textit{re-training}$.\n","authors":["Samir Khaki","Konstantinos N. Plataniotis"],"pdf_url":"https://arxiv.org/pdf/2403.17921v1.pdf","comment":"Accepted in the International Conference on Learning Representations\n  (ICLR) 2024"},{"id":"http://arxiv.org/abs/2403.17919v1","updated":"2024-03-26T17:55:02Z","published":"2024-03-26T17:55:02Z","title":"LISA: Layerwise Importance Sampling for Memory-Efficient Large Language\n  Model Fine-Tuning","summary":"  The machine learning community has witnessed impressive advancements since\nthe first appearance of large language models (LLMs), yet their huge memory\nconsumption has become a major roadblock to large-scale training. Parameter\nEfficient Fine-Tuning techniques such as Low-Rank Adaptation (LoRA) have been\nproposed to alleviate this problem, but their performance still fails to match\nfull parameter training in most large-scale fine-tuning settings. Attempting to\ncomplement this deficiency, we investigate layerwise properties of LoRA on\nfine-tuning tasks and observe an uncommon skewness of weight norms across\ndifferent layers. Utilizing this key observation, a surprisingly simple\ntraining strategy is discovered, which outperforms both LoRA and full parameter\ntraining in a wide range of settings with memory costs as low as LoRA. We name\nit Layerwise Importance Sampled AdamW (LISA), a promising alternative for LoRA,\nwhich applies the idea of importance sampling to different layers in LLMs and\nrandomly freeze most middle layers during optimization. Experimental results\nshow that with similar or less GPU memory consumption, LISA surpasses LoRA or\neven full parameter tuning in downstream fine-tuning tasks, where LISA\nconsistently outperforms LoRA by over $11\\%$-$37\\%$ in terms of MT-Bench\nscores. On large models, specifically LLaMA-2-70B, LISA achieves on-par or\nbetter performance than LoRA on MT-Bench, GSM8K, and PubMedQA, demonstrating\nits effectiveness across different domains.\n","authors":["Rui Pan","Xiang Liu","Shizhe Diao","Renjie Pi","Jipeng Zhang","Chi Han","Tong Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.17919v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17916v1","updated":"2024-03-26T17:53:27Z","published":"2024-03-26T17:53:27Z","title":"CMP: Cooperative Motion Prediction with Multi-Agent Communication","summary":"  The confluence of the advancement of Autonomous Vehicles (AVs) and the\nmaturity of Vehicle-to-Everything (V2X) communication has enabled the\ncapability of cooperative connected and automated vehicles (CAVs). Building on\ntop of cooperative perception, this paper explores the feasibility and\neffectiveness of cooperative motion prediction. Our method, CMP, takes LiDAR\nsignals as input to enhance tracking and prediction capabilities. Unlike\nprevious work that focuses separately on either cooperative perception or\nmotion prediction, our framework, to the best of our knowledge, is the first to\naddress the unified problem where CAVs share information in both perception and\nprediction modules. Incorporated into our design is the unique capability to\ntolerate realistic V2X bandwidth limitations and transmission delays, while\ndealing with bulky perception representations. We also propose a prediction\naggregation module, which unifies the predictions obtained by different CAVs\nand generates the final prediction. Through extensive experiments and ablation\nstudies, we demonstrate the effectiveness of our method in cooperative\nperception, tracking, and motion prediction tasks. In particular, CMP reduces\nthe average prediction error by 17.2\\% with fewer missing detections compared\nwith the no cooperation setting. Our work marks a significant step forward in\nthe cooperative capabilities of CAVs, showcasing enhanced performance in\ncomplex scenarios.\n","authors":["Zhuoyuan Wu","Yuping Wang","Hengbo Ma","Zhaowei Li","Hang Qiu","Jiachen Li"],"pdf_url":"https://arxiv.org/pdf/2403.17916v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.01364v3","updated":"2024-03-26T17:45:01Z","published":"2022-11-02T17:59:09Z","title":"An optimal control perspective on diffusion-based generative modeling","summary":"  We establish a connection between stochastic optimal control and generative\nmodels based on stochastic differential equations (SDEs), such as recently\ndeveloped diffusion probabilistic models. In particular, we derive a\nHamilton-Jacobi-Bellman equation that governs the evolution of the\nlog-densities of the underlying SDE marginals. This perspective allows to\ntransfer methods from optimal control theory to generative modeling. First, we\nshow that the evidence lower bound is a direct consequence of the well-known\nverification theorem from control theory. Further, we can formulate\ndiffusion-based generative modeling as a minimization of the Kullback-Leibler\ndivergence between suitable measures in path space. Finally, we develop a novel\ndiffusion-based method for sampling from unnormalized densities -- a problem\nfrequently occurring in statistics and computational sciences. We demonstrate\nthat our time-reversed diffusion sampler (DIS) can outperform other\ndiffusion-based sampling approaches on multiple numerical examples.\n","authors":["Julius Berner","Lorenz Richter","Karen Ullrich"],"pdf_url":"https://arxiv.org/pdf/2211.01364v3.pdf","comment":"Accepted for oral presentation at NeurIPS 2022 Workshop on\n  Score-Based Methods"},{"id":"http://arxiv.org/abs/2401.15059v2","updated":"2024-03-26T17:44:45Z","published":"2024-01-26T18:42:01Z","title":"Fully Independent Communication in Multi-Agent Reinforcement Learning","summary":"  Multi-Agent Reinforcement Learning (MARL) comprises a broad area of research\nwithin the field of multi-agent systems. Several recent works have focused\nspecifically on the study of communication approaches in MARL. While multiple\ncommunication methods have been proposed, these might still be too complex and\nnot easily transferable to more practical contexts. One of the reasons for that\nis due to the use of the famous parameter sharing trick. In this paper, we\ninvestigate how independent learners in MARL that do not share parameters can\ncommunicate. We demonstrate that this setting might incur into some problems,\nto which we propose a new learning scheme as a solution. Our results show that,\ndespite the challenges, independent agents can still learn communication\nstrategies following our method. Additionally, we use this method to\ninvestigate how communication in MARL is affected by different network\ncapacities, both for sharing and not sharing parameters. We observe that\ncommunication may not always be needed and that the chosen agent network sizes\nneed to be considered when used together with communication in order to achieve\nefficient learning.\n","authors":["Rafael Pina","Varuna De Silva","Corentin Artaud","Xiaolan Liu"],"pdf_url":"https://arxiv.org/pdf/2401.15059v2.pdf","comment":"Extended version of the paper appearing on AAMAS 2024 with the same\n  title. 11 pages, 8 figures"},{"id":"http://arxiv.org/abs/2403.17902v1","updated":"2024-03-26T17:43:15Z","published":"2024-03-26T17:43:15Z","title":"Serpent: Scalable and Efficient Image Restoration via Multi-scale\n  Structured State Space Models","summary":"  The landscape of computational building blocks of efficient image restoration\narchitectures is dominated by a combination of convolutional processing and\nvarious attention mechanisms. However, convolutional filters are inherently\nlocal and therefore struggle at modeling long-range dependencies in images. On\nthe other hand, attention excels at capturing global interactions between\narbitrary image regions, however at a quadratic cost in image dimension. In\nthis work, we propose Serpent, an architecture that leverages recent advances\nin state space models (SSMs) in its core computational block. SSMs, originally\nintroduced for sequence modeling, can maintain a global receptive field with a\nfavorable linear scaling in input size. Our preliminary results demonstrate\nthat Serpent can achieve reconstruction quality on par with state-of-the-art\ntechniques, while requiring orders of magnitude less compute (up to $150$ fold\nreduction in FLOPS) and a factor of up to $5\\times$ less GPU memory while\nmaintaining a compact model size.\n","authors":["Mohammad Shahab Sepehri","Zalan Fabian","Mahdi Soltanolkotabi"],"pdf_url":"https://arxiv.org/pdf/2403.17902v1.pdf","comment":"7 pages, 5 figures, preliminary workshop submission of a\n  comprehensive work to be released soon"},{"id":"http://arxiv.org/abs/2310.18841v2","updated":"2024-03-26T17:39:30Z","published":"2023-10-28T22:57:56Z","title":"A randomized algorithm for nonconvex minimization with inexact\n  evaluations and complexity guarantees","summary":"  We consider minimization of a smooth nonconvex function with inexact oracle\naccess to gradient and Hessian (without assuming access to the function value)\nto achieve approximate second-order optimality. A novel feature of our method\nis that if an approximate direction of negative curvature is chosen as the\nstep, we choose its sense to be positive or negative with equal probability. We\nallow gradients to be inexact in a relative sense and relax the coupling\nbetween inexactness thresholds for the first- and second-order optimality\nconditions. Our convergence analysis includes both an expectation bound based\non martingale analysis and a high-probability bound based on concentration\ninequalities. We apply our algorithm to empirical risk minimization problems\nand obtain improved gradient sample complexity over existing works.\n","authors":["Shuyao Li","Stephen J. Wright"],"pdf_url":"https://arxiv.org/pdf/2310.18841v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09428v2","updated":"2024-03-26T17:38:38Z","published":"2024-03-14T14:19:48Z","title":"Borrowing Treasures from Neighbors: In-Context Learning for Multimodal\n  Learning with Missing Modalities and Data Scarcity","summary":"  Multimodal machine learning with missing modalities is an increasingly\nrelevant challenge arising in various applications such as healthcare. This\npaper extends the current research into missing modalities to the low-data\nregime, i.e., a downstream task has both missing modalities and limited sample\nsize issues. This problem setting is particularly challenging and also\npractical as it is often expensive to get full-modality data and sufficient\nannotated training samples. We propose to use retrieval-augmented in-context\nlearning to address these two crucial issues by unleashing the potential of a\ntransformer's in-context learning ability. Diverging from existing methods,\nwhich primarily belong to the parametric paradigm and often require sufficient\ntraining samples, our work exploits the value of the available full-modality\ndata, offering a novel perspective on resolving the challenge. The proposed\ndata-dependent framework exhibits a higher degree of sample efficiency and is\nempirically demonstrated to enhance the classification model's performance on\nboth full- and missing-modality data in the low-data regime across various\nmultimodal learning tasks. When only 1% of the training data are available, our\nproposed method demonstrates an average improvement of 6.1% over a recent\nstrong baseline across various datasets and missing states. Notably, our method\nalso reduces the performance gap between full-modality and missing-modality\ndata compared with the baseline.\n","authors":["Zhuo Zhi","Ziquan Liu","Moe Elbadawi","Adam Daneshmend","Mine Orlu","Abdul Basit","Andreas Demosthenous","Miguel Rodrigues"],"pdf_url":"https://arxiv.org/pdf/2403.09428v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.02156v4","updated":"2024-03-26T17:36:54Z","published":"2023-10-03T15:43:59Z","title":"Probabilistically Rewired Message-Passing Neural Networks","summary":"  Message-passing graph neural networks (MPNNs) emerged as powerful tools for\nprocessing graph-structured input. However, they operate on a fixed input graph\nstructure, ignoring potential noise and missing information. Furthermore, their\nlocal aggregation mechanism can lead to problems such as over-squashing and\nlimited expressive power in capturing relevant graph structures. Existing\nsolutions to these challenges have primarily relied on heuristic methods, often\ndisregarding the underlying data distribution. Hence, devising principled\napproaches for learning to infer graph structures relevant to the given\nprediction task remains an open challenge. In this work, leveraging recent\nprogress in exact and differentiable $k$-subset sampling, we devise\nprobabilistically rewired MPNNs (PR-MPNNs), which learn to add relevant edges\nwhile omitting less beneficial ones. For the first time, our theoretical\nanalysis explores how PR-MPNNs enhance expressive power, and we identify\nprecise conditions under which they outperform purely randomized approaches.\nEmpirically, we demonstrate that our approach effectively mitigates issues like\nover-squashing and under-reaching. In addition, on established real-world\ndatasets, our method exhibits competitive or superior predictive performance\ncompared to traditional MPNN models and recent graph transformer architectures.\n","authors":["Chendi Qian","Andrei Manolache","Kareem Ahmed","Zhe Zeng","Guy Van den Broeck","Mathias Niepert","Christopher Morris"],"pdf_url":"https://arxiv.org/pdf/2310.02156v4.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2401.07809v2","updated":"2024-03-26T17:29:07Z","published":"2024-01-15T16:30:12Z","title":"Optimal Data Splitting in Distributed Optimization for Machine Learning","summary":"  The distributed optimization problem has become increasingly relevant\nrecently. It has a lot of advantages such as processing a large amount of data\nin less time compared to non-distributed methods. However, most distributed\napproaches suffer from a significant bottleneck - the cost of communications.\nTherefore, a large amount of research has recently been directed at solving\nthis problem. One such approach uses local data similarity. In particular,\nthere exists an algorithm provably optimally exploiting the similarity\nproperty. But this result, as well as results from other works solve the\ncommunication bottleneck by focusing only on the fact that communication is\nsignificantly more expensive than local computing and does not take into\naccount the various capacities of network devices and the different\nrelationship between communication time and local computing expenses. We\nconsider this setup and the objective of this study is to achieve an optimal\nratio of distributed data between the server and local machines for any costs\nof communications and local computations. The running times of the network are\ncompared between uniform and optimal distributions. The superior theoretical\nperformance of our solutions is experimentally validated.\n","authors":["Daniil Medyakov","Gleb Molodtsov","Aleksandr Beznosikov","Alexander Gasnikov"],"pdf_url":"https://arxiv.org/pdf/2401.07809v2.pdf","comment":"17 pages, 2 figures"},{"id":"http://arxiv.org/abs/2403.17891v1","updated":"2024-03-26T17:22:29Z","published":"2024-03-26T17:22:29Z","title":"Image-based Novel Fault Detection with Deep Learning Classifiers using\n  Hierarchical Labels","summary":"  One important characteristic of modern fault classification systems is the\nability to flag the system when faced with previously unseen fault types. This\nwork considers the unknown fault detection capabilities of deep neural\nnetwork-based fault classifiers. Specifically, we propose a methodology on how,\nwhen available, labels regarding the fault taxonomy can be used to increase\nunknown fault detection performance without sacrificing model performance. To\nachieve this, we propose to utilize soft label techniques to improve the\nstate-of-the-art deep novel fault detection techniques during the training\nprocess and novel hierarchically consistent detection statistics for online\nnovel fault detection. Finally, we demonstrated increased detection performance\non novel fault detection in inspection images from the hot steel rolling\nprocess, with results well replicated across multiple scenarios and baseline\ndetection methods.\n","authors":["Nurettin Sergin","Jiayu Huang","Tzyy-Shuh Chang","Hao Yan"],"pdf_url":"https://arxiv.org/pdf/2403.17891v1.pdf","comment":"Accepted in IISE Transaction"},{"id":"http://arxiv.org/abs/2403.17889v1","updated":"2024-03-26T17:21:54Z","published":"2024-03-26T17:21:54Z","title":"Large scale paired antibody language models","summary":"  Antibodies are proteins produced by the immune system that can identify and\nneutralise a wide variety of antigens with high specificity and affinity, and\nconstitute the most successful class of biotherapeutics. With the advent of\nnext-generation sequencing, billions of antibody sequences have been collected\nin recent years, though their application in the design of better therapeutics\nhas been constrained by the sheer volume and complexity of the data. To address\nthis challenge, we present IgBert and IgT5, the best performing\nantibody-specific language models developed to date which can consistently\nhandle both paired and unpaired variable region sequences as input. These\nmodels are trained comprehensively using the more than two billion unpaired\nsequences and two million paired sequences of light and heavy chains present in\nthe Observed Antibody Space dataset. We show that our models outperform\nexisting antibody and protein language models on a diverse range of design and\nregression tasks relevant to antibody engineering. This advancement marks a\nsignificant leap forward in leveraging machine learning, large scale data sets\nand high-performance computing for enhancing antibody design for therapeutic\ndevelopment.\n","authors":["Henry Kenlay","Frédéric A. Dreyer","Aleksandr Kovaltsuk","Dom Miketa","Douglas Pires","Charlotte M. Deane"],"pdf_url":"https://arxiv.org/pdf/2403.17889v1.pdf","comment":"14 pages, 2 figures, 6 tables, model weights available at\n  https://zenodo.org/doi/10.5281/zenodo.10876908"},{"id":"http://arxiv.org/abs/2403.17887v1","updated":"2024-03-26T17:20:04Z","published":"2024-03-26T17:20:04Z","title":"The Unreasonable Ineffectiveness of the Deeper Layers","summary":"  We empirically study a simple layer-pruning strategy for popular families of\nopen-weight pretrained LLMs, finding minimal degradation of performance on\ndifferent question-answering benchmarks until after a large fraction (up to\nhalf) of the layers are removed. To prune these models, we identify the optimal\nblock of layers to prune by considering similarity across layers; then, to\n\"heal\" the damage, we perform a small amount of finetuning. In particular, we\nuse parameter-efficient finetuning (PEFT) methods, specifically quantization\nand Low Rank Adapters (QLoRA), such that each of our experiments can be\nperformed on a single A100 GPU. From a practical perspective, these results\nsuggest that layer pruning methods can complement other PEFT strategies to\nfurther reduce computational resources of finetuning on the one hand, and can\nimprove the memory and latency of inference on the other hand. From a\nscientific perspective, the robustness of these LLMs to the deletion of layers\nimplies either that current pretraining methods are not properly leveraging the\nparameters in the deeper layers of the network or that the shallow layers play\na critical role in storing knowledge.\n","authors":["Andrey Gromov","Kushal Tirumala","Hassan Shapourian","Paolo Glorioso","Daniel A. Roberts"],"pdf_url":"https://arxiv.org/pdf/2403.17887v1.pdf","comment":"12 + 10 pages, 5 + 4 figures"},{"id":"http://arxiv.org/abs/2403.17886v1","updated":"2024-03-26T17:19:23Z","published":"2024-03-26T17:19:23Z","title":"Compressed Multi-task embeddings for Data-Efficient Downstream training\n  and inference in Earth Observation","summary":"  As repositories of large scale data in earth observation (EO) have grown, so\nhave transfer and storage costs for model training and inference, expending\nsignificant resources. We introduce Neural Embedding Compression (NEC), based\non the transfer of compressed embeddings to data consumers instead of raw data.\nWe adapt foundation models (FM) through learned neural compression to generate\nmulti-task embeddings while navigating the tradeoff between compression rate\nand embedding utility. We update only a small fraction of the FM parameters\n(10%) for a short training period (1% of the iterations of pre-training). We\nevaluate NEC on two EO tasks: scene classification and semantic segmentation.\nCompared with applying traditional compression to the raw data, NEC achieves\nsimilar accuracy with a 75% to 90% reduction in data. Even at 99.7%\ncompression, performance drops by only 5% on the scene classification task.\nOverall, NEC is a data-efficient yet performant approach for multi-task EO\nmodelling.\n","authors":["Carlos Gomes","Thomas Brunschwiler"],"pdf_url":"https://arxiv.org/pdf/2403.17886v1.pdf","comment":"Published at IGARSS 2024"},{"id":"http://arxiv.org/abs/2403.04202v3","updated":"2024-03-26T17:18:33Z","published":"2024-03-07T04:12:24Z","title":"Dynamics of Moral Behavior in Heterogeneous Populations of Learning\n  Agents","summary":"  Growing concerns about safety and alignment of AI systems highlight the\nimportance of embedding moral capabilities in artificial agents. A promising\nsolution is the use of learning from experience, i.e., Reinforcement Learning.\nIn multi-agent (social) environments, complex population-level phenomena may\nemerge from interactions between individual learning agents. Many of the\nexisting studies rely on simulated social dilemma environments to study the\ninteractions of independent learning agents. However, they tend to ignore the\nmoral heterogeneity that is likely to be present in societies of agents in\npractice. For example, at different points in time a single learning agent may\nface opponents who are consequentialist (i.e., caring about maximizing some\noutcome over time) or norm-based (i.e., focusing on conforming to a specific\nnorm here and now). The extent to which agents' co-development may be impacted\nby such moral heterogeneity in populations is not well understood. In this\npaper, we present a study of the learning dynamics of morally heterogeneous\npopulations interacting in a social dilemma setting. Using a Prisoner's Dilemma\nenvironment with a partner selection mechanism, we investigate the extent to\nwhich the prevalence of diverse moral agents in populations affects individual\nagents' learning behaviors and emergent population-level outcomes. We observe\nseveral types of non-trivial interactions between pro-social and anti-social\nagents, and find that certain classes of moral agents are able to steer selfish\nagents towards more cooperative behavior.\n","authors":["Elizaveta Tennant","Stephen Hailes","Mirco Musolesi"],"pdf_url":"https://arxiv.org/pdf/2403.04202v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17878v1","updated":"2024-03-26T17:10:15Z","published":"2024-03-26T17:10:15Z","title":"Empowering Data Mesh with Federated Learning","summary":"  The evolution of data architecture has seen the rise of data lakes, aiming to\nsolve the bottlenecks of data management and promote intelligent\ndecision-making. However, this centralized architecture is limited by the\nproliferation of data sources and the growing demand for timely analysis and\nprocessing. A new data paradigm, Data Mesh, is proposed to overcome these\nchallenges. Data Mesh treats domains as a first-class concern by distributing\nthe data ownership from the central team to each data domain, while keeping the\nfederated governance to monitor domains and their data products. Many\nmulti-million dollar organizations like Paypal, Netflix, and Zalando have\nalready transformed their data analysis pipelines based on this new\narchitecture. In this decentralized architecture where data is locally\npreserved by each domain team, traditional centralized machine learning is\nincapable of conducting effective analysis across multiple domains, especially\nfor security-sensitive organizations. To this end, we introduce a pioneering\napproach that incorporates Federated Learning into Data Mesh. To the best of\nour knowledge, this is the first open-source applied work that represents a\ncritical advancement toward the integration of federated learning methods into\nthe Data Mesh paradigm, underscoring the promising prospects for\nprivacy-preserving and decentralized data analysis strategies within Data Mesh\narchitecture.\n","authors":["Haoyuan Li","Salman Toor"],"pdf_url":"https://arxiv.org/pdf/2403.17878v1.pdf","comment":"In Proceedings of ACM Knowledge Discovery and Data Mining, Barcelona,\n  Spain, 25th - 29th August, 2024 (Conference acronym KDD), 9 pages"},{"id":"http://arxiv.org/abs/2402.04866v2","updated":"2024-03-26T16:57:46Z","published":"2024-02-01T21:16:40Z","title":"Room Transfer Function Reconstruction Using Complex-valued Neural\n  Networks and Irregularly Distributed Microphones","summary":"  Reconstructing the room transfer functions needed to calculate the complex\nsound field in a room has several impor- tant real-world applications. However,\nan unpractical number of microphones is often required. Recently, in addition\nto classical signal processing methods, deep learning techniques have been\napplied to reconstruct the room transfer function starting from a very limited\nset of measurements at scattered points in the room. In this paper, we employ\ncomplex-valued neural networks to estimate room transfer functions in the\nfrequency range of the first room resonances, using a few irregularly\ndistributed microphones. To the best of our knowledge, this is the first time\nthat complex-valued neural networks are used to estimate room transfer\nfunctions. To analyze the benefits of applying complex- valued optimization to\nthe considered task, we compare the proposed technique with a state-of-the-art\nkernel-based signal processing approach for sound field reconstruction, showing\nthat the proposed technique exhibits relevant advantages in terms of phase\naccuracy and overall quality of the reconstructed sound field. For informative\npurposes, we also compare the model with a similarly-structured data-driven\napproach that, however, applies a real-valued neural network to reconstruct\nonly the magnitude of the sound field.\n","authors":["Francesca Ronchini","Luca Comanducci","Mirco Pezzoli","Fabio Antonacci","Augusto Sarti"],"pdf_url":"https://arxiv.org/pdf/2402.04866v2.pdf","comment":"Submitted to EUSIPCO 2024"},{"id":"http://arxiv.org/abs/2403.17868v1","updated":"2024-03-26T16:57:01Z","published":"2024-03-26T16:57:01Z","title":"Sample complexity of quantum hypothesis testing","summary":"  Quantum hypothesis testing has been traditionally studied from the\ninformation-theoretic perspective, wherein one is interested in the optimal\ndecay rate of error probabilities as a function of the number of samples of an\nunknown state. In this paper, we study the sample complexity of quantum\nhypothesis testing, wherein the goal is to determine the minimum number of\nsamples needed to reach a desired error probability. By making use of the\nwealth of knowledge that already exists in the literature on quantum hypothesis\ntesting, we characterize the sample complexity of binary quantum hypothesis\ntesting in the symmetric and asymmetric settings, and we provide bounds on the\nsample complexity of multiple quantum hypothesis testing. In more detail, we\nprove that the sample complexity of symmetric binary quantum hypothesis testing\ndepends logarithmically on the inverse error probability and inversely on the\nnegative logarithm of the fidelity. As a counterpart of the quantum Stein's\nlemma, we also find that the sample complexity of asymmetric binary quantum\nhypothesis testing depends logarithmically on the inverse type~II error\nprobability and inversely on the quantum relative entropy. Finally, we provide\nlower and upper bounds on the sample complexity of multiple quantum hypothesis\ntesting, with it remaining an intriguing open question to improve these bounds.\n","authors":["Hao-Chung Cheng","Nilanjana Datta","Nana Liu","Theshani Nuradha","Robert Salzmann","Mark M. Wilde"],"pdf_url":"https://arxiv.org/pdf/2403.17868v1.pdf","comment":"38 pages, 1 figure, preliminary version; see independent and\n  concurrent work of Pensia, Jog, Loh at arXiv:2403.16981"},{"id":"http://arxiv.org/abs/2401.07788v2","updated":"2024-03-26T16:49:44Z","published":"2024-01-15T15:54:54Z","title":"Activations and Gradients Compression for Model-Parallel Training","summary":"  Large neural networks require enormous computational clusters of machines.\nModel-parallel training, when the model architecture is partitioned\nsequentially between workers, is a popular approach for training modern models.\nInformation compression can be applied to decrease workers communication time,\nas it is often a bottleneck in such systems. This work explores how\nsimultaneous compression of activations and gradients in model-parallel\ndistributed training setup affects convergence. We analyze compression methods\nsuch as quantization and TopK compression, and also experiment with error\ncompensation techniques. Moreover, we employ TopK with AQ-SGD per-batch error\nfeedback approach. We conduct experiments on image classification and language\nmodel fine-tuning tasks. Our findings demonstrate that gradients require milder\ncompression rates than activations. We observe that $K=10\\%$ is the lowest TopK\ncompression level, which does not harm model convergence severely. Experiments\nalso show that models trained with TopK perform well only when compression is\nalso applied during inference. We find that error feedback techniques do not\nimprove model-parallel training compared to plain compression, but allow model\ninference without compression with almost no quality drop. Finally, when\napplied with the AQ-SGD approach, TopK stronger than with $ K=30\\%$ worsens\nmodel performance significantly.\n","authors":["Mikhail Rudakov","Aleksandr Beznosikov","Yaroslav Kholodov","Alexander Gasnikov"],"pdf_url":"https://arxiv.org/pdf/2401.07788v2.pdf","comment":"17 pages, 6 figures, 5 tables"},{"id":"http://arxiv.org/abs/2210.06459v2","updated":"2024-03-26T16:49:11Z","published":"2022-10-12T17:56:04Z","title":"Differentially private multivariate medians","summary":"  Statistical tools which satisfy rigorous privacy guarantees are necessary for\nmodern data analysis. It is well-known that robustness against contamination is\nlinked to differential privacy. Despite this fact, using multivariate medians\nfor differentially private and robust multivariate location estimation has not\nbeen systematically studied. We develop novel finite-sample performance\nguarantees for differentially private multivariate depth-based medians, which\nare essentially sharp. Our results cover commonly used depth functions, such as\nthe halfspace (or Tukey) depth, spatial depth, and the integrated dual depth.\nWe show that under Cauchy marginals, the cost of heavy-tailed location\nestimation outweighs the cost of privacy. We demonstrate our results\nnumerically using a Gaussian contamination model in dimensions up to d = 100,\nand compare them to a state-of-the-art private mean estimation algorithm. As a\nby-product of our investigation, we prove concentration inequalities for the\noutput of the exponential mechanism about the maximizer of the population\nobjective function. This bound applies to objective functions that satisfy a\nmild regularity condition.\n","authors":["Kelly Ramsay","Aukosh Jagannath","Shoja'eddin Chenouri"],"pdf_url":"https://arxiv.org/pdf/2210.06459v2.pdf","comment":"42 pages, 3 figures, 2 tables"},{"id":"http://arxiv.org/abs/2401.06795v2","updated":"2024-03-26T16:44:34Z","published":"2024-01-08T18:42:55Z","title":"AI and Generative AI for Research Discovery and Summarization","summary":"  AI and generative AI tools, including chatbots like ChatGPT that rely on\nlarge language models (LLMs), have burst onto the scene this year, creating\nincredible opportunities to increase work productivity and improve our lives.\nStatisticians and data scientists have begun experiencing the benefits from the\navailability of these tools in numerous ways, such as the generation of\nprogramming code from text prompts to analyze data or fit statistical models.\nOne area that these tools can make a substantial impact is in research\ndiscovery and summarization. Standalone tools and plugins to chatbots are being\ndeveloped that allow researchers to more quickly find relevant literature than\npre-2023 search tools. Furthermore, generative AI tools have improved to the\npoint where they can summarize and extract the key points from research\narticles in succinct language. Finally, chatbots based on highly parameterized\nLLMs can be used to simulate abductive reasoning, which provides researchers\nthe ability to make connections among related technical topics, which can also\nbe used for research discovery. We review the developments in AI and generative\nAI for research discovery and summarization, and propose directions where these\ntypes of tools are likely to head in the future that may be of interest to\nstatistician and data scientists.\n","authors":["Mark Glickman","Yi Zhang"],"pdf_url":"https://arxiv.org/pdf/2401.06795v2.pdf","comment":"29 pages, 9 figures"},{"id":"http://arxiv.org/abs/2403.17853v1","updated":"2024-03-26T16:42:30Z","published":"2024-03-26T16:42:30Z","title":"Using Domain Knowledge to Guide Dialog Structure Induction via Neural\n  Probabilistic Soft Logic","summary":"  Dialog Structure Induction (DSI) is the task of inferring the latent dialog\nstructure (i.e., a set of dialog states and their temporal transitions) of a\ngiven goal-oriented dialog. It is a critical component for modern dialog system\ndesign and discourse analysis. Existing DSI approaches are often purely\ndata-driven, deploy models that infer latent states without access to domain\nknowledge, underperform when the training corpus is limited/noisy, or have\ndifficulty when test dialogs exhibit distributional shifts from the training\ndomain. This work explores a neural-symbolic approach as a potential solution\nto these problems. We introduce Neural Probabilistic Soft Logic Dialogue\nStructure Induction (NEUPSL DSI), a principled approach that injects symbolic\nknowledge into the latent space of a generative neural model. We conduct a\nthorough empirical investigation on the effect of NEUPSL DSI learning on hidden\nrepresentation quality, few-shot learning, and out-of-domain generalization\nperformance. Over three dialog structure induction datasets and across\nunsupervised and semi-supervised settings for standard and cross-domain\ngeneralization, the injection of symbolic knowledge using NEUPSL DSI provides a\nconsistent boost in performance over the canonical baselines.\n","authors":["Connor Pryor","Quan Yuan","Jeremiah Liu","Mehran Kazemi","Deepak Ramachandran","Tania Bedrax-Weiss","Lise Getoor"],"pdf_url":"https://arxiv.org/pdf/2403.17853v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17852v1","updated":"2024-03-26T16:40:08Z","published":"2024-03-26T16:40:08Z","title":"Counterfactual Fairness through Transforming Data Orthogonal to Bias","summary":"  Machine learning models have shown exceptional prowess in solving complex\nissues across various domains. Nonetheless, these models can sometimes exhibit\nbiased decision-making, leading to disparities in treatment across different\ngroups. Despite the extensive research on fairness, the nuanced effects of\nmultivariate and continuous sensitive variables on decision-making outcomes\nremain insufficiently studied. We introduce a novel data pre-processing\nalgorithm, Orthogonal to Bias (OB), designed to remove the influence of a group\nof continuous sensitive variables, thereby facilitating counterfactual fairness\nin machine learning applications. Our approach is grounded in the assumption of\na jointly normal distribution within a structural causal model (SCM), proving\nthat counterfactual fairness can be achieved by ensuring the data is\nuncorrelated with sensitive variables. The OB algorithm is model-agnostic,\ncatering to a wide array of machine learning models and tasks, and includes a\nsparse variant to enhance numerical stability through regularization. Through\nempirical evaluation on simulated and real-world datasets - including the adult\nincome and the COMPAS recidivism datasets - our methodology demonstrates its\ncapacity to enable fairer outcomes without compromising accuracy.\n","authors":["Shuyi Chen","Shixiang Zhu"],"pdf_url":"https://arxiv.org/pdf/2403.17852v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17847v1","updated":"2024-03-26T16:36:50Z","published":"2024-03-26T16:36:50Z","title":"Climate Downscaling: A Deep-Learning Based Super-resolution Model of\n  Precipitation Data with Attention Block and Skip Connections","summary":"  Human activities accelerate consumption of fossil fuels and produce\ngreenhouse gases, resulting in urgent issues today: global warming and the\nclimate change. These indirectly cause severe natural disasters, plenty of\nlives suffering and huge losses of agricultural properties. To mitigate impacts\non our lands, scientists are developing renewable, reusable, and clean energies\nand climatologists are trying to predict the extremes. Meanwhile, governments\nare publicizing resource-saving policies for a more eco-friendly society and\narousing environment awareness. One of the most influencing factors is the\nprecipitation, bringing condensed water vapor onto lands. Water resources are\nthe most significant but basic needs in society, not only supporting our\nlivings, but also economics. In Taiwan, although the average annual\nprecipitation is up to 2,500 millimeter (mm), the water allocation for each\nperson is lower than the global average due to drastically geographical\nelevation changes and uneven distribution through the year. Thus, it is crucial\nto track and predict the rainfall to make the most use of it and to prevent the\nfloods. However, climate models have limited resolution and require intensive\ncomputational power for local-scale use. Therefore, we proposed a deep\nconvolutional neural network with skip connections, attention blocks, and\nauxiliary data concatenation, in order to downscale the low-resolution\nprecipitation data into high-resolution one. Eventually, we compare with other\nclimate downscaling methods and show better performance in metrics of Mean\nAbsolute Error (MAE), Root Mean Square Error (RMSE), Pearson Correlation,\nstructural similarity index (SSIM), and forecast indicators.\n","authors":["Chia-Hao Chiang","Zheng-Han Huang","Liwen Liu","Hsin-Chien Liang","Yi-Chi Wang","Wan-Ling Tseng","Chao Wang","Che-Ta Chen","Ko-Chih Wang"],"pdf_url":"https://arxiv.org/pdf/2403.17847v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17846v1","updated":"2024-03-26T16:36:43Z","published":"2024-03-26T16:36:43Z","title":"Hierarchical Open-Vocabulary 3D Scene Graphs for Language-Grounded Robot\n  Navigation","summary":"  Recent open-vocabulary robot mapping methods enrich dense geometric maps with\npre-trained visual-language features. While these maps allow for the prediction\nof point-wise saliency maps when queried for a certain language concept,\nlarge-scale environments and abstract queries beyond the object level still\npose a considerable hurdle, ultimately limiting language-grounded robotic\nnavigation. In this work, we present HOV-SG, a hierarchical open-vocabulary 3D\nscene graph mapping approach for language-grounded robot navigation. Leveraging\nopen-vocabulary vision foundation models, we first obtain state-of-the-art\nopen-vocabulary segment-level maps in 3D and subsequently construct a 3D scene\ngraph hierarchy consisting of floor, room, and object concepts, each enriched\nwith open-vocabulary features. Our approach is able to represent multi-story\nbuildings and allows robotic traversal of those using a cross-floor Voronoi\ngraph. HOV-SG is evaluated on three distinct datasets and surpasses previous\nbaselines in open-vocabulary semantic accuracy on the object, room, and floor\nlevel while producing a 75% reduction in representation size compared to dense\nopen-vocabulary maps. In order to prove the efficacy and generalization\ncapabilities of HOV-SG, we showcase successful long-horizon\nlanguage-conditioned robot navigation within real-world multi-storage\nenvironments. We provide code and trial video data at http://hovsg.github.io/.\n","authors":["Abdelrhman Werby","Chenguang Huang","Martin Büchner","Abhinav Valada","Wolfram Burgard"],"pdf_url":"https://arxiv.org/pdf/2403.17846v1.pdf","comment":"Code and video are available at http://hovsg.github.io/"},{"id":"http://arxiv.org/abs/2312.17336v2","updated":"2024-03-26T16:35:15Z","published":"2023-12-28T19:28:23Z","title":"PINN surrogate of Li-ion battery models for parameter inference. Part\n  II: Regularization and application of the pseudo-2D model","summary":"  Bayesian parameter inference is useful to improve Li-ion battery diagnostics\nand can help formulate battery aging models. However, it is computationally\nintensive and cannot be easily repeated for multiple cycles, multiple operating\nconditions, or multiple replicate cells. To reduce the computational cost of\nBayesian calibration, numerical solvers for physics-based models can be\nreplaced with faster surrogates. A physics-informed neural network (PINN) is\ndeveloped as a surrogate for the pseudo-2D (P2D) battery model calibration. For\nthe P2D surrogate, additional training regularization was needed as compared to\nthe PINN single-particle model (SPM) developed in Part I. Both the PINN SPM and\nP2D surrogate models are exercised for parameter inference and compared to data\nobtained from a direct numerical solution of the governing equations. A\nparameter inference study highlights the ability to use these PINNs to\ncalibrate scaling parameters for the cathode Li diffusion and the anode\nexchange current density. By realizing computational speed-ups of 2250x for the\nP2D model, as compared to using standard integrating methods, the PINN\nsurrogates enable rapid state-of-health diagnostics. In the low-data\navailability scenario, the testing error was estimated to 2mV for the SPM\nsurrogate and 10mV for the P2D surrogate which could be mitigated with\nadditional data.\n","authors":["Malik Hassanaly","Peter J. Weddle","Ryan N. King","Subhayan De","Alireza Doostan","Corey R. Randall","Eric J. Dufek","Andrew M. Colclasure","Kandler Smith"],"pdf_url":"https://arxiv.org/pdf/2312.17336v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17845v1","updated":"2024-03-26T16:34:05Z","published":"2024-03-26T16:34:05Z","title":"TractOracle: towards an anatomically-informed reward function for\n  RL-based tractography","summary":"  Reinforcement learning (RL)-based tractography is a competitive alternative\nto machine learning and classical tractography algorithms due to its high\nanatomical accuracy obtained without the need for any annotated data. However,\nthe reward functions so far used to train RL agents do not encapsulate\nanatomical knowledge which causes agents to generate spurious false positives\ntracts. In this paper, we propose a new RL tractography system, TractOracle,\nwhich relies on a reward network trained for streamline classification. This\nnetwork is used both as a reward function during training as well as a mean for\nstopping the tracking process early and thus reduce the number of false\npositive streamlines. This makes our system a unique method that evaluates and\nreconstructs WM streamlines at the same time. We report an improvement of true\npositive ratios by almost 20\\% and a reduction of 3x of false positive ratios\non one dataset and an increase between 2x and 7x in the number true positive\nstreamlines on another dataset.\n","authors":["Antoine Théberge","Maxime Descoteaux","Pierre-Marc Jodoin"],"pdf_url":"https://arxiv.org/pdf/2403.17845v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17844v1","updated":"2024-03-26T16:33:12Z","published":"2024-03-26T16:33:12Z","title":"Mechanistic Design and Scaling of Hybrid Architectures","summary":"  The development of deep learning architectures is a resource-demanding\nprocess, due to a vast design space, long prototyping times, and high compute\ncosts associated with at-scale model training and evaluation. We set out to\nsimplify this process by grounding it in an end-to-end mechanistic architecture\ndesign (MAD) pipeline, encompassing small-scale capability unit tests\npredictive of scaling laws. Through a suite of synthetic token manipulation\ntasks such as compression and recall, designed to probe capabilities, we\nidentify and test new hybrid architectures constructed from a variety of\ncomputational primitives. We experimentally validate the resulting\narchitectures via an extensive compute-optimal and a new state-optimal scaling\nlaw analysis, training over 500 language models between 70M to 7B parameters.\nSurprisingly, we find MAD synthetics to correlate with compute-optimal\nperplexity, enabling accurate evaluation of new architectures via isolated\nproxy tasks. The new architectures found via MAD, based on simple ideas such as\nhybridization and sparsity, outperform state-of-the-art Transformer,\nconvolutional, and recurrent architectures (Transformer++, Hyena, Mamba) in\nscaling, both at compute-optimal budgets and in overtrained regimes. Overall,\nthese results provide evidence that performance on curated synthetic tasks can\nbe predictive of scaling laws, and that an optimal architecture should leverage\nspecialized layers via a hybrid topology.\n","authors":["Michael Poli","Armin W Thomas","Eric Nguyen","Pragaash Ponnusamy","Björn Deiseroth","Kristian Kersting","Taiji Suzuki","Brian Hie","Stefano Ermon","Christopher Ré","Ce Zhang","Stefano Massaroli"],"pdf_url":"https://arxiv.org/pdf/2403.17844v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17837v1","updated":"2024-03-26T16:24:42Z","published":"2024-03-26T16:24:42Z","title":"GTA-HDR: A Large-Scale Synthetic Dataset for HDR Image Reconstruction","summary":"  High Dynamic Range (HDR) content (i.e., images and videos) has a broad range\nof applications. However, capturing HDR content from real-world scenes is\nexpensive and time- consuming. Therefore, the challenging task of\nreconstructing visually accurate HDR images from their Low Dynamic Range (LDR)\ncounterparts is gaining attention in the vision research community. A major\nchallenge in this research problem is the lack of datasets, which capture\ndiverse scene conditions (e.g., lighting, shadows, weather, locations,\nlandscapes, objects, humans, buildings) and various image features (e.g.,\ncolor, contrast, saturation, hue, luminance, brightness, radiance). To address\nthis gap, in this paper, we introduce GTA-HDR, a large-scale synthetic dataset\nof photo-realistic HDR images sampled from the GTA-V video game. We perform\nthorough evaluation of the proposed dataset, which demonstrates significant\nqualitative and quantitative improvements of the state-of-the-art HDR image\nreconstruction methods. Furthermore, we demonstrate the effectiveness of the\nproposed dataset and its impact on additional computer vision tasks including\n3D human pose estimation, human body part segmentation, and holistic scene\nsegmentation. The dataset, data collection pipeline, and evaluation code are\navailable at: https://github.com/HrishavBakulBarua/GTA-HDR.\n","authors":["Hrishav Bakul Barua","Kalin Stefanov","KokSheik Wong","Abhinav Dhall","Ganesh Krishnasamy"],"pdf_url":"https://arxiv.org/pdf/2403.17837v1.pdf","comment":"Submitted to IEEE"},{"id":"http://arxiv.org/abs/2305.03123v2","updated":"2024-03-26T16:22:54Z","published":"2023-04-13T16:01:28Z","title":"ChatGPT Needs SPADE (Sustainability, PrivAcy, Digital divide, and\n  Ethics) Evaluation: A Review","summary":"  ChatGPT is another large language model (LLM) vastly available for the\nconsumers on their devices but due to its performance and ability to converse\neffectively, it has gained a huge popularity amongst research as well as\nindustrial community. Recently, many studies have been published to show the\neffectiveness, efficiency, integration, and sentiments of chatGPT and other\nLLMs. In contrast, this study focuses on the important aspects that are mostly\noverlooked, i.e. sustainability, privacy, digital divide, and ethics and\nsuggests that not only chatGPT but every subsequent entry in the category of\nconversational bots should undergo Sustainability, PrivAcy, Digital divide, and\nEthics (SPADE) evaluation. This paper discusses in detail the issues and\nconcerns raised over chatGPT in line with aforementioned characteristics. We\nalso discuss the recent EU AI Act briefly in accordance with the SPADE\nevaluation. We support our hypothesis by some preliminary data collection and\nvisualizations along with hypothesized facts. We also suggest mitigations and\nrecommendations for each of the concerns. Furthermore, we also suggest some\npolicies and recommendations for AI policy act, if designed by the governments.\n","authors":["Sunder Ali Khowaja","Parus Khuwaja","Kapal Dev","Weizheng Wang","Lewis Nkenyereye"],"pdf_url":"https://arxiv.org/pdf/2305.03123v2.pdf","comment":"29 pages, 8 figures, 4 tables"},{"id":"http://arxiv.org/abs/2312.17329v2","updated":"2024-03-26T16:22:36Z","published":"2023-12-28T19:09:56Z","title":"PINN surrogate of Li-ion battery models for parameter inference. Part I:\n  Implementation and multi-fidelity hierarchies for the single-particle model","summary":"  To plan and optimize energy storage demands that account for Li-ion battery\naging dynamics, techniques need to be developed to diagnose battery internal\nstates accurately and rapidly. This study seeks to reduce the computational\nresources needed to determine a battery's internal states by replacing\nphysics-based Li-ion battery models -- such as the single-particle model (SPM)\nand the pseudo-2D (P2D) model -- with a physics-informed neural network (PINN)\nsurrogate. The surrogate model makes high-throughput techniques, such as\nBayesian calibration, tractable to determine battery internal parameters from\nvoltage responses. This manuscript is the first of a two-part series that\nintroduces PINN surrogates of Li-ion battery models for parameter inference\n(i.e., state-of-health diagnostics). In this first part, a method is presented\nfor constructing a PINN surrogate of the SPM. A multi-fidelity hierarchical\ntraining, where several neural nets are trained with multiple physics-loss\nfidelities is shown to significantly improve the surrogate accuracy when only\ntraining on the governing equation residuals. The implementation is made\navailable in a companion repository (https://github.com/NREL/pinnstripes). The\ntechniques used to develop a PINN surrogate of the SPM are extended in Part II\nfor the PINN surrogate for the P2D battery model, and explore the Bayesian\ncalibration capabilities of both surrogates.\n","authors":["Malik Hassanaly","Peter J. Weddle","Ryan N. King","Subhayan De","Alireza Doostan","Corey R. Randall","Eric J. Dufek","Andrew M. Colclasure","Kandler Smith"],"pdf_url":"https://arxiv.org/pdf/2312.17329v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.14427v2","updated":"2024-03-26T16:15:40Z","published":"2023-11-24T12:00:50Z","title":"Disentangling the Spectral Properties of the Hodge Laplacian: Not All\n  Small Eigenvalues Are Equal","summary":"  The rich spectral information of the graph Laplacian has been instrumental in\ngraph theory, machine learning, and graph signal processing for applications\nsuch as graph classification, clustering, or eigenmode analysis. Recently, the\nHodge Laplacian has come into focus as a generalisation of the ordinary\nLaplacian for higher-order graph models such as simplicial and cellular\ncomplexes. Akin to the traditional analysis of graph Laplacians, many authors\nanalyse the smallest eigenvalues of the Hodge Laplacian, which are connected to\nimportant topological properties such as homology. However, small eigenvalues\nof the Hodge Laplacian can carry different information depending on whether\nthey are related to curl or gradient eigenmodes, and thus may not be\ncomparable. We therefore introduce the notion of persistent eigenvector\nsimilarity and provide a method to track individual harmonic, curl, and\ngradient eigenvectors/-values through the so-called persistence filtration,\nleveraging the full information contained in the Hodge-Laplacian spectrum\nacross all possible scales of a point cloud. Finally, we use our insights (a)\nto introduce a novel form of Hodge spectral clustering and (b) to classify\nedges and higher-order simplices based on their relationship to the smallest\nharmonic, curl, and gradient eigenvectors.\n","authors":["Vincent P. Grande","Michael T. Schaub"],"pdf_url":"https://arxiv.org/pdf/2311.14427v2.pdf","comment":"5 pages, 4 figures, comments welcome"},{"id":"http://arxiv.org/abs/2403.17833v1","updated":"2024-03-26T16:14:43Z","published":"2024-03-26T16:14:43Z","title":"GPFL: A Gradient Projection-Based Client Selection Framework for\n  Efficient Federated Learning","summary":"  Federated learning client selection is crucial for determining participant\nclients while balancing model accuracy and communication efficiency. Existing\nmethods have limitations in handling data heterogeneity, computational burdens,\nand independent client treatment. To address these challenges, we propose GPFL,\nwhich measures client value by comparing local and global descent directions.\nWe also employ an Exploit-Explore mechanism to enhance performance.\nExperimental results on FEMINST and CIFAR-10 datasets demonstrate that GPFL\noutperforms baselines in Non-IID scenarios, achieving over 9\\% improvement in\nFEMINST test accuracy. Moreover, GPFL exhibits shorter computation times\nthrough pre-selection and parameter reuse in federated learning.\n","authors":["Shijie Na","Yuzhi Liang","Siu-Ming Yiu"],"pdf_url":"https://arxiv.org/pdf/2403.17833v1.pdf","comment":"8 pages, 5 figures"},{"id":"http://arxiv.org/abs/2403.17831v1","updated":"2024-03-26T16:13:55Z","published":"2024-03-26T16:13:55Z","title":"Learning the Optimal Power Flow: Environment Design Matters","summary":"  To solve the optimal power flow (OPF) problem, reinforcement learning (RL)\nemerges as a promising new approach. However, the RL-OPF literature is strongly\ndivided regarding the exact formulation of the OPF problem as an RL\nenvironment. In this work, we collect and implement diverse environment design\ndecisions from the literature regarding training data, observation space,\nepisode definition, and reward function choice. In an experimental analysis, we\nshow the significant impact of these environment design options on RL-OPF\ntraining performance. Further, we derive some first recommendations regarding\nthe choice of these design decisions. The created environment framework is\nfully open-source and can serve as a benchmark for future research in the\nRL-OPF field.\n","authors":["Thomas Wolgast","Astrid Nieße"],"pdf_url":"https://arxiv.org/pdf/2403.17831v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2108.13969v3","updated":"2024-03-26T16:13:26Z","published":"2021-08-31T16:51:00Z","title":"Semi-Supervised Crowd Counting from Unlabeled Data","summary":"  Automatic Crowd behavior analysis can be applied to effectively help the\ndaily transportation statistics and planning, which helps the smart city\nconstruction. As one of the most important keys, crowd counting has drawn\nincreasing attention. Recent works achieved promising performance but relied on\nthe supervised paradigm with expensive crowd annotations. To alleviate the\nannotation cost in real-world transportation scenarios, in this work we\nproposed a semi-supervised learning framework $S^{4}\\textit{Crowd}$, which can\nleverage both unlabeled/labeled data for robust crowd counting. In the\nunsupervised pathway, two \\textit{self-supervised losses} were proposed to\nsimulate the crowd variations such as scale, illumination, based on which\nsupervised information pseudo labels were generated and gradually refined. We\nalso proposed a crowd-driven recurrent unit \\textit{Gated-Crowd-Recurrent-Unit\n(GCRU)}, which can preserve discriminant crowd information by extracting\nsecond-order statistics, yielding pseudo labels with improved quality. A joint\nloss including both unsupervised/supervised information was proposed, and a\ndynamic weighting strategy was employed to balance the importance of the\nunsupervised loss and supervised loss at different training stages. We\nconducted extensive experiments on four popular crowd counting datasets in\nsemi-supervised settings. Experimental results supported the effectiveness of\neach proposed component in our $S^{4}$Crowd framework. Our method achieved\ncompetitive performance in semi-supervised learning approaches on these crowd\ncounting datasets.\n","authors":["Haoran Duan","Fan Wan","Rui Sun","Zeyu Wang","Varun Ojha","Yu Guan","Hubert P. H. Shum","Bingzhang Hu","Yang Long"],"pdf_url":"https://arxiv.org/pdf/2108.13969v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17827v1","updated":"2024-03-26T16:06:42Z","published":"2024-03-26T16:06:42Z","title":"DiffH2O: Diffusion-Based Synthesis of Hand-Object Interactions from\n  Textual Descriptions","summary":"  Generating natural hand-object interactions in 3D is challenging as the\nresulting hand and object motions are expected to be physically plausible and\nsemantically meaningful. Furthermore, generalization to unseen objects is\nhindered by the limited scale of available hand-object interaction datasets. We\npropose DiffH2O, a novel method to synthesize realistic, one or two-handed\nobject interactions from provided text prompts and geometry of the object. The\nmethod introduces three techniques that enable effective learning from limited\ndata. First, we decompose the task into a grasping stage and a text-based\ninteraction stage and use separate diffusion models for each. In the grasping\nstage, the model only generates hand motions, whereas in the interaction phase\nboth hand and object poses are synthesized. Second, we propose a compact\nrepresentation that tightly couples hand and object poses. Third, we propose\ntwo different guidance schemes to allow more control of the generated motions:\ngrasp guidance and detailed textual guidance. Grasp guidance takes a single\ntarget grasping pose and guides the diffusion model to reach this grasp at the\nend of the grasping stage, which provides control over the grasping pose. Given\na grasping motion from this stage, multiple different actions can be prompted\nin the interaction phase. For textual guidance, we contribute comprehensive\ntext descriptions to the GRAB dataset and show that they enable our method to\nhave more fine-grained control over hand-object interactions. Our quantitative\nand qualitative evaluation demonstrates that the proposed method outperforms\nbaseline methods and leads to natural hand-object motions. Moreover, we\ndemonstrate the practicality of our framework by utilizing a hand pose estimate\nfrom an off-the-shelf pose estimator for guidance, and then sampling multiple\ndifferent actions in the interaction stage.\n","authors":["Sammy Christen","Shreyas Hampali","Fadime Sener","Edoardo Remelli","Tomas Hodan","Eric Sauser","Shugao Ma","Bugra Tekin"],"pdf_url":"https://arxiv.org/pdf/2403.17827v1.pdf","comment":"Project Page: https://diffh2o.github.io/"},{"id":"http://arxiv.org/abs/2311.15964v2","updated":"2024-03-26T15:58:26Z","published":"2023-11-27T16:07:37Z","title":"Efficient Pre-training for Localized Instruction Generation of Videos","summary":"  Procedural videos show step-by-step demonstrations of tasks like recipe\npreparation. Understanding such videos is challenging, involving the precise\nlocalization of steps and the generation of textual instructions. Manually\nannotating steps and writing instructions is costly, which limits the size of\ncurrent datasets and hinders effective learning. Leveraging large but noisy\nvideo-transcript datasets for pre-training can boost performance, but demands\nsignificant computational resources. Furthermore, transcripts contain\nirrelevant content and exhibit style variation compared to instructions written\nby human annotators. To mitigate both issues, we propose a technique,\nSieve-&-Swap, to automatically curate a smaller dataset: (i) Sieve filters\nirrelevant transcripts and (ii) Swap enhances the quality of the text\ninstruction by automatically replacing the transcripts with human-written\ninstructions from a text-only recipe dataset. The curated dataset, three orders\nof magnitude smaller than current web-scale datasets, enables efficient\ntraining of large-scale models with competitive performance. We complement our\nSieve-\\&-Swap approach with a Procedure Transformer (ProcX) for end-to-end step\nlocalization and instruction generation for procedural videos. When this model\nis pre-trained on our curated dataset, it achieves state-of-the-art performance\nin zero-shot and finetuning settings on YouCook2 and Tasty, while using a\nfraction of the computational resources.\n","authors":["Anil Batra","Davide Moltisanti","Laura Sevilla-Lara","Marcus Rohrbach","Frank Keller"],"pdf_url":"https://arxiv.org/pdf/2311.15964v2.pdf","comment":"This version has some missing experiments and elaborative technical\n  details"},{"id":"http://arxiv.org/abs/2403.17811v1","updated":"2024-03-26T15:50:37Z","published":"2024-03-26T15:50:37Z","title":"Are Compressed Language Models Less Subgroup Robust?","summary":"  To reduce the inference cost of large language models, model compression is\nincreasingly used to create smaller scalable models. However, little is known\nabout their robustness to minority subgroups defined by the labels and\nattributes of a dataset. In this paper, we investigate the effects of 18\ndifferent compression methods and settings on the subgroup robustness of BERT\nlanguage models. We show that worst-group performance does not depend on model\nsize alone, but also on the compression method used. Additionally, we find that\nmodel compression does not always worsen the performance on minority subgroups.\nAltogether, our analysis serves to further research into the subgroup\nrobustness of model compression.\n","authors":["Leonidas Gee","Andrea Zugarini","Novi Quadrianto"],"pdf_url":"https://arxiv.org/pdf/2403.17811v1.pdf","comment":"The 2023 Conference on Empirical Methods in Natural Language\n  Processing (EMNLP 2023)"},{"id":"http://arxiv.org/abs/2403.17808v1","updated":"2024-03-26T15:45:29Z","published":"2024-03-26T15:45:29Z","title":"Annotated Biomedical Video Generation using Denoising Diffusion\n  Probabilistic Models and Flow Fields","summary":"  The segmentation and tracking of living cells play a vital role within the\nbiomedical domain, particularly in cancer research, drug development, and\ndevelopmental biology. These are usually tedious and time-consuming tasks that\nare traditionally done by biomedical experts. Recently, to automatize these\nprocesses, deep learning based segmentation and tracking methods have been\nproposed. These methods require large-scale datasets and their full potential\nis constrained by the scarcity of annotated data in the biomedical imaging\ndomain. To address this limitation, we propose Biomedical Video Diffusion Model\n(BVDM), capable of generating realistic-looking synthetic microscopy videos.\nTrained only on a single real video, BVDM can generate videos of arbitrary\nlength with pixel-level annotations that can be used for training data-hungry\nmodels. It is composed of a denoising diffusion probabilistic model (DDPM)\ngenerating high-fidelity synthetic cell microscopy images and a flow prediction\nmodel (FPM) predicting the non-rigid transformation between consecutive video\nframes. During inference, initially, the DDPM imposes realistic cell textures\non synthetic cell masks which are generated based on real data statistics. The\nflow prediction model predicts the flow field between consecutive masks and\napplies that to the DDPM output from the previous time frame to create the next\none while keeping temporal consistency. BVDM outperforms state-of-the-art\nsynthetic live cell microscopy video generation models. Furthermore, we\ndemonstrate that a sufficiently large synthetic dataset enhances the\nperformance of cell segmentation and tracking models compared to using a\nlimited amount of available real data.\n","authors":["Rüveyda Yilmaz","Dennis Eschweiler","Johannes Stegmaier"],"pdf_url":"https://arxiv.org/pdf/2403.17808v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17806v1","updated":"2024-03-26T15:44:58Z","published":"2024-03-26T15:44:58Z","title":"Have Faith in Faithfulness: Going Beyond Circuit Overlap When Finding\n  Model Mechanisms","summary":"  Many recent language model (LM) interpretability studies have adopted the\ncircuits framework, which aims to find the minimal computational subgraph, or\ncircuit, that explains LM behavior on a given task. Most studies determine\nwhich edges belong in a LM's circuit by performing causal interventions on each\nedge independently, but this scales poorly with model size. Edge attribution\npatching (EAP), gradient-based approximation to interventions, has emerged as a\nscalable but imperfect solution to this problem. In this paper, we introduce a\nnew method - EAP with integrated gradients (EAP-IG) - that aims to better\nmaintain a core property of circuits: faithfulness. A circuit is faithful if\nall model edges outside the circuit can be ablated without changing the model's\nperformance on the task; faithfulness is what justifies studying circuits,\nrather than the full model. Our experiments demonstrate that circuits found\nusing EAP are less faithful than those found using EAP-IG, even though both\nhave high node overlap with circuits found previously using causal\ninterventions. We conclude more generally that when using circuits to compare\nthe mechanisms models use to solve tasks, faithfulness, not overlap, is what\nshould be measured.\n","authors":["Michael Hanna","Sandro Pezzelle","Yonatan Belinkov"],"pdf_url":"https://arxiv.org/pdf/2403.17806v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17805v1","updated":"2024-03-26T15:42:04Z","published":"2024-03-26T15:42:04Z","title":"Scenario-Based Curriculum Generation for Multi-Agent Autonomous Driving","summary":"  The automated generation of diverse and complex training scenarios has been\nan important ingredient in many complex learning tasks. Especially in\nreal-world application domains, such as autonomous driving, auto-curriculum\ngeneration is considered vital for obtaining robust and general policies.\nHowever, crafting traffic scenarios with multiple, heterogeneous agents is\ntypically considered as a tedious and time-consuming task, especially in more\ncomplex simulation environments. In our work, we introduce MATS-Gym, a\nMulti-Agent Traffic Scenario framework to train agents in CARLA, a\nhigh-fidelity driving simulator. MATS-Gym is a multi-agent training framework\nfor autonomous driving that uses partial scenario specifications to generate\ntraffic scenarios with variable numbers of agents. This paper unifies various\nexisting approaches to traffic scenario description into a single training\nframework and demonstrates how it can be integrated with techniques from\nunsupervised environment design to automate the generation of adaptive\nauto-curricula. The code is available at\nhttps://github.com/AutonomousDrivingExaminer/mats-gym.\n","authors":["Axel Brunnbauer","Luigi Berducci","Peter Priller","Dejan Nickovic","Radu Grosu"],"pdf_url":"https://arxiv.org/pdf/2403.17805v1.pdf","comment":"7 Pages, Under Review"},{"id":"http://arxiv.org/abs/2302.03788v4","updated":"2024-03-26T15:41:28Z","published":"2023-02-07T22:56:58Z","title":"Toward a Theory of Causation for Interpreting Neural Code Models","summary":"  Neural Language Models of Code, or Neural Code Models (NCMs), are rapidly\nprogressing from research prototypes to commercial developer tools. As such,\nunderstanding the capabilities and limitations of such models is becoming\ncritical. However, the abilities of these models are typically measured using\nautomated metrics that often only reveal a portion of their real-world\nperformance. While, in general, the performance of NCMs appears promising,\ncurrently much is unknown about how such models arrive at decisions. To this\nend, this paper introduces $do_{code}$, a post hoc interpretability method\nspecific to NCMs that is capable of explaining model predictions. $do_{code}$\nis based upon causal inference to enable programming language-oriented\nexplanations. While the theoretical underpinnings of $do_{code}$ are extensible\nto exploring different model properties, we provide a concrete instantiation\nthat aims to mitigate the impact of spurious correlations by grounding\nexplanations of model behavior in properties of programming languages. To\ndemonstrate the practical benefit of $do_{code}$, we illustrate the insights\nthat our framework can provide by performing a case study on two popular deep\nlearning architectures and ten NCMs. The results of this case study illustrate\nthat our studied NCMs are sensitive to changes in code syntax. All our NCMs,\nexcept for the BERT-like model, statistically learn to predict tokens related\nto blocks of code (\\eg brackets, parenthesis, semicolon) with less confounding\nbias as compared to other programming language constructs. These insights\ndemonstrate the potential of $do_{code}$ as a useful method to detect and\nfacilitate the elimination of confounding bias in NCMs.\n","authors":["David N. Palacio","Alejandro Velasco","Nathan Cooper","Alvaro Rodriguez","Kevin Moran","Denys Poshyvanyk"],"pdf_url":"https://arxiv.org/pdf/2302.03788v4.pdf","comment":"Accepted to appear in IEEE Transactions on Software Engineering"},{"id":"http://arxiv.org/abs/2306.15909v4","updated":"2024-03-26T15:13:20Z","published":"2023-06-28T04:16:16Z","title":"RL$^3$: Boosting Meta Reinforcement Learning via RL inside RL$^2$","summary":"  Meta reinforcement learning (meta-RL) methods such as RL$^2$ have emerged as\npromising approaches for learning data-efficient RL algorithms tailored to a\ngiven task distribution. However, they show poor asymptotic performance and\nstruggle with out-of-distribution tasks because they rely on sequence models,\nsuch as recurrent neural networks or transformers, to process experiences\nrather than summarize them using general-purpose RL components such as value\nfunctions. In contrast, traditional RL algorithms are data-inefficient as they\ndo not use domain knowledge, but they do converge to an optimal policy in the\nlimit. We propose RL$^3$, a principled hybrid approach that incorporates\naction-values, learned per task through traditional RL, in the inputs to\nmeta-RL. We show that RL$^3$ earns greater cumulative reward in the long term,\ncompared to RL$^2$, while maintaining data-efficiency in the short term, and\ngeneralizes better to out-of-distribution tasks. Experiments are conducted on\nboth custom and benchmark discrete domains from the meta-RL literature that\nexhibit a range of short-term, long-term, and complex dependencies.\n","authors":["Abhinav Bhatia","Samer B. Nashed","Shlomo Zilberstein"],"pdf_url":"https://arxiv.org/pdf/2306.15909v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.12243v4","updated":"2024-03-26T15:12:19Z","published":"2023-08-23T16:42:27Z","title":"Multi-Objective Optimization for Sparse Deep Multi-Task Learning","summary":"  Different conflicting optimization criteria arise naturally in various Deep\nLearning scenarios. These can address different main tasks (i.e., in the\nsetting of Multi-Task Learning), but also main and secondary tasks such as loss\nminimization versus sparsity. The usual approach is a simple weighting of the\ncriteria, which formally only works in the convex setting. In this paper, we\npresent a Multi-Objective Optimization algorithm using a modified Weighted\nChebyshev scalarization for training Deep Neural Networks (DNNs) with respect\nto several tasks. By employing this scalarization technique, the algorithm can\nidentify all optimal solutions of the original problem while reducing its\ncomplexity to a sequence of single-objective problems. The simplified problems\nare then solved using an Augmented Lagrangian method, enabling the use of\npopular optimization techniques such as Adam and Stochastic Gradient Descent,\nwhile efficaciously handling constraints. Our work aims to address the\n(economical and also ecological) sustainability issue of DNN models, with a\nparticular focus on Deep Multi-Task models, which are typically designed with a\nvery large number of weights to perform equally well on multiple tasks. Through\nexperiments conducted on two Machine Learning datasets, we demonstrate the\npossibility of adaptively sparsifying the model during training without\nsignificantly impacting its performance, if we are willing to apply\ntask-specific adaptations to the network weights. Code is available at\nhttps://github.com/salomonhotegni/MDMTN\n","authors":["S. S. Hotegni","M. Berkemeier","S. Peitz"],"pdf_url":"https://arxiv.org/pdf/2308.12243v4.pdf","comment":"12 pages, 7 figures"},{"id":"http://arxiv.org/abs/2403.17775v1","updated":"2024-03-26T15:07:58Z","published":"2024-03-26T15:07:58Z","title":"Secure Aggregation is Not Private Against Membership Inference Attacks","summary":"  Secure aggregation (SecAgg) is a commonly-used privacy-enhancing mechanism in\nfederated learning, affording the server access only to the aggregate of model\nupdates while safeguarding the confidentiality of individual updates. Despite\nwidespread claims regarding SecAgg's privacy-preserving capabilities, a formal\nanalysis of its privacy is lacking, making such presumptions unjustified. In\nthis paper, we delve into the privacy implications of SecAgg by treating it as\na local differential privacy (LDP) mechanism for each local update. We design a\nsimple attack wherein an adversarial server seeks to discern which update\nvector a client submitted, out of two possible ones, in a single training round\nof federated learning under SecAgg. By conducting privacy auditing, we assess\nthe success probability of this attack and quantify the LDP guarantees provided\nby SecAgg. Our numerical results unveil that, contrary to prevailing claims,\nSecAgg offers weak privacy against membership inference attacks even in a\nsingle training round. Indeed, it is difficult to hide a local update by adding\nother independent local updates when the updates are of high dimension. Our\nfindings underscore the imperative for additional privacy-enhancing mechanisms,\nsuch as noise injection, in federated learning.\n","authors":["Khac-Hoang Ngo","Johan Östman","Giuseppe Durisi","Alexandre Graell i Amat"],"pdf_url":"https://arxiv.org/pdf/2403.17775v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17768v1","updated":"2024-03-26T14:54:48Z","published":"2024-03-26T14:54:48Z","title":"SciNews: From Scholarly Complexities to Public Narratives -- A Dataset\n  for Scientific News Report Generation","summary":"  Scientific news reports serve as a bridge, adeptly translating complex\nresearch articles into reports that resonate with the broader public. The\nautomated generation of such narratives enhances the accessibility of scholarly\ninsights. In this paper, we present a new corpus to facilitate this paradigm\ndevelopment. Our corpus comprises a parallel compilation of academic\npublications and their corresponding scientific news reports across nine\ndisciplines. To demonstrate the utility and reliability of our dataset, we\nconduct an extensive analysis, highlighting the divergences in readability and\nbrevity between scientific news narratives and academic manuscripts. We\nbenchmark our dataset employing state-of-the-art text generation models. The\nevaluation process involves both automatic and human evaluation, which lays the\ngroundwork for future explorations into the automated generation of scientific\nnews reports. The dataset and code related to this work are available at\nhttps://dongqi.me/projects/SciNews.\n","authors":["Dongqi Pu","Yifan Wang","Jia Loy","Vera Demberg"],"pdf_url":"https://arxiv.org/pdf/2403.17768v1.pdf","comment":"LREC-COLING 2024 Main Conference Paper"}],"Computer Science and Game Theory":[{"id":"http://arxiv.org/abs/2403.17515v1","updated":"2024-03-26T09:18:50Z","published":"2024-03-26T09:18:50Z","title":"Prediction-sharing During Training and Inference","summary":"  Two firms are engaged in a competitive prediction task. Each firm has two\nsources of data -- labeled historical data and unlabeled inference-time data --\nand uses the former to derive a prediction model, and the latter to make\npredictions on new instances. We study data-sharing contracts between the\nfirms. The novelty of our study is to introduce and highlight the differences\nbetween contracts that share prediction models only, contracts to share\ninference-time predictions only, and contracts to share both. Our analysis\nproceeds on three levels. First, we develop a general Bayesian framework that\nfacilitates our study. Second, we narrow our focus to two natural settings\nwithin this framework: (i) a setting in which the accuracy of each firm's\nprediction model is common knowledge, but the correlation between the\nrespective models is unknown; and (ii) a setting in which two hypotheses exist\nregarding the optimal predictor, and one of the firms has a structural\nadvantage in deducing it. Within these two settings we study optimal contract\nchoice. More specifically, we find the individually rational and Pareto-optimal\ncontracts for some notable cases, and describe specific settings where each of\nthe different sharing contracts emerge as optimal. Finally, in the third level\nof our analysis we demonstrate the applicability of our concepts in a synthetic\nsimulation using real loan data.\n","authors":["Yotam Gafni","Ronen Gradwohl","Moshe Tennenholtz"],"pdf_url":"https://arxiv.org/pdf/2403.17515v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17302v1","updated":"2024-03-26T01:14:18Z","published":"2024-03-26T01:14:18Z","title":"So Long Sucker: Endgame Analysis","summary":"  So Long Sucker is a strategy board game requiring 4 players, each with $c$\nchips of their designated color, and a board made of $k$ empty piles. With a\nclear set-up come intricate rules, such as: players taking turns but not in a\nfixed order, agreements between some players being made and broken at any time,\nand a player winning the game even without any chips in hand.\n  One of the main points of interest in studying this game, is finding when a\nplayer has a winning strategy. The game begins with four players that get\neliminated successively until the winner is left. To study winning strategies,\nit is of interest to look at endgame situations. We present the following game\nset-up: there are two players left in the game, Blue and Red, and only their\nrespective chip colors. In this paper, we characterize Blue's winning\nsituations and strategies through inductive reasoning.\n","authors":["Jean-Lou De Carufel","Marie Rose Jerade"],"pdf_url":"https://arxiv.org/pdf/2403.17302v1.pdf","comment":"49 pages"}],"Information Theory":[{"id":"http://arxiv.org/abs/2403.17908v1","updated":"2024-03-26T17:46:03Z","published":"2024-03-26T17:46:03Z","title":"Reciprocity Calibration of Dual-Antenna Repeaters","summary":"  We present a reciprocity calibration method for dual-antenna repeaters in\nwireless networks. The method uses bi-directional measurements between two\nnetwork nodes, A and B, where for each bi-directional measurement, the\nrepeaters are configured in different states. The nodes A and B could be two\naccess points in a distributed MIMO system, or they could be a base station and\na mobile user terminal, for example. From the calibration measurements, the\ndifferences between the repeaters' forward and reverse gains are estimated. The\nrepeaters are then (re-)configured to compensate for these differences such\nthat the repeaters appear, transparently to the network, as reciprocal\ncomponents of the propagation environment, enabling reciprocity-based\nbeamforming in the network.\n","authors":["Erik G. Larsson","Joao Vieira","Pål Frenger"],"pdf_url":"https://arxiv.org/pdf/2403.17908v1.pdf","comment":"IEEE Wireless Communications Letters, 2024"},{"id":"http://arxiv.org/abs/2306.15470v3","updated":"2024-03-26T17:32:47Z","published":"2023-06-27T13:41:54Z","title":"Task-oriented and Semantics-aware Communication Framework for\n  Avatar-centric Augmented Reality","summary":"  Upon the advent of the emerging metaverse and its related applications in\nAugmented Reality (AR), the current bit-oriented network struggles to support\nreal-time changes for the vast amount of associated information, hindering its\ndevelopment. Thus, a critical revolution in the Sixth Generation (6G) networks\nis envisioned through the joint exploitation of information context and its\nimportance to the task, leading to a communication paradigm shift towards\nsemantic and effectiveness levels. However, current research has not yet\nproposed any explicit and systematic communication framework for AR\napplications that incorporate these two levels. To fill this research gap, this\npaper presents a task-oriented and semantics-aware communication framework for\naugmented reality (TSAR) to enhance communication efficiency and effectiveness\nin 6G. Specifically, we first analyse the traditional wireless AR point cloud\ncommunication framework and then summarize our proposed semantic information\nalong with the end-to-end wireless communication. We then detail the design\nblocks of the TSAR framework, covering both semantic and effectiveness levels.\nFinally, numerous experiments have been conducted to demonstrate that, compared\nto the traditional point cloud communication framework, our proposed TSAR\nsignificantly reduces wireless AR application transmission latency by 95.6%,\nwhile improving communication effectiveness in geometry and color aspects by up\nto 82.4% and 20.4%, respectively.\n","authors":["Zhe Wang","Yansha Deng","A. Hamid Aghvami"],"pdf_url":"https://arxiv.org/pdf/2306.15470v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17868v1","updated":"2024-03-26T16:57:01Z","published":"2024-03-26T16:57:01Z","title":"Sample complexity of quantum hypothesis testing","summary":"  Quantum hypothesis testing has been traditionally studied from the\ninformation-theoretic perspective, wherein one is interested in the optimal\ndecay rate of error probabilities as a function of the number of samples of an\nunknown state. In this paper, we study the sample complexity of quantum\nhypothesis testing, wherein the goal is to determine the minimum number of\nsamples needed to reach a desired error probability. By making use of the\nwealth of knowledge that already exists in the literature on quantum hypothesis\ntesting, we characterize the sample complexity of binary quantum hypothesis\ntesting in the symmetric and asymmetric settings, and we provide bounds on the\nsample complexity of multiple quantum hypothesis testing. In more detail, we\nprove that the sample complexity of symmetric binary quantum hypothesis testing\ndepends logarithmically on the inverse error probability and inversely on the\nnegative logarithm of the fidelity. As a counterpart of the quantum Stein's\nlemma, we also find that the sample complexity of asymmetric binary quantum\nhypothesis testing depends logarithmically on the inverse type~II error\nprobability and inversely on the quantum relative entropy. Finally, we provide\nlower and upper bounds on the sample complexity of multiple quantum hypothesis\ntesting, with it remaining an intriguing open question to improve these bounds.\n","authors":["Hao-Chung Cheng","Nilanjana Datta","Nana Liu","Theshani Nuradha","Robert Salzmann","Mark M. Wilde"],"pdf_url":"https://arxiv.org/pdf/2403.17868v1.pdf","comment":"38 pages, 1 figure, preliminary version; see independent and\n  concurrent work of Pensia, Jog, Loh at arXiv:2403.16981"},{"id":"http://arxiv.org/abs/2403.17792v1","updated":"2024-03-26T15:25:21Z","published":"2024-03-26T15:25:21Z","title":"A Novel Temperature-based Model for SWIPT","summary":"  In this letter, a novel communication paradigm for simultaneous wireless\ninformation and power transfer (SWIPT) is proposed, which leverages the thermal\ncharacteristics of electromagnetic signals. In particular, the proposed scheme\nexploits the inherent thermal dynamics of electromagnetic signals, enabling the\nseamless integration of information decoding and energy harvesting (EH). As a\nconsequence, in contrast to conventional SWIPT techniques, the proposed model\neliminates the need to divide the received signal into orthogonal components.\nBy exploiting the thermal correlation between consecutive time slots, the\ncommunication channel is converted to a virtual multiple-input multiple-output\n(MIMO) channel with memory. We evaluate the achievable rate of the proposed\ntemperature-modulated channel for uniform and exponential input distributions\nand assess its performance in terms of harvested energy through a non-linear\nharvesting model. Our numerical results reveal that the exponential\ndistribution outperforms the uniform distribution in rate and harvested energy\nat low input power levels, while the uniform distribution achieves a better EH\nperformance at high input power levels.\n","authors":["Elio Faddoul","Yuan Guo","Christodoulos Skouroumounis","Ioannis Krikidis"],"pdf_url":"https://arxiv.org/pdf/2403.17792v1.pdf","comment":"This work has been accepted for publication in IEEE Wireless\n  Communications Letters"},{"id":"http://arxiv.org/abs/2403.17751v1","updated":"2024-03-26T14:43:48Z","published":"2024-03-26T14:43:48Z","title":"Robust Analysis of Full-Duplex Two-Way Space Shift Keying With RIS\n  Systems","summary":"  Reconfigurable intelligent surface (RIS)-assisted index modulation system\nschemes are considered a promising technology for sixth-generation (6G)\nwireless communication systems, which can enhance various system capabilities\nsuch as coverage and reliability. However, obtaining perfect channel state\ninformation (CSI) is challenging due to the lack of a radio frequency chain in\nRIS. In this paper, we investigate the RIS-assisted full-duplex (FD) two-way\nspace shift keying (SSK) system under imperfect CSI, where the signal emissions\nare augmented by deploying RISs in the vicinity of two FD users. The maximum\nlikelihood detector is utilized to recover the transmit antenna index. With\nthis in mind, we derive closed-form average bit error probability (ABEP)\nexpression based on the Gaussian-Chebyshev quadrature (GCQ) method and provide\nthe upper bound and asymptotic ABEP expressions in the presence of channel\nestimation errors. To gain more insights, we also derive the outage probability\nand provide the throughput of the proposed scheme with imperfect CSI. The\ncorrectness of the analytical derivation results is confirmed via Monte Carlo\nsimulations. It is demonstrated that increasing the number of elements of RIS\ncan significantly improve the ABEP performance of the FD system over the\nhalf-duplex (HD) system. Furthermore, in the high SNR region, the ABEP\nperformance of the FD system is better than that of the HD system.\n","authors":["Xusheng Zhu","Wen Chen","Qingqing Wu","Wen Fang","Chaoying Huang","Jun Li"],"pdf_url":"https://arxiv.org/pdf/2403.17751v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17652v1","updated":"2024-03-26T12:36:00Z","published":"2024-03-26T12:36:00Z","title":"Leveraging A Variety of Anchors in Cellular Network for Ubiquitous\n  Sensing","summary":"  Integrated sensing and communication (ISAC) has recently attracted tremendous\nattention from both academia and industry, being envisioned as a key part of\nthe standards for the sixth-generation (6G) cellular network. A key challenge\nof 6G-oriented ISAC lies in how to perform ubiquitous sensing based on the\ncommunication signals and devices. Previous works have made great progresses on\nstudying the signal waveform design that leads to optimal communication-sensing\nperformance tradeoff. In this article, we aim to focus on issues arising from\nthe exploitation of the communication devices for sensing in 6G network.\nParticularly, we will discuss about how to leverage various nodes available in\nthe cellular network as anchors to perform ubiquitous sensing. On one hand, the\nbase stations (BSs) will be the most important anchors in the future 6G ISAC\nnetwork, since they can generate/process radio signals with high range/angle\nresolutions, and their positions are precisely known. Correspondingly, we will\nfirst study the BS-based sensing technique. On the other hand, the BSs alone\nmay not enable ubiquitous sensing, since they cannot cover all the places with\nstrong line-of-sight (LOS) links. This motivates us to investigate the\npossibility of using other nodes that are with higher density in the network to\nact as the anchors. Along this line, we are interested in two types of new\nanchors - user equipments (UEs) and reconfigurable intelligent surfaces (RISs).\nThis paper will shed light on the opportunities and challenges brought by\nUE-assisted sensing and RIS-assisted sensing. Our goal is to devise a novel\n6G-oriented sensing architecture where BSs, UEs, and RISs can work together to\nprovide ubiquitous sensing services.\n","authors":["Liang Liu","Shuowen Zhang","Shuguang Cui"],"pdf_url":"https://arxiv.org/pdf/2403.17652v1.pdf","comment":"to appear in IEEE Communications Magazine"},{"id":"http://arxiv.org/abs/2309.09012v2","updated":"2024-03-26T10:51:08Z","published":"2023-09-16T14:50:31Z","title":"Modelling Irrational Behaviour of Residential End Users using\n  Non-Stationary Gaussian Processes","summary":"  Demand response (DR) plays a critical role in ensuring efficient electricity\nconsumption and optimal use of network assets. Yet, existing DR models often\noverlook a crucial element, the irrational behaviour of electricity end users.\nIn this work, we propose a price-responsive model that incorporates key aspects\nof end-user irrationality, specifically loss aversion, time inconsistency, and\nbounded rationality. To this end, we first develop a framework that uses\nMultiple Seasonal-Trend decomposition using Loess (MSTL) and non-stationary\nGaussian processes to model the randomness in the electricity consumption by\nresidential consumers. The impact of this model is then evaluated through a\ncommunity battery storage (CBS) business model. Additionally, we apply a\nchance-constrained optimisation model for CBS operation that deals with the\nunpredictability of the end-user irrationality. Our simulations using\nreal-world data show that the proposed DR model provides a more realistic\nestimate of end-user price-responsive behaviour when considering irrationality.\nCompared to a deterministic model that cannot fully take into account the\nirrational behaviour of end users, the chance-constrained CBS operation model\nyields an additional 19% revenue. Lastly, the business model reduces the\nelectricity costs of solar end users by 11%.\n","authors":["Nam Trong Dinh","Sahand Karimi-Arpanahi","Rui Yuan","S. Ali Pourmousavi","Mingyu Guo","Jon A. R. Liisberg","Julian Lemos-Vinasco"],"pdf_url":"https://arxiv.org/pdf/2309.09012v2.pdf","comment":"This manuscript has been accepted for publication in IEEE\n  Transactions on Smart Grid"},{"id":"http://arxiv.org/abs/2102.01412v4","updated":"2024-03-26T10:37:22Z","published":"2021-02-02T10:06:41Z","title":"On Codes for the Noisy Substring Channel","summary":"  We consider the problem of coding for the substring channel, in which\ninformation strings are observed only through their (multisets of) substrings.\nDue to existing DNA sequencing techniques and applications in DNA-based storage\nsystems, interest in this channel has renewed in recent years. In contrast to\nexisting literature, we consider a noisy channel model where information is\nsubject to noise before its substrings are sampled, motivated by in-vivo\nstorage. We study two separate noise models, substitutions or deletions. In\nboth cases, we examine families of codes which may be utilized for\nerror-correction and present combinatorial bounds on their sizes. Through a\ngeneralization of the concept of repeat-free strings, we show that the added\nrequired redundancy due to this imperfect observation assumption is sublinear,\neither when the fraction of errors in the observed substring length is\nsufficiently small, or when that length is sufficiently long. This suggests\nthat no asymptotic cost in rate is incurred by this channel model in these\ncases. Moreover, we develop an efficient encoder for such constrained strings\nin some cases. Finally, we show how a similar encoder can be used to avoid\nformation of secondary-structures in coded DNA strands, even when accounting\nfor imperfect structures.\n","authors":["Yonatan Yehezkeally","Nikita Polyanskii"],"pdf_url":"https://arxiv.org/pdf/2102.01412v4.pdf","comment":"Author submitted, peer-reviewed, version"},{"id":"http://arxiv.org/abs/2307.01525v2","updated":"2024-03-26T09:37:12Z","published":"2023-07-04T07:22:23Z","title":"OTFS-based Robust MMSE Precoding Design in Over-the-air Computation","summary":"  Over-the-air computation (AirComp), as a data aggregation method that can\nimprove network efficiency by exploiting the superposition characteristics of\nwireless channels, has received much attention recently. Meanwhile, the\northogonal time frequency space (OTFS) modulation can provide a strong Doppler\nresilience and facilitate reliable transmission for high-mobility\ncommunications. Hence, in this work, we investigate an OTFS-based AirComp\nsystem in the presence of time-frequency dual-selective channels. In\nparticular, we commence from the development of a novel transmission framework\nfor the considered system, where the pilot signal is sent together with data,\nand the channel estimation is implemented according to the echo from the access\npoint to the sensor, thereby reducing the overhead of channel state information\n(CSI) feedback. Hereafter, based on the CSI estimated from the previous frame,\na robust precoding matrix aiming at minimizing mean square error in the current\nframe is designed, which takes into account the estimation error from the\nreceiver noise and the outdated CSI. The simulation results demonstrate the\neffectiveness of the proposed robust precoding scheme by comparing it with the\nnon-robust precoding. The performance gain is more obvious in a high\nsignal-to-noise ratio in case of large channel estimation errors.\n","authors":["Dongkai Zhou","Jing Guo","Siqiang Wang","Zhong Zheng","Zesong Fei","Weijie Yuan","Xinyi Wang"],"pdf_url":"https://arxiv.org/pdf/2307.01525v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17509v1","updated":"2024-03-26T09:11:27Z","published":"2024-03-26T09:11:27Z","title":"Computer classification of linear codes based on lattice point\n  enumeration and integer linear programming","summary":"  Linear codes play a central role in coding theory and have applications in\nseveral branches of mathematics. For error correction purposes the minimum\nHamming distance should be as large as possible. Linear codes related to\napplications in Galois Geometry often require a certain divisibility of the\noccurring weights. In this paper we present an algorithmic framework for the\nclassification of linear codes over finite fields with restricted sets of\nweights. The underlying algorithms are based on lattice point enumeration and\ninteger linear programming. We present new enumeration and non-existence\nresults for projective two-weight codes, divisible codes, and additive\n$\\mathbb{F}_4$-codes.\n","authors":["Sascha Kurz"],"pdf_url":"https://arxiv.org/pdf/2403.17509v1.pdf","comment":"9 pages"}]},"2024-03-27T00:00:00Z":{"Machine Learning":[{"id":"http://arxiv.org/abs/2403.17905v2","updated":"2024-03-27T09:07:02Z","published":"2024-03-26T17:45:06Z","title":"Scalable Non-Cartesian Magnetic Resonance Imaging with R2D2","summary":"  We propose a new approach for non-Cartesian magnetic resonance image\nreconstruction. While unrolled architectures provide robustness via\ndata-consistency layers, embedding measurement operators in Deep Neural Network\n(DNN) can become impractical at large scale. Alternative Plug-and-Play (PnP)\napproaches, where the denoising DNNs are blind to the measurement setting, are\nnot affected by this limitation and have also proven effective, but their\nhighly iterative nature also affects scalability. To address this scalability\nchallenge, we leverage the \"Residual-to-Residual DNN series for high-Dynamic\nrange imaging (R2D2)\" approach recently introduced in astronomical imaging.\nR2D2's reconstruction is formed as a series of residual images, iteratively\nestimated as outputs of DNNs taking the previous iteration's image estimate and\nassociated data residual as inputs. The method can be interpreted as a learned\nversion of the Matching Pursuit algorithm. We demonstrate R2D2 in simulation,\nconsidering radial k-space sampling acquisition sequences. Our preliminary\nresults suggest that R2D2 achieves: (i) suboptimal performance compared to its\nunrolled incarnation R2D2-Net, which is however non-scalable due to the\nnecessary embedding of NUFFT-based data-consistency layers; (ii) superior\nreconstruction quality to a scalable version of R2D2-Net embedding an FFT-based\napproximation for data consistency; (iii) superior reconstruction quality to\nPnP, while only requiring few iterations.\n","authors":["Yiwei Chen","Chao Tang","Amir Aghabiglou","Chung San Chu","Yves Wiaux"],"pdf_url":"https://arxiv.org/pdf/2403.17905v2.pdf","comment":"submitted to IEEE EUSIPCO 2024"}]}}