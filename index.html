<!DOCTYPE html>
<html lang="en">

<head>
    <title>MyArxiv</title>
    <meta charset="utf-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="robots" content="noindex, nofollow"/>
    <meta name="viewport" content="width=device-width, initial-scale=1"/>
    <link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
    <link href="index.css" rel="stylesheet"/>
    <link href="https://cdn.jsdelivr.net/npm/remixicon@2.5.0/fonts/remixicon.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"
            integrity="sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx"
            crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js"
            integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR"
            crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function () {
            renderMathInElement(document.body, {
                // customised options
                // • auto-render specific keys, e.g.:
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '$', right: '$', display: false},
                    {left: '\\(', right: '\\)', display: false},
                    {left: '\\[', right: '\\]', display: true},
                    {left: "\\begin{equation}", right: "\\end{equation}", display: true},
                    {left: "\\begin{align}", right: "\\end{align}", display: true},
                    {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
                    {left: "\\begin{gather}", right: "\\end{gather}", display: true},
                    {left: "\\begin{CD}", right: "\\end{CD}", display: true},
                ],
                // • rendering keys, e.g.:
                throwOnError: false
            });
        });
    </script>
</head>

<body>
<section class="header-container">
    <div style="display:flex; justify-content:space-between; align-items:flex-end;">
        <div>
            <div class="header-title">
                MyArxiv
            </div>
        </div>

        <div class=icons>
            <label class="theme-switch" for="checkbox">
                <input type="checkbox" id="checkbox"/>
                <i id="theme-icon" class="ri-moon-line" style="font-size: 32px" rel="noopener noreferrer"></i>
            </label>
        </div>
    </div>
</section>

    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-27T00:00:00Z">2024-03-27</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Machine Learning
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scalable Non-Cartesian Magnetic Resonance Imaging with R2D2 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17905v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17905v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiwei Chen, Chao Tang, Amir Aghabiglou, Chung San Chu, Yves Wiaux
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a new approach for non-Cartesian magnetic resonance image
reconstruction. While unrolled architectures provide robustness via
data-consistency layers, embedding measurement operators in Deep Neural Network
(DNN) can become impractical at large scale. Alternative Plug-and-Play (PnP)
approaches, where the denoising DNNs are blind to the measurement setting, are
not affected by this limitation and have also proven effective, but their
highly iterative nature also affects scalability. To address this scalability
challenge, we leverage the "Residual-to-Residual DNN series for high-Dynamic
range imaging (R2D2)" approach recently introduced in astronomical imaging.
R2D2's reconstruction is formed as a series of residual images, iteratively
estimated as outputs of DNNs taking the previous iteration's image estimate and
associated data residual as inputs. The method can be interpreted as a learned
version of the Matching Pursuit algorithm. We demonstrate R2D2 in simulation,
considering radial k-space sampling acquisition sequences. Our preliminary
results suggest that R2D2 achieves: (i) suboptimal performance compared to its
unrolled incarnation R2D2-Net, which is however non-scalable due to the
necessary embedding of NUFFT-based data-consistency layers; (ii) superior
reconstruction quality to a scalable version of R2D2-Net embedding an FFT-based
approximation for data consistency; (iii) superior reconstruction quality to
PnP, while only requiring few iterations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to IEEE EUSIPCO 2024</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-26T00:00:00Z">2024-03-26</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Artificial Intelligence
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SLEDGE: Synthesizing Simulation Environments for Driving Agents with
  Generative Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17933v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17933v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kashyap Chitta, Daniel Dauner, Andreas Geiger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  SLEDGE is the first generative simulator for vehicle motion planning trained
on real-world driving logs. Its core component is a learned model that is able
to generate agent bounding boxes and lane graphs. The model's outputs serve as
an initial state for traffic simulation. The unique properties of the entities
to be generated for SLEDGE, such as their connectivity and variable count per
scene, render the naive application of most modern generative models to this
task non-trivial. Therefore, together with a systematic study of existing lane
graph representations, we introduce a novel raster-to-vector autoencoder
(RVAE). It encodes agents and the lane graph into distinct channels in a
rasterized latent map. This facilitates both lane-conditioned agent generation
and combined generation of lanes and agents with a Diffusion Transformer. Using
generated entities in SLEDGE enables greater control over the simulation, e.g.
upsampling turns or increasing traffic density. Further, SLEDGE can support
500m long routes, a capability not found in existing data-driven simulators
like nuPlan. It presents new challenges for planning algorithms, evidenced by
failure rates of over 40% for PDM, the winner of the 2023 nuPlan challenge,
when tested on hard routes and dense traffic generated by our model. Compared
to nuPlan, SLEDGE requires 500$\times$ less storage to set up (<4GB), making it
a more accessible option and helping with democratizing future research in this
field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MAGIS: LLM-Based Multi-Agent Framework for GitHub Issue Resolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17927v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17927v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Tao, Yucheng Zhou, Wenqiang Zhang, Yu Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In software evolution, resolving the emergent issues within GitHub
repositories is a complex challenge that involves not only the incorporation of
new code but also the maintenance of existing functionalities. Large Language
Models (LLMs) have shown promise in code generation and understanding but face
difficulties in code change, particularly at the repository level. To overcome
these challenges, we empirically study the reason why LLMs mostly fail to
resolve GitHub issues and analyze some impact factors. Motivated by the
empirical findings, we propose a novel LLM-based Multi-Agent framework for
GitHub Issue reSolution, MAGIS, consisting of four kinds of agents customized
for the software evolution: Manager, Repository Custodian, Developer, and
Quality Assurance Engineer agents. This framework leverages the collaboration
of various agents in the planning and coding process to unlock the potential of
LLMs to resolve GitHub issues. In experiments, we employ the SWE-bench
benchmark to compare MAGIS with popular LLMs, including GPT-3.5, GPT-4, and
Claude-2. MAGIS can resolve 13.94% GitHub issues, which significantly
outperforms the baselines. Specifically, MAGIS achieves an eight-fold increase
in resolved ratio over the direct application of GPT-4, the based LLM of our
method. We also analyze the factors for improving GitHub issue resolution
rates, such as line location, task allocation, etc.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AID: Attention Interpolation of Text-to-Image Diffusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17924v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17924v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiyuan He, Jinghao Wang, Ziwei Liu, Angela Yao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conditional diffusion models can create unseen images in various settings,
aiding image interpolation. Interpolation in latent spaces is well-studied, but
interpolation with specific conditions like text or poses is less understood.
Simple approaches, such as linear interpolation in the space of conditions,
often result in images that lack consistency, smoothness, and fidelity. To that
end, we introduce a novel training-free technique named Attention Interpolation
via Diffusion (AID). Our key contributions include 1) proposing an inner/outer
interpolated attention layer; 2) fusing the interpolated attention with
self-attention to boost fidelity; and 3) applying beta distribution to
selection to increase smoothness. We also present a variant, Prompt-guided
Attention Interpolation via Diffusion (PAID), that considers interpolation as a
condition-dependent generative process. This method enables the creation of new
images with greater consistency, smoothness, and efficiency, and offers control
over the exact path of interpolation. Our approach demonstrates effectiveness
for conceptual and spatial interpolation. Code and demo are available at
https://github.com/QY-H00/attention-interpolation-diffusion.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LISA: Layerwise Importance Sampling for Memory-Efficient Large Language
  Model Fine-Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17919v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17919v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Pan, Xiang Liu, Shizhe Diao, Renjie Pi, Jipeng Zhang, Chi Han, Tong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The machine learning community has witnessed impressive advancements since
the first appearance of large language models (LLMs), yet their huge memory
consumption has become a major roadblock to large-scale training. Parameter
Efficient Fine-Tuning techniques such as Low-Rank Adaptation (LoRA) have been
proposed to alleviate this problem, but their performance still fails to match
full parameter training in most large-scale fine-tuning settings. Attempting to
complement this deficiency, we investigate layerwise properties of LoRA on
fine-tuning tasks and observe an uncommon skewness of weight norms across
different layers. Utilizing this key observation, a surprisingly simple
training strategy is discovered, which outperforms both LoRA and full parameter
training in a wide range of settings with memory costs as low as LoRA. We name
it Layerwise Importance Sampled AdamW (LISA), a promising alternative for LoRA,
which applies the idea of importance sampling to different layers in LLMs and
randomly freeze most middle layers during optimization. Experimental results
show that with similar or less GPU memory consumption, LISA surpasses LoRA or
even full parameter tuning in downstream fine-tuning tasks, where LISA
consistently outperforms LoRA by over $11\%$-$37\%$ in terms of MT-Bench
scores. On large models, specifically LLaMA-2-70B, LISA achieves on-par or
better performance than LoRA on MT-Bench, GSM8K, and PubMedQA, demonstrating
its effectiveness across different domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AgentStudio: A Toolkit for Building General Virtual Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17918v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17918v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Longtao Zheng, Zhiyuan Huang, Zhenghai Xue, Xinrun Wang, Bo An, Shuicheng Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Creating autonomous virtual agents capable of using arbitrary software on any
digital device remains a major challenge for artificial intelligence. Two key
obstacles hinder progress: insufficient infrastructure for building virtual
agents in real-world environments, and the need for in-the-wild evaluation of
fundamental agent abilities. To address this, we introduce AgentStudio, an
online, realistic, and multimodal toolkit that covers the entire lifecycle of
agent development. This includes environment setups, data collection, agent
evaluation, and visualization. The observation and action spaces are highly
generic, supporting both function calling and human-computer interfaces. This
versatility is further enhanced by AgentStudio's graphical user interfaces,
which allow efficient development of datasets and benchmarks in real-world
settings. To illustrate, we introduce a visual grounding dataset and a
real-world benchmark suite, both created with our graphical interfaces.
Furthermore, we present several actionable insights derived from AgentStudio,
e.g., general visual grounding, open-ended tool creation, learning from videos,
etc. We have open-sourced the environments, datasets, benchmarks, and
interfaces to promote research towards developing general virtual agents for
the future.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CMP: Cooperative Motion Prediction with Multi-Agent Communication 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17916v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17916v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuoyuan Wu, Yuping Wang, Hengbo Ma, Zhaowei Li, Hang Qiu, Jiachen Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The confluence of the advancement of Autonomous Vehicles (AVs) and the
maturity of Vehicle-to-Everything (V2X) communication has enabled the
capability of cooperative connected and automated vehicles (CAVs). Building on
top of cooperative perception, this paper explores the feasibility and
effectiveness of cooperative motion prediction. Our method, CMP, takes LiDAR
signals as input to enhance tracking and prediction capabilities. Unlike
previous work that focuses separately on either cooperative perception or
motion prediction, our framework, to the best of our knowledge, is the first to
address the unified problem where CAVs share information in both perception and
prediction modules. Incorporated into our design is the unique capability to
tolerate realistic V2X bandwidth limitations and transmission delays, while
dealing with bulky perception representations. We also propose a prediction
aggregation module, which unifies the predictions obtained by different CAVs
and generates the final prediction. Through extensive experiments and ablation
studies, we demonstrate the effectiveness of our method in cooperative
perception, tracking, and motion prediction tasks. In particular, CMP reduces
the average prediction error by 17.2\% with fewer missing detections compared
with the no cooperation setting. Our work marks a significant step forward in
the cooperative capabilities of CAVs, showcasing enhanced performance in
complex scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hierarchical Multi-label Classification for Fine-level Event Extraction
  from Aviation Accident Reports 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17914v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17914v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyu Zhao, Hao Yan, Yongming Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A large volume of accident reports is recorded in the aviation domain, which
greatly values improving aviation safety. To better use those reports, we need
to understand the most important events or impact factors according to the
accident reports. However, the increasing number of accident reports requires
large efforts from domain experts to label those reports. In order to make the
labeling process more efficient, many researchers have started developing
algorithms to identify the underlying events from accident reports
automatically. This article argues that we can identify the events more
accurately by leveraging the event taxonomy. More specifically, we consider the
problem a hierarchical classification task where we first identify the
coarse-level information and then predict the fine-level information. We
achieve this hierarchical classification process by incorporating a novel
hierarchical attention module into BERT. To further utilize the information
from event taxonomy, we regularize the proposed model according to the
relationship and distribution among labels. The effectiveness of our framework
is evaluated with the data collected by National Transportation Safety Board
(NTSB). It has been shown that fine-level prediction accuracy is highly
improved, and the regularization term can be beneficial to the rare event
identification problem.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in INFORMS Journal of Data Science</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Image-based Novel Fault Detection with Deep Learning Classifiers using
  Hierarchical Labels 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17891v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17891v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nurettin Sergin, Jiayu Huang, Tzyy-Shuh Chang, Hao Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One important characteristic of modern fault classification systems is the
ability to flag the system when faced with previously unseen fault types. This
work considers the unknown fault detection capabilities of deep neural
network-based fault classifiers. Specifically, we propose a methodology on how,
when available, labels regarding the fault taxonomy can be used to increase
unknown fault detection performance without sacrificing model performance. To
achieve this, we propose to utilize soft label techniques to improve the
state-of-the-art deep novel fault detection techniques during the training
process and novel hierarchically consistent detection statistics for online
novel fault detection. Finally, we demonstrated increased detection performance
on novel fault detection in inspection images from the hot steel rolling
process, with results well replicated across multiple scenarios and baseline
detection methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in IISE Transaction</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Addressing Social Misattributions of Large Language Models: An
  HCXAI-based Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17873v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17873v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrea Ferrario, Alberto Termine, Alessandro Facchini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human-centered explainable AI (HCXAI) advocates for the integration of social
aspects into AI explanations. Central to the HCXAI discourse is the Social
Transparency (ST) framework, which aims to make the socio-organizational
context of AI systems accessible to their users. In this work, we suggest
extending the ST framework to address the risks of social misattributions in
Large Language Models (LLMs), particularly in sensitive areas like mental
health. In fact LLMs, which are remarkably capable of simulating roles and
personas, may lead to mismatches between designers' intentions and users'
perceptions of social attributes, risking to promote emotional manipulation and
dangerous behaviors, cases of epistemic injustice, and unwarranted trust. To
address these issues, we propose enhancing the ST framework with a fifth
'W-question' to clarify the specific social attributions assigned to LLMs by
its designers and users. This addition aims to bridge the gap between LLM
capabilities and user perceptions, promoting the ethically responsible
development and use of LLM-based technology.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Extended version of the manuscript accepted for the ACM CHI Workshop
  on Human-Centered Explainable AI 2024 (HCXAI24)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Climate Downscaling: A Deep-Learning Based Super-resolution Model of
  Precipitation Data with Attention Block and Skip Connections 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17847v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17847v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chia-Hao Chiang, Zheng-Han Huang, Liwen Liu, Hsin-Chien Liang, Yi-Chi Wang, Wan-Ling Tseng, Chao Wang, Che-Ta Chen, Ko-Chih Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human activities accelerate consumption of fossil fuels and produce
greenhouse gases, resulting in urgent issues today: global warming and the
climate change. These indirectly cause severe natural disasters, plenty of
lives suffering and huge losses of agricultural properties. To mitigate impacts
on our lands, scientists are developing renewable, reusable, and clean energies
and climatologists are trying to predict the extremes. Meanwhile, governments
are publicizing resource-saving policies for a more eco-friendly society and
arousing environment awareness. One of the most influencing factors is the
precipitation, bringing condensed water vapor onto lands. Water resources are
the most significant but basic needs in society, not only supporting our
livings, but also economics. In Taiwan, although the average annual
precipitation is up to 2,500 millimeter (mm), the water allocation for each
person is lower than the global average due to drastically geographical
elevation changes and uneven distribution through the year. Thus, it is crucial
to track and predict the rainfall to make the most use of it and to prevent the
floods. However, climate models have limited resolution and require intensive
computational power for local-scale use. Therefore, we proposed a deep
convolutional neural network with skip connections, attention blocks, and
auxiliary data concatenation, in order to downscale the low-resolution
precipitation data into high-resolution one. Eventually, we compare with other
climate downscaling methods and show better performance in metrics of Mean
Absolute Error (MAE), Root Mean Square Error (RMSE), Pearson Correlation,
structural similarity index (SSIM), and forecast indicators.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hierarchical Open-Vocabulary 3D Scene Graphs for Language-Grounded Robot
  Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17846v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17846v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdelrhman Werby, Chenguang Huang, Martin Büchner, Abhinav Valada, Wolfram Burgard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent open-vocabulary robot mapping methods enrich dense geometric maps with
pre-trained visual-language features. While these maps allow for the prediction
of point-wise saliency maps when queried for a certain language concept,
large-scale environments and abstract queries beyond the object level still
pose a considerable hurdle, ultimately limiting language-grounded robotic
navigation. In this work, we present HOV-SG, a hierarchical open-vocabulary 3D
scene graph mapping approach for language-grounded robot navigation. Leveraging
open-vocabulary vision foundation models, we first obtain state-of-the-art
open-vocabulary segment-level maps in 3D and subsequently construct a 3D scene
graph hierarchy consisting of floor, room, and object concepts, each enriched
with open-vocabulary features. Our approach is able to represent multi-story
buildings and allows robotic traversal of those using a cross-floor Voronoi
graph. HOV-SG is evaluated on three distinct datasets and surpasses previous
baselines in open-vocabulary semantic accuracy on the object, room, and floor
level while producing a 75% reduction in representation size compared to dense
open-vocabulary maps. In order to prove the efficacy and generalization
capabilities of HOV-SG, we showcase successful long-horizon
language-conditioned robot navigation within real-world multi-storage
environments. We provide code and trial video data at http://hovsg.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code and video are available at http://hovsg.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ReMamber: Referring Image Segmentation with Mamba Twister 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17839v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17839v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhuan Yang, Chaofan Ma, Jiangchao Yao, Zhun Zhong, Ya Zhang, Yanfeng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Referring Image Segmentation (RIS) leveraging transformers has achieved great
success on the interpretation of complex visual-language tasks. However, the
quadratic computation cost makes it resource-consuming in capturing long-range
visual-language dependencies. Fortunately, Mamba addresses this with efficient
linear complexity in processing. However, directly applying Mamba to
multi-modal interactions presents challenges, primarily due to inadequate
channel interactions for the effective fusion of multi-modal data. In this
paper, we propose ReMamber, a novel RIS architecture that integrates the power
of Mamba with a multi-modal Mamba Twister block. The Mamba Twister explicitly
models image-text interaction, and fuses textual and visual features through
its unique channel and spatial twisting mechanism. We achieve the
state-of-the-art on three challenging benchmarks. Moreover, we conduct thorough
analyses of ReMamber and discuss other fusion designs using Mamba. These
provide valuable perspectives for future research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DiffH2O: Diffusion-Based Synthesis of Hand-Object Interactions from
  Textual Descriptions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17827v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17827v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sammy Christen, Shreyas Hampali, Fadime Sener, Edoardo Remelli, Tomas Hodan, Eric Sauser, Shugao Ma, Bugra Tekin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating natural hand-object interactions in 3D is challenging as the
resulting hand and object motions are expected to be physically plausible and
semantically meaningful. Furthermore, generalization to unseen objects is
hindered by the limited scale of available hand-object interaction datasets. We
propose DiffH2O, a novel method to synthesize realistic, one or two-handed
object interactions from provided text prompts and geometry of the object. The
method introduces three techniques that enable effective learning from limited
data. First, we decompose the task into a grasping stage and a text-based
interaction stage and use separate diffusion models for each. In the grasping
stage, the model only generates hand motions, whereas in the interaction phase
both hand and object poses are synthesized. Second, we propose a compact
representation that tightly couples hand and object poses. Third, we propose
two different guidance schemes to allow more control of the generated motions:
grasp guidance and detailed textual guidance. Grasp guidance takes a single
target grasping pose and guides the diffusion model to reach this grasp at the
end of the grasping stage, which provides control over the grasping pose. Given
a grasping motion from this stage, multiple different actions can be prompted
in the interaction phase. For textual guidance, we contribute comprehensive
text descriptions to the GRAB dataset and show that they enable our method to
have more fine-grained control over hand-object interactions. Our quantitative
and qualitative evaluation demonstrates that the proposed method outperforms
baseline methods and leads to natural hand-object motions. Moreover, we
demonstrate the practicality of our framework by utilizing a hand pose estimate
from an off-the-shelf pose estimator for guidance, and then sampling multiple
different actions in the interaction stage.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://diffh2o.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Computational Complexity of Stackelberg Planning and
  Meta-Operator Verification: Technical Report <span class="chip">ICAPS24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17826v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17826v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gregor Behnke, Marcel Steinmetz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stackelberg planning is a recently introduced single-turn two-player
adversarial planning model, where two players are acting in a joint classical
planning task, the objective of the first player being hampering the second
player from achieving its goal. This places the Stackelberg planning problem
somewhere between classical planning and general combinatorial two-player
games. But, where exactly? All investigations of Stackelberg planning so far
focused on practical aspects. We close this gap by conducting the first
theoretical complexity analysis of Stackelberg planning. We show that in
general Stackelberg planning is actually no harder than classical planning.
Under a polynomial plan-length restriction, however, Stackelberg planning is a
level higher up in the polynomial complexity hierarchy, suggesting that
compilations into classical planning come with a worst-case exponential
plan-length increase. In attempts to identify tractable fragments, we further
study its complexity under various planning task restrictions, showing that
Stackelberg planning remains intractable where classical planning is not. We
finally inspect the complexity of meta-operator verification, a problem that
has been recently connected to Stackelberg planning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at ICAPS24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Accelerating Radio Spectrum Regulation Workflows with Large Language
  Models (LLMs) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17819v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17819v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amir Ghasemi, Paul Guinand
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Wireless spectrum regulation is a complex and demanding process due to the
rapid pace of technological progress, increasing demand for spectrum, and a
multitude of stakeholders with potentially conflicting interests, alongside
significant economic implications. To navigate this, regulators must engage
effectively with all parties, keep pace with global technology trends, conduct
technical evaluations, issue licenses in a timely manner, and comply with
various legal and policy frameworks.
  In light of these challenges, this paper demonstrates example applications of
Large Language Models (LLMs) to expedite spectrum regulatory processes. We
explore various roles that LLMs can play in this context while identifying some
of the challenges to address. The paper also offers practical case studies and
insights, with appropriate experiments, highlighting the transformative
potential of LLMs in spectrum management.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ D-PAD: Deep-Shallow Multi-Frequency Patterns Disentangling for Time
  Series Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17814v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17814v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaobing Yuan, Ling Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In time series forecasting, effectively disentangling intricate temporal
patterns is crucial. While recent works endeavor to combine decomposition
techniques with deep learning, multiple frequencies may still be mixed in the
decomposed components, e.g., trend and seasonal. Furthermore, frequency domain
analysis methods, e.g., Fourier and wavelet transforms, have limitations in
resolution in the time domain and adaptability. In this paper, we propose
D-PAD, a deep-shallow multi-frequency patterns disentangling neural network for
time series forecasting. Specifically, a multi-component decomposing (MCD)
block is introduced to decompose the series into components with different
frequency ranges, corresponding to the "shallow" aspect. A
decomposition-reconstruction-decomposition (D-R-D) module is proposed to
progressively extract the information of frequencies mixed in the components,
corresponding to the "deep" aspect. After that, an interaction and fusion (IF)
module is used to further analyze the components. Extensive experiments on
seven real-world datasets demonstrate that D-PAD achieves the state-of-the-art
performance, outperforming the best baseline by an average of 9.48% and 7.15%
in MSE and MAE, respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating the Efficacy of <span class="highlight-title">Prompt</span>-Engineered Large Multimodal Models
  Versus Fine-Tuned Vision <span class="highlight-title">Transformer</span>s in Image-Based Security Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17787v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17787v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fouad Trad, Ali Chehab
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The success of Large Language Models (LLMs) has led to a parallel rise in the
development of Large Multimodal Models (LMMs), such as Gemini-pro, which have
begun to transform a variety of applications. These sophisticated multimodal
models are designed to interpret and analyze complex data, integrating both
textual and visual information on a scale previously unattainable, opening new
avenues for a range of applications. This paper investigates the applicability
and effectiveness of prompt-engineered Gemini-pro LMMs versus fine-tuned Vision
Transformer (ViT) models in addressing critical security challenges. We focus
on two distinct tasks: a visually evident task of detecting simple triggers,
such as small squares in images, indicative of potential backdoors, and a
non-visually evident task of malware classification through visual
representations. Our results highlight a significant divergence in performance,
with Gemini-pro falling short in accuracy and reliability when compared to
fine-tuned ViT models. The ViT models, on the other hand, demonstrate
exceptional accuracy, achieving near-perfect performance on both tasks. This
study not only showcases the strengths and limitations of prompt-engineered
LMMs in cybersecurity applications but also emphasizes the unmatched efficacy
of fine-tuned ViT models for precise and dependable tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SciCapenter: Supporting Caption Composition for Scientific Figures with
  Machine-Generated Captions and Ratings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17784v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17784v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ting-Yao Hsu, Chieh-Yang Huang, Shih-Hong Huang, Ryan Rossi, Sungchul Kim, Tong Yu, C. Lee Giles, Ting-Hao K. Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Crafting effective captions for figures is important. Readers heavily depend
on these captions to grasp the figure's message. However, despite a
well-developed set of AI technologies for figures and captions, these have
rarely been tested for usefulness in aiding caption writing. This paper
introduces SciCapenter, an interactive system that puts together cutting-edge
AI technologies for scientific figure captions to aid caption composition.
SciCapenter generates a variety of captions for each figure in a scholarly
article, providing scores and a comprehensive checklist to assess caption
quality across multiple critical aspects, such as helpfulness, OCR mention, key
takeaways, and visual properties reference. Users can directly edit captions in
SciCapenter, resubmit for revised evaluations, and iteratively refine them. A
user study with Ph.D. students indicates that SciCapenter significantly lowers
the cognitive load of caption writing. Participants' feedback further offers
valuable design insights for future systems aiming to enhance caption writing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CHI EA '24: Extended Abstracts of the 2024 CHI Conference on Human
  Factors in Computing Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards a FAIR Documentation of Workflows and Models in Applied
  Mathematics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17778v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17778v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marco Reidelbach, Björn Schembera, Marcus Weber
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modeling-Simulation-Optimization workflows play a fundamental role in applied
mathematics. The Mathematical Research Data Initiative, MaRDI, responded to
this by developing a FAIR and machine-interpretable template for a
comprehensive documentation of such workflows. MaRDMO, a Plugin for the
Research Data Management Organiser, enables scientists from diverse fields to
document and publish their workflows on the MaRDI Portal seamlessly using the
MaRDI template. Central to these workflows are mathematical models. MaRDI
addresses them with the MathModDB ontology, offering a structured formal model
description. Here, we showcase the interaction between MaRDMO and the MathModDB
Knowledge Graph through an algebraic modeling workflow from the Digital
Humanities. This demonstration underscores the versatility of both services
beyond their original numerical domain.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SciNews: From Scholarly Complexities to Public Narratives -- A <span class="highlight-title">Dataset</span>
  for Scientific News Report Generation <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17768v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17768v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongqi Pu, Yifan Wang, Jia Loy, Vera Demberg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scientific news reports serve as a bridge, adeptly translating complex
research articles into reports that resonate with the broader public. The
automated generation of such narratives enhances the accessibility of scholarly
insights. In this paper, we present a new corpus to facilitate this paradigm
development. Our corpus comprises a parallel compilation of academic
publications and their corresponding scientific news reports across nine
disciplines. To demonstrate the utility and reliability of our dataset, we
conduct an extensive analysis, highlighting the divergences in readability and
brevity between scientific news narratives and academic manuscripts. We
benchmark our dataset employing state-of-the-art text generation models. The
evaluation process involves both automatic and human evaluation, which lays the
groundwork for future explorations into the automated generation of scientific
news reports. The dataset and code related to this work are available at
https://dongqi.me/projects/SciNews.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LREC-COLING 2024 Main Conference Paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DataCook: Crafting Anti-Adversarial Examples for Healthcare Data
  Copyright Protection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17755v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17755v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sihan Shang, Jiancheng Yang, Zhenglong Sun, Pascal Fua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the realm of healthcare, the challenges of copyright protection and
unauthorized third-party misuse are increasingly significant. Traditional
methods for data copyright protection are applied prior to data distribution,
implying that models trained on these data become uncontrollable. This paper
introduces a novel approach, named DataCook, designed to safeguard the
copyright of healthcare data during the deployment phase. DataCook operates by
"cooking" the raw data before distribution, enabling the development of models
that perform normally on this processed data. However, during the deployment
phase, the original test data must be also "cooked" through DataCook to ensure
normal model performance. This process grants copyright holders control over
authorization during the deployment phase. The mechanism behind DataCook is by
crafting anti-adversarial examples (AntiAdv), which are designed to enhance
model confidence, as opposed to standard adversarial examples (Adv) that aim to
confuse models. Similar to Adv, AntiAdv introduces imperceptible perturbations,
ensuring that the data processed by DataCook remains easily understandable. We
conducted extensive experiments on MedMNIST datasets, encompassing both 2D/3D
data and the high-resolution variants. The outcomes indicate that DataCook
effectively meets its objectives, preventing models trained on AntiAdv from
analyzing unauthorized data effectively, without compromising the validity and
accuracy of the data in legitimate scenarios. Code and data are available at
https://github.com/MedMNIST/DataCook.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Using Stratified Sampling to Improve LIME Image Explanations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17742v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17742v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Rashid, Elvio G. Amparore, Enrico Ferrari, Damiano Verda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate the use of a stratified sampling approach for LIME Image, a
popular model-agnostic explainable AI method for computer vision tasks, in
order to reduce the artifacts generated by typical Monte Carlo sampling. Such
artifacts are due to the undersampling of the dependent variable in the
synthetic neighborhood around the image being explained, which may result in
inadequate explanations due to the impossibility of fitting a linear regressor
on the sampled data. We then highlight a connection with the Shapley theory,
where similar arguments about undersampling and sample relevance were suggested
in the past. We derive all the formulas and adjustment factors required for an
unbiased stratified sampling estimator. Experiments show the efficacy of the
proposed approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ All-in-One: Heterogeneous Interaction Modeling for Cold-Start Rating
  Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17740v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17740v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuheng Fang, Kangfei Zhao, Yu Rong, Zhixun Li, Jeffrey Xu Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cold-start rating prediction is a fundamental problem in recommender systems
that has been extensively studied. Many methods have been proposed that exploit
explicit relations among existing data, such as collaborative filtering, social
recommendations and heterogeneous information network, to alleviate the data
insufficiency issue for cold-start users and items. However, the explicit
relations constructed based on data between different roles may be unreliable
and irrelevant, which limits the performance ceiling of the specific
recommendation task. Motivated by this, in this paper, we propose a flexible
framework dubbed heterogeneous interaction rating network (HIRE). HIRE dose not
solely rely on the pre-defined interaction pattern or the manually constructed
heterogeneous information network. Instead, we devise a Heterogeneous
Interaction Module (HIM) to jointly model the heterogeneous interactions and
directly infer the important interactions via the observed data. In the
experiments, we evaluate our model under three cold-start settings on three
real-world datasets. The experimental results show that HIRE outperforms other
baselines by a large margin. Furthermore, we visualize the inferred
interactions of HIRE to confirm the contribution of our model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Out-of-distribution Rumor Detection via Test-Time Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17735v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17735v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Tao, Mingqing Zhang, Qiang Liu, Shu Wu, Liang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to the rapid spread of rumors on social media, rumor detection has become
an extremely important challenge. Existing methods for rumor detection have
achieved good performance, as they have collected enough corpus from the same
data distribution for model training. However, significant distribution shifts
between the training data and real-world test data occur due to differences in
news topics, social media platforms, languages and the variance in propagation
scale caused by news popularity. This leads to a substantial decline in the
performance of these existing methods in Out-Of-Distribution (OOD) situations.
To address this problem, we propose a simple and efficient method named
Test-time Adaptation for Rumor Detection under distribution shifts (TARD). This
method models the propagation of news in the form of a propagation graph, and
builds propagation graph test-time adaptation framework, enhancing the model's
adaptability and robustness when facing OOD problems. Extensive experiments
conducted on two group datasets collected from real-world social platforms
demonstrate that our framework outperforms the state-of-the-art methods in
performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tiny Models are the Computational Saver for Large Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17726v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17726v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingyuan Wang, Barry Cardiff, Antoine Frappé, Benoit Larras, Deepu John
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces TinySaver, an early-exit-like dynamic model compression
approach which employs tiny models to substitute large models adaptively.
Distinct from traditional compression techniques, dynamic methods like
TinySaver can leverage the difficulty differences to allow certain inputs to
complete their inference processes early, thereby conserving computational
resources. Most existing early exit designs are implemented by attaching
additional network branches to the model's backbone. Our study, however,
reveals that completely independent tiny models can replace a substantial
portion of the larger models' job with minimal impact on performance. Employing
them as the first exit can remarkably enhance computational efficiency. By
searching and employing the most appropriate tiny model as the computational
saver for a given large model, the proposed approaches work as a novel and
generic method to model compression. This finding will help the research
community in exploring new compression methods to address the escalating
computational demands posed by rapidly evolving AI models. Our evaluation of
this approach in ImageNet-1k classification demonstrates its potential to
reduce the number of compute operations by up to 90%, with only negligible
losses in performance, across various modern vision models. The code of this
work will be available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimization-based <span class="highlight-title">Prompt</span> Injection Attack to LLM-as-a-Judge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17710v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17710v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiawen Shi, Zenghui Yuan, Yinuo Liu, Yue Huang, Pan Zhou, Lichao Sun, Neil Zhenqiang Gong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LLM-as-a-Judge is a novel solution that can assess textual information with
large language models (LLMs). Based on existing research studies, LLMs
demonstrate remarkable performance in providing a compelling alternative to
traditional human assessment. However, the robustness of these systems against
prompt injection attacks remains an open question. In this work, we introduce
JudgeDeceiver, a novel optimization-based prompt injection attack tailored to
LLM-as-a-Judge. Our method formulates a precise optimization objective for
attacking the decision-making process of LLM-as-a-Judge and utilizes an
optimization algorithm to efficiently automate the generation of adversarial
sequences, achieving targeted and effective manipulation of model evaluations.
Compared to handcraft prompt injection attacks, our method demonstrates
superior efficacy, posing a significant challenge to the current security
paradigms of LLM-based judgment systems. Through extensive experiments, we
showcase the capability of JudgeDeceiver in altering decision outcomes across
various cases, highlighting the vulnerability of LLM-as-a-Judge systems to the
optimization-based prompt injection attack.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhanced Short Text Modeling: Leveraging Large Language Models for Topic
  Refinement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17706v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17706v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuyu Chang, Rui Wang, Peng Ren, Haiping Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Crafting effective topic models for brief texts, like tweets and news
headlines, is essential for capturing the swift shifts in social dynamics.
Traditional topic models, however, often fall short in accurately representing
the semantic intricacies of short texts due to their brevity and lack of
contextual data. In our study, we harness the advanced capabilities of Large
Language Models (LLMs) to introduce a novel approach termed "Topic Refinement".
This approach does not directly involve itself in the initial modeling of
topics but focuses on improving topics after they have been mined. By employing
prompt engineering, we direct LLMs to eliminate off-topic words within a given
topic, ensuring that only contextually relevant words are preserved or
substituted with ones that fit better semantically. This method emulates
human-like scrutiny and improvement of topics, thereby elevating the semantic
quality of the topics generated by various models. Our comprehensive evaluation
across three unique datasets has shown that our topic refinement approach
significantly enhances the semantic coherence of topics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CLHA: A Simple yet Effective Contrastive Learning Framework for Human
  Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16649v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16649v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Feiteng Fang, Liang Zhu, Min Yang, Xi Feng, Jinchang Hou, Qixuan Zhao, Chengming Li, Xiping Hu, Ruifeng Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning from human feedback (RLHF) is a crucial technique in
aligning large language models (LLMs) with human preferences, ensuring these
LLMs behave in beneficial and comprehensible ways to users. However, a
longstanding challenge in human alignment techniques based on reinforcement
learning lies in their inherent complexity and difficulty in training. To
address this challenge, we present a simple yet effective Contrastive Learning
Framework for Human Alignment (CLHA) to align LLMs with human preferences
directly. CLHA employs a novel rescoring strategy to evaluate the noise within
the data by considering its inherent quality and dynamically adjusting the
training process. Simultaneously, CLHA utilizes pairwise contrastive loss and
adaptive supervised fine-tuning loss to adaptively modify the likelihood of
generating responses, ensuring enhanced alignment with human preferences. Using
advanced methods, CLHA surpasses other algorithms, showcasing superior
performance in terms of reward model scores, automatic evaluations, and human
assessments on the widely used ``Helpful and Harmless'' dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Re2LLM: Reflective Reinforcement Large Language Model for Session-based
  Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16427v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16427v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyan Wang, Yingpeng Du, Zhu Sun, Haoyan Chua, Kaidong Feng, Wenya Wang, Jie Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are emerging as promising approaches to enhance
session-based recommendation (SBR), where both prompt-based and
fine-tuning-based methods have been widely investigated to align LLMs with SBR.
However, the former methods struggle with optimal prompts to elicit the correct
reasoning of LLMs due to the lack of task-specific feedback, leading to
unsatisfactory recommendations. Although the latter methods attempt to
fine-tune LLMs with domain-specific knowledge, they face limitations such as
high computational costs and reliance on open-source backbones. To address such
issues, we propose a \underline{Re}flective \underline{Re}inforcement
\underline{L}arge \underline{L}anguage \underline{M}odel (Re2LLM) for SBR,
guiding LLMs to focus on specialized knowledge essential for more accurate
recommendations effectively and efficiently. In particular, we first design the
Reflective Exploration Module to effectively extract knowledge that is readily
understandable and digestible by LLMs. To be specific, we direct LLMs to
examine recommendation errors through self-reflection and construct a knowledge
base (KB) comprising hints capable of rectifying these errors. To efficiently
elicit the correct reasoning of LLMs, we further devise the Reinforcement
Utilization Module to train a lightweight retrieval agent. It learns to select
hints from the constructed KB based on the task-specific feedback, where the
hints can serve as guidance to help correct LLMs reasoning for better
recommendations. Extensive experiments on multiple real-world datasets
demonstrate that our method consistently outperforms state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Simple and Scalable Strategies to Continually <span class="highlight-title">Pre-train</span> Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08763v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08763v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adam Ibrahim, Benjamin Thérien, Kshitij Gupta, Mats L. Richter, Quentin Anthony, Timothée Lesort, Eugene Belilovsky, Irina Rish
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are routinely pre-trained on billions of tokens,
only to start the process over again once new data becomes available. A much
more efficient solution is to continually pre-train these models, saving
significant compute compared to re-training. However, the distribution shift
induced by new data typically results in degraded performance on previous data
or poor adaptation to the new data. In this work, we show that a simple and
scalable combination of learning rate (LR) re-warming, LR re-decaying, and
replay of previous data is sufficient to match the performance of fully
re-training from scratch on all available data, as measured by the final loss
and the average score on several language model (LM) evaluation benchmarks.
Specifically, we show this for a weak but realistic distribution shift between
two commonly used LLM pre-training datasets (English$\rightarrow$English) and a
stronger distribution shift (English$\rightarrow$German) at the $405$M
parameter model scale with large dataset sizes (hundreds of billions of
tokens). Selecting the weak but realistic shift for larger-scale experiments,
we also find that our continual learning strategies match the re-training
baseline for a 10B parameter LLM. Our results demonstrate that LLMs can be
successfully updated via simple and scalable continual learning strategies,
matching the re-training baseline using only a fraction of the compute.
Finally, inspired by previous work, we propose alternatives to the cosine
learning rate schedule that help circumvent forgetting induced by LR re-warming
and that are not bound to a fixed token budget.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DiVa-360: The Dynamic Visual <span class="highlight-title">Dataset</span> for Immersive Neural Fields 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.16897v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.16897v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cheng-You Lu, Peisen Zhou, Angela Xing, Chandradeep Pokhariya, Arnab Dey, Ishaan Shah, Rugved Mavidipalli, Dylan Hu, Andrew Comport, Kefan Chen, Srinath Sridhar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advances in neural fields are enabling high-fidelity capture of the shape and
appearance of dynamic 3D scenes. However, their capabilities lag behind those
offered by conventional representations such as 2D videos because of
algorithmic challenges and the lack of large-scale multi-view real-world
datasets. We address the dataset limitation with DiVa-360, a real-world 360
dynamic visual dataset that contains synchronized high-resolution and
long-duration multi-view video sequences of table-scale scenes captured using a
customized low-cost system with 53 cameras. It contains 21 object-centric
sequences categorized by different motion types, 25 intricate hand-object
interaction sequences, and 8 long-duration sequences for a total of 17.4 M
image frames. In addition, we provide foreground-background segmentation masks,
synchronized audio, and text descriptions. We benchmark the state-of-the-art
dynamic neural field methods on DiVa-360 and provide insights about existing
methods and future challenges on long-duration neural field capture.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dynamics of Moral Behavior in Heterogeneous Populations of Learning
  Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.04202v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.04202v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elizaveta Tennant, Stephen Hailes, Mirco Musolesi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Growing concerns about safety and alignment of AI systems highlight the
importance of embedding moral capabilities in artificial agents. A promising
solution is the use of learning from experience, i.e., Reinforcement Learning.
In multi-agent (social) environments, complex population-level phenomena may
emerge from interactions between individual learning agents. Many of the
existing studies rely on simulated social dilemma environments to study the
interactions of independent learning agents. However, they tend to ignore the
moral heterogeneity that is likely to be present in societies of agents in
practice. For example, at different points in time a single learning agent may
face opponents who are consequentialist (i.e., caring about maximizing some
outcome over time) or norm-based (i.e., focusing on conforming to a specific
norm here and now). The extent to which agents' co-development may be impacted
by such moral heterogeneity in populations is not well understood. In this
paper, we present a study of the learning dynamics of morally heterogeneous
populations interacting in a social dilemma setting. Using a Prisoner's Dilemma
environment with a partner selection mechanism, we investigate the extent to
which the prevalence of diverse moral agents in populations affects individual
agents' learning behaviors and emergent population-level outcomes. We observe
several types of non-trivial interactions between pro-social and anti-social
agents, and find that certain classes of moral agents are able to steer selfish
agents towards more cooperative behavior.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Safe Explicable Planning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.03773v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.03773v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akkamahadevi Hanni, Andrew Boateng, Yu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human expectations arise from their understanding of others and the world. In
the context of human-AI interaction, this understanding may not align with
reality, leading to the AI agent failing to meet expectations and compromising
team performance. Explicable planning, introduced as a method to bridge this
gap, aims to reconcile human expectations with the agent's optimal behavior,
facilitating interpretable decision-making. However, an unresolved critical
issue is ensuring safety in explicable planning, as it could result in
explicable behaviors that are unsafe. To address this, we propose Safe
Explicable Planning (SEP), which extends the prior work to support the
specification of a safety bound. The goal of SEP is to find behaviors that
align with human expectations while adhering to the specified safety criterion.
Our approach generalizes the consideration of multiple objectives stemming from
multiple models rather than a single model, yielding a Pareto set of safe
explicable policies. We present both an exact method, guaranteeing finding the
Pareto set, and a more efficient greedy method that finds one of the policies
in the Pareto set. Additionally, we offer approximate solutions based on state
aggregation to improve scalability. We provide formal proofs that validate the
desired theoretical properties of these methods. Evaluation through simulations
and physical robot experiments confirms the effectiveness of our approach for
safe explicable planning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AI and Generative AI for Research Discovery and Summarization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.06795v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.06795v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mark Glickman, Yi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  AI and generative AI tools, including chatbots like ChatGPT that rely on
large language models (LLMs), have burst onto the scene this year, creating
incredible opportunities to increase work productivity and improve our lives.
Statisticians and data scientists have begun experiencing the benefits from the
availability of these tools in numerous ways, such as the generation of
programming code from text prompts to analyze data or fit statistical models.
One area that these tools can make a substantial impact is in research
discovery and summarization. Standalone tools and plugins to chatbots are being
developed that allow researchers to more quickly find relevant literature than
pre-2023 search tools. Furthermore, generative AI tools have improved to the
point where they can summarize and extract the key points from research
articles in succinct language. Finally, chatbots based on highly parameterized
LLMs can be used to simulate abductive reasoning, which provides researchers
the ability to make connections among related technical topics, which can also
be used for research discovery. We review the developments in AI and generative
AI for research discovery and summarization, and propose directions where these
types of tools are likely to head in the future that may be of interest to
statistician and data scientists.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Chat<span class="highlight-title">GPT</span> Needs SPADE (Sustainability, PrivAcy, Digital divide, and
  Ethics) Evaluation: A <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.03123v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.03123v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sunder Ali Khowaja, Parus Khuwaja, Kapal Dev, Weizheng Wang, Lewis Nkenyereye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  ChatGPT is another large language model (LLM) vastly available for the
consumers on their devices but due to its performance and ability to converse
effectively, it has gained a huge popularity amongst research as well as
industrial community. Recently, many studies have been published to show the
effectiveness, efficiency, integration, and sentiments of chatGPT and other
LLMs. In contrast, this study focuses on the important aspects that are mostly
overlooked, i.e. sustainability, privacy, digital divide, and ethics and
suggests that not only chatGPT but every subsequent entry in the category of
conversational bots should undergo Sustainability, PrivAcy, Digital divide, and
Ethics (SPADE) evaluation. This paper discusses in detail the issues and
concerns raised over chatGPT in line with aforementioned characteristics. We
also discuss the recent EU AI Act briefly in accordance with the SPADE
evaluation. We support our hypothesis by some preliminary data collection and
visualizations along with hypothesized facts. We also suggest mitigations and
recommendations for each of the concerns. Furthermore, we also suggest some
policies and recommendations for AI policy act, if designed by the governments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 8 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can Chat<span class="highlight-title">GPT</span> Detect DeepFakes? A Study of Using Multimodal Large Language
  Models for Media Forensics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14077v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14077v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shan Jia, Reilin Lyu, Kangran Zhao, Yize Chen, Zhiyuan Yan, Yan Ju, Chuanbo Hu, Xin Li, Baoyuan Wu, Siwei Lyu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  DeepFakes, which refer to AI-generated media content, have become an
increasing concern due to their use as a means for disinformation. Detecting
DeepFakes is currently solved with programmed machine learning algorithms. In
this work, we investigate the capabilities of multimodal large language models
(LLMs) in DeepFake detection. We conducted qualitative and quantitative
experiments to demonstrate multimodal LLMs and show that they can expose
AI-generated images through careful experimental design and prompt engineering.
This is interesting, considering that LLMs are not inherently tailored for
media forensic tasks, and the process does not require programming. We discuss
the limitations of multimodal LLMs for these tasks and suggest possible
improvements.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient <span class="highlight-title">Pre-train</span>ing for Localized Instruction Generation of Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.15964v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.15964v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anil Batra, Davide Moltisanti, Laura Sevilla-Lara, Marcus Rohrbach, Frank Keller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Procedural videos show step-by-step demonstrations of tasks like recipe
preparation. Understanding such videos is challenging, involving the precise
localization of steps and the generation of textual instructions. Manually
annotating steps and writing instructions is costly, which limits the size of
current datasets and hinders effective learning. Leveraging large but noisy
video-transcript datasets for pre-training can boost performance, but demands
significant computational resources. Furthermore, transcripts contain
irrelevant content and exhibit style variation compared to instructions written
by human annotators. To mitigate both issues, we propose a technique,
Sieve-&-Swap, to automatically curate a smaller dataset: (i) Sieve filters
irrelevant transcripts and (ii) Swap enhances the quality of the text
instruction by automatically replacing the transcripts with human-written
instructions from a text-only recipe dataset. The curated dataset, three orders
of magnitude smaller than current web-scale datasets, enables efficient
training of large-scale models with competitive performance. We complement our
Sieve-\&-Swap approach with a Procedure Transformer (ProcX) for end-to-end step
localization and instruction generation for procedural videos. When this model
is pre-trained on our curated dataset, it achieves state-of-the-art performance
in zero-shot and finetuning settings on YouCook2 and Tasty, while using a
fraction of the computational resources.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This version has some missing experiments and elaborative technical
  details</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Analyzing the Quality Attributes of AI Vision Models in Open
  Repositories Under Adversarial Attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.12261v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.12261v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zerui Wang, Yan Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As AI models rapidly evolve, they are frequently released to open
repositories, such as HuggingFace. It is essential to perform quality assurance
validation on these models before integrating them into the production
development lifecycle. In addition to evaluating efficiency in terms of
balanced accuracy and computing costs, adversarial attacks are potential
threats to the robustness and explainability of AI models. Meanwhile, XAI
applies algorithms that approximate inputs to outputs post-hoc to identify the
contributing features. Adversarial perturbations may also degrade the utility
of XAI explanations that require further investigation. In this paper, we
present an integrated process designed for downstream evaluation tasks,
including validating AI model accuracy, evaluating robustness with benchmark
perturbations, comparing explanation utility, and assessing overhead. We
demonstrate an evaluation scenario involving six computer vision models, which
include CNN-based, Transformer-based, and hybrid architectures, three types of
perturbations, and five XAI methods, resulting in ninety unique combinations.
The process reveals the explanation utility among the XAI methods in terms of
the identified key areas responding to the adversarial perturbation. The
process produces aggregated results that illustrate multiple attributes of each
AI model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Blinded by Generated Contexts: How Language Models Merge Generated and
  Retrieved Contexts for Open-Domain QA? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.11911v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.11911v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hexiang Tan, Fei Sun, Wanli Yang, Yuanzhuo Wang, Qi Cao, Xueqi Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While auxiliary information has become a key to enhancing Large Language
Models (LLMs), relatively little is known about how LLMs merge these contexts,
specifically contexts generated by LLMs and those retrieved from external
sources. To investigate this, we formulate a systematic framework to identify
whether LLMs' responses, derived from the integration of generated and
retrieved contexts, are attributed to either generated or retrieved contexts.
To easily trace the origin of the response, we construct datasets with
conflicting contexts, i.e., each question is paired with both generated and
retrieved contexts, yet only one of them contains the correct answer. Our
experiments reveal a significant bias in several LLMs (GPT-4/3.5 and Llama2) to
favor generated contexts, even when they provide incorrect information. We
further identify two key factors contributing to this bias: i) contexts
generated by LLMs typically show greater similarity to the questions,
increasing their likelihood of being selected; ii) the segmentation process
used in retrieved contexts disrupts their completeness, thereby hindering their
full utilization in LLMs. Our analysis enhances the understanding of how LLMs
merge diverse contexts, offering valuable insights for advancing current
augmentation methods for LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Toward a Theory of Causation for Interpreting Neural Code Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.03788v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.03788v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David N. Palacio, Alejandro Velasco, Nathan Cooper, Alvaro Rodriguez, Kevin Moran, Denys Poshyvanyk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural Language Models of Code, or Neural Code Models (NCMs), are rapidly
progressing from research prototypes to commercial developer tools. As such,
understanding the capabilities and limitations of such models is becoming
critical. However, the abilities of these models are typically measured using
automated metrics that often only reveal a portion of their real-world
performance. While, in general, the performance of NCMs appears promising,
currently much is unknown about how such models arrive at decisions. To this
end, this paper introduces $do_{code}$, a post hoc interpretability method
specific to NCMs that is capable of explaining model predictions. $do_{code}$
is based upon causal inference to enable programming language-oriented
explanations. While the theoretical underpinnings of $do_{code}$ are extensible
to exploring different model properties, we provide a concrete instantiation
that aims to mitigate the impact of spurious correlations by grounding
explanations of model behavior in properties of programming languages. To
demonstrate the practical benefit of $do_{code}$, we illustrate the insights
that our framework can provide by performing a case study on two popular deep
learning architectures and ten NCMs. The results of this case study illustrate
that our studied NCMs are sensitive to changes in code syntax. All our NCMs,
except for the BERT-like model, statistically learn to predict tokens related
to blocks of code (\eg brackets, parenthesis, semicolon) with less confounding
bias as compared to other programming language constructs. These insights
demonstrate the potential of $do_{code}$ as a useful method to detect and
facilitate the elimination of confounding bias in NCMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to appear in IEEE Transactions on Software Engineering</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cyber-Security Knowledge Graph Generation by Hierarchical Nonnegative
  Matrix Factorization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16222v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16222v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryan Barron, Maksim E. Eren, Manish Bhattarai, Selma Wanna, Nicholas Solovyev, Kim Rasmussen, Boian S. Alexandrov, Charles Nicholas, Cynthia Matuszek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Much of human knowledge in cybersecurity is encapsulated within the
ever-growing volume of scientific papers. As this textual data continues to
expand, the importance of document organization methods becomes increasingly
crucial for extracting actionable insights hidden within large text datasets.
Knowledge Graphs (KGs) serve as a means to store factual information in a
structured manner, providing explicit, interpretable knowledge that includes
domain-specific information from the cybersecurity scientific literature. One
of the challenges in constructing a KG from scientific literature is the
extraction of ontology from unstructured text. In this paper, we address this
topic and introduce a method for building a multi-modal KG by extracting
structured ontology from scientific papers. We demonstrate this concept in the
cybersecurity domain. One modality of the KG represents observable information
from the papers, such as the categories in which they were published or the
authors. The second modality uncovers latent (hidden) patterns of text
extracted through hierarchical and semantic non-negative matrix factorization
(NMF), such as named entities, topics or clusters, and keywords. We illustrate
this concept by consolidating more than two million scientific papers uploaded
to arXiv into the cyber-domain, using hierarchical and semantic NMF, and by
building a cyber-domain-specific KG.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at IEEE ISDFS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Decode Neural signal as Speech 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.01748v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.01748v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiqian Yang, Yiqun Duan, Qiang Zhang, Renjing Xu, Hui Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Decoding language from brain dynamics is an important open direction in the
realm of brain-computer interface (BCI), especially considering the rapid
growth of large language models. Compared to invasive-based signals which
require electrode implantation surgery, non-invasive neural signals (e.g. EEG,
MEG) have attracted increasing attention considering their safety and
generality. However, the exploration is not adequate in three aspects: 1)
previous methods mainly focus on EEG but none of the previous works address
this problem on MEG with better signal quality; 2) prior works have
predominantly used ``teacher-forcing" during generative decoding, which is
impractical; 3) prior works are mostly ``BART-based" not fully auto-regressive,
which performs better in other sequence tasks. In this paper, we explore the
brain-to-text translation of MEG signals in a speech-decoding formation. Here
we are the first to investigate a cross-attention-based ``whisper" model for
generating text directly from MEG signals without teacher forcing. Our model
achieves impressive BLEU-1 scores of 60.30 and 52.89 without pretraining \&
teacher-forcing on two major datasets (\textit{GWilliams} and
\textit{Schoffelen}). This paper conducts a comprehensive review to understand
how speech decoding formation performs on the neural decoding tasks, including
pretraining initialization, training \& evaluation set splitting, augmentation,
and scaling law.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ As Good As A Coin Toss: Human detection of AI-generated images, videos,
  audio, and audiovisual stimuli 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16760v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16760v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Di Cooke, Abigail Edwards, Sophia Barkoff, Kathryn Kelly
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As synthetic media becomes progressively more realistic and barriers to using
it continue to lower, the technology has been increasingly utilized for
malicious purposes, from financial fraud to nonconsensual pornography. Today,
the principal defense against being misled by synthetic media relies on the
ability of the human observer to visually and auditorily discern between real
and fake. However, it remains unclear just how vulnerable people actually are
to deceptive synthetic media in the course of their day to day lives. We
conducted a perceptual study with 1276 participants to assess how accurate
people were at distinguishing synthetic images, audio only, video only, and
audiovisual stimuli from authentic. To reflect the circumstances under which
people would likely encounter synthetic media in the wild, testing conditions
and stimuli emulated a typical online platform, while all synthetic media used
in the survey was sourced from publicly accessible generative AI technology.
  We find that overall, participants struggled to meaningfully discern between
synthetic and authentic content. We also find that detection performance
worsens when the stimuli contains synthetic content as compared to authentic
content, images featuring human faces as compared to non face objects, a single
modality as compared to multimodal stimuli, mixed authenticity as compared to
being fully synthetic for audiovisual stimuli, and features foreign languages
as compared to languages the observer is fluent in. Finally, we also find that
prior knowledge of synthetic media does not meaningfully impact their detection
performance. Collectively, these results indicate that people are highly
susceptible to being tricked by synthetic media in their daily lives and that
human perceptual detection capabilities can no longer be relied upon as an
effective counterdefense.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>For study pre-registration, see https://osf.io/fnhr3</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RL$^3$: Boosting Meta Reinforcement Learning via RL inside RL$^2$ 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.15909v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.15909v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhinav Bhatia, Samer B. Nashed, Shlomo Zilberstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Meta reinforcement learning (meta-RL) methods such as RL$^2$ have emerged as
promising approaches for learning data-efficient RL algorithms tailored to a
given task distribution. However, they show poor asymptotic performance and
struggle with out-of-distribution tasks because they rely on sequence models,
such as recurrent neural networks or transformers, to process experiences
rather than summarize them using general-purpose RL components such as value
functions. In contrast, traditional RL algorithms are data-inefficient as they
do not use domain knowledge, but they do converge to an optimal policy in the
limit. We propose RL$^3$, a principled hybrid approach that incorporates
action-values, learned per task through traditional RL, in the inputs to
meta-RL. We show that RL$^3$ earns greater cumulative reward in the long term,
compared to RL$^2$, while maintaining data-efficiency in the short term, and
generalizes better to out-of-distribution tasks. Experiments are conducted on
both custom and benchmark discrete domains from the meta-RL literature that
exhibit a range of short-term, long-term, and complex dependencies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-Objective Optimization for Sparse Deep Multi-Task Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.12243v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.12243v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        S. S. Hotegni, M. Berkemeier, S. Peitz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Different conflicting optimization criteria arise naturally in various Deep
Learning scenarios. These can address different main tasks (i.e., in the
setting of Multi-Task Learning), but also main and secondary tasks such as loss
minimization versus sparsity. The usual approach is a simple weighting of the
criteria, which formally only works in the convex setting. In this paper, we
present a Multi-Objective Optimization algorithm using a modified Weighted
Chebyshev scalarization for training Deep Neural Networks (DNNs) with respect
to several tasks. By employing this scalarization technique, the algorithm can
identify all optimal solutions of the original problem while reducing its
complexity to a sequence of single-objective problems. The simplified problems
are then solved using an Augmented Lagrangian method, enabling the use of
popular optimization techniques such as Adam and Stochastic Gradient Descent,
while efficaciously handling constraints. Our work aims to address the
(economical and also ecological) sustainability issue of DNN models, with a
particular focus on Deep Multi-Task models, which are typically designed with a
very large number of weights to perform equally well on multiple tasks. Through
experiments conducted on two Machine Learning datasets, we demonstrate the
possibility of adaptively sparsifying the model during training without
significantly impacting its performance, if we are willing to apply
task-specific adaptations to the network weights. Code is available at
https://github.com/salomonhotegni/MDMTN
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Med<span class="highlight-title">Prompt</span>X: Grounded Multimodal <span class="highlight-title">Prompt</span>ing for Chest X-ray Diagnosis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15585v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15585v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mai A. Shaaban, Adnan Khan, Mohammad Yaqub
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chest X-ray images are commonly used for predicting acute and chronic
cardiopulmonary conditions, but efforts to integrate them with structured
clinical data face challenges due to incomplete electronic health records
(EHR). This paper introduces \textbf{MedPromptX}, the first model to integrate
multimodal large language models (MLLMs), few-shot prompting (FP) and visual
grounding (VG) to combine imagery with EHR data for chest X-ray diagnosis. A
pre-trained MLLM is utilized to complement the missing EHR information,
providing a comprehensive understanding of patients' medical history.
Additionally, FP reduces the necessity for extensive training of MLLMs while
effectively tackling the issue of hallucination. Nevertheless, the process of
determining the optimal number of few-shot examples and selecting high-quality
candidates can be burdensome, yet it profoundly influences model performance.
Hence, we propose a new technique that dynamically refines few-shot data for
real-time adjustment to new patient scenarios. Moreover, VG aids in focusing
the model's attention on relevant regions of interest in X-ray images,
enhancing the identification of abnormalities. We release MedPromptX-VQA, a new
in-context visual question answering dataset encompassing interleaved image and
EHR data derived from MIMIC-IV and MIMIC-CXR databases. Results demonstrate the
SOTA performance of MedPromptX, achieving an 11% improvement in F1-score
compared to the baselines. Code and data are available at
https://github.com/BioMedIA-MBZUAI/MedPromptX
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Accelerating Scientific Discovery with Generative Knowledge Extraction,
  Graph-Based Representation, and Multimodal Intelligent Graph Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11996v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11996v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Markus J. Buehler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Leveraging generative Artificial Intelligence (AI), we have transformed a
dataset comprising 1,000 scientific papers into an ontological knowledge graph.
Through an in-depth structural analysis, we have calculated node degrees,
identified communities and connectivities, and evaluated clustering
coefficients and betweenness centrality of pivotal nodes, uncovering
fascinating knowledge architectures. The graph has an inherently scale-free
nature, is highly connected, and can be used for graph reasoning by taking
advantage of transitive and isomorphic properties that reveal unprecedented
interdisciplinary relationships that can be used to answer queries, identify
gaps in knowledge, propose never-before-seen material designs, and predict
material behaviors. We compute deep node embeddings for combinatorial node
similarity ranking for use in a path sampling strategy links dissimilar
concepts that have previously not been related. One comparison revealed
structural parallels between biological materials and Beethoven's 9th Symphony,
highlighting shared patterns of complexity through isomorphic mapping. In
another example, the algorithm proposed a hierarchical mycelium-based composite
based on integrating path sampling with principles extracted from Kandinsky's
'Composition VII' painting. The resulting material integrates an innovative set
of concepts that include a balance of chaos/order, adjustable porosity,
mechanical strength, and complex patterned chemical functionalization. We
uncover other isomorphisms across science, technology and art, revealing a
nuanced ontology of immanence that reveal a context-dependent heterarchical
interplay of constituents. Graph-based generative AI achieves a far higher
degree of novelty, explorative capacity, and technical detail, than
conventional approaches and establishes a widely useful framework for
innovation by revealing hidden connections.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Text-Guided Variational Image Generation for Industrial Anomaly
  Detection and Segmentation <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06247v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06247v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingyu Lee, Jongwon Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a text-guided variational image generation method to address the
challenge of getting clean data for anomaly detection in industrial
manufacturing. Our method utilizes text information about the target object,
learned from extensive text library documents, to generate non-defective data
images resembling the input image. The proposed framework ensures that the
generated non-defective images align with anticipated distributions derived
from textual and image-based knowledge, ensuring stability and generality.
Experimental results demonstrate the effectiveness of our approach, surpassing
previous methods even with limited non-defective data. Our approach is
validated through generalization tests across four baseline models and three
distinct datasets. We present an additional analysis to enhance the
effectiveness of anomaly detection models by utilizing the generated images.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, Accepted to CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unveiling the Pitfalls of Knowledge Editing for Large Language Models <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02129v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02129v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhoubo Li, Ningyu Zhang, Yunzhi Yao, Mengru Wang, Xi Chen, Huajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the cost associated with fine-tuning Large Language Models (LLMs)
continues to rise, recent research efforts have pivoted towards developing
methodologies to edit implicit knowledge embedded within LLMs. Yet, there's
still a dark cloud lingering overhead -- will knowledge editing trigger
butterfly effect? since it is still unclear whether knowledge editing might
introduce side effects that pose potential risks or not. This paper pioneers
the investigation into the potential pitfalls associated with knowledge editing
for LLMs. To achieve this, we introduce new benchmark datasets and propose
innovative evaluation metrics. Our results underline two pivotal concerns: (1)
Knowledge Conflict: Editing groups of facts that logically clash can magnify
the inherent inconsistencies in LLMs-a facet neglected by previous methods. (2)
Knowledge Distortion: Altering parameters with the aim of editing factual
knowledge can irrevocably warp the innate knowledge structure of LLMs.
Experimental results vividly demonstrate that knowledge editing might
inadvertently cast a shadow of unintended consequences on LLMs, which warrant
attention and efforts for future works. Code and data are available at
https://github.com/zjunlp/PitfallsKnowledgeEditing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unleashing the Emergent Cognitive Synergy in Large Language Models: A
  Task-Solving Agent through Multi-Persona Self-Collaboration <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05300v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05300v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenhailong Wang, Shaoguang Mao, Wenshan Wu, Tao Ge, Furu Wei, Heng Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human intelligence thrives on cognitive synergy, where collaboration among
different minds yield superior outcomes compared to isolated individuals. In
this work, we propose Solo Performance Prompting (SPP), which transforms a
single LLM into a cognitive synergist by engaging in multi-turn
self-collaboration with multiple personas. A cognitive synergist is an
intelligent agent that collaboratively combines multiple minds' strengths and
knowledge to enhance problem-solving in complex tasks. By dynamically
identifying and simulating different personas based on task inputs, SPP
unleashes the potential of cognitive synergy in LLMs. Our in-depth analysis
shows that assigning multiple fine-grained personas in LLMs improves
problem-solving abilities compared to using a single or fixed number of
personas. We evaluate SPP on three challenging tasks: Trivia Creative Writing,
Codenames Collaborative, and Logic Grid Puzzle, encompassing both
knowledge-intensive and reasoning-intensive types. Unlike previous works, such
as Chain-of-Thought, that solely enhance the reasoning abilities in LLMs,
experimental results demonstrate that SPP effectively reduces factual
hallucination, and maintains strong reasoning capabilities. Additionally,
comparative experiments show that cognitive synergy only emerges in GPT-4 and
does not appear in less capable models, such as GPT-3.5-turbo and
Llama2-13b-chat, which draws an interesting analogy to human development. Code,
data, and prompts can be found at:
https://github.com/MikeWangWZHL/Solo-Performance-Prompting.git.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as a main conference paper at NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Safe Preference Learning Approach for Personalization with
  Applications to Autonomous Vehicles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.02099v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.02099v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruya Karagulle, Nikos Arechiga, Andrew Best, Jonathan DeCastro, Necmiye Ozay
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work introduces a preference learning method that ensures adherence to
given specifications, with an application to autonomous vehicles. Our approach
incorporates the priority ordering of Signal Temporal Logic (STL) formulas
describing traffic rules into a learning framework. By leveraging Parametric
Weighted Signal Temporal Logic (PWSTL), we formulate the problem of
safety-guaranteed preference learning based on pairwise comparisons and propose
an approach to solve this learning problem. Our approach finds a feasible
valuation for the weights of the given PWSTL formula such that, with these
weights, preferred signals have weighted quantitative satisfaction measures
greater than their non-preferred counterparts. The feasible valuation of
weights given by our approach leads to a weighted STL formula that can be used
in correct-and-custom-by-construction controller synthesis. We demonstrate the
performance of our method with a pilot human subject study in two different
simulated driving scenarios involving a stop sign and a pedestrian crossing.
Our approach yields competitive results compared to existing preference
learning methods in terms of capturing preferences and notably outperforms them
when safety is considered.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 3 figures, 2 tables. This work has been published at IEEE
  Robotics and Automation Letters. Copyright may be transferred without notice,
  after which this version may no longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Artificial Neural Nets and the Representation of Human Concepts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.05337v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.05337v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Timo Freiesleben
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  What do artificial neural networks (ANNs) learn? The machine learning (ML)
community shares the narrative that ANNs must develop abstract human concepts
to perform complex tasks. Some go even further and believe that these concepts
are stored in individual units of the network. Based on current research, I
systematically investigate the assumptions underlying this narrative. I
conclude that ANNs are indeed capable of performing complex prediction tasks,
and that they may learn human and non-human concepts to do so. However,
evidence indicates that ANNs do not represent these concepts in individual
units.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>For: Philosophy of Science for Machine Learning: Core Issues and New
  Perspectives, edited by Juan Duran and Giorgia Pozzi</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Distributed, Parallel, and Cluster Computing
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Empowering Data Mesh with Federated Learning <span class="chip">KDD</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17878v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17878v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoyuan Li, Salman Toor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The evolution of data architecture has seen the rise of data lakes, aiming to
solve the bottlenecks of data management and promote intelligent
decision-making. However, this centralized architecture is limited by the
proliferation of data sources and the growing demand for timely analysis and
processing. A new data paradigm, Data Mesh, is proposed to overcome these
challenges. Data Mesh treats domains as a first-class concern by distributing
the data ownership from the central team to each data domain, while keeping the
federated governance to monitor domains and their data products. Many
multi-million dollar organizations like Paypal, Netflix, and Zalando have
already transformed their data analysis pipelines based on this new
architecture. In this decentralized architecture where data is locally
preserved by each domain team, traditional centralized machine learning is
incapable of conducting effective analysis across multiple domains, especially
for security-sensitive organizations. To this end, we introduce a pioneering
approach that incorporates Federated Learning into Data Mesh. To the best of
our knowledge, this is the first open-source applied work that represents a
critical advancement toward the integration of federated learning methods into
the Data Mesh paradigm, underscoring the promising prospects for
privacy-preserving and decentralized data analysis strategies within Data Mesh
architecture.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In Proceedings of ACM Knowledge Discovery and Data Mining, Barcelona,
  Spain, 25th - 29th August, 2024 (Conference acronym KDD), 9 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An AI-Native Runtime for Multi-Wearable Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17863v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17863v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chulhong Min, Utku Günay Acer, SiYoung Jang, Sangwon Choi, Diana A. Vasile, Taesik Gong, Juheon Yi, Fahim Kawsar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The miniaturization of AI accelerators is paving the way for next-generation
wearable applications within wearable technologies. We introduce Mojito, an
AI-native runtime with advanced MLOps designed to facilitate the development
and deployment of these applications on wearable devices. It emphasizes the
necessity of dynamic orchestration of distributed resources equipped with
ultra-low-power AI accelerators to overcome challenges associated with
unpredictable runtime environments. Through its innovative approaches, Mojito
demonstrates how future wearable technologies can evolve to be more autonomous.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GPFL: A Gradient Projection-Based Client Selection Framework for
  Efficient Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17833v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17833v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shijie Na, Yuzhi Liang, Siu-Ming Yiu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning client selection is crucial for determining participant
clients while balancing model accuracy and communication efficiency. Existing
methods have limitations in handling data heterogeneity, computational burdens,
and independent client treatment. To address these challenges, we propose GPFL,
which measures client value by comparing local and global descent directions.
We also employ an Exploit-Explore mechanism to enhance performance.
Experimental results on FEMINST and CIFAR-10 datasets demonstrate that GPFL
outperforms baselines in Non-IID scenarios, achieving over 9\% improvement in
FEMINST test accuracy. Moreover, GPFL exhibits shorter computation times
through pre-selection and parameter reuse in federated learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SPES: Towards Optimizing Performance-Resource Trade-Off for Serverless
  Functions <span class="chip">ICDE 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17574v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17574v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cheryl Lee, Zhouruixin Zhu, Tianyi Yang, Yintong Huo, Yuxin Su, Pinjia He, Michael R. Lyu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As an emerging cloud computing deployment paradigm, serverless computing is
gaining traction due to its efficiency and ability to harness on-demand cloud
resources. However, a significant hurdle remains in the form of the cold start
problem, causing latency when launching new function instances from scratch.
Existing solutions tend to use over-simplistic strategies for function
pre-loading/unloading without full invocation pattern exploitation, rendering
unsatisfactory optimization of the trade-off between cold start latency and
resource waste. To bridge this gap, we propose SPES, the first differentiated
scheduler for runtime cold start mitigation by optimizing serverless function
provision. Our insight is that the common architecture of serverless systems
prompts the con- centration of certain invocation patterns, leading to
predictable invocation behaviors. This allows us to categorize functions and
pre-load/unload proper function instances with finer-grained strategies based
on accurate invocation prediction. Experiments demonstrate the success of SPES
in optimizing serverless function provision on both sides: reducing the
75th-percentile cold start rates by 49.77% and the wasted memory time by
56.43%, compared to the state-of-the-art. By mitigating the cold start issue,
SPES is a promising advancement in facilitating cloud services deployed on
serverless architectures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, accepted by ICDE 2024 (40th IEEE International Conference
  on Data Engineering)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Survey</span> on Resource Management in Joint Communication and
  Computing-Embedded SAGIN 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17400v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17400v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qian Chen, Zheng Guo, Weixiao Meng, Shuai Han, Cheng Li, Tony Q. S. Quek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of the 6G era aims for ubiquitous connectivity, with the
integration of non-terrestrial networks (NTN) offering extensive coverage and
enhanced capacity. As manufacturing advances and user demands evolve,
space-air-ground integrated networks (SAGIN) with computational capabilities
emerge as a viable solution for services requiring low latency and high
computational power. Resource management within joint communication and
computing-embedded SAGIN (JCC-SAGIN) presents greater complexity than
traditional terrestrial networks. This complexity arises from the
spatiotemporal dynamics of network topology and service demand, the
interdependency of large-scale resource variables, and intricate tradeoffs
among various performance metrics. Thus, a thorough examination of resource
management strategies in JCC-SAGIN is crucial, emphasizing the role of
non-terrestrial platforms with processing capabilities in 6G. This paper begins
by reviewing the architecture, enabling technologies, and applications in
JCC-SAGIN. Then, we offer a detailed overview of resource management modeling
and optimization methods, encompassing both traditional optimization approaches
and learning-based intelligent decision-making frameworks. Finally, we outline
the prospective research directions in JCC-SAGIN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>43 pages, 17 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FedMIL: Federated-Multiple Instance Learning for Video Analysis with
  Optimized DPP Scheduling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17331v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17331v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ashish Bastola, Hao Wang, Xiwen Chen, Abolfazl Razi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many AI platforms, including traffic monitoring systems, use Federated
Learning (FL) for decentralized sensor data processing for learning-based
applications while preserving privacy and ensuring secured information
transfer. On the other hand, applying supervised learning to large data
samples, like high-resolution images requires intensive human labor to label
different parts of a data sample. Multiple Instance Learning (MIL) alleviates
this challenge by operating over labels assigned to the 'bag' of instances. In
this paper, we introduce Federated Multiple-Instance Learning (FedMIL). This
framework applies federated learning to boost the training performance in
video-based MIL tasks such as vehicle accident detection using distributed CCTV
networks. However, data sources in decentralized settings are not typically
Independently and Identically Distributed (IID), making client selection
imperative to collectively represent the entire dataset with minimal clients.
To address this challenge, we propose DPPQ, a framework based on the
Determinantal Point Process (DPP) with a quality-based kernel to select clients
with the most diverse datasets that achieve better performance compared to both
random selection and current DPP-based client selection methods even with less
data utilization in the majority of non-IID cases. This offers a significant
advantage for deployment on edge devices with limited computational resources,
providing a reliable solution for training AI models in massive smart sensor
networks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Not All Federated Learning Algorithms Are Created Equal: A Performance
  Evaluation Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17287v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17287v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gustav A. Baumgart, Jaemin Shin, Ali Payani, Myungjin Lee, Ramana Rao Kompella
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) emerged as a practical approach to training a model
from decentralized data. The proliferation of FL led to the development of
numerous FL algorithms and mechanisms. Many prior efforts have given their
primary focus on accuracy of those approaches, but there exists little
understanding of other aspects such as computational overheads, performance and
training stability, etc. To bridge this gap, we conduct extensive performance
evaluation on several canonical FL algorithms (FedAvg, FedProx, FedYogi,
FedAdam, SCAFFOLD, and FedDyn) by leveraging an open-source federated learning
framework called Flame. Our comprehensive measurement study reveals that no
single algorithm works best across different performance metrics. A few key
observations are: (1) While some state-of-the-art algorithms achieve higher
accuracy than others, they incur either higher computation overheads (FedDyn)
or communication overheads (SCAFFOLD). (2) Recent algorithms present smaller
standard deviation in accuracy across clients than FedAvg, indicating that the
advanced algorithms' performances are stable. (3) However, algorithms such as
FedDyn and SCAFFOLD are more prone to catastrophic failures without the support
of additional techniques such as gradient clipping. We hope that our empirical
study can help the community to build best practices in evaluating FL
algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Activations and Gradients Compression for Model-Parallel Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.07788v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.07788v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mikhail Rudakov, Aleksandr Beznosikov, Yaroslav Kholodov, Alexander Gasnikov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large neural networks require enormous computational clusters of machines.
Model-parallel training, when the model architecture is partitioned
sequentially between workers, is a popular approach for training modern models.
Information compression can be applied to decrease workers communication time,
as it is often a bottleneck in such systems. This work explores how
simultaneous compression of activations and gradients in model-parallel
distributed training setup affects convergence. We analyze compression methods
such as quantization and TopK compression, and also experiment with error
compensation techniques. Moreover, we employ TopK with AQ-SGD per-batch error
feedback approach. We conduct experiments on image classification and language
model fine-tuning tasks. Our findings demonstrate that gradients require milder
compression rates than activations. We observe that $K=10\%$ is the lowest TopK
compression level, which does not harm model convergence severely. Experiments
also show that models trained with TopK perform well only when compression is
also applied during inference. We find that error feedback techniques do not
improve model-parallel training compared to plain compression, but allow model
inference without compression with almost no quality drop. Finally, when
applied with the AQ-SGD approach, TopK stronger than with $ K=30\%$ worsens
model performance significantly.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 6 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A simple GPU implementation of spectral-element methods for solving 3D
  Poisson type equations on rectangular domains and its applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.00226v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.00226v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyu Liu, Jie Shen, Xiangxiong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It is well known since 1960s that by exploring the tensor product structure
of the discrete Laplacian on Cartesian meshes, one can develop a simple direct
Poisson solver with an $\mathcal O(N^{\frac{d+1}d})$ complexity in d-dimension,
where N is the number of the total unknowns. The GPU acceleration of
numerically solving PDEs has been explored successfully around fifteen years
ago and become more and more popular in the past decade, driven by significant
advancement in both hardware and software technologies, especially in the
recent few years. We present in this paper a simple but extremely fast MATLAB
implementation on a modern GPU, which can be easily reproduced, for solving 3D
Poisson type equations using a spectral-element method. In particular, it costs
less than one second on a Nvidia A100 for solving a Poisson equation with one
billion degree of freedoms. We also present applications of this fast solver to
solve a linear (time-independent) Schr\"odinger equation and a nonlinear
(time-dependent) Cahn-Hilliard equation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Parallel Self-Avoiding Walks for a Low-Autocorrelation Binary Sequences
  Problem 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.15962v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.15962v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Borko Bošković, Jana Herzog, Janez Brest
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A low-autocorrelation binary sequences problem with a high figure of merit
factor represents a formidable computational challenge. An efficient parallel
computing algorithm is required to reach the new best-known solutions for this
problem. Therefore, we developed the $\mathit{sokol}_{\mathit{skew}}$ solver
for the skew-symmetric search space. The developed solver takes the advantage
of parallel computing on graphics processing units. The solver organized the
search process as a sequence of parallel and contiguous self-avoiding walks and
achieved a speedup factor of 387 compared with $\mathit{lssOrel}$, its
predecessor. The $\mathit{sokol}_{\mathit{skew}}$ solver belongs to stochastic
solvers and can not guarantee the optimality of solutions. To mitigate this
problem, we established the predictive model of stopping conditions according
to the small instances for which the optimal skew-symmetric solutions are
known. With its help and 99% probability, the $\mathit{sokol}_{\mathit{skew}}$
solver found all the known and seven new best-known skew-symmetric sequences
for odd instances from $L=121$ to $L=223$. For larger instances, the solver can
not reach 99% probability within our limitations, but it still found several
new best-known binary sequences. We also analyzed the trend of the best merit
factor values, and it shows that as sequence size increases, the value of the
merit factor also increases, and this trend is flatter for larger instances.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DISL: Fueling Research with A Large <span class="highlight-title">Dataset</span> of Solidity Smart Contracts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16861v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16861v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gabriele Morello, Mojtaba Eshghie, Sofia Bobadilla, Martin Monperrus
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The DISL dataset features a collection of $514,506$ unique Solidity files
that have been deployed to Ethereum mainnet. It caters to the need for a large
and diverse dataset of real-world smart contracts. DISL serves as a resource
for developing machine learning systems and for benchmarking software
engineering tools designed for smart contracts. By aggregating every verified
smart contract from Etherscan up to January 15, 2024, DISL surpasses existing
datasets in size and recency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ P-TimeSync: A Precise Time Synchronization Simulation with Network
  Propagation Delays 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01412v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01412v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Dai, Rui Zhang, Jinwei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Time serves as the foundation of modern society and will continue to grow in
value in the future world. Unlike previous research papers, authors delve into
various time sources, ranging from atomic time and GPS time to quartz time.
Specifically, we explore the time uncertainty associated with the four major
Global Navigation Satellite Systems. In existing time synchronization
simulations provide partial usages. However, our research introduces a
comprehensive and precise time synchronization simulation named P-TimeSync,
leading to a better understanding of time synchronization in distributed
environments. It is a state-of-the-art simulation tool for time because (1) it
can simulate atomic clocks and quartz clocks with user-defined software clock
algorithms, (2) the simulation provides nanosecond-level precision time across
different network propagation paths and distances, (3) the tool offers a
visualization platform with classic algorithms for distributed time
synchronization, such as Cristian's algorithm and Berkeley algorithm. The
simulation easily allows for the redefinition of configurations and functions,
supporting advanced research and development. The simulation tool could be
downloaded via the website: https://github.com/rui5097/purdue_timesync
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 5 figures, conference paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Securing Blockchain Systems: A Novel Collaborative Learning Framework to
  Detect Attacks in Transactions and Smart Contracts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.15804v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.15804v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tran Viet Khoa, Do Hai Son, Chi-Hieu Nguyen, Dinh Thai Hoang, Diep N. Nguyen, Nguyen Linh Trung, Tran Thi Thuy Quynh, Trong-Minh Hoang, Nguyen Viet Ha, Eryk Dutkiewicz, Mohammad Abu Alsheikh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the escalating prevalence of malicious activities exploiting
vulnerabilities in blockchain systems, there is an urgent requirement for
robust attack detection mechanisms. To address this challenge, this paper
presents a novel collaborative learning framework designed to detect attacks in
blockchain transactions and smart contracts by analyzing transaction features.
Our framework exhibits the capability to classify various types of blockchain
attacks, including intricate attacks at the machine code level (e.g., injecting
malicious codes to withdraw coins from users unlawfully), which typically
necessitate significant time and security expertise to detect. To achieve that,
the proposed framework incorporates a unique tool that transforms transaction
features into visual representations, facilitating efficient analysis and
classification of low-level machine codes. Furthermore, we propose a customized
collaborative learning model to enable real-time detection of diverse attack
types at distributed mining nodes. In order to create a comprehensive dataset,
we deploy a pilot system based on a private Ethereum network and conduct
multiple attack scenarios. To the best of our knowledge, our dataset is the
most comprehensive and diverse collection of transactions and smart contracts
synthesized in a laboratory for cyberattack detection in blockchain systems.
Our framework achieves a detection accuracy of approximately 94\% through
extensive simulations and real-time experiments with a throughput of over 2,150
transactions per second. These compelling results validate the efficacy of our
framework and showcase its adaptability in addressing real-world cyberattack
scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Performance
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ALISA: Accelerating Large Language Model Inference via Sparsity-Aware KV
  Caching <span class="chip">ISCA 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17312v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17312v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Youpeng Zhao, Di Wu, Jun Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Transformer architecture has significantly advanced natural language
processing (NLP) and has been foundational in developing large language models
(LLMs) such as LLaMA and OPT, which have come to dominate a broad range of NLP
tasks. Despite their superior accuracy, LLMs present unique challenges in
practical inference, concerning the compute and memory-intensive nature. Thanks
to the autoregressive characteristic of LLM inference, KV caching for the
attention layers in Transformers can effectively accelerate LLM inference by
substituting quadratic-complexity computation with linear-complexity memory
accesses. Yet, this approach requires increasing memory as demand grows for
processing longer sequences. The overhead leads to reduced throughput due to
I/O bottlenecks and even out-of-memory errors, particularly on
resource-constrained systems like a single commodity GPU. In this paper, we
propose ALISA, a novel algorithm-system co-design solution to address the
challenges imposed by KV caching. On the algorithm level, ALISA prioritizes
tokens that are most important in generating a new token via a Sparse Window
Attention (SWA) algorithm. SWA introduces high sparsity in attention layers and
reduces the memory footprint of KV caching at negligible accuracy loss. On the
system level, ALISA employs three-phase token-level dynamical scheduling and
optimizes the trade-off between caching and recomputation, thus maximizing the
overall performance in resource-constrained systems. In a single GPU-CPU
system, we demonstrate that under varying workloads, ALISA improves the
throughput of baseline systems such as FlexGen and vLLM by up to 3X and 1.9X,
respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ISCA 2024</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Operating Systems
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AIOS: LLM Agent Operating System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16971v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16971v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Mei, Zelong Li, Shuyuan Xu, Ruosong Ye, Yingqiang Ge, Yongfeng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integration and deployment of large language model (LLM)-based
intelligent agents have been fraught with challenges that compromise their
efficiency and efficacy. Among these issues are sub-optimal scheduling and
resource allocation of agent requests over the LLM, the difficulties in
maintaining context during interactions between agent and LLM, and the
complexities inherent in integrating heterogeneous agents with different
capabilities and specializations. The rapid increase of agent quantity and
complexity further exacerbates these issues, often leading to bottlenecks and
sub-optimal utilization of resources. Inspired by these challenges, this paper
presents AIOS, an LLM agent operating system, which embeds large language model
into operating systems (OS) as the brain of the OS, enabling an operating
system "with soul" -- an important step towards AGI. Specifically, AIOS is
designed to optimize resource allocation, facilitate context switch across
agents, enable concurrent execution of agents, provide tool service for agents,
and maintain access control for agents. We present the architecture of such an
operating system, outline the core challenges it aims to resolve, and provide
the basic design and implementation of the AIOS. Our experiments on concurrent
execution of multiple agents demonstrate the reliability and efficiency of our
AIOS modules. Through this, we aim to not only improve the performance and
efficiency of LLM agents but also to pioneer for better development and
deployment of the AIOS ecosystem in the future. The project is open-source at
https://github.com/agiresearch/AIOS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 5 figures, 5 tables; comments and suggestions are
  appreciated</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Databases
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Empirical Analysis of EIP-3675: Miner Dynamics, Transaction Fees, and
  Transaction Time 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17885v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17885v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Umesh Bhatt, Sarvesh Pandey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Ethereum Improvement Proposal 3675 (EIP-3675) marks a significant shift,
transitioning from a Proof of Work (PoW) to a Proof of Stake (PoS) consensus
mechanism. This transition resulted in a staggering 99.95% decrease in energy
consumption. However, the transition prompts two critical questions: (1). How
does EIP-3675 affect miners' dynamics? and (2). How do users determine priority
fees, considering that paying too little may cause delays or non-inclusion, yet
paying too much wastes money with little to no benefits? To address the first
question, we present a comprehensive empirical study examining EIP-3675's
effect on miner dynamics (i.e., miner participation, distribution, and the
degree of randomness in miner selection). Our findings reveal that the
transition has encouraged broader participation of miners in block append
operation, resulting in a larger pool of unique miners ($\approx50\times$ PoW),
and the change in miner distribution with the increased number of unique small
category miners ($\approx60\times$ PoW). However, there is an unintended
consequence: a reduction in the miner selection randomness, which signifies the
negative impact of the transition to PoS-Ethereum on network decentralization.
Regarding the second question, we employed regression-based machine learning
models; the Gradient Boosting Regressor performed best in predicting priority
fees, while the K-Neighbours Regressor was worst.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted as a full paper at the IEEE
  International Conference on Blockchain and Cryptocurrency (ICBC) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Query Refinement for Diverse Top-$k$ Selection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17786v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17786v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Felix S. Campbell, Alon Silberstein, Yuval Moskovitch, Julia Stoyanovich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Database queries are often used to select and rank items as decision support
for many applications. As automated decision-making tools become more
prevalent, there is a growing recognition of the need to diversify their
outcomes. In this paper, we define and study the problem of modifying the
selection conditions of an ORDER BY query so that the result of the modified
query closely fits some user-defined notion of diversity while simultaneously
maintaining the intent of the original query. We show the hardness of this
problem and propose a Mixed Integer Linear Programming (MILP) based solution.
We further present optimizations designed to enhance the scalability and
applicability of the solution in real-life scenarios. We investigate the
performance characteristics of our algorithm and show its efficiency and the
usefulness of our optimizations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards a FAIR Documentation of Workflows and Models in Applied
  Mathematics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17778v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17778v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marco Reidelbach, Björn Schembera, Marcus Weber
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modeling-Simulation-Optimization workflows play a fundamental role in applied
mathematics. The Mathematical Research Data Initiative, MaRDI, responded to
this by developing a FAIR and machine-interpretable template for a
comprehensive documentation of such workflows. MaRDMO, a Plugin for the
Research Data Management Organiser, enables scientists from diverse fields to
document and publish their workflows on the MaRDI Portal seamlessly using the
MaRDI template. Central to these workflows are mathematical models. MaRDI
addresses them with the MathModDB ontology, offering a structured formal model
description. Here, we showcase the interaction between MaRDMO and the MathModDB
Knowledge Graph through an algebraic modeling workflow from the Digital
Humanities. This demonstration underscores the versatility of both services
beyond their original numerical domain.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ When View- and Conflict-Robustness Coincide for Multiversion Concurrency
  Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17665v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17665v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brecht Vandevoort, Bas Ketsman, Frank Neven
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A DBMS allows trading consistency for efficiency through the allocation of
isolation levels that are strictly weaker than serializability. The robustness
problem asks whether, for a given set of transactions and a given allocation of
isolation levels, every possible interleaved execution of those transactions
that is allowed under the provided allocation, is always safe. In the
literature, safe is interpreted as conflict-serializable (to which we refer
here as conflict-robustness). In this paper, we study the view-robustness
problem, interpreting safe as view-serializable. View-serializability is a more
permissive notion that allows for a greater number of schedules to be
serializable and aligns more closely with the intuitive understanding of what
it means for a database to be consistent. However, view-serializability is more
complex to analyze (e.g., conflict-serializability can be decided in polynomial
time whereas deciding view-serializability is NP-complete). While
conflict-robustness implies view-robustness, the converse does not hold in
general. In this paper, we provide a sufficient condition for isolation levels
guaranteeing that conflict- and view-robustness coincide and show that this
condition is satisfied by the isolation levels occurring in Postgres and
Oracle: read committed (RC), snapshot isolation (SI) and serializable snapshot
isolation (SSI). It hence follows that for these systems, widening from
conflict- to view-serializability does not allow for more sets of transactions
to become robust. Interestingly, the complexity of deciding serializability
within these isolation levels is still quite different. Indeed, deciding
conflict-serializability for schedules allowed under RC and SI remains in
polynomial time, while we show that deciding view-serializability within these
isolation levels remains NP-complete.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Geometric planted matchings beyond the Gaussian model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17469v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17469v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lucas da Rocha Schwengber, Roberto Imbuzeiro Oliveira
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of recovering an unknown matching between a set of
$n$ randomly placed points in $\mathbb{R}^d$ and random perturbations of these
points. This can be seen as a model for particle tracking and more generally,
entity resolution. We use matchings in random geometric graphs to derive
minimax lower bounds for this problem that hold under great generality. Using
these results we show that for a broad class of distributions, the order of the
number of mistakes made by an estimator that minimizes the sum of squared
Euclidean distances is minimax optimal when $d$ is fixed and is optimal up to
$n^{o(1)}$ factors when $d = o(\log n)$. In the high-dimensional regime we
consider a setup where both initial positions and perturbations have
independent sub-Gaussian coordinates. In this setup we give sufficient
conditions under which the same estimator makes no mistakes with high
probability. We prove an analogous result for an adapted version of this
estimator that incorporates information on the covariance matrix of the
perturbations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>36 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Disambiguate Entity Matching through Relation Discovery with Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17344v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17344v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zezhou Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Entity matching is a critical challenge in data integration and cleaning,
central to tasks like fuzzy joins and deduplication. Traditional approaches
have focused on overcoming fuzzy term representations through methods such as
edit distance, Jaccard similarity, and more recently, embeddings and deep
neural networks, including advancements from large language models (LLMs) like
GPT. However, the core challenge in entity matching extends beyond term
fuzziness to the ambiguity in defining what constitutes a "match," especially
when integrating with external databases. This ambiguity arises due to varying
levels of detail and granularity among entities, complicating exact matches. We
propose a novel approach that shifts focus from purely identifying semantic
similarities to understanding and defining the "relations" between entities as
crucial for resolving ambiguities in matching. By predefining a set of
relations relevant to the task at hand, our method allows analysts to navigate
the spectrum of similarity more effectively, from exact matches to conceptually
related entities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Decade of Scholarly Research on Open Knowledge Graphs <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.13186v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.13186v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Houcemeddine Turki, Abraham Toluwase Owodunni, Mohamed Ali Hadj Taieb, René Fabrice Bile, Mohamed Ben Aouicha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The proliferation of open knowledge graphs has led to a surge in scholarly
research on the topic over the past decade. This paper presents a bibliometric
analysis of the scholarly literature on open knowledge graphs published between
2013 and 2023. The study aims to identify the trends, patterns, and impact of
research in this field, as well as the key topics and research questions that
have emerged. The work uses bibliometric techniques to analyze a sample of 4445
scholarly articles retrieved from Scopus. The findings reveal an
ever-increasing number of publications on open knowledge graphs published every
year, particularly in developed countries (+50 per year). These outputs are
published in highly-referred scholarly journals and conferences. The study
identifies three main research themes: (1) knowledge graph construction and
enrichment, (2) evaluation and reuse, and (3) fusion of knowledge graphs into
NLP systems. Within these themes, the study identifies specific tasks that have
received considerable attention, including entity linking, knowledge graph
embedding, and graph neural networks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Camera-ready edition for LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unveiling the Pitfalls of Knowledge Editing for Large Language Models <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02129v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02129v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhoubo Li, Ningyu Zhang, Yunzhi Yao, Mengru Wang, Xi Chen, Huajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the cost associated with fine-tuning Large Language Models (LLMs)
continues to rise, recent research efforts have pivoted towards developing
methodologies to edit implicit knowledge embedded within LLMs. Yet, there's
still a dark cloud lingering overhead -- will knowledge editing trigger
butterfly effect? since it is still unclear whether knowledge editing might
introduce side effects that pose potential risks or not. This paper pioneers
the investigation into the potential pitfalls associated with knowledge editing
for LLMs. To achieve this, we introduce new benchmark datasets and propose
innovative evaluation metrics. Our results underline two pivotal concerns: (1)
Knowledge Conflict: Editing groups of facts that logically clash can magnify
the inherent inconsistencies in LLMs-a facet neglected by previous methods. (2)
Knowledge Distortion: Altering parameters with the aim of editing factual
knowledge can irrevocably warp the innate knowledge structure of LLMs.
Experimental results vividly demonstrate that knowledge editing might
inadvertently cast a shadow of unintended consequences on LLMs, which warrant
attention and efforts for future works. Code and data are available at
https://github.com/zjunlp/PitfallsKnowledgeEditing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ByteCard: Enhancing ByteDance's Data Warehouse with Learned Cardinality
  Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16110v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16110v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxing Han, Haoyu Wang, Lixiang Chen, Yifeng Dong, Xing Chen, Benquan Yu, Chengcheng Yang, Weining Qian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cardinality estimation is a critical component and a longstanding challenge
in modern data warehouses. ByteHouse, ByteDance's cloud-native engine for big
data analysis in exabyte-scale environments, serves numerous internal
decision-making business scenarios. With the increasing demand of ByteHouse,
cardinality estimation becomes the bottleneck for efficiently processing
queries. Specifically, the existing query optimizer of ByteHouse uses the
traditional Selinger-like cardinality estimator, which can produce huge
estimation errors, resulting in sub-optimal query plans. To improve cardinality
estimation accuracy while maintaining a practical inference overhead, we
develop ByteCard framework that enables efficient training/updating and
integration of cardinality estimators. Furthermore, ByteCard adapts recent
advances in cardinality estimation to build models that can balance accuracy
and practicality (e.g., inference latency, model size, training/updating
overhead). We observe significant query processing speed-up in ByteHouse after
replacing the system's existing cardinality estimation with ByteCard's
estimations for several optimization strategies. Evaluations on real-world
datasets show the integration of ByteCard leads to an improvement of up to 30%
in the 99th quantile of latency. At last, we share our valuable experience in
engineering advanced cardinality estimators. We believe this experience can
help other data warehouses integrate more accurate and sophisticated solutions
on the critical path of query execution.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A proof-of-concept online metadata catalogue service of Earth
  observation <span class="highlight-title">dataset</span>s for human health research in exposomics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.08770v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.08770v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keumseok Koh, Maged N. Kamel Boulos, Gang Zheng, Hongsheng Zhang, Muralikrishna V. Iyyanki, Bosco Bwambale, Ashraf Dewan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This article describes research carried out during 2023 under an
International Society for Photogrammetry and Remote Sensing (ISPRS)-funded
project to develop and disseminate a metadata catalogue of Earth observation
data sources/products and types that are relevant to human health research in
exposomics, as a free service to interested researchers worldwide. The
proof-of-concept catalogue was informed by input from existing research
literature on the subject (desk research), as well as online communications
with, and relevant research publications collected from, a small panel (n = 5)
of select experts from the academia in three countries (China, UK and USA). It
has 90 metadata records of relevant Earth observation datasets (n = 40) and
associated health-focused research publications (n = 50). The project's online
portal offers a searchable version of the catalogue featuring a number of
search modes and filtering options. It is hoped future, more comprehensive
versions of this service will enable more researchers and studies to discover
and use remote sensing data about population-level exposures to disease
determinants (exposomic determinants of disease) in combination with other
relevant data to reveal fresh insights that could improve our understanding of
relevant diseases, and hence contribute to the development of better-optimized
prevention and management plans to tackle them.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SLEDGE: Synthesizing Simulation Environments for Driving Agents with
  Generative Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17933v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17933v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kashyap Chitta, Daniel Dauner, Andreas Geiger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  SLEDGE is the first generative simulator for vehicle motion planning trained
on real-world driving logs. Its core component is a learned model that is able
to generate agent bounding boxes and lane graphs. The model's outputs serve as
an initial state for traffic simulation. The unique properties of the entities
to be generated for SLEDGE, such as their connectivity and variable count per
scene, render the naive application of most modern generative models to this
task non-trivial. Therefore, together with a systematic study of existing lane
graph representations, we introduce a novel raster-to-vector autoencoder
(RVAE). It encodes agents and the lane graph into distinct channels in a
rasterized latent map. This facilitates both lane-conditioned agent generation
and combined generation of lanes and agents with a Diffusion Transformer. Using
generated entities in SLEDGE enables greater control over the simulation, e.g.
upsampling turns or increasing traffic density. Further, SLEDGE can support
500m long routes, a capability not found in existing data-driven simulators
like nuPlan. It presents new challenges for planning algorithms, evidenced by
failure rates of over 40% for PDM, the winner of the 2023 nuPlan challenge,
when tested on hard routes and dense traffic generated by our model. Compared
to nuPlan, SLEDGE requires 500$\times$ less storage to set up (<4GB), making it
a more accessible option and helping with democratizing future research in this
field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Need for Speed: Pruning <span class="highlight-title">Transformer</span>s with One Recipe <span class="chip">ICLR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17921v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17921v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samir Khaki, Konstantinos N. Plataniotis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce the $\textbf{O}$ne-shot $\textbf{P}$runing $\textbf{T}$echnique
for $\textbf{I}$nterchangeable $\textbf{N}$etworks ($\textbf{OPTIN}$) framework
as a tool to increase the efficiency of pre-trained transformer architectures
$\textit{without requiring re-training}$. Recent works have explored improving
transformer efficiency, however often incur computationally expensive
re-training procedures or depend on architecture-specific characteristics, thus
impeding practical wide-scale adoption. To address these shortcomings, the
OPTIN framework leverages intermediate feature distillation, capturing the
long-range dependencies of model parameters (coined $\textit{trajectory}$), to
produce state-of-the-art results on natural language, image classification,
transfer learning, and semantic segmentation tasks $\textit{without
re-training}$. Given a FLOP constraint, the OPTIN framework will compress the
network while maintaining competitive accuracy performance and improved
throughput. Particularly, we show a $\leq 2$% accuracy degradation from NLP
baselines and a $0.5$% improvement from state-of-the-art methods on image
classification at competitive FLOPs reductions. We further demonstrate the
generalization of tasks and architecture with comparative performance using
Mask2Former for semantic segmentation and cnn-style networks. OPTIN presents
one of the first one-shot efficient frameworks for compressing transformer
architectures that generalizes well across different class domains, in
particular: natural language and image-related tasks, without
$\textit{re-training}$.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in the International Conference on Learning Representations
  (ICLR) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LISA: Layerwise Importance Sampling for Memory-Efficient Large Language
  Model Fine-Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17919v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17919v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Pan, Xiang Liu, Shizhe Diao, Renjie Pi, Jipeng Zhang, Chi Han, Tong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The machine learning community has witnessed impressive advancements since
the first appearance of large language models (LLMs), yet their huge memory
consumption has become a major roadblock to large-scale training. Parameter
Efficient Fine-Tuning techniques such as Low-Rank Adaptation (LoRA) have been
proposed to alleviate this problem, but their performance still fails to match
full parameter training in most large-scale fine-tuning settings. Attempting to
complement this deficiency, we investigate layerwise properties of LoRA on
fine-tuning tasks and observe an uncommon skewness of weight norms across
different layers. Utilizing this key observation, a surprisingly simple
training strategy is discovered, which outperforms both LoRA and full parameter
training in a wide range of settings with memory costs as low as LoRA. We name
it Layerwise Importance Sampled AdamW (LISA), a promising alternative for LoRA,
which applies the idea of importance sampling to different layers in LLMs and
randomly freeze most middle layers during optimization. Experimental results
show that with similar or less GPU memory consumption, LISA surpasses LoRA or
even full parameter tuning in downstream fine-tuning tasks, where LISA
consistently outperforms LoRA by over $11\%$-$37\%$ in terms of MT-Bench
scores. On large models, specifically LLaMA-2-70B, LISA achieves on-par or
better performance than LoRA on MT-Bench, GSM8K, and PubMedQA, demonstrating
its effectiveness across different domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CMP: Cooperative Motion Prediction with Multi-Agent Communication 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17916v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17916v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuoyuan Wu, Yuping Wang, Hengbo Ma, Zhaowei Li, Hang Qiu, Jiachen Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The confluence of the advancement of Autonomous Vehicles (AVs) and the
maturity of Vehicle-to-Everything (V2X) communication has enabled the
capability of cooperative connected and automated vehicles (CAVs). Building on
top of cooperative perception, this paper explores the feasibility and
effectiveness of cooperative motion prediction. Our method, CMP, takes LiDAR
signals as input to enhance tracking and prediction capabilities. Unlike
previous work that focuses separately on either cooperative perception or
motion prediction, our framework, to the best of our knowledge, is the first to
address the unified problem where CAVs share information in both perception and
prediction modules. Incorporated into our design is the unique capability to
tolerate realistic V2X bandwidth limitations and transmission delays, while
dealing with bulky perception representations. We also propose a prediction
aggregation module, which unifies the predictions obtained by different CAVs
and generates the final prediction. Through extensive experiments and ablation
studies, we demonstrate the effectiveness of our method in cooperative
perception, tracking, and motion prediction tasks. In particular, CMP reduces
the average prediction error by 17.2\% with fewer missing detections compared
with the no cooperation setting. Our work marks a significant step forward in
the cooperative capabilities of CAVs, showcasing enhanced performance in
complex scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Serpent: Scalable and Efficient Image Restoration via Multi-scale
  Structured State Space Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17902v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17902v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Shahab Sepehri, Zalan Fabian, Mahdi Soltanolkotabi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The landscape of computational building blocks of efficient image restoration
architectures is dominated by a combination of convolutional processing and
various attention mechanisms. However, convolutional filters are inherently
local and therefore struggle at modeling long-range dependencies in images. On
the other hand, attention excels at capturing global interactions between
arbitrary image regions, however at a quadratic cost in image dimension. In
this work, we propose Serpent, an architecture that leverages recent advances
in state space models (SSMs) in its core computational block. SSMs, originally
introduced for sequence modeling, can maintain a global receptive field with a
favorable linear scaling in input size. Our preliminary results demonstrate
that Serpent can achieve reconstruction quality on par with state-of-the-art
techniques, while requiring orders of magnitude less compute (up to $150$ fold
reduction in FLOPS) and a factor of up to $5\times$ less GPU memory while
maintaining a compact model size.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 5 figures, preliminary workshop submission of a
  comprehensive work to be released soon</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Image-based Novel Fault Detection with Deep Learning Classifiers using
  Hierarchical Labels 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17891v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17891v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nurettin Sergin, Jiayu Huang, Tzyy-Shuh Chang, Hao Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One important characteristic of modern fault classification systems is the
ability to flag the system when faced with previously unseen fault types. This
work considers the unknown fault detection capabilities of deep neural
network-based fault classifiers. Specifically, we propose a methodology on how,
when available, labels regarding the fault taxonomy can be used to increase
unknown fault detection performance without sacrificing model performance. To
achieve this, we propose to utilize soft label techniques to improve the
state-of-the-art deep novel fault detection techniques during the training
process and novel hierarchically consistent detection statistics for online
novel fault detection. Finally, we demonstrated increased detection performance
on novel fault detection in inspection images from the hot steel rolling
process, with results well replicated across multiple scenarios and baseline
detection methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in IISE Transaction</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large scale paired antibody language models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17889v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17889v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Henry Kenlay, Frédéric A. Dreyer, Aleksandr Kovaltsuk, Dom Miketa, Douglas Pires, Charlotte M. Deane
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Antibodies are proteins produced by the immune system that can identify and
neutralise a wide variety of antigens with high specificity and affinity, and
constitute the most successful class of biotherapeutics. With the advent of
next-generation sequencing, billions of antibody sequences have been collected
in recent years, though their application in the design of better therapeutics
has been constrained by the sheer volume and complexity of the data. To address
this challenge, we present IgBert and IgT5, the best performing
antibody-specific language models developed to date which can consistently
handle both paired and unpaired variable region sequences as input. These
models are trained comprehensively using the more than two billion unpaired
sequences and two million paired sequences of light and heavy chains present in
the Observed Antibody Space dataset. We show that our models outperform
existing antibody and protein language models on a diverse range of design and
regression tasks relevant to antibody engineering. This advancement marks a
significant leap forward in leveraging machine learning, large scale data sets
and high-performance computing for enhancing antibody design for therapeutic
development.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 2 figures, 6 tables, model weights available at
  https://zenodo.org/doi/10.5281/zenodo.10876908</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Unreasonable Ineffectiveness of the Deeper Layers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17887v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17887v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrey Gromov, Kushal Tirumala, Hassan Shapourian, Paolo Glorioso, Daniel A. Roberts
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We empirically study a simple layer-pruning strategy for popular families of
open-weight pretrained LLMs, finding minimal degradation of performance on
different question-answering benchmarks until after a large fraction (up to
half) of the layers are removed. To prune these models, we identify the optimal
block of layers to prune by considering similarity across layers; then, to
"heal" the damage, we perform a small amount of finetuning. In particular, we
use parameter-efficient finetuning (PEFT) methods, specifically quantization
and Low Rank Adapters (QLoRA), such that each of our experiments can be
performed on a single A100 GPU. From a practical perspective, these results
suggest that layer pruning methods can complement other PEFT strategies to
further reduce computational resources of finetuning on the one hand, and can
improve the memory and latency of inference on the other hand. From a
scientific perspective, the robustness of these LLMs to the deletion of layers
implies either that current pretraining methods are not properly leveraging the
parameters in the deeper layers of the network or that the shallow layers play
a critical role in storing knowledge.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 + 10 pages, 5 + 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Compressed Multi-task embeddings for Data-Efficient Downstream training
  and inference in Earth Observation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17886v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17886v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Carlos Gomes, Thomas Brunschwiler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As repositories of large scale data in earth observation (EO) have grown, so
have transfer and storage costs for model training and inference, expending
significant resources. We introduce Neural Embedding Compression (NEC), based
on the transfer of compressed embeddings to data consumers instead of raw data.
We adapt foundation models (FM) through learned neural compression to generate
multi-task embeddings while navigating the tradeoff between compression rate
and embedding utility. We update only a small fraction of the FM parameters
(10%) for a short training period (1% of the iterations of pre-training). We
evaluate NEC on two EO tasks: scene classification and semantic segmentation.
Compared with applying traditional compression to the raw data, NEC achieves
similar accuracy with a 75% to 90% reduction in data. Even at 99.7%
compression, performance drops by only 5% on the scene classification task.
Overall, NEC is a data-efficient yet performant approach for multi-task EO
modelling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at IGARSS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Empowering Data Mesh with Federated Learning <span class="chip">KDD</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17878v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17878v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoyuan Li, Salman Toor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The evolution of data architecture has seen the rise of data lakes, aiming to
solve the bottlenecks of data management and promote intelligent
decision-making. However, this centralized architecture is limited by the
proliferation of data sources and the growing demand for timely analysis and
processing. A new data paradigm, Data Mesh, is proposed to overcome these
challenges. Data Mesh treats domains as a first-class concern by distributing
the data ownership from the central team to each data domain, while keeping the
federated governance to monitor domains and their data products. Many
multi-million dollar organizations like Paypal, Netflix, and Zalando have
already transformed their data analysis pipelines based on this new
architecture. In this decentralized architecture where data is locally
preserved by each domain team, traditional centralized machine learning is
incapable of conducting effective analysis across multiple domains, especially
for security-sensitive organizations. To this end, we introduce a pioneering
approach that incorporates Federated Learning into Data Mesh. To the best of
our knowledge, this is the first open-source applied work that represents a
critical advancement toward the integration of federated learning methods into
the Data Mesh paradigm, underscoring the promising prospects for
privacy-preserving and decentralized data analysis strategies within Data Mesh
architecture.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In Proceedings of ACM Knowledge Discovery and Data Mining, Barcelona,
  Spain, 25th - 29th August, 2024 (Conference acronym KDD), 9 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sample complexity of quantum hypothesis testing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17868v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17868v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao-Chung Cheng, Nilanjana Datta, Nana Liu, Theshani Nuradha, Robert Salzmann, Mark M. Wilde
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantum hypothesis testing has been traditionally studied from the
information-theoretic perspective, wherein one is interested in the optimal
decay rate of error probabilities as a function of the number of samples of an
unknown state. In this paper, we study the sample complexity of quantum
hypothesis testing, wherein the goal is to determine the minimum number of
samples needed to reach a desired error probability. By making use of the
wealth of knowledge that already exists in the literature on quantum hypothesis
testing, we characterize the sample complexity of binary quantum hypothesis
testing in the symmetric and asymmetric settings, and we provide bounds on the
sample complexity of multiple quantum hypothesis testing. In more detail, we
prove that the sample complexity of symmetric binary quantum hypothesis testing
depends logarithmically on the inverse error probability and inversely on the
negative logarithm of the fidelity. As a counterpart of the quantum Stein's
lemma, we also find that the sample complexity of asymmetric binary quantum
hypothesis testing depends logarithmically on the inverse type~II error
probability and inversely on the quantum relative entropy. Finally, we provide
lower and upper bounds on the sample complexity of multiple quantum hypothesis
testing, with it remaining an intriguing open question to improve these bounds.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>38 pages, 1 figure, preliminary version; see independent and
  concurrent work of Pensia, Jog, Loh at arXiv:2403.16981</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Using Domain Knowledge to Guide Dialog Structure Induction via Neural
  Probabilistic Soft Logic 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17853v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17853v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Connor Pryor, Quan Yuan, Jeremiah Liu, Mehran Kazemi, Deepak Ramachandran, Tania Bedrax-Weiss, Lise Getoor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dialog Structure Induction (DSI) is the task of inferring the latent dialog
structure (i.e., a set of dialog states and their temporal transitions) of a
given goal-oriented dialog. It is a critical component for modern dialog system
design and discourse analysis. Existing DSI approaches are often purely
data-driven, deploy models that infer latent states without access to domain
knowledge, underperform when the training corpus is limited/noisy, or have
difficulty when test dialogs exhibit distributional shifts from the training
domain. This work explores a neural-symbolic approach as a potential solution
to these problems. We introduce Neural Probabilistic Soft Logic Dialogue
Structure Induction (NEUPSL DSI), a principled approach that injects symbolic
knowledge into the latent space of a generative neural model. We conduct a
thorough empirical investigation on the effect of NEUPSL DSI learning on hidden
representation quality, few-shot learning, and out-of-domain generalization
performance. Over three dialog structure induction datasets and across
unsupervised and semi-supervised settings for standard and cross-domain
generalization, the injection of symbolic knowledge using NEUPSL DSI provides a
consistent boost in performance over the canonical baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Counterfactual Fairness through Transforming Data Orthogonal to Bias 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17852v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17852v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuyi Chen, Shixiang Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning models have shown exceptional prowess in solving complex
issues across various domains. Nonetheless, these models can sometimes exhibit
biased decision-making, leading to disparities in treatment across different
groups. Despite the extensive research on fairness, the nuanced effects of
multivariate and continuous sensitive variables on decision-making outcomes
remain insufficiently studied. We introduce a novel data pre-processing
algorithm, Orthogonal to Bias (OB), designed to remove the influence of a group
of continuous sensitive variables, thereby facilitating counterfactual fairness
in machine learning applications. Our approach is grounded in the assumption of
a jointly normal distribution within a structural causal model (SCM), proving
that counterfactual fairness can be achieved by ensuring the data is
uncorrelated with sensitive variables. The OB algorithm is model-agnostic,
catering to a wide array of machine learning models and tasks, and includes a
sparse variant to enhance numerical stability through regularization. Through
empirical evaluation on simulated and real-world datasets - including the adult
income and the COMPAS recidivism datasets - our methodology demonstrates its
capacity to enable fairer outcomes without compromising accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Climate Downscaling: A Deep-Learning Based Super-resolution Model of
  Precipitation Data with Attention Block and Skip Connections 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17847v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17847v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chia-Hao Chiang, Zheng-Han Huang, Liwen Liu, Hsin-Chien Liang, Yi-Chi Wang, Wan-Ling Tseng, Chao Wang, Che-Ta Chen, Ko-Chih Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human activities accelerate consumption of fossil fuels and produce
greenhouse gases, resulting in urgent issues today: global warming and the
climate change. These indirectly cause severe natural disasters, plenty of
lives suffering and huge losses of agricultural properties. To mitigate impacts
on our lands, scientists are developing renewable, reusable, and clean energies
and climatologists are trying to predict the extremes. Meanwhile, governments
are publicizing resource-saving policies for a more eco-friendly society and
arousing environment awareness. One of the most influencing factors is the
precipitation, bringing condensed water vapor onto lands. Water resources are
the most significant but basic needs in society, not only supporting our
livings, but also economics. In Taiwan, although the average annual
precipitation is up to 2,500 millimeter (mm), the water allocation for each
person is lower than the global average due to drastically geographical
elevation changes and uneven distribution through the year. Thus, it is crucial
to track and predict the rainfall to make the most use of it and to prevent the
floods. However, climate models have limited resolution and require intensive
computational power for local-scale use. Therefore, we proposed a deep
convolutional neural network with skip connections, attention blocks, and
auxiliary data concatenation, in order to downscale the low-resolution
precipitation data into high-resolution one. Eventually, we compare with other
climate downscaling methods and show better performance in metrics of Mean
Absolute Error (MAE), Root Mean Square Error (RMSE), Pearson Correlation,
structural similarity index (SSIM), and forecast indicators.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hierarchical Open-Vocabulary 3D Scene Graphs for Language-Grounded Robot
  Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17846v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17846v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdelrhman Werby, Chenguang Huang, Martin Büchner, Abhinav Valada, Wolfram Burgard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent open-vocabulary robot mapping methods enrich dense geometric maps with
pre-trained visual-language features. While these maps allow for the prediction
of point-wise saliency maps when queried for a certain language concept,
large-scale environments and abstract queries beyond the object level still
pose a considerable hurdle, ultimately limiting language-grounded robotic
navigation. In this work, we present HOV-SG, a hierarchical open-vocabulary 3D
scene graph mapping approach for language-grounded robot navigation. Leveraging
open-vocabulary vision foundation models, we first obtain state-of-the-art
open-vocabulary segment-level maps in 3D and subsequently construct a 3D scene
graph hierarchy consisting of floor, room, and object concepts, each enriched
with open-vocabulary features. Our approach is able to represent multi-story
buildings and allows robotic traversal of those using a cross-floor Voronoi
graph. HOV-SG is evaluated on three distinct datasets and surpasses previous
baselines in open-vocabulary semantic accuracy on the object, room, and floor
level while producing a 75% reduction in representation size compared to dense
open-vocabulary maps. In order to prove the efficacy and generalization
capabilities of HOV-SG, we showcase successful long-horizon
language-conditioned robot navigation within real-world multi-storage
environments. We provide code and trial video data at http://hovsg.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code and video are available at http://hovsg.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TractOracle: towards an anatomically-informed reward function for
  RL-based tractography 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17845v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17845v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antoine Théberge, Maxime Descoteaux, Pierre-Marc Jodoin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning (RL)-based tractography is a competitive alternative
to machine learning and classical tractography algorithms due to its high
anatomical accuracy obtained without the need for any annotated data. However,
the reward functions so far used to train RL agents do not encapsulate
anatomical knowledge which causes agents to generate spurious false positives
tracts. In this paper, we propose a new RL tractography system, TractOracle,
which relies on a reward network trained for streamline classification. This
network is used both as a reward function during training as well as a mean for
stopping the tracking process early and thus reduce the number of false
positive streamlines. This makes our system a unique method that evaluates and
reconstructs WM streamlines at the same time. We report an improvement of true
positive ratios by almost 20\% and a reduction of 3x of false positive ratios
on one dataset and an increase between 2x and 7x in the number true positive
streamlines on another dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mechanistic Design and Scaling of Hybrid Architectures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17844v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17844v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Poli, Armin W Thomas, Eric Nguyen, Pragaash Ponnusamy, Björn Deiseroth, Kristian Kersting, Taiji Suzuki, Brian Hie, Stefano Ermon, Christopher Ré, Ce Zhang, Stefano Massaroli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The development of deep learning architectures is a resource-demanding
process, due to a vast design space, long prototyping times, and high compute
costs associated with at-scale model training and evaluation. We set out to
simplify this process by grounding it in an end-to-end mechanistic architecture
design (MAD) pipeline, encompassing small-scale capability unit tests
predictive of scaling laws. Through a suite of synthetic token manipulation
tasks such as compression and recall, designed to probe capabilities, we
identify and test new hybrid architectures constructed from a variety of
computational primitives. We experimentally validate the resulting
architectures via an extensive compute-optimal and a new state-optimal scaling
law analysis, training over 500 language models between 70M to 7B parameters.
Surprisingly, we find MAD synthetics to correlate with compute-optimal
perplexity, enabling accurate evaluation of new architectures via isolated
proxy tasks. The new architectures found via MAD, based on simple ideas such as
hybridization and sparsity, outperform state-of-the-art Transformer,
convolutional, and recurrent architectures (Transformer++, Hyena, Mamba) in
scaling, both at compute-optimal budgets and in overtrained regimes. Overall,
these results provide evidence that performance on curated synthetic tasks can
be predictive of scaling laws, and that an optimal architecture should leverage
specialized layers via a hybrid topology.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GTA-HDR: A Large-Scale Synthetic <span class="highlight-title">Dataset</span> for HDR Image Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17837v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17837v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hrishav Bakul Barua, Kalin Stefanov, KokSheik Wong, Abhinav Dhall, Ganesh Krishnasamy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High Dynamic Range (HDR) content (i.e., images and videos) has a broad range
of applications. However, capturing HDR content from real-world scenes is
expensive and time- consuming. Therefore, the challenging task of
reconstructing visually accurate HDR images from their Low Dynamic Range (LDR)
counterparts is gaining attention in the vision research community. A major
challenge in this research problem is the lack of datasets, which capture
diverse scene conditions (e.g., lighting, shadows, weather, locations,
landscapes, objects, humans, buildings) and various image features (e.g.,
color, contrast, saturation, hue, luminance, brightness, radiance). To address
this gap, in this paper, we introduce GTA-HDR, a large-scale synthetic dataset
of photo-realistic HDR images sampled from the GTA-V video game. We perform
thorough evaluation of the proposed dataset, which demonstrates significant
qualitative and quantitative improvements of the state-of-the-art HDR image
reconstruction methods. Furthermore, we demonstrate the effectiveness of the
proposed dataset and its impact on additional computer vision tasks including
3D human pose estimation, human body part segmentation, and holistic scene
segmentation. The dataset, data collection pipeline, and evaluation code are
available at: https://github.com/HrishavBakulBarua/GTA-HDR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GPFL: A Gradient Projection-Based Client Selection Framework for
  Efficient Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17833v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17833v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shijie Na, Yuzhi Liang, Siu-Ming Yiu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning client selection is crucial for determining participant
clients while balancing model accuracy and communication efficiency. Existing
methods have limitations in handling data heterogeneity, computational burdens,
and independent client treatment. To address these challenges, we propose GPFL,
which measures client value by comparing local and global descent directions.
We also employ an Exploit-Explore mechanism to enhance performance.
Experimental results on FEMINST and CIFAR-10 datasets demonstrate that GPFL
outperforms baselines in Non-IID scenarios, achieving over 9\% improvement in
FEMINST test accuracy. Moreover, GPFL exhibits shorter computation times
through pre-selection and parameter reuse in federated learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning the Optimal Power Flow: Environment Design Matters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17831v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17831v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Wolgast, Astrid Nieße
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To solve the optimal power flow (OPF) problem, reinforcement learning (RL)
emerges as a promising new approach. However, the RL-OPF literature is strongly
divided regarding the exact formulation of the OPF problem as an RL
environment. In this work, we collect and implement diverse environment design
decisions from the literature regarding training data, observation space,
episode definition, and reward function choice. In an experimental analysis, we
show the significant impact of these environment design options on RL-OPF
training performance. Further, we derive some first recommendations regarding
the choice of these design decisions. The created environment framework is
fully open-source and can serve as a benchmark for future research in the
RL-OPF field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DiffH2O: Diffusion-Based Synthesis of Hand-Object Interactions from
  Textual Descriptions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17827v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17827v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sammy Christen, Shreyas Hampali, Fadime Sener, Edoardo Remelli, Tomas Hodan, Eric Sauser, Shugao Ma, Bugra Tekin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating natural hand-object interactions in 3D is challenging as the
resulting hand and object motions are expected to be physically plausible and
semantically meaningful. Furthermore, generalization to unseen objects is
hindered by the limited scale of available hand-object interaction datasets. We
propose DiffH2O, a novel method to synthesize realistic, one or two-handed
object interactions from provided text prompts and geometry of the object. The
method introduces three techniques that enable effective learning from limited
data. First, we decompose the task into a grasping stage and a text-based
interaction stage and use separate diffusion models for each. In the grasping
stage, the model only generates hand motions, whereas in the interaction phase
both hand and object poses are synthesized. Second, we propose a compact
representation that tightly couples hand and object poses. Third, we propose
two different guidance schemes to allow more control of the generated motions:
grasp guidance and detailed textual guidance. Grasp guidance takes a single
target grasping pose and guides the diffusion model to reach this grasp at the
end of the grasping stage, which provides control over the grasping pose. Given
a grasping motion from this stage, multiple different actions can be prompted
in the interaction phase. For textual guidance, we contribute comprehensive
text descriptions to the GRAB dataset and show that they enable our method to
have more fine-grained control over hand-object interactions. Our quantitative
and qualitative evaluation demonstrates that the proposed method outperforms
baseline methods and leads to natural hand-object motions. Moreover, we
demonstrate the practicality of our framework by utilizing a hand pose estimate
from an off-the-shelf pose estimator for guidance, and then sampling multiple
different actions in the interaction stage.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://diffh2o.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Are Compressed Language Models Less Subgroup Robust? <span class="chip">EMNLP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17811v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17811v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leonidas Gee, Andrea Zugarini, Novi Quadrianto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To reduce the inference cost of large language models, model compression is
increasingly used to create smaller scalable models. However, little is known
about their robustness to minority subgroups defined by the labels and
attributes of a dataset. In this paper, we investigate the effects of 18
different compression methods and settings on the subgroup robustness of BERT
language models. We show that worst-group performance does not depend on model
size alone, but also on the compression method used. Additionally, we find that
model compression does not always worsen the performance on minority subgroups.
Altogether, our analysis serves to further research into the subgroup
robustness of model compression.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The 2023 Conference on Empirical Methods in Natural Language
  Processing (EMNLP 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Annotated Biomedical Video Generation using Denoising Diffusion
  Probabilistic Models and Flow Fields 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17808v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17808v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rüveyda Yilmaz, Dennis Eschweiler, Johannes Stegmaier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The segmentation and tracking of living cells play a vital role within the
biomedical domain, particularly in cancer research, drug development, and
developmental biology. These are usually tedious and time-consuming tasks that
are traditionally done by biomedical experts. Recently, to automatize these
processes, deep learning based segmentation and tracking methods have been
proposed. These methods require large-scale datasets and their full potential
is constrained by the scarcity of annotated data in the biomedical imaging
domain. To address this limitation, we propose Biomedical Video Diffusion Model
(BVDM), capable of generating realistic-looking synthetic microscopy videos.
Trained only on a single real video, BVDM can generate videos of arbitrary
length with pixel-level annotations that can be used for training data-hungry
models. It is composed of a denoising diffusion probabilistic model (DDPM)
generating high-fidelity synthetic cell microscopy images and a flow prediction
model (FPM) predicting the non-rigid transformation between consecutive video
frames. During inference, initially, the DDPM imposes realistic cell textures
on synthetic cell masks which are generated based on real data statistics. The
flow prediction model predicts the flow field between consecutive masks and
applies that to the DDPM output from the previous time frame to create the next
one while keeping temporal consistency. BVDM outperforms state-of-the-art
synthetic live cell microscopy video generation models. Furthermore, we
demonstrate that a sufficiently large synthetic dataset enhances the
performance of cell segmentation and tracking models compared to using a
limited amount of available real data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Have Faith in Faithfulness: Going Beyond Circuit Overlap When Finding
  Model Mechanisms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17806v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17806v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Hanna, Sandro Pezzelle, Yonatan Belinkov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many recent language model (LM) interpretability studies have adopted the
circuits framework, which aims to find the minimal computational subgraph, or
circuit, that explains LM behavior on a given task. Most studies determine
which edges belong in a LM's circuit by performing causal interventions on each
edge independently, but this scales poorly with model size. Edge attribution
patching (EAP), gradient-based approximation to interventions, has emerged as a
scalable but imperfect solution to this problem. In this paper, we introduce a
new method - EAP with integrated gradients (EAP-IG) - that aims to better
maintain a core property of circuits: faithfulness. A circuit is faithful if
all model edges outside the circuit can be ablated without changing the model's
performance on the task; faithfulness is what justifies studying circuits,
rather than the full model. Our experiments demonstrate that circuits found
using EAP are less faithful than those found using EAP-IG, even though both
have high node overlap with circuits found previously using causal
interventions. We conclude more generally that when using circuits to compare
the mechanisms models use to solve tasks, faithfulness, not overlap, is what
should be measured.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scenario-Based Curriculum Generation for Multi-Agent Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17805v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17805v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Axel Brunnbauer, Luigi Berducci, Peter Priller, Dejan Nickovic, Radu Grosu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The automated generation of diverse and complex training scenarios has been
an important ingredient in many complex learning tasks. Especially in
real-world application domains, such as autonomous driving, auto-curriculum
generation is considered vital for obtaining robust and general policies.
However, crafting traffic scenarios with multiple, heterogeneous agents is
typically considered as a tedious and time-consuming task, especially in more
complex simulation environments. In our work, we introduce MATS-Gym, a
Multi-Agent Traffic Scenario framework to train agents in CARLA, a
high-fidelity driving simulator. MATS-Gym is a multi-agent training framework
for autonomous driving that uses partial scenario specifications to generate
traffic scenarios with variable numbers of agents. This paper unifies various
existing approaches to traffic scenario description into a single training
framework and demonstrates how it can be integrated with techniques from
unsupervised environment design to automate the generation of adaptive
auto-curricula. The code is available at
https://github.com/AutonomousDrivingExaminer/mats-gym.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 Pages, Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Secure Aggregation is Not Private Against Membership Inference Attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17775v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17775v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Khac-Hoang Ngo, Johan Östman, Giuseppe Durisi, Alexandre Graell i Amat
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Secure aggregation (SecAgg) is a commonly-used privacy-enhancing mechanism in
federated learning, affording the server access only to the aggregate of model
updates while safeguarding the confidentiality of individual updates. Despite
widespread claims regarding SecAgg's privacy-preserving capabilities, a formal
analysis of its privacy is lacking, making such presumptions unjustified. In
this paper, we delve into the privacy implications of SecAgg by treating it as
a local differential privacy (LDP) mechanism for each local update. We design a
simple attack wherein an adversarial server seeks to discern which update
vector a client submitted, out of two possible ones, in a single training round
of federated learning under SecAgg. By conducting privacy auditing, we assess
the success probability of this attack and quantify the LDP guarantees provided
by SecAgg. Our numerical results unveil that, contrary to prevailing claims,
SecAgg offers weak privacy against membership inference attacks even in a
single training round. Indeed, it is difficult to hide a local update by adding
other independent local updates when the updates are of high dimension. Our
findings underscore the imperative for additional privacy-enhancing mechanisms,
such as noise injection, in federated learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SciNews: From Scholarly Complexities to Public Narratives -- A <span class="highlight-title">Dataset</span>
  for Scientific News Report Generation <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17768v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17768v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongqi Pu, Yifan Wang, Jia Loy, Vera Demberg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scientific news reports serve as a bridge, adeptly translating complex
research articles into reports that resonate with the broader public. The
automated generation of such narratives enhances the accessibility of scholarly
insights. In this paper, we present a new corpus to facilitate this paradigm
development. Our corpus comprises a parallel compilation of academic
publications and their corresponding scientific news reports across nine
disciplines. To demonstrate the utility and reliability of our dataset, we
conduct an extensive analysis, highlighting the divergences in readability and
brevity between scientific news narratives and academic manuscripts. We
benchmark our dataset employing state-of-the-art text generation models. The
evaluation process involves both automatic and human evaluation, which lays the
groundwork for future explorations into the automated generation of scientific
news reports. The dataset and code related to this work are available at
https://dongqi.me/projects/SciNews.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LREC-COLING 2024 Main Conference Paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LocalTweets to LocalHealth: A Mental Health Surveillance Framework Based
  on Twitter Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.13452v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.13452v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vijeta Deshpande, Minhwa Lee, Zonghai Yao, Zihao Zhang, Jason Brian Gibbons, Hong Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prior research on Twitter (now X) data has provided positive evidence of its
utility in developing supplementary health surveillance systems. In this study,
we present a new framework to surveil public health, focusing on mental health
(MH) outcomes. We hypothesize that locally posted tweets are indicative of
local MH outcomes and collect tweets posted from 765 neighborhoods (census
block groups) in the USA. We pair these tweets from each neighborhood with the
corresponding MH outcome reported by the Center for Disease Control (CDC) to
create a benchmark dataset, LocalTweets. With LocalTweets, we present the first
population-level evaluation task for Twitter-based MH surveillance systems. We
then develop an efficient and effective method, LocalHealth, for predicting MH
outcomes based on LocalTweets. When used with GPT3.5, LocalHealth achieves the
highest F1-score and accuracy of 0.7429 and 79.78\%, respectively, a 59\%
improvement in F1-score over the GPT3.5 in zero-shot setting. We also utilize
LocalHealth to extrapolate CDC's estimates to proxy unreported neighborhoods,
achieving an F1-score of 0.7291. Our work suggests that Twitter data can be
effectively leveraged to simulate neighborhood-level MH outcomes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Simple and Scalable Strategies to Continually <span class="highlight-title">Pre-train</span> Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08763v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08763v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adam Ibrahim, Benjamin Thérien, Kshitij Gupta, Mats L. Richter, Quentin Anthony, Timothée Lesort, Eugene Belilovsky, Irina Rish
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are routinely pre-trained on billions of tokens,
only to start the process over again once new data becomes available. A much
more efficient solution is to continually pre-train these models, saving
significant compute compared to re-training. However, the distribution shift
induced by new data typically results in degraded performance on previous data
or poor adaptation to the new data. In this work, we show that a simple and
scalable combination of learning rate (LR) re-warming, LR re-decaying, and
replay of previous data is sufficient to match the performance of fully
re-training from scratch on all available data, as measured by the final loss
and the average score on several language model (LM) evaluation benchmarks.
Specifically, we show this for a weak but realistic distribution shift between
two commonly used LLM pre-training datasets (English$\rightarrow$English) and a
stronger distribution shift (English$\rightarrow$German) at the $405$M
parameter model scale with large dataset sizes (hundreds of billions of
tokens). Selecting the weak but realistic shift for larger-scale experiments,
we also find that our continual learning strategies match the re-training
baseline for a 10B parameter LLM. Our results demonstrate that LLMs can be
successfully updated via simple and scalable continual learning strategies,
matching the re-training baseline using only a fraction of the compute.
Finally, inspired by previous work, we propose alternatives to the cosine
learning rate schedule that help circumvent forgetting induced by LR re-warming
and that are not bound to a fixed token budget.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An optimal control perspective on diffusion-based generative modeling <span class="chip">NeurIPS 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.01364v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.01364v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julius Berner, Lorenz Richter, Karen Ullrich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We establish a connection between stochastic optimal control and generative
models based on stochastic differential equations (SDEs), such as recently
developed diffusion probabilistic models. In particular, we derive a
Hamilton-Jacobi-Bellman equation that governs the evolution of the
log-densities of the underlying SDE marginals. This perspective allows to
transfer methods from optimal control theory to generative modeling. First, we
show that the evidence lower bound is a direct consequence of the well-known
verification theorem from control theory. Further, we can formulate
diffusion-based generative modeling as a minimization of the Kullback-Leibler
divergence between suitable measures in path space. Finally, we develop a novel
diffusion-based method for sampling from unnormalized densities -- a problem
frequently occurring in statistics and computational sciences. We demonstrate
that our time-reversed diffusion sampler (DIS) can outperform other
diffusion-based sampling approaches on multiple numerical examples.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for oral presentation at NeurIPS 2022 Workshop on
  Score-Based Methods</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fully Independent Communication in Multi-Agent Reinforcement Learning <span class="chip">AAMAS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.15059v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.15059v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rafael Pina, Varuna De Silva, Corentin Artaud, Xiaolan Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-Agent Reinforcement Learning (MARL) comprises a broad area of research
within the field of multi-agent systems. Several recent works have focused
specifically on the study of communication approaches in MARL. While multiple
communication methods have been proposed, these might still be too complex and
not easily transferable to more practical contexts. One of the reasons for that
is due to the use of the famous parameter sharing trick. In this paper, we
investigate how independent learners in MARL that do not share parameters can
communicate. We demonstrate that this setting might incur into some problems,
to which we propose a new learning scheme as a solution. Our results show that,
despite the challenges, independent agents can still learn communication
strategies following our method. Additionally, we use this method to
investigate how communication in MARL is affected by different network
capacities, both for sharing and not sharing parameters. We observe that
communication may not always be needed and that the chosen agent network sizes
need to be considered when used together with communication in order to achieve
efficient learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Extended version of the paper appearing on AAMAS 2024 with the same
  title. 11 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A randomized algorithm for nonconvex minimization with inexact
  evaluations and complexity guarantees 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.18841v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.18841v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuyao Li, Stephen J. Wright
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider minimization of a smooth nonconvex function with inexact oracle
access to gradient and Hessian (without assuming access to the function value)
to achieve approximate second-order optimality. A novel feature of our method
is that if an approximate direction of negative curvature is chosen as the
step, we choose its sense to be positive or negative with equal probability. We
allow gradients to be inexact in a relative sense and relax the coupling
between inexactness thresholds for the first- and second-order optimality
conditions. Our convergence analysis includes both an expectation bound based
on martingale analysis and a high-probability bound based on concentration
inequalities. We apply our algorithm to empirical risk minimization problems
and obtain improved gradient sample complexity over existing works.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Borrowing Treasures from Neighbors: In-Context Learning for Multimodal
  Learning with Missing Modalities and Data Scarcity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09428v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09428v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuo Zhi, Ziquan Liu, Moe Elbadawi, Adam Daneshmend, Mine Orlu, Abdul Basit, Andreas Demosthenous, Miguel Rodrigues
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal machine learning with missing modalities is an increasingly
relevant challenge arising in various applications such as healthcare. This
paper extends the current research into missing modalities to the low-data
regime, i.e., a downstream task has both missing modalities and limited sample
size issues. This problem setting is particularly challenging and also
practical as it is often expensive to get full-modality data and sufficient
annotated training samples. We propose to use retrieval-augmented in-context
learning to address these two crucial issues by unleashing the potential of a
transformer's in-context learning ability. Diverging from existing methods,
which primarily belong to the parametric paradigm and often require sufficient
training samples, our work exploits the value of the available full-modality
data, offering a novel perspective on resolving the challenge. The proposed
data-dependent framework exhibits a higher degree of sample efficiency and is
empirically demonstrated to enhance the classification model's performance on
both full- and missing-modality data in the low-data regime across various
multimodal learning tasks. When only 1% of the training data are available, our
proposed method demonstrates an average improvement of 6.1% over a recent
strong baseline across various datasets and missing states. Notably, our method
also reduces the performance gap between full-modality and missing-modality
data compared with the baseline.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Probabilistically Rewired Message-Passing Neural Networks <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02156v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02156v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chendi Qian, Andrei Manolache, Kareem Ahmed, Zhe Zeng, Guy Van den Broeck, Mathias Niepert, Christopher Morris
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Message-passing graph neural networks (MPNNs) emerged as powerful tools for
processing graph-structured input. However, they operate on a fixed input graph
structure, ignoring potential noise and missing information. Furthermore, their
local aggregation mechanism can lead to problems such as over-squashing and
limited expressive power in capturing relevant graph structures. Existing
solutions to these challenges have primarily relied on heuristic methods, often
disregarding the underlying data distribution. Hence, devising principled
approaches for learning to infer graph structures relevant to the given
prediction task remains an open challenge. In this work, leveraging recent
progress in exact and differentiable $k$-subset sampling, we devise
probabilistically rewired MPNNs (PR-MPNNs), which learn to add relevant edges
while omitting less beneficial ones. For the first time, our theoretical
analysis explores how PR-MPNNs enhance expressive power, and we identify
precise conditions under which they outperform purely randomized approaches.
Empirically, we demonstrate that our approach effectively mitigates issues like
over-squashing and under-reaching. In addition, on established real-world
datasets, our method exhibits competitive or superior predictive performance
compared to traditional MPNN models and recent graph transformer architectures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimal Data Splitting in Distributed Optimization for Machine Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.07809v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.07809v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniil Medyakov, Gleb Molodtsov, Aleksandr Beznosikov, Alexander Gasnikov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The distributed optimization problem has become increasingly relevant
recently. It has a lot of advantages such as processing a large amount of data
in less time compared to non-distributed methods. However, most distributed
approaches suffer from a significant bottleneck - the cost of communications.
Therefore, a large amount of research has recently been directed at solving
this problem. One such approach uses local data similarity. In particular,
there exists an algorithm provably optimally exploiting the similarity
property. But this result, as well as results from other works solve the
communication bottleneck by focusing only on the fact that communication is
significantly more expensive than local computing and does not take into
account the various capacities of network devices and the different
relationship between communication time and local computing expenses. We
consider this setup and the objective of this study is to achieve an optimal
ratio of distributed data between the server and local machines for any costs
of communications and local computations. The running times of the network are
compared between uniform and optimal distributions. The superior theoretical
performance of our solutions is experimentally validated.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dynamics of Moral Behavior in Heterogeneous Populations of Learning
  Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.04202v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.04202v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elizaveta Tennant, Stephen Hailes, Mirco Musolesi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Growing concerns about safety and alignment of AI systems highlight the
importance of embedding moral capabilities in artificial agents. A promising
solution is the use of learning from experience, i.e., Reinforcement Learning.
In multi-agent (social) environments, complex population-level phenomena may
emerge from interactions between individual learning agents. Many of the
existing studies rely on simulated social dilemma environments to study the
interactions of independent learning agents. However, they tend to ignore the
moral heterogeneity that is likely to be present in societies of agents in
practice. For example, at different points in time a single learning agent may
face opponents who are consequentialist (i.e., caring about maximizing some
outcome over time) or norm-based (i.e., focusing on conforming to a specific
norm here and now). The extent to which agents' co-development may be impacted
by such moral heterogeneity in populations is not well understood. In this
paper, we present a study of the learning dynamics of morally heterogeneous
populations interacting in a social dilemma setting. Using a Prisoner's Dilemma
environment with a partner selection mechanism, we investigate the extent to
which the prevalence of diverse moral agents in populations affects individual
agents' learning behaviors and emergent population-level outcomes. We observe
several types of non-trivial interactions between pro-social and anti-social
agents, and find that certain classes of moral agents are able to steer selfish
agents towards more cooperative behavior.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Room Transfer Function Reconstruction Using Complex-valued Neural
  Networks and Irregularly Distributed Microphones 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.04866v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.04866v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francesca Ronchini, Luca Comanducci, Mirco Pezzoli, Fabio Antonacci, Augusto Sarti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reconstructing the room transfer functions needed to calculate the complex
sound field in a room has several impor- tant real-world applications. However,
an unpractical number of microphones is often required. Recently, in addition
to classical signal processing methods, deep learning techniques have been
applied to reconstruct the room transfer function starting from a very limited
set of measurements at scattered points in the room. In this paper, we employ
complex-valued neural networks to estimate room transfer functions in the
frequency range of the first room resonances, using a few irregularly
distributed microphones. To the best of our knowledge, this is the first time
that complex-valued neural networks are used to estimate room transfer
functions. To analyze the benefits of applying complex- valued optimization to
the considered task, we compare the proposed technique with a state-of-the-art
kernel-based signal processing approach for sound field reconstruction, showing
that the proposed technique exhibits relevant advantages in terms of phase
accuracy and overall quality of the reconstructed sound field. For informative
purposes, we also compare the model with a similarly-structured data-driven
approach that, however, applies a real-valued neural network to reconstruct
only the magnitude of the sound field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to EUSIPCO 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Activations and Gradients Compression for Model-Parallel Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.07788v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.07788v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mikhail Rudakov, Aleksandr Beznosikov, Yaroslav Kholodov, Alexander Gasnikov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large neural networks require enormous computational clusters of machines.
Model-parallel training, when the model architecture is partitioned
sequentially between workers, is a popular approach for training modern models.
Information compression can be applied to decrease workers communication time,
as it is often a bottleneck in such systems. This work explores how
simultaneous compression of activations and gradients in model-parallel
distributed training setup affects convergence. We analyze compression methods
such as quantization and TopK compression, and also experiment with error
compensation techniques. Moreover, we employ TopK with AQ-SGD per-batch error
feedback approach. We conduct experiments on image classification and language
model fine-tuning tasks. Our findings demonstrate that gradients require milder
compression rates than activations. We observe that $K=10\%$ is the lowest TopK
compression level, which does not harm model convergence severely. Experiments
also show that models trained with TopK perform well only when compression is
also applied during inference. We find that error feedback techniques do not
improve model-parallel training compared to plain compression, but allow model
inference without compression with almost no quality drop. Finally, when
applied with the AQ-SGD approach, TopK stronger than with $ K=30\%$ worsens
model performance significantly.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 6 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Differentially private multivariate medians 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.06459v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.06459v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kelly Ramsay, Aukosh Jagannath, Shoja'eddin Chenouri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Statistical tools which satisfy rigorous privacy guarantees are necessary for
modern data analysis. It is well-known that robustness against contamination is
linked to differential privacy. Despite this fact, using multivariate medians
for differentially private and robust multivariate location estimation has not
been systematically studied. We develop novel finite-sample performance
guarantees for differentially private multivariate depth-based medians, which
are essentially sharp. Our results cover commonly used depth functions, such as
the halfspace (or Tukey) depth, spatial depth, and the integrated dual depth.
We show that under Cauchy marginals, the cost of heavy-tailed location
estimation outweighs the cost of privacy. We demonstrate our results
numerically using a Gaussian contamination model in dimensions up to d = 100,
and compare them to a state-of-the-art private mean estimation algorithm. As a
by-product of our investigation, we prove concentration inequalities for the
output of the exponential mechanism about the maximizer of the population
objective function. This bound applies to objective functions that satisfy a
mild regularity condition.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>42 pages, 3 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AI and Generative AI for Research Discovery and Summarization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.06795v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.06795v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mark Glickman, Yi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  AI and generative AI tools, including chatbots like ChatGPT that rely on
large language models (LLMs), have burst onto the scene this year, creating
incredible opportunities to increase work productivity and improve our lives.
Statisticians and data scientists have begun experiencing the benefits from the
availability of these tools in numerous ways, such as the generation of
programming code from text prompts to analyze data or fit statistical models.
One area that these tools can make a substantial impact is in research
discovery and summarization. Standalone tools and plugins to chatbots are being
developed that allow researchers to more quickly find relevant literature than
pre-2023 search tools. Furthermore, generative AI tools have improved to the
point where they can summarize and extract the key points from research
articles in succinct language. Finally, chatbots based on highly parameterized
LLMs can be used to simulate abductive reasoning, which provides researchers
the ability to make connections among related technical topics, which can also
be used for research discovery. We review the developments in AI and generative
AI for research discovery and summarization, and propose directions where these
types of tools are likely to head in the future that may be of interest to
statistician and data scientists.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PINN surrogate of Li-ion battery models for parameter inference. Part
  II: Regularization and application of the pseudo-2D model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.17336v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.17336v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Malik Hassanaly, Peter J. Weddle, Ryan N. King, Subhayan De, Alireza Doostan, Corey R. Randall, Eric J. Dufek, Andrew M. Colclasure, Kandler Smith
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bayesian parameter inference is useful to improve Li-ion battery diagnostics
and can help formulate battery aging models. However, it is computationally
intensive and cannot be easily repeated for multiple cycles, multiple operating
conditions, or multiple replicate cells. To reduce the computational cost of
Bayesian calibration, numerical solvers for physics-based models can be
replaced with faster surrogates. A physics-informed neural network (PINN) is
developed as a surrogate for the pseudo-2D (P2D) battery model calibration. For
the P2D surrogate, additional training regularization was needed as compared to
the PINN single-particle model (SPM) developed in Part I. Both the PINN SPM and
P2D surrogate models are exercised for parameter inference and compared to data
obtained from a direct numerical solution of the governing equations. A
parameter inference study highlights the ability to use these PINNs to
calibrate scaling parameters for the cathode Li diffusion and the anode
exchange current density. By realizing computational speed-ups of 2250x for the
P2D model, as compared to using standard integrating methods, the PINN
surrogates enable rapid state-of-health diagnostics. In the low-data
availability scenario, the testing error was estimated to 2mV for the SPM
surrogate and 10mV for the P2D surrogate which could be mitigated with
additional data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Chat<span class="highlight-title">GPT</span> Needs SPADE (Sustainability, PrivAcy, Digital divide, and
  Ethics) Evaluation: A <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.03123v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.03123v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sunder Ali Khowaja, Parus Khuwaja, Kapal Dev, Weizheng Wang, Lewis Nkenyereye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  ChatGPT is another large language model (LLM) vastly available for the
consumers on their devices but due to its performance and ability to converse
effectively, it has gained a huge popularity amongst research as well as
industrial community. Recently, many studies have been published to show the
effectiveness, efficiency, integration, and sentiments of chatGPT and other
LLMs. In contrast, this study focuses on the important aspects that are mostly
overlooked, i.e. sustainability, privacy, digital divide, and ethics and
suggests that not only chatGPT but every subsequent entry in the category of
conversational bots should undergo Sustainability, PrivAcy, Digital divide, and
Ethics (SPADE) evaluation. This paper discusses in detail the issues and
concerns raised over chatGPT in line with aforementioned characteristics. We
also discuss the recent EU AI Act briefly in accordance with the SPADE
evaluation. We support our hypothesis by some preliminary data collection and
visualizations along with hypothesized facts. We also suggest mitigations and
recommendations for each of the concerns. Furthermore, we also suggest some
policies and recommendations for AI policy act, if designed by the governments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 8 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PINN surrogate of Li-ion battery models for parameter inference. Part I:
  Implementation and multi-fidelity hierarchies for the single-particle model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.17329v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.17329v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Malik Hassanaly, Peter J. Weddle, Ryan N. King, Subhayan De, Alireza Doostan, Corey R. Randall, Eric J. Dufek, Andrew M. Colclasure, Kandler Smith
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To plan and optimize energy storage demands that account for Li-ion battery
aging dynamics, techniques need to be developed to diagnose battery internal
states accurately and rapidly. This study seeks to reduce the computational
resources needed to determine a battery's internal states by replacing
physics-based Li-ion battery models -- such as the single-particle model (SPM)
and the pseudo-2D (P2D) model -- with a physics-informed neural network (PINN)
surrogate. The surrogate model makes high-throughput techniques, such as
Bayesian calibration, tractable to determine battery internal parameters from
voltage responses. This manuscript is the first of a two-part series that
introduces PINN surrogates of Li-ion battery models for parameter inference
(i.e., state-of-health diagnostics). In this first part, a method is presented
for constructing a PINN surrogate of the SPM. A multi-fidelity hierarchical
training, where several neural nets are trained with multiple physics-loss
fidelities is shown to significantly improve the surrogate accuracy when only
training on the governing equation residuals. The implementation is made
available in a companion repository (https://github.com/NREL/pinnstripes). The
techniques used to develop a PINN surrogate of the SPM are extended in Part II
for the PINN surrogate for the P2D battery model, and explore the Bayesian
calibration capabilities of both surrogates.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Disentangling the Spectral Properties of the Hodge Laplacian: Not All
  Small Eigenvalues Are Equal 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.14427v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.14427v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vincent P. Grande, Michael T. Schaub
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rich spectral information of the graph Laplacian has been instrumental in
graph theory, machine learning, and graph signal processing for applications
such as graph classification, clustering, or eigenmode analysis. Recently, the
Hodge Laplacian has come into focus as a generalisation of the ordinary
Laplacian for higher-order graph models such as simplicial and cellular
complexes. Akin to the traditional analysis of graph Laplacians, many authors
analyse the smallest eigenvalues of the Hodge Laplacian, which are connected to
important topological properties such as homology. However, small eigenvalues
of the Hodge Laplacian can carry different information depending on whether
they are related to curl or gradient eigenmodes, and thus may not be
comparable. We therefore introduce the notion of persistent eigenvector
similarity and provide a method to track individual harmonic, curl, and
gradient eigenvectors/-values through the so-called persistence filtration,
leveraging the full information contained in the Hodge-Laplacian spectrum
across all possible scales of a point cloud. Finally, we use our insights (a)
to introduce a novel form of Hodge spectral clustering and (b) to classify
edges and higher-order simplices based on their relationship to the smallest
harmonic, curl, and gradient eigenvectors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 4 figures, comments welcome</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Semi-Supervised Crowd Counting from Unlabeled Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2108.13969v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2108.13969v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoran Duan, Fan Wan, Rui Sun, Zeyu Wang, Varun Ojha, Yu Guan, Hubert P. H. Shum, Bingzhang Hu, Yang Long
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic Crowd behavior analysis can be applied to effectively help the
daily transportation statistics and planning, which helps the smart city
construction. As one of the most important keys, crowd counting has drawn
increasing attention. Recent works achieved promising performance but relied on
the supervised paradigm with expensive crowd annotations. To alleviate the
annotation cost in real-world transportation scenarios, in this work we
proposed a semi-supervised learning framework $S^{4}\textit{Crowd}$, which can
leverage both unlabeled/labeled data for robust crowd counting. In the
unsupervised pathway, two \textit{self-supervised losses} were proposed to
simulate the crowd variations such as scale, illumination, based on which
supervised information pseudo labels were generated and gradually refined. We
also proposed a crowd-driven recurrent unit \textit{Gated-Crowd-Recurrent-Unit
(GCRU)}, which can preserve discriminant crowd information by extracting
second-order statistics, yielding pseudo labels with improved quality. A joint
loss including both unsupervised/supervised information was proposed, and a
dynamic weighting strategy was employed to balance the importance of the
unsupervised loss and supervised loss at different training stages. We
conducted extensive experiments on four popular crowd counting datasets in
semi-supervised settings. Experimental results supported the effectiveness of
each proposed component in our $S^{4}$Crowd framework. Our method achieved
competitive performance in semi-supervised learning approaches on these crowd
counting datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient <span class="highlight-title">Pre-train</span>ing for Localized Instruction Generation of Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.15964v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.15964v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anil Batra, Davide Moltisanti, Laura Sevilla-Lara, Marcus Rohrbach, Frank Keller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Procedural videos show step-by-step demonstrations of tasks like recipe
preparation. Understanding such videos is challenging, involving the precise
localization of steps and the generation of textual instructions. Manually
annotating steps and writing instructions is costly, which limits the size of
current datasets and hinders effective learning. Leveraging large but noisy
video-transcript datasets for pre-training can boost performance, but demands
significant computational resources. Furthermore, transcripts contain
irrelevant content and exhibit style variation compared to instructions written
by human annotators. To mitigate both issues, we propose a technique,
Sieve-&-Swap, to automatically curate a smaller dataset: (i) Sieve filters
irrelevant transcripts and (ii) Swap enhances the quality of the text
instruction by automatically replacing the transcripts with human-written
instructions from a text-only recipe dataset. The curated dataset, three orders
of magnitude smaller than current web-scale datasets, enables efficient
training of large-scale models with competitive performance. We complement our
Sieve-\&-Swap approach with a Procedure Transformer (ProcX) for end-to-end step
localization and instruction generation for procedural videos. When this model
is pre-trained on our curated dataset, it achieves state-of-the-art performance
in zero-shot and finetuning settings on YouCook2 and Tasty, while using a
fraction of the computational resources.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This version has some missing experiments and elaborative technical
  details</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Toward a Theory of Causation for Interpreting Neural Code Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.03788v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.03788v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David N. Palacio, Alejandro Velasco, Nathan Cooper, Alvaro Rodriguez, Kevin Moran, Denys Poshyvanyk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural Language Models of Code, or Neural Code Models (NCMs), are rapidly
progressing from research prototypes to commercial developer tools. As such,
understanding the capabilities and limitations of such models is becoming
critical. However, the abilities of these models are typically measured using
automated metrics that often only reveal a portion of their real-world
performance. While, in general, the performance of NCMs appears promising,
currently much is unknown about how such models arrive at decisions. To this
end, this paper introduces $do_{code}$, a post hoc interpretability method
specific to NCMs that is capable of explaining model predictions. $do_{code}$
is based upon causal inference to enable programming language-oriented
explanations. While the theoretical underpinnings of $do_{code}$ are extensible
to exploring different model properties, we provide a concrete instantiation
that aims to mitigate the impact of spurious correlations by grounding
explanations of model behavior in properties of programming languages. To
demonstrate the practical benefit of $do_{code}$, we illustrate the insights
that our framework can provide by performing a case study on two popular deep
learning architectures and ten NCMs. The results of this case study illustrate
that our studied NCMs are sensitive to changes in code syntax. All our NCMs,
except for the BERT-like model, statistically learn to predict tokens related
to blocks of code (\eg brackets, parenthesis, semicolon) with less confounding
bias as compared to other programming language constructs. These insights
demonstrate the potential of $do_{code}$ as a useful method to detect and
facilitate the elimination of confounding bias in NCMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to appear in IEEE Transactions on Software Engineering</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RL$^3$: Boosting Meta Reinforcement Learning via RL inside RL$^2$ 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.15909v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.15909v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhinav Bhatia, Samer B. Nashed, Shlomo Zilberstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Meta reinforcement learning (meta-RL) methods such as RL$^2$ have emerged as
promising approaches for learning data-efficient RL algorithms tailored to a
given task distribution. However, they show poor asymptotic performance and
struggle with out-of-distribution tasks because they rely on sequence models,
such as recurrent neural networks or transformers, to process experiences
rather than summarize them using general-purpose RL components such as value
functions. In contrast, traditional RL algorithms are data-inefficient as they
do not use domain knowledge, but they do converge to an optimal policy in the
limit. We propose RL$^3$, a principled hybrid approach that incorporates
action-values, learned per task through traditional RL, in the inputs to
meta-RL. We show that RL$^3$ earns greater cumulative reward in the long term,
compared to RL$^2$, while maintaining data-efficiency in the short term, and
generalizes better to out-of-distribution tasks. Experiments are conducted on
both custom and benchmark discrete domains from the meta-RL literature that
exhibit a range of short-term, long-term, and complex dependencies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-Objective Optimization for Sparse Deep Multi-Task Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.12243v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.12243v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        S. S. Hotegni, M. Berkemeier, S. Peitz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Different conflicting optimization criteria arise naturally in various Deep
Learning scenarios. These can address different main tasks (i.e., in the
setting of Multi-Task Learning), but also main and secondary tasks such as loss
minimization versus sparsity. The usual approach is a simple weighting of the
criteria, which formally only works in the convex setting. In this paper, we
present a Multi-Objective Optimization algorithm using a modified Weighted
Chebyshev scalarization for training Deep Neural Networks (DNNs) with respect
to several tasks. By employing this scalarization technique, the algorithm can
identify all optimal solutions of the original problem while reducing its
complexity to a sequence of single-objective problems. The simplified problems
are then solved using an Augmented Lagrangian method, enabling the use of
popular optimization techniques such as Adam and Stochastic Gradient Descent,
while efficaciously handling constraints. Our work aims to address the
(economical and also ecological) sustainability issue of DNN models, with a
particular focus on Deep Multi-Task models, which are typically designed with a
very large number of weights to perform equally well on multiple tasks. Through
experiments conducted on two Machine Learning datasets, we demonstrate the
possibility of adaptively sparsifying the model during training without
significantly impacting its performance, if we are willing to apply
task-specific adaptations to the network weights. Code is available at
https://github.com/salomonhotegni/MDMTN
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Science and Game Theory
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Prediction-sharing During Training and Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17515v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17515v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yotam Gafni, Ronen Gradwohl, Moshe Tennenholtz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Two firms are engaged in a competitive prediction task. Each firm has two
sources of data -- labeled historical data and unlabeled inference-time data --
and uses the former to derive a prediction model, and the latter to make
predictions on new instances. We study data-sharing contracts between the
firms. The novelty of our study is to introduce and highlight the differences
between contracts that share prediction models only, contracts to share
inference-time predictions only, and contracts to share both. Our analysis
proceeds on three levels. First, we develop a general Bayesian framework that
facilitates our study. Second, we narrow our focus to two natural settings
within this framework: (i) a setting in which the accuracy of each firm's
prediction model is common knowledge, but the correlation between the
respective models is unknown; and (ii) a setting in which two hypotheses exist
regarding the optimal predictor, and one of the firms has a structural
advantage in deducing it. Within these two settings we study optimal contract
choice. More specifically, we find the individually rational and Pareto-optimal
contracts for some notable cases, and describe specific settings where each of
the different sharing contracts emerge as optimal. Finally, in the third level
of our analysis we demonstrate the applicability of our concepts in a synthetic
simulation using real loan data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ So Long Sucker: Endgame Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17302v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17302v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jean-Lou De Carufel, Marie Rose Jerade
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  So Long Sucker is a strategy board game requiring 4 players, each with $c$
chips of their designated color, and a board made of $k$ empty piles. With a
clear set-up come intricate rules, such as: players taking turns but not in a
fixed order, agreements between some players being made and broken at any time,
and a player winning the game even without any chips in hand.
  One of the main points of interest in studying this game, is finding when a
player has a winning strategy. The game begins with four players that get
eliminated successively until the winner is left. To study winning strategies,
it is of interest to look at endgame situations. We present the following game
set-up: there are two players left in the game, Blue and Red, and only their
respective chip colors. In this paper, we characterize Blue's winning
situations and strategies through inductive reasoning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>49 pages</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Theory
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reciprocity Calibration of Dual-Antenna Repeaters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17908v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17908v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Erik G. Larsson, Joao Vieira, Pål Frenger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a reciprocity calibration method for dual-antenna repeaters in
wireless networks. The method uses bi-directional measurements between two
network nodes, A and B, where for each bi-directional measurement, the
repeaters are configured in different states. The nodes A and B could be two
access points in a distributed MIMO system, or they could be a base station and
a mobile user terminal, for example. From the calibration measurements, the
differences between the repeaters' forward and reverse gains are estimated. The
repeaters are then (re-)configured to compensate for these differences such
that the repeaters appear, transparently to the network, as reciprocal
components of the propagation environment, enabling reciprocity-based
beamforming in the network.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE Wireless Communications Letters, 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sample complexity of quantum hypothesis testing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17868v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17868v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao-Chung Cheng, Nilanjana Datta, Nana Liu, Theshani Nuradha, Robert Salzmann, Mark M. Wilde
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantum hypothesis testing has been traditionally studied from the
information-theoretic perspective, wherein one is interested in the optimal
decay rate of error probabilities as a function of the number of samples of an
unknown state. In this paper, we study the sample complexity of quantum
hypothesis testing, wherein the goal is to determine the minimum number of
samples needed to reach a desired error probability. By making use of the
wealth of knowledge that already exists in the literature on quantum hypothesis
testing, we characterize the sample complexity of binary quantum hypothesis
testing in the symmetric and asymmetric settings, and we provide bounds on the
sample complexity of multiple quantum hypothesis testing. In more detail, we
prove that the sample complexity of symmetric binary quantum hypothesis testing
depends logarithmically on the inverse error probability and inversely on the
negative logarithm of the fidelity. As a counterpart of the quantum Stein's
lemma, we also find that the sample complexity of asymmetric binary quantum
hypothesis testing depends logarithmically on the inverse type~II error
probability and inversely on the quantum relative entropy. Finally, we provide
lower and upper bounds on the sample complexity of multiple quantum hypothesis
testing, with it remaining an intriguing open question to improve these bounds.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>38 pages, 1 figure, preliminary version; see independent and
  concurrent work of Pensia, Jog, Loh at arXiv:2403.16981</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Novel Temperature-based Model for SWIPT 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17792v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17792v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elio Faddoul, Yuan Guo, Christodoulos Skouroumounis, Ioannis Krikidis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this letter, a novel communication paradigm for simultaneous wireless
information and power transfer (SWIPT) is proposed, which leverages the thermal
characteristics of electromagnetic signals. In particular, the proposed scheme
exploits the inherent thermal dynamics of electromagnetic signals, enabling the
seamless integration of information decoding and energy harvesting (EH). As a
consequence, in contrast to conventional SWIPT techniques, the proposed model
eliminates the need to divide the received signal into orthogonal components.
By exploiting the thermal correlation between consecutive time slots, the
communication channel is converted to a virtual multiple-input multiple-output
(MIMO) channel with memory. We evaluate the achievable rate of the proposed
temperature-modulated channel for uniform and exponential input distributions
and assess its performance in terms of harvested energy through a non-linear
harvesting model. Our numerical results reveal that the exponential
distribution outperforms the uniform distribution in rate and harvested energy
at low input power levels, while the uniform distribution achieves a better EH
performance at high input power levels.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been accepted for publication in IEEE Wireless
  Communications Letters</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust Analysis of Full-Duplex Two-Way Space Shift Keying With RIS
  Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17751v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17751v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xusheng Zhu, Wen Chen, Qingqing Wu, Wen Fang, Chaoying Huang, Jun Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reconfigurable intelligent surface (RIS)-assisted index modulation system
schemes are considered a promising technology for sixth-generation (6G)
wireless communication systems, which can enhance various system capabilities
such as coverage and reliability. However, obtaining perfect channel state
information (CSI) is challenging due to the lack of a radio frequency chain in
RIS. In this paper, we investigate the RIS-assisted full-duplex (FD) two-way
space shift keying (SSK) system under imperfect CSI, where the signal emissions
are augmented by deploying RISs in the vicinity of two FD users. The maximum
likelihood detector is utilized to recover the transmit antenna index. With
this in mind, we derive closed-form average bit error probability (ABEP)
expression based on the Gaussian-Chebyshev quadrature (GCQ) method and provide
the upper bound and asymptotic ABEP expressions in the presence of channel
estimation errors. To gain more insights, we also derive the outage probability
and provide the throughput of the proposed scheme with imperfect CSI. The
correctness of the analytical derivation results is confirmed via Monte Carlo
simulations. It is demonstrated that increasing the number of elements of RIS
can significantly improve the ABEP performance of the FD system over the
half-duplex (HD) system. Furthermore, in the high SNR region, the ABEP
performance of the FD system is better than that of the HD system.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging A Variety of Anchors in Cellular Network for Ubiquitous
  Sensing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17652v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17652v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liang Liu, Shuowen Zhang, Shuguang Cui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Integrated sensing and communication (ISAC) has recently attracted tremendous
attention from both academia and industry, being envisioned as a key part of
the standards for the sixth-generation (6G) cellular network. A key challenge
of 6G-oriented ISAC lies in how to perform ubiquitous sensing based on the
communication signals and devices. Previous works have made great progresses on
studying the signal waveform design that leads to optimal communication-sensing
performance tradeoff. In this article, we aim to focus on issues arising from
the exploitation of the communication devices for sensing in 6G network.
Particularly, we will discuss about how to leverage various nodes available in
the cellular network as anchors to perform ubiquitous sensing. On one hand, the
base stations (BSs) will be the most important anchors in the future 6G ISAC
network, since they can generate/process radio signals with high range/angle
resolutions, and their positions are precisely known. Correspondingly, we will
first study the BS-based sensing technique. On the other hand, the BSs alone
may not enable ubiquitous sensing, since they cannot cover all the places with
strong line-of-sight (LOS) links. This motivates us to investigate the
possibility of using other nodes that are with higher density in the network to
act as the anchors. Along this line, we are interested in two types of new
anchors - user equipments (UEs) and reconfigurable intelligent surfaces (RISs).
This paper will shed light on the opportunities and challenges brought by
UE-assisted sensing and RIS-assisted sensing. Our goal is to devise a novel
6G-oriented sensing architecture where BSs, UEs, and RISs can work together to
provide ubiquitous sensing services.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>to appear in IEEE Communications Magazine</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Computer classification of linear codes based on lattice point
  enumeration and integer linear programming 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17509v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17509v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sascha Kurz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Linear codes play a central role in coding theory and have applications in
several branches of mathematics. For error correction purposes the minimum
Hamming distance should be as large as possible. Linear codes related to
applications in Galois Geometry often require a certain divisibility of the
occurring weights. In this paper we present an algorithmic framework for the
classification of linear codes over finite fields with restricted sets of
weights. The underlying algorithms are based on lattice point enumeration and
integer linear programming. We present new enumeration and non-existence
results for projective two-weight codes, divisible codes, and additive
$\mathbb{F}_4$-codes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Task-oriented and Semantics-aware Communication Framework for
  Avatar-centric Augmented Reality 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.15470v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.15470v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhe Wang, Yansha Deng, A. Hamid Aghvami
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Upon the advent of the emerging metaverse and its related applications in
Augmented Reality (AR), the current bit-oriented network struggles to support
real-time changes for the vast amount of associated information, hindering its
development. Thus, a critical revolution in the Sixth Generation (6G) networks
is envisioned through the joint exploitation of information context and its
importance to the task, leading to a communication paradigm shift towards
semantic and effectiveness levels. However, current research has not yet
proposed any explicit and systematic communication framework for AR
applications that incorporate these two levels. To fill this research gap, this
paper presents a task-oriented and semantics-aware communication framework for
augmented reality (TSAR) to enhance communication efficiency and effectiveness
in 6G. Specifically, we first analyse the traditional wireless AR point cloud
communication framework and then summarize our proposed semantic information
along with the end-to-end wireless communication. We then detail the design
blocks of the TSAR framework, covering both semantic and effectiveness levels.
Finally, numerous experiments have been conducted to demonstrate that, compared
to the traditional point cloud communication framework, our proposed TSAR
significantly reduces wireless AR application transmission latency by 95.6%,
while improving communication effectiveness in geometry and color aspects by up
to 82.4% and 20.4%, respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Modelling Irrational Behaviour of Residential End Users using
  Non-Stationary Gaussian Processes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.09012v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.09012v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nam Trong Dinh, Sahand Karimi-Arpanahi, Rui Yuan, S. Ali Pourmousavi, Mingyu Guo, Jon A. R. Liisberg, Julian Lemos-Vinasco
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Demand response (DR) plays a critical role in ensuring efficient electricity
consumption and optimal use of network assets. Yet, existing DR models often
overlook a crucial element, the irrational behaviour of electricity end users.
In this work, we propose a price-responsive model that incorporates key aspects
of end-user irrationality, specifically loss aversion, time inconsistency, and
bounded rationality. To this end, we first develop a framework that uses
Multiple Seasonal-Trend decomposition using Loess (MSTL) and non-stationary
Gaussian processes to model the randomness in the electricity consumption by
residential consumers. The impact of this model is then evaluated through a
community battery storage (CBS) business model. Additionally, we apply a
chance-constrained optimisation model for CBS operation that deals with the
unpredictability of the end-user irrationality. Our simulations using
real-world data show that the proposed DR model provides a more realistic
estimate of end-user price-responsive behaviour when considering irrationality.
Compared to a deterministic model that cannot fully take into account the
irrational behaviour of end users, the chance-constrained CBS operation model
yields an additional 19% revenue. Lastly, the business model reduces the
electricity costs of solar end users by 11%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This manuscript has been accepted for publication in IEEE
  Transactions on Smart Grid</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On Codes for the Noisy Substring Channel 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2102.01412v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2102.01412v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yonatan Yehezkeally, Nikita Polyanskii
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of coding for the substring channel, in which
information strings are observed only through their (multisets of) substrings.
Due to existing DNA sequencing techniques and applications in DNA-based storage
systems, interest in this channel has renewed in recent years. In contrast to
existing literature, we consider a noisy channel model where information is
subject to noise before its substrings are sampled, motivated by in-vivo
storage. We study two separate noise models, substitutions or deletions. In
both cases, we examine families of codes which may be utilized for
error-correction and present combinatorial bounds on their sizes. Through a
generalization of the concept of repeat-free strings, we show that the added
required redundancy due to this imperfect observation assumption is sublinear,
either when the fraction of errors in the observed substring length is
sufficiently small, or when that length is sufficiently long. This suggests
that no asymptotic cost in rate is incurred by this channel model in these
cases. Moreover, we develop an efficient encoder for such constrained strings
in some cases. Finally, we show how a similar encoder can be used to avoid
formation of secondary-structures in coded DNA strands, even when accounting
for imperfect structures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Author submitted, peer-reviewed, version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OTFS-based Robust MMSE Precoding Design in Over-the-air Computation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.01525v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.01525v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongkai Zhou, Jing Guo, Siqiang Wang, Zhong Zheng, Zesong Fei, Weijie Yuan, Xinyi Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Over-the-air computation (AirComp), as a data aggregation method that can
improve network efficiency by exploiting the superposition characteristics of
wireless channels, has received much attention recently. Meanwhile, the
orthogonal time frequency space (OTFS) modulation can provide a strong Doppler
resilience and facilitate reliable transmission for high-mobility
communications. Hence, in this work, we investigate an OTFS-based AirComp
system in the presence of time-frequency dual-selective channels. In
particular, we commence from the development of a novel transmission framework
for the considered system, where the pilot signal is sent together with data,
and the channel estimation is implemented according to the echo from the access
point to the sensor, thereby reducing the overhead of channel state information
(CSI) feedback. Hereafter, based on the CSI estimated from the previous frame,
a robust precoding matrix aiming at minimizing mean square error in the current
frame is designed, which takes into account the estimation error from the
receiver noise and the outdated CSI. The simulation results demonstrate the
effectiveness of the proposed robust precoding scheme by comparing it with the
non-robust precoding. The performance gain is more obvious in a high
signal-to-noise ratio in case of large channel estimation errors.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-25T00:00:00Z">2024-03-25</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Distributed, Parallel, and Cluster Computing
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FOOL: Addressing the Downlink Bottleneck in Satellite Computing with
  Neural Feature Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16677v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16677v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alireza Furutanpey, Qiyang Zhang, Philipp Raith, Tobias Pfandzelter, Shangguang Wang, Schahram Dustdar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nanosatellite constellations equipped with sensors capturing large geographic
regions provide unprecedented opportunities for Earth observation. As
constellation sizes increase, network contention poses a downlink bottleneck.
Orbital Edge Computing (OEC) leverages limited onboard compute resources to
reduce transfer costs by processing the raw captures at the source. However,
current solutions have limited practicability due to reliance on crude
filtering methods or over-prioritizing particular downstream tasks.
  This work presents FOOL, an OEC-native and task-agnostic feature compression
method that preserves prediction performance. FOOL partitions high-resolution
satellite imagery to maximize throughput. Further, it embeds context and
leverages inter-tile dependencies to lower transfer costs with negligible
overhead. While FOOL is a feature compressor, it can recover images with
competitive scores on perceptual quality measures at lower bitrates. We
extensively evaluate transfer cost reduction by including the peculiarity of
intermittently available network connections in low earth orbit. Lastly, we
test the feasibility of our system for standardized nanosatellite form factors.
We demonstrate that FOOL permits downlinking over 100x the data volume without
relying on prior information on the downstream tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, double column, 19 figures, 7 tables, Initial Submission to
  IEEE Transactions on Mobile Computing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Accelerating Federated Learning by Selecting Beneficial Herd of Local
  Gradients 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16557v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16557v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ping Luo, Xiaoge Deng, Ziqing Wen, Tao Sun, Dongsheng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) is a distributed machine learning framework in
communication network systems. However, the systems' Non-Independent and
Identically Distributed (Non-IID) data negatively affect the convergence
efficiency of the global model, since only a subset of these data samples are
beneficial for model convergence. In pursuit of this subset, a reliable
approach involves determining a measure of validity to rank the samples within
the dataset. In this paper, We propose the BHerd strategy which selects a
beneficial herd of local gradients to accelerate the convergence of the FL
model. Specifically, we map the distribution of the local dataset to the local
gradients and use the Herding strategy to obtain a permutation of the set of
gradients, where the more advanced gradients in the permutation are closer to
the average of the set of gradients. These top portion of the gradients will be
selected and sent to the server for global aggregation. We conduct experiments
on different datasets, models and scenarios by building a prototype system, and
experimental results demonstrate that our BHerd strategy is effective in
selecting beneficial local gradients to mitigate the effects brought by the
Non-IID dataset, thus accelerating model convergence.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Differentially Private Online Federated Learning with Correlated Noise 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16542v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16542v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaojiao Zhang, Linglingzhi Zhu, Mikael Johansson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel differentially private algorithm for online federated
learning that employs temporally correlated noise to improve the utility while
ensuring the privacy of the continuously released models. To address challenges
stemming from DP noise and local updates with streaming noniid data, we develop
a perturbed iterate analysis to control the impact of the DP noise on the
utility. Moreover, we demonstrate how the drift errors from local updates can
be effectively managed under a quasi-strong convexity condition. Subject to an
$(\epsilon, \delta)$-DP budget, we establish a dynamic regret bound over the
entire time horizon that quantifies the impact of key parameters and the
intensity of changes in dynamic environments. Numerical experiments validate
the efficacy of the proposed algorithm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ColonyOS -- A Meta-Operating System for Distributed Computing Across
  Heterogeneous Platform 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16486v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16486v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Johan Kristiansson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents ColonyOS, an open-source meta-operating system designed
to improve integration and utilization of diverse computing platforms,
including IoT, edge, cloud, and HPC. Operating as an overlay, ColonyOS can
interface with a wide range of computing environments, fostering creation of
so-called compute continuums. This makes it possible to develop AI workflows
and applications that can operate across platforms. At its core, ColonyOS
consists of distributed executors that integrate with various underlying
platforms based on a distributed microservice architecture. These executors
collectively form a colony, serving as a unified computing unit. To enable
secure integration of various platforms, each colony is provisioned with
precisely the resources needed, and all communication is confined within the
colony governed by a strict zero-trust security protocol. Interaction with
ColonyOS is done by submitting functional meta-descriptions of computational
tasks, called function specifications. These are sent to a Colonies server,
which acts as intermediary between applications and the executors. Upon
assignment, an executor interprets the meta-description and translates it into
an executable format, e.g. a Kubernetes deployment description, a Slurm script,
or a direct function call within the executor. Furthermore, a built-in
meta-file system enables data synchronization directives to be included in
meta-descriptions, enabling seamless data management across platforms.
Ultimately, ColonyOS paves the way for development of hyper-distributed
applications and workflows, which can seamlessly operate in a computing
continuum. The paper describes design principles and implementation details of
ColonyOS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>See https://colonyos.io for more information</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FedAC: A Adaptive Clustered Federated Learning Framework for
  Heterogeneous Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16460v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16460v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxin Zhang, Haoyu Chen, Zheng Lin, Zhe Chen, Jin Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Clustered federated learning (CFL) is proposed to mitigate the performance
deterioration stemming from data heterogeneity in federated learning (FL) by
grouping similar clients for cluster-wise model training. However, current CFL
methods struggle due to inadequate integration of global and intra-cluster
knowledge and the absence of an efficient online model similarity metric, while
treating the cluster count as a fixed hyperparameter limits flexibility and
robustness. In this paper, we propose an adaptive CFL framework, named FedAC,
which (1) efficiently integrates global knowledge into intra-cluster learning
by decoupling neural networks and utilizing distinct aggregation methods for
each submodule, significantly enhancing performance; (2) includes a
costeffective online model similarity metric based on dimensionality reduction;
(3) incorporates a cluster number fine-tuning module for improved adaptability
and scalability in complex, heterogeneous environments. Extensive experiments
show that FedAC achieves superior empirical performance, increasing the test
accuracy by around 1.82% and 12.67% on CIFAR-10 and CIFAR-100 datasets,
respectively, under different non-IID settings compared to SOTA methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Raptor: Distributed Scheduling for Serverless Functions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16457v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16457v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kevin Exton, Maria Read
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Serverless platforms that poorly schedule function requests inspire
developers to implement workarounds to issues like high cold start latencies,
poor fault tolerance, and limited support for parallel processing. These
solutions litter environments with idle containers and add unnecessary pressure
to the already underperforming scheduling services. An effective serverless
scheduling policy should encourage developers to write small and reusable
snippets of code, and give operators the freedom to administer cluster
workloads however necessary in order to meet their operational demands. To this
end, we have designed a distributed scheduling service that integrates with
existing serverless frameworks. Our service addresses three key issues that
affect modern serverless platforms; high cold start latencies, poor fault
tolerance, and limited native support for parallel processing patterns like
fork-join and map-reduce. We have built a prototype that integrates with the
existing OpenWhisk services, and is fully backwards compatible with the
existing implementation. The updated architecture improves performance and adds
new scheduling and security features. Our empirical results demonstrate that
our scheduler reduces cold start execution latencies by up to 80, steady state
latencies by up to 10, and does so with negligible time and memory overhead.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Distributed Simulation of Large Multi-body Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17261v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17261v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manas Kale, Paul G. Kry
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a technique designed for parallelizing large rigid body
simulations, capable of exploiting multiple CPU cores within a computer and
across a network. Our approach can be applied to simulate both unilateral and
bilateral constraints, requiring straightforward modifications to the
underlying physics engine. Starting from an approximate partitioning, we
identify interface bodies and add them to overlapping sets such that they are
simulated by multiple workers. At each timestep, we blend the states of overlap
bodies using weights based on graph geodesic distances within the constraint
graph. The use of overlap simulation also allows us to perform load balancing
using efficient local evaluations of the constraint graph. We demonstrate our
technique's scalability and load-balancing capabilities using several
large-scale scenes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>For associated video, see https://www.youtube.com/watch?v=2gg-YnIGJ-w</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Design Principles of Dynamic Resource Management for High-Performance
  Parallel Programming Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17107v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17107v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dominik Huber, Martin Schreiber, Martin Schulz, Howard Pritchard, Daniel Holmes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With Dynamic Resource Management (DRM) the resources assigned to a job can be
changed dynamically during its execution. From the system's perspective, DRM
opens a new level of flexibility in resource allocation and job scheduling and
therefore has the potential to improve system efficiency metrics such as the
utilization rate, job throughput, energy efficiency, and responsiveness. From
the application perspective, users can tailor the resources they request to
their needs offering potential optimizations in queuing time or charged costs.
Despite these obvious advantages and many attempts over the last decade to
establish DRM in HPC, it remains a concept discussed in academia rather than
being successfully deployed on production systems. This stems from the fact
that support for DRM requires changes in all the layers of the HPC system
software stack including applications, programming models, process managers,
and resource management software, as well as an extensive and holistic
co-design process to establish new techniques and policies for scheduling and
resource optimization. In this work, we therefore start with the assumption
that resources are accessible by processes executed either on them (e.g., on
CPU) or controlling them (e.g., GPU-offloading). Then, the overall DRM problem
can be decomposed into dynamic process management (DPM) and dynamic resource
mapping or allocation (DRA). The former determines which processes (or which
change in processes) must be managed and the latter identifies the resources
where they will be executed. The interfaces for such \mbox{DPM/DPA} in these
layers need to be standardized, which requires a careful design to be
interoperable while providing high flexibility. Based on a survey of existing
approaches we propose design principles, that form the basis of a holistic
approach to DMR in HPC and provide a prototype implementation using MPI.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Unified CPU-GPU Protocol for GNN Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17092v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17092v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi-Chien Lin, Gangda Deng, Viktor Prasanna
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training a Graph Neural Network (GNN) model on large-scale graphs involves a
high volume of data communication and compu- tations. While state-of-the-art
CPUs and GPUs feature high computing power, the Standard GNN training protocol
adopted in existing GNN frameworks cannot efficiently utilize the platform
resources. To this end, we propose a novel Unified CPU-GPU protocol that can
improve the resource utilization of GNN training on a CPU-GPU platform. The
Unified CPU-GPU protocol instantiates multiple GNN training processes in
parallel on both the CPU and the GPU. By allocating training processes on the
CPU to perform GNN training collaboratively with the GPU, the proposed protocol
improves the platform resource utilization and reduces the CPU-GPU data
transfer overhead. Since the performance of a CPU and a GPU varies, we develop
a novel load balancer that balances the workload dynamically between CPUs and
GPUs during runtime. We evaluate our protocol using two representative GNN
sampling algorithms, with two widely-used GNN models, on three datasets.
Compared with the standard training protocol adopted in the state-of-the-art
GNN frameworks, our protocol effectively improves resource utilization and
overall training time. On a platform where the GPU moderately outperforms the
CPU, our protocol speeds up GNN training by up to 1.41x. On a platform where
the GPU significantly outperforms the CPU, our protocol speeds up GNN training
by up to 1.26x. Our protocol is open-sourced and can be seamlessly integrated
into state-of-the-art GNN frameworks and accelerate GNN training. Our protocol
particularly benefits those with limited GPU access due to its high demand.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in 21st ACM International Conference on Computing Frontiers
  (CF' 24)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Secure and Trusted-by-Design Smart Contracts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16903v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16903v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zaynah Dargaye, Önder Gürcan, Florent Kirchner, Sara Tucci-Piergiovanni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Distributed immutable ledgers, or blockchains, allow the secure digitization
of evidential transactions without relying on a trusted third-party. Evidential
transactions involve the exchange of any form of physical evidence, such as
money, birth certificate, visas, tickets, etc. Most of the time, evidential
transactions occur in the context of complex procedures, called evidential
protocols, among physical agents. The blockchain provides the mechanisms to
transfer evidence, while smart contracts - programs executing within the
blockchain in a decentralized and replicated fashion - allow encoding
evidential protocols on top of a blockchain.
  As a smart contract foregoes trusted third-parties and runs on several
machines anonymously, it constitutes a highly critical program that has to be
secure and trusted-by-design. While most of the current smart contract
languages focus on easy programmability, they do not directly address the need
of guaranteeing trust and accountability, which becomes a significant issue
when evidential protocols are encoded as smart contracts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 1 algorithm, The 29th Francophone Days of Application
  Languages - JFLA 2018</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lessons Learned from Building Edge Software System Testbeds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16869v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16869v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tobias Pfandzelter, David Bermbach
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Edge computing requires the complex software interaction of geo-distributed,
heterogeneous components. The growing research and industry interest in edge
computing software systems has necessitated exploring ways of testing and
evaluating edge software at scale without relying on physical infrastructure.
Beyond simulation, virtual testbeds that emulate edge infrastructure can
provide a cost-efficient yet realistic environment to evaluate edge software.
  In this experience paper, we share lessons learned from building a total of
five edge software testbeds. We describe pitfalls in architecture and
development as well as experiences from having students use our testbed tooling
in distributed systems prototyping classes. While we remain confident that
building custom testbed tooling is the right approach for edge computing
researchers and practitioners alike, we hope this paper allows others to avoid
common mistakes and benefit from our experience.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Union: An Automatic Workload Manager for Accelerating Network Simulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17036v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17036v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Wang, Misbah Mubarak, Yao Kang, Robert B. Ross, Zhiling Lan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid growth of the machine learning applications, the workloads of
future HPC systems are anticipated to be a mix of scientific simulation, big
data analytics, and machine learning applications. Simulation is a great
research vehicle to understand the performance implications of co-running
scientific applications with big data and machine learning workloads on
large-scale systems. In this paper, we present Union, a workload manager that
provides an automatic framework to facilitate hybrid workload simulation in
CODES. Furthermore, we use Union, along with CODES, to investigate various
hybrid workloads composed of traditional simulation applications and emerging
learning applications on two dragonfly systems. The experiment results show
that both message latency and communication time are important performance
metrics to evaluate network interference. Network interference on HPC
applications is more reflected by the message latency variation, whereas ML
application performance depends more on the communication time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SignSGD with Federated Voting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16372v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16372v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chanho Park, H. Vincent Poor, Namyoon Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Distributed learning is commonly used for accelerating model training by
harnessing the computational capabilities of multiple-edge devices. However, in
practical applications, the communication delay emerges as a bottleneck due to
the substantial information exchange required between workers and a central
parameter server. SignSGD with majority voting (signSGD-MV) is an effective
distributed learning algorithm that can significantly reduce communication
costs by one-bit quantization. However, due to heterogeneous computational
capabilities, it fails to converge when the mini-batch sizes differ among
workers. To overcome this, we propose a novel signSGD optimizer with
\textit{federated voting} (signSGD-FV). The idea of federated voting is to
exploit learnable weights to perform weighted majority voting. The server
learns the weights assigned to the edge devices in an online fashion based on
their computational capabilities. Subsequently, these weights are employed to
decode the signs of the aggregated local gradients in such a way to minimize
the sign decoding error probability. We provide a unified convergence rate
analysis framework applicable to scenarios where the estimated weights are
known to the parameter server either perfectly or imperfectly. We demonstrate
that the proposed signSGD-FV algorithm has a theoretical convergence guarantee
even when edge devices use heterogeneous mini-batch sizes. Experimental results
show that signSGD-FV outperforms signSGD-MV, exhibiting a faster convergence
rate, especially in heterogeneous mini-batch sizes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ProvDeploy: Provenance-oriented Containerization of High Performance
  Computing Scientific Workflows 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15324v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15324v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liliane Kunstmann, Débora Pina, Daniel de Oliveira, Marta Mattoso
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many existing scientific workflows require High Performance Computing
environments to produce results in a timely manner. These workflows have
several software library components and use different environments, making the
deployment and execution of the software stack not trivial. This complexity
increases if the user needs to add provenance data capture services to the
workflow. This manuscript introduces ProvDeploy to assist the user in
configuring containers for scientific workflows with integrated provenance data
capture. ProvDeploy was evaluated with a Scientific Machine Learning workflow,
exploring containerization strategies focused on provenance in two distinct HPC
environments
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Implications of Decentralization in Blockchained Federated Learning:
  Evaluating the Impact of Model Staleness and Inconsistencies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07471v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07471v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francesc Wilhelmi, Nima Afraz, Elia Guerra, Paolo Dini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Blockchain promises to enhance distributed machine learning (ML) approaches
such as federated learning (FL) by providing further decentralization,
security, immutability, and trust, which are key properties for enabling
collaborative intelligence in next-generation applications. Nonetheless, the
intrinsic decentralized operation of peer-to-peer (P2P) blockchain nodes leads
to an uncharted setting for FL, whereby the concepts of FL round and global
model become meaningless, as devices' synchronization is lost without the
figure of a central orchestrating server. In this paper, we study the practical
implications of outsourcing the orchestration of FL to a democratic setting
such as in a blockchain. In particular, we focus on the effects that model
staleness and inconsistencies, endorsed by blockchains' modus operandi, have on
the training procedure held by FL devices asynchronously. Using simulation, we
evaluate the blockchained FL operation by applying two different ML models
(ranging from low to high complexity) on the well-known MNIST and CIFAR-10
datasets, respectively, and focus on the accuracy and timeliness of the
solutions. Our results show the high impact of model inconsistencies on the
accuracy of the models (up to a ~35% decrease in prediction accuracy), which
underscores the importance of properly designing blockchain systems based on
the characteristics of the underlying FL application.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-Agent Online Graph Exploration on Cycles and Tadpole Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.13845v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.13845v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Erik van den Akker, Kevin Buchin, Klaus-Tycho Foerster
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the problem of multi-agent online graph exploration, in which a team
of k agents has to explore a given graph, starting and ending on the same node.
The graph is initially unknown. Whenever a node is visited by an agent, its
neighborhood and adjacent edges are revealed. The agents share a global view of
the explored parts of the graph. The cost of the exploration has to be
minimized, where cost either describes the time needed for the entire
exploration (time model), or the length of the longest path traversed by any
agent (energy model). We investigate graph exploration on cycles and tadpole
graphs for 2-4 agents, providing optimal results on the competitive ratio in
the energy model (1-competitive with two agents on cycles and three agents on
tadpole graphs), and for tadpole graphs in the time model (1.5-competitive with
four agents). We also show competitive upper bounds of 2 for the exploration of
tadpole graphs with three agents, and 2.5 for the exploration of tadpole graphs
with two agents in the time model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>v2: Update Related Work, more detailed description of models in
  Motivation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Plotinus: A Satellite Internet Digital Twin System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08515v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08515v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Gao, Kun Qiu, Zhe Chen, Wenjun Zhu, Qi Zhang, Handong Luo, Quanwei Lin, Ziheng Yang, Wenhao Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The development of an integrated space-air-ground network (SAGIN) requires
sophisticated satellite Internet emulation tools that can handle complex,
dynamic topologies and offer in-depth analysis. Existing emulation platforms
struggle with challenges like the need for detailed implementation across all
network layers, real-time response, and scalability. This paper proposes a
digital twin system based on microservices for satellite Internet emulation,
namely Plotinus, which aims to solve these problems. Plotinus features a
modular design, allowing for easy replacement of the physical layer to emulate
different aerial vehicles and analyze channel interference. It also enables
replacing path computation methods to simplify testing and deploying
algorithms. In particular, Plotinus allows for real-time emulation with live
network traffic, enhancing practical network models. The evaluation result
shows Plotinus's effective emulation of dynamic satellite networks with
real-world devices. Its adaptability for various communication models and
algorithm testing highlights Plotinus's role as a vital tool for developing and
analyzing SAGIN systems, offering a cross-layer, real-time and scalable digital
twin system.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Accelerating Graph Neural Networks on Real Processing-In-Memory Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16731v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16731v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christina Giannoula, Peiming Yang, Ivan Fernandez Vega, Jiacheng Yang, Yu Xin Li, Juan Gomez Luna, Mohammad Sadrosadati, Onur Mutlu, Gennady Pekhimenko
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks (GNNs) are emerging ML models to analyze
graph-structure data. Graph Neural Network (GNN) execution involves both
compute-intensive and memory-intensive kernels, the latter dominates the total
time, being significantly bottlenecked by data movement between memory and
processors. Processing-In-Memory (PIM) systems can alleviate this data movement
bottleneck by placing simple processors near or inside to memory arrays. In
this work, we introduce PyGim, an efficient ML framework that accelerates GNNs
on real PIM systems. We propose intelligent parallelization techniques for
memory-intensive kernels of GNNs tailored for real PIM systems, and develop
handy Python API for them. We provide hybrid GNN execution, in which the
compute-intensive and memory-intensive kernels are executed in
processor-centric and memory-centric computing systems, respectively, to match
their algorithmic nature. We extensively evaluate PyGim on a real-world PIM
system with 1992 PIM cores using emerging GNN models, and demonstrate that it
outperforms its state-of-the-art CPU counterpart on Intel Xeon by on average
3.04x, and achieves higher resource utilization than CPU and GPU systems. Our
work provides useful recommendations for software, system and hardware
designers. PyGim will be open-sourced to enable the widespread use of PIM
systems in GNNs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PICO: Pipeline Inference Framework for Versatile CNNs on Diverse Mobile
  Devices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.08662v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.08662v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Yang, Zikang Xu, Qi Qi, Jingyu Wang, Haifeng Sun, Jianxin Liao, Song Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Distributing the inference of convolutional neural network (CNN) to multiple
mobile devices has been studied in recent years to achieve real-time inference
without losing accuracy. However, how to map CNN to devices remains a
challenge. On the one hand, scheduling the workload of state-of-the-art CNNs
with multiple devices is NP-Hard because the structures of CNNs are directed
acyclic graphs (DAG) rather than simple chains. On the other hand, distributing
the inference workload suffers from expensive communication and unbalanced
computation due to the wireless environment and heterogeneous devices. This
paper presents PICO, a pipeline cooperation framework to accelerate the
inference of versatile CNNs on diverse mobile devices. At its core, PICO
features: (1) a generic graph partition algorithm that considers the
characteristics of any given CNN and orchestrates it into a list of model
pieces with suitable granularity, and (2) a many-to-many mapping algorithm that
produces the best pipeline configuration for heterogeneous devices. In our
experiment with 2 ~ 8 Raspberry-Pi devices, the throughput can be improved by
1.8 ~ 6.8x under different CPU frequencies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE Transactions on Mobile Computing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Toward parallel intelligence: an interdisciplinary solution for complex
  systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12838v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12838v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yong Zhao, Zhengqiu Zhu, Bin Chen, Sihang Qiu, Jincai Huang, Xin Lu, Weiyi Yang, Chuan Ai, Kuihua Huang, Cheng He, Yucheng Jin, Zhong Liu, Fei-Yue Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The growing complexity of real-world systems necessitates interdisciplinary
solutions to confront myriad challenges in modeling, analysis, management, and
control. To meet these demands, the parallel systems method rooted in
Artificial systems, Computational experiments, and Parallel execution (ACP)
approach has been developed. The method cultivates a cycle, termed parallel
intelligence, which iteratively creates data, acquires knowledge, and refines
the actual system. Over the past two decades, the parallel systems method has
continuously woven advanced knowledge and technologies from various
disciplines, offering versatile interdisciplinary solutions for complex systems
across diverse fields. This review explores the origins and fundamental
concepts of the parallel systems method, showcasing its accomplishments as a
diverse array of parallel technologies and applications, while also
prognosticating potential challenges. We posit that this method will
considerably augment sustainable development while enhancing interdisciplinary
communication and cooperation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>41 pages, 6 figures. The Innovation (2023)</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Performance
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multilevel Modeling as a Methodology for the Simulation of Human
  Mobility 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16745v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16745v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luca Serena, Moreno Marzolla, Gabriele D'Angelo, Stefano Ferretti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multilevel modeling is increasingly relevant in the context of modelling and
simulation since it leads to several potential benefits, such as software reuse
and integration, the split of semantically separated levels into sub-models,
the possibility to employ different levels of detail, and the potential for
parallel execution. The coupling that inevitably exists between the sub-models,
however, implies the need for maintaining consistency between the various
components, more so when different simulation paradigms are employed (e.g.,
sequential vs parallel, discrete vs continuous). In this paper we argue that
multilevel modelling is well suited for the simulation of human mobility, since
it naturally leads to the decomposition of the model into two layers, the
"micro" and "macro" layer, where individual entities (micro) and long-range
interactions (macro) are described. In this paper we investigate the challenges
of multilevel modeling, and describe some preliminary results using prototype
implementations of multilayer simulators in the context of epidemic diffusion
and vehicle pollution.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Implementing and Evaluating E2LSH on Storage 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16404v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16404v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Nakanishi, Kazuhiro Hiwada, Yosuke Bando, Tomoya Suzuki, Hirotsugu Kajihara, Shintaro Sano, Tatsuro Endo, Tatsuo Shiozawa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Locality sensitive hashing (LSH) is one of the widely-used approaches to
approximate nearest neighbor search (ANNS) in high-dimensional spaces. The
first work on LSH for the Euclidean distance, E2LSH, showed how ANNS can be
solved efficiently at a sublinear query time in the database size with
theoretically-guaranteed accuracy, although it required a large hash index
size. Since then, several LSH variants having much smaller index sizes have
been proposed. Their query time is linear or superlinear, but they have been
shown to run effectively faster because they require fewer I/Os when the index
is stored on hard disk drives and because they also permit in-memory execution
with modern DRAM capacity.
  In this paper, we show that E2LSH is regaining the advantage in query speed
with the advent of modern flash storage devices such as solid-state drives
(SSDs). We evaluate E2LSH on a modern single-node computing environment and
analyze its computational cost and I/O cost, from which we derive storage
performance requirements for its external memory execution. Our analysis
indicates that E2LSH on a single consumer-grade SSD can run faster than the
state-of-the-art small-index methods executed in-memory. It also indicates that
E2LSH with emerging high-performance storage devices and interfaces can
approach in-memory E2LSH speeds. We implement a simple adaptation of E2LSH to
external memory, E2LSH-on-Storage (E2LSHoS), and evaluate it for practical
large datasets of up to one billion objects using different combinations of
modern storage devices and interfaces. We demonstrate that our E2LSHoS
implementation runs much faster than small-index methods and can approach
in-memory E2LSH speeds, and also that its query time scales sublinearly with
the database size beyond the index size limit of in-memory E2LSH.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Accelerating Graph Neural Networks on Real Processing-In-Memory Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16731v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16731v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christina Giannoula, Peiming Yang, Ivan Fernandez Vega, Jiacheng Yang, Yu Xin Li, Juan Gomez Luna, Mohammad Sadrosadati, Onur Mutlu, Gennady Pekhimenko
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks (GNNs) are emerging ML models to analyze
graph-structure data. Graph Neural Network (GNN) execution involves both
compute-intensive and memory-intensive kernels, the latter dominates the total
time, being significantly bottlenecked by data movement between memory and
processors. Processing-In-Memory (PIM) systems can alleviate this data movement
bottleneck by placing simple processors near or inside to memory arrays. In
this work, we introduce PyGim, an efficient ML framework that accelerates GNNs
on real PIM systems. We propose intelligent parallelization techniques for
memory-intensive kernels of GNNs tailored for real PIM systems, and develop
handy Python API for them. We provide hybrid GNN execution, in which the
compute-intensive and memory-intensive kernels are executed in
processor-centric and memory-centric computing systems, respectively, to match
their algorithmic nature. We extensively evaluate PyGim on a real-world PIM
system with 1992 PIM cores using emerging GNN models, and demonstrate that it
outperforms its state-of-the-art CPU counterpart on Intel Xeon by on average
3.04x, and achieves higher resource utilization than CPU and GPU systems. Our
work provides useful recommendations for software, system and hardware
designers. PyGim will be open-sourced to enable the widespread use of PIM
systems in GNNs.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Artificial Intelligence
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enabling Uncertainty Estimation in Iterative Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16732v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16732v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikita Durasov, Doruk Oner, Jonathan Donier, Hieu Le, Pascal Fua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Turning pass-through network architectures into iterative ones, which use
their own output as input, is a well-known approach for boosting performance.
In this paper, we argue that such architectures offer an additional benefit:
The convergence rate of their successive outputs is highly correlated with the
accuracy of the value to which they converge. Thus, we can use the convergence
rate as a useful proxy for uncertainty. This results in an approach to
uncertainty estimation that provides state-of-the-art estimates at a much lower
computational cost than techniques like Ensembles, and without requiring any
modifications to the original iterative model. We demonstrate its practical
value by embedding it in two application domains: road detection in aerial
images and the estimation of aerodynamic properties of 2D and 3D shapes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Diffusion Models's Data-Corruption Resistance using Scheduled
  Pseudo-Huber Loss 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16728v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16728v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Artem Khrapov, Vadim Popov, Tasnima Sadekova, Assel Yermekova, Mikhail Kudinov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models are known to be vulnerable to outliers in training data. In
this paper we study an alternative diffusion loss function, which can preserve
the high quality of generated data like the original squared $L_{2}$ loss while
at the same time being robust to outliers. We propose to use pseudo-Huber loss
function with a time-dependent parameter to allow for the trade-off between
robustness on the most vulnerable early reverse-diffusion steps and fine
details restoration on the final steps. We show that pseudo-Huber loss with the
time-dependent parameter exhibits better performance on corrupted datasets in
both image and audio domains. In addition, the loss function we propose can
potentially help diffusion models to resist dataset corruption while not
requiring data filtering or purification compared to conventional training
algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 16 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards a Formalisation of Value-based Actions and Consequentialist
  Ethics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16719v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16719v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adam Wyner, Tomasz Zurek, DOrota Stachura-Zurek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Agents act to bring about a state of the world that is more compatible with
their personal or institutional values. To formalise this intuition, the paper
proposes an action framework based on the STRIPS formalisation. Technically,
the contribution expresses actions in terms of Value-based Formal Reasoning
(VFR), which provides a set of propositions derived from an Agent's value
profile and the Agent's assessment of propositions with respect to the profile.
Conceptually, the contribution provides a computational framework for a form of
consequentialist ethics which is satisficing, luralistic, act-based, and
preferential.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ One-Shot Domain Incremental Learning <span class="chip">IJCNN</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16707v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16707v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yasushi Esaki, Satoshi Koide, Takuro Kutsuna
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Domain incremental learning (DIL) has been discussed in previous studies on
deep neural network models for classification. In DIL, we assume that samples
on new domains are observed over time. The models must classify inputs on all
domains. In practice, however, we may encounter a situation where we need to
perform DIL under the constraint that the samples on the new domain are
observed only infrequently. Therefore, in this study, we consider the extreme
case where we have only one sample from the new domain, which we call one-shot
DIL. We first empirically show that existing DIL methods do not work well in
one-shot DIL. We have analyzed the reason for this failure through various
investigations. According to our analysis, we clarify that the difficulty of
one-shot DIL is caused by the statistics in the batch normalization layers.
Therefore, we propose a technique regarding these statistics and demonstrate
the effectiveness of our technique through experiments on open datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted at IEEE International Joint Conference on Neural Networks
  (IJCNN) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Investigation of the effectiveness of applying Chat<span class="highlight-title">GPT</span> in Dialogic
  Teaching Using Electroencephalography 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16687v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16687v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiayue Zhang, Yiheng Liu, Wenqi Cai, Yali Peng, Senqing Qi, Taotao Long, Bao Ge
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, the rapid development of artificial intelligence technology,
especially the emergence of large language models (LLMs) such as ChatGPT, has
presented significant prospects for application in the field of education. LLMs
possess the capability to interpret knowledge, answer questions, and consider
context, thus providing support for dialogic teaching to students. Therefore,
an examination of the capacity of LLMs to effectively fulfill instructional
roles, thereby facilitating student learning akin to human educators within
dialogic teaching scenarios, is an exceptionally valuable research topic. This
research recruited 34 undergraduate students as participants, who were randomly
divided into two groups. The experimental group engaged in dialogic teaching
using ChatGPT, while the control group interacted with human teachers. Both
groups learned the histogram equalization unit in the information-related
course "Digital Image Processing". The research findings show comparable scores
between the two groups on the retention test. However, students who engaged in
dialogue with ChatGPT exhibited lower performance on the transfer test.
Electroencephalography data revealed that students who interacted with ChatGPT
exhibited higher levels of cognitive activity, suggesting that ChatGPT could
help students establish a knowledge foundation and stimulate cognitive
activity. However, its strengths on promoting students. knowledge application
and creativity were insignificant. Based upon the research findings, it is
evident that ChatGPT cannot fully excel in fulfilling teaching tasks in the
dialogue teaching in information related courses. Combining ChatGPT with
traditional human teachers might be a more ideal approach. The synergistic use
of both can provide students with more comprehensive learning support, thus
contributing to enhancing the quality of teaching.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding the Functional Roles of Modelling Components in Spiking
  Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16674v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16674v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huifeng Yin, Hanle Zheng, Jiayi Mao, Siyuan Ding, Xing Liu, Mingkun Xu, Yifan Hu, Jing Pei, Lei Deng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spiking neural networks (SNNs), inspired by the neural circuits of the brain,
are promising in achieving high computational efficiency with biological
fidelity. Nevertheless, it is quite difficult to optimize SNNs because the
functional roles of their modelling components remain unclear. By designing and
evaluating several variants of the classic model, we systematically investigate
the functional roles of key modelling components, leakage, reset, and
recurrence, in leaky integrate-and-fire (LIF) based SNNs. Through extensive
experiments, we demonstrate how these components influence the accuracy,
generalization, and robustness of SNNs. Specifically, we find that the leakage
plays a crucial role in balancing memory retention and robustness, the reset
mechanism is essential for uninterrupted temporal processing and computational
efficiency, and the recurrence enriches the capability to model complex
dynamics at a cost of robustness degradation. With these interesting
observations, we provide optimization suggestions for enhancing the performance
of SNNs in different scenarios. This work deepens the understanding of how SNNs
work, which offers valuable guidance for the development of more effective and
robust neuromorphic models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Reinforcement Learning and Mean-Variance Strategies for Responsible
  Portfolio Optimization <span class="chip">AAAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16667v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16667v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fernando Acero, Parisa Zehtabi, Nicolas Marchesotti, Michael Cashmore, Daniele Magazzeni, Manuela Veloso
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Portfolio optimization involves determining the optimal allocation of
portfolio assets in order to maximize a given investment objective.
Traditionally, some form of mean-variance optimization is used with the aim of
maximizing returns while minimizing risk, however, more recently, deep
reinforcement learning formulations have been explored. Increasingly, investors
have demonstrated an interest in incorporating ESG objectives when making
investment decisions, and modifications to the classical mean-variance
optimization framework have been developed. In this work, we study the use of
deep reinforcement learning for responsible portfolio optimization, by
incorporating ESG states and objectives, and provide comparisons against
modified mean-variance approaches. Our results show that deep reinforcement
learning policies can provide competitive performance against mean-variance
approaches for responsible portfolio allocation across additive and
multiplicative utility functions of financial and ESG responsibility
objectives.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at the AAAI 2024 Workshop on AI in Finance for Social
  Impact</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Revisiting the Sleeping Beauty problem 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16666v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16666v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paulo S. Piva, Gabriel Ruffolo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Sleeping Beauty problem is a probability riddle with no definite solution
for more than two decades and its solution is of great interest in many fields
of knowledge. There are two main competing solutions to the problem: the halfer
approach, and the thirder approach. The main reason for disagreement in the
literature is connected to the use of different probability spaces to represent
the same probabilistic riddle. In this work, we analyse the problem from a
mathematical perspective, identifying probability distributions induced
directly from the thought experiment's rules. The precise choices of
probability spaces provide both halfer and thirder solutions to the problem. To
try and decide on which approach to follow, a criterion involving the
information available to Sleeping Beauty is proposed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deciphering the Interplay between Local Differential Privacy, Average
  Bayesian Privacy, and Maximum Bayesian Privacy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16591v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16591v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaojin Zhang, Yulin Fei, Wei Chen, Hai Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The swift evolution of machine learning has led to emergence of various
definitions of privacy due to the threats it poses to privacy, including the
concept of local differential privacy (LDP). Although widely embraced and
utilized across numerous domains, this conventional approach to measure privacy
still exhibits certain limitations, spanning from failure to prevent
inferential disclosure to lack of consideration for the adversary's background
knowledge. In this comprehensive study, we introduce Bayesian privacy and delve
into the intricate relationship between local differential privacy and its
Bayesian counterparts, unveiling novel insights into utility-privacy
trade-offs. We introduce a framework that encapsulates both attack and defense
strategies, highlighting their interplay and effectiveness. Our theoretical
contributions are anchored in the rigorous definitions and relationships
between Average Bayesian Privacy (ABP) and Maximum Bayesian Privacy (MBP),
encapsulated by equations $\epsilon_{p,a} \leq
\frac{1}{\sqrt{2}}\sqrt{(\epsilon_{p,m} + \epsilon)\cdot(e^{\epsilon_{p,m} +
\epsilon} - 1)}$ and the equivalence between $\xi$-MBP and $2\xi$-LDP
established under uniform prior distribution. These relationships fortify our
understanding of the privacy guarantees provided by various mechanisms, leading
to the realization that a mechanism satisfying $\xi$-LDP also confers
$\xi$-MBP, and vice versa. Our work not only lays the groundwork for future
empirical exploration but also promises to enhance the design of
privacy-preserving algorithms that do not compromise on utility, thereby
fostering the development of trustworthy machine learning solutions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ In the Search for Optimal Multi-view Learning Models for Crop
  Classification with Global Remote Sensing Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16582v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16582v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francisco Mena, Diego Arenas, Andreas Dengel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Crop classification is of critical importance due to its role in studying
crop pattern changes, resource management, and carbon sequestration. When
employing data-driven techniques for its prediction, utilizing various temporal
data sources is necessary. Deep learning models have proven to be effective for
this task by mapping time series data to high-level representation for
prediction. However, they face substantial challenges when dealing with
multiple input patterns. The literature offers limited guidance for Multi-View
Learning (MVL) scenarios, as it has primarily focused on exploring fusion
strategies with specific encoders and validating them in local regions. In
contrast, we investigate the impact of simultaneous selection of the fusion
strategy and the encoder architecture evaluated on a global-scale cropland and
crop-type classifications. We use a range of five fusion strategies (Input,
Feature, Decision, Ensemble, Hybrid) and five temporal encoder architectures
(LSTM, GRU, TempCNN, TAE, L-TAE) as possible MVL model configurations. The
validation is on the CropHarvest dataset that provides optical, radar, and
weather time series, and topographic information as input data. We found that
in scenarios with a limited number of labeled samples, a unique configuration
is insufficient for all the cases. Instead, a specialized combination,
including encoder and fusion strategy, should be meticulously sought. To
streamline this search process, we suggest initially identifying the optimal
encoder architecture tailored for a particular fusion strategy, and then
determining the most suitable fusion strategy for the classification task. We
provide a technical framework for researchers exploring crop classification or
related tasks through a MVL approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SegICL: A Universal In-context Learning Framework for Enhanced
  Segmentation in Medical Imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16578v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16578v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lingdong Shen, Fangxin Shang, Yehui Yang, Xiaoshuang Huang, Shining Xiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical image segmentation models adapting to new tasks in a training-free
manner through in-context learning is an exciting advancement. Universal
segmentation models aim to generalize across the diverse modality of medical
images, yet their effectiveness often diminishes when applied to
out-of-distribution (OOD) data modalities and tasks, requiring intricate
fine-tuning of model for optimal performance. For addressing this challenge, we
introduce SegICL, a novel approach leveraging In-Context Learning (ICL) for
image segmentation. Unlike existing methods, SegICL has the capability to
employ text-guided segmentation and conduct in-context learning with a small
set of image-mask pairs, eliminating the need for training the model from
scratch or fine-tuning for OOD tasks (including OOD modality and dataset).
Extensive experimental validation of SegICL demonstrates a positive correlation
between the number of prompt samples and segmentation performance on OOD
modalities and tasks. This indicates that SegICL effectively address new
segmentation tasks based on contextual information. Additionally, SegICL also
exhibits comparable segmentation performance to mainstream models on OOD and
in-distribution tasks. Our code will be released soon.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NSINA: A News Corpus for Sinhala <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16571v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16571v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hansi Hettiarachchi, Damith Premasiri, Lasitha Uyangodage, Tharindu Ranasinghe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The introduction of large language models (LLMs) has advanced natural
language processing (NLP), but their effectiveness is largely dependent on
pre-training resources. This is especially evident in low-resource languages,
such as Sinhala, which face two primary challenges: the lack of substantial
training data and limited benchmarking datasets. In response, this study
introduces NSINA, a comprehensive news corpus of over 500,000 articles from
popular Sinhala news websites, along with three NLP tasks: news media
identification, news category prediction, and news headline generation. The
release of NSINA aims to provide a solution to challenges in adapting LLMs to
Sinhala, offering valuable resources and benchmarks for improving NLP in the
Sinhala language. NSINA is the largest news corpus for Sinhala, available up to
date.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to LREC-COLING 2024 (The 2024 Joint International Conference
  on Computational Linguistics, Language Resources and Evaluation)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FedFixer: Mitigating Heterogeneous Label Noise in Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16561v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16561v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyuan Ji, Zhaowei Zhu, Wei Xi, Olga Gadyatskaya, Zilong Song, Yong Cai, Yang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) heavily depends on label quality for its performance.
However, the label distribution among individual clients is always both noisy
and heterogeneous. The high loss incurred by client-specific samples in
heterogeneous label noise poses challenges for distinguishing between
client-specific and noisy label samples, impacting the effectiveness of
existing label noise learning approaches. To tackle this issue, we propose
FedFixer, where the personalized model is introduced to cooperate with the
global model to effectively select clean client-specific samples. In the dual
models, updating the personalized model solely at a local level can lead to
overfitting on noisy data due to limited samples, consequently affecting both
the local and global models' performance. To mitigate overfitting, we address
this concern from two perspectives. Firstly, we employ a confidence regularizer
to alleviate the impact of unconfident predictions caused by label noise.
Secondly, a distance regularizer is implemented to constrain the disparity
between the personalized and global models. We validate the effectiveness of
FedFixer through extensive experiments on benchmark datasets. The results
demonstrate that FedFixer can perform well in filtering noisy label samples on
different clients, especially in highly heterogeneous label noise scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by AAA24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PE: A Poincare Explanation Method for Fast Text Hierarchy Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16554v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16554v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qian Chen, Xiaofeng He, Hongzhao Li, Hongyu Yi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The black-box nature of deep learning models in NLP hinders their widespread
application. The research focus has shifted to Hierarchical Attribution (HA)
for its ability to model feature interactions. Recent works model
non-contiguous combinations with a time-costly greedy search in Eculidean
spaces, neglecting underlying linguistic information in feature
representations. In this work, we introduce a novel method, namely Poincar\'e
Explanation (PE), for modeling feature interactions using hyperbolic spaces in
an $O(n^2logn)$ time complexity. Inspired by Poincar\'e model, we propose a
framework to project the embeddings into hyperbolic spaces, which exhibit
better inductive biases for syntax and semantic hierarchical structures.
Eventually, we prove that the hierarchical clustering process in the projected
space could be viewed as building a minimum spanning tree and propose a time
efficient algorithm. Experimental results demonstrate the effectiveness of our
approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ QKFormer: Hierarchical Spiking <span class="highlight-title">Transformer</span> using Q-K Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16552v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16552v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenlin Zhou, Han Zhang, Zhaokun Zhou, Liutao Yu, Liwei Huang, Xiaopeng Fan, Li Yuan, Zhengyu Ma, Huihui Zhou, Yonghong Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spiking Transformers, which integrate Spiking Neural Networks (SNNs) with
Transformer architectures, have attracted significant attention due to their
potential for energy efficiency and high performance. However, existing models
in this domain still suffer from suboptimal performance. We introduce several
innovations to improve the performance: i) We propose a novel spike-form Q-K
attention mechanism, tailored for SNNs, which efficiently models the importance
of token or channel dimensions through binary vectors with linear complexity.
ii) We incorporate the hierarchical structure, which significantly benefits the
performance of both the brain and artificial neural networks, into spiking
transformers to obtain multi-scale spiking representation. iii) We design a
versatile and powerful patch embedding module with a deformed shortcut
specifically for spiking transformers. Together, we develop QKFormer, a
hierarchical spiking transformer based on Q-K attention with direct training.
QKFormer shows significantly superior performance over existing
state-of-the-art SNN models on various mainstream datasets. Notably, with
comparable size to Spikformer (66.34 M, 74.81%), QKFormer (64.96 M) achieves a
groundbreaking top-1 accuracy of 85.65% on ImageNet-1k, substantially
outperforming Spikformer by 10.84%. To our best knowledge, this is the first
time that directly training SNNs have exceeded 85% accuracy on ImageNet-1K. The
code and models are publicly available at
https://github.com/zhouchenlin2096/QKFormer
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, code: https://github.com/zhouchenlin2096/QKFormer</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Information Extraction in Few-Shot Relation Classification
  through Contrastive Representation Learning <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16543v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16543v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philipp Borchert, Jochen De Weerdt, Marie-Francine Moens
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Differentiating relationships between entity pairs with limited labeled
instances poses a significant challenge in few-shot relation classification.
Representations of textual data extract rich information spanning the domain,
entities, and relations. In this paper, we introduce a novel approach to
enhance information extraction combining multiple sentence representations and
contrastive learning. While representations in relation classification are
commonly extracted using entity marker tokens, we argue that substantial
information within the internal model representations remains untapped. To
address this, we propose aligning multiple sentence representations, such as
the [CLS] token, the [MASK] token used in prompting, and entity marker tokens.
Our method employs contrastive learning to extract complementary discriminative
information from these individual representations. This is particularly
relevant in low-resource settings where information is scarce. Leveraging
multiple sentence representations is especially effective in distilling
discriminative information for relation classification when additional
information, like relation descriptions, are not available. We validate the
adaptability of our approach, maintaining robust performance in scenarios that
include relation descriptions, and showcasing its flexibility to adapt to
different resource constraints.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Intermediate Fusion ViT Enables Efficient Text-Image Alignment in
  Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16530v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16530v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zizhao Hu, Shaochong Jia, Mohammad Rostami
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have been widely used for conditional data cross-modal
generation tasks such as text-to-image and text-to-video. However,
state-of-the-art models still fail to align the generated visual concepts with
high-level semantics in a language such as object count, spatial relationship,
etc. We approach this problem from a multimodal data fusion perspective and
investigate how different fusion strategies can affect vision-language
alignment. We discover that compared to the widely used early fusion of
conditioning text in a pretrained image feature space, a specially designed
intermediate fusion can: (i) boost text-to-image alignment with improved
generation quality and (ii) improve training and inference efficiency by
reducing low-rank text-to-image attention calculations. We perform experiments
using a text-to-image generation task on the MS-COCO dataset. We compare our
intermediate fusion mechanism with the classic early fusion mechanism on two
common conditioning methods on a U-shaped ViT backbone. Our intermediate fusion
model achieves a higher CLIP Score and lower FID, with 20% reduced FLOPs, and
50% increased training speed compared to a strong U-ViT baseline with an early
fusion.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hallucination Detection in Foundation Models for Decision-Making: A
  Flexible Definition and <span class="highlight-title">Review</span> of the State of the Art 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16527v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16527v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Neeloy Chakraborty, Melkior Ornik, Katherine Driggs-Campbell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous systems are soon to be ubiquitous, from manufacturing autonomy to
agricultural field robots, and from health care assistants to the entertainment
industry. The majority of these systems are developed with modular
sub-components for decision-making, planning, and control that may be
hand-engineered or learning-based. While these existing approaches have been
shown to perform well under the situations they were specifically designed for,
they can perform especially poorly in rare, out-of-distribution scenarios that
will undoubtedly arise at test-time. The rise of foundation models trained on
multiple tasks with impressively large datasets from a variety of fields has
led researchers to believe that these models may provide common sense reasoning
that existing planners are missing. Researchers posit that this common sense
reasoning will bridge the gap between algorithm development and deployment to
out-of-distribution tasks, like how humans adapt to unexpected scenarios. Large
language models have already penetrated the robotics and autonomous systems
domains as researchers are scrambling to showcase their potential use cases in
deployment. While this application direction is very promising empirically,
foundation models are known to hallucinate and generate decisions that may
sound reasonable, but are in fact poor. We argue there is a need to step back
and simultaneously design systems that can quantify the certainty of a model's
decision, and detect when it may be hallucinating. In this work, we discuss the
current use cases of foundation models for decision-making tasks, provide a
general definition for hallucinations with examples, discuss existing
approaches to hallucination detection and mitigation with a focus on decision
problems, and explore areas for further research in this exciting field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Harnessing the power of LLMs for normative reasoning in MASs <span class="chip">AAMAS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16524v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16524v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bastin Tony Roy Savarimuthu, Surangika Ranathunga, Stephen Cranefield
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Software agents, both human and computational, do not exist in isolation and
often need to collaborate or coordinate with others to achieve their goals. In
human society, social mechanisms such as norms ensure efficient functioning,
and these techniques have been adopted by researchers in multi-agent systems
(MAS) to create socially aware agents. However, traditional techniques have
limitations, such as operating in limited environments often using brittle
symbolic reasoning. The advent of Large Language Models (LLMs) offers a
promising solution, providing a rich and expressive vocabulary for norms and
enabling norm-capable agents that can perform a range of tasks such as norm
discovery, normative reasoning and decision-making. This paper examines the
potential of LLM-based agents to acquire normative capabilities, drawing on
recent Natural Language Processing (NLP) and LLM research. We present our
vision for creating normative LLM agents. In particular, we discuss how the
recently proposed "LLM agent" approaches can be extended to implement such
normative LLM agents. We also highlight challenges in this emerging field. This
paper thus aims to foster collaboration between MAS, NLP and LLM researchers in
order to advance the field of normative agents.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 1 figure, accepted to COINE 2024 workshop at AAMAS 2024
  (https://coin-workshop.github.io/coine-2024-auckland/accepted_papers.html)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Causal Discovery from Poisson Branching Structural Causal Model Using
  High-Order Cumulant with Path Analysis <span class="chip">AAAI-2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16523v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16523v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jie Qiao, Yu Xiang, Zhengming Chen, Ruichu Cai, Zhifeng Hao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Count data naturally arise in many fields, such as finance, neuroscience, and
epidemiology, and discovering causal structure among count data is a crucial
task in various scientific and industrial scenarios. One of the most common
characteristics of count data is the inherent branching structure described by
a binomial thinning operator and an independent Poisson distribution that
captures both branching and noise. For instance, in a population count
scenario, mortality and immigration contribute to the count, where survival
follows a Bernoulli distribution, and immigration follows a Poisson
distribution. However, causal discovery from such data is challenging due to
the non-identifiability issue: a single causal pair is Markov equivalent, i.e.,
$X\rightarrow Y$ and $Y\rightarrow X$ are distributed equivalent. Fortunately,
in this work, we found that the causal order from $X$ to its child $Y$ is
identifiable if $X$ is a root vertex and has at least two directed paths to
$Y$, or the ancestor of $X$ with the most directed path to $X$ has a directed
path to $Y$ without passing $X$. Specifically, we propose a Poisson Branching
Structure Causal Model (PB-SCM) and perform a path analysis on PB-SCM using
high-order cumulants. Theoretical results establish the connection between the
path and cumulant and demonstrate that the path information can be obtained
from the cumulant. With the path information, causal order is identifiable
under some graphical conditions. A practical algorithm for learning causal
structure under PB-SCM is proposed and the experiments demonstrate and verify
the effectiveness of the proposed method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI-2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLMs Are Few-Shot In-Context Low-Resource Language Learners 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16512v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16512v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samuel Cahyawijaya, Holy Lovenia, Pascale Fung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In-context learning (ICL) empowers large language models (LLMs) to perform
diverse tasks in underrepresented languages using only short in-context
information, offering a crucial avenue for narrowing the gap between
high-resource and low-resource languages. Nonetheless, there is only a handful
of works explored ICL for low-resource languages with most of them focusing on
relatively high-resource languages, such as French and Spanish. In this work,
we extensively study ICL and its cross-lingual variation (X-ICL) on 25
low-resource and 7 relatively higher-resource languages. Our study not only
assesses the effectiveness of ICL with LLMs in low-resource languages but also
identifies the shortcomings of in-context label alignment, and introduces a
more effective alternative: query alignment. Moreover, we provide valuable
insights into various facets of ICL for low-resource languages. Our study
concludes the significance of few-shot in-context information on enhancing the
low-resource understanding quality of LLMs through semantically relevant
information by closing the language gap in the target language and aligning the
semantics between the targeted low-resource and the high-resource language that
the model is proficient in. Our work highlights the importance of advancing ICL
research, particularly for low-resource languages.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Return to Tradition: Learning Reliable Heuristics with Classical Machine
  Learning <span class="chip">ICAPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16508v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16508v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dillon Z. Chen, Felipe Trevizan, Sylvie Thiébaux
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current approaches for learning for planning have yet to achieve competitive
performance against classical planners in several domains, and have poor
overall performance. In this work, we construct novel graph representations of
lifted planning tasks and use the WL algorithm to generate features from them.
These features are used with classical machine learning methods which have up
to 2 orders of magnitude fewer parameters and train up to 3 orders of magnitude
faster than the state-of-the-art deep learning for planning models. Our novel
approach, WL-GOOSE, reliably learns heuristics from scratch and outperforms the
$h^{\text{FF}}$ heuristic in a fair competition setting. It also outperforms or
ties with LAMA on 4 out of 10 domains on coverage and 7 out of 10 domains on
plan quality. WL-GOOSE is the first learning for planning model which achieves
these feats. Furthermore, we study the connections between our novel WL feature
generation method, previous theoretically flavoured learning architectures, and
Description Logic Features for planning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Extended version of ICAPS 2024 paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning To Guide Human Decision Makers With Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16501v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16501v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Debodeep Banerjee, Stefano Teso, Burcu Sayin Grunel, Andrea Passerini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There is increasing interest in developing AIs for assisting human decision
making in \textit{high-stakes} tasks, such as medical diagnosis, for the
purpose of improving decision quality and reducing cognitive strain.
  %
  Mainstream approaches team up an expert with a machine learning model to
which safer decisions are offloaded, thus letting the former focus on cases
that demand their attention.
  %
  This \textit{separation of responsibilities} setup, however, is inadequate
for high-stakes scenarios. On the one hand, the expert may end up over-relying
on the machine's decisions due to \textit{anchoring bias}, thus losing the
human oversight that is increasingly being required by regulatory agencies to
ensure trustworthy AI. On the other hand, the expert is left entirely
unassisted on the (typically hardest) decisions on which the model abstained.
  %
  As a remedy, we introduce \textit{learning to guide} (LTG), an alternative
framework in which -- rather than taking control from the human expert -- the
machine provides \textit{guidance} useful for decision making, and the human is
entirely responsible for coming up with a decision.
  %
  In order to ensure guidance is \textit{interpretable} and
\textit{task-specific}, we develop \method, an approach for turning
\textit{any} vision-language model into a capable generator of textual guidance
by leveraging a modicum of human feedback.
  %
  Our empirical evaluation highlights the promise of \method on a challenging,
real-world medical diagnosis task.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LSTTN: A Long-Short Term <span class="highlight-title">Transformer</span>-based Spatio-temporal Neural
  Network for Traffic Flow Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16495v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16495v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qinyao Luo, Silu He, Xing Han, Yuhan Wang, Haifeng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate traffic forecasting is a fundamental problem in intelligent
transportation systems and learning long-range traffic representations with key
information through spatiotemporal graph neural networks (STGNNs) is a basic
assumption of current traffic flow prediction models. However, due to
structural limitations, existing STGNNs can only utilize short-range traffic
flow data; therefore, the models cannot adequately learn the complex trends and
periodic features in traffic flow. Besides, it is challenging to extract the
key temporal information from the long historical traffic series and obtain a
compact representation. To solve the above problems, we propose a novel LSTTN
(Long-Short Term Transformer-based Network) framework comprehensively
considering the long- and short-term features in historical traffic flow.
First, we employ a masked subseries Transformer to infer the content of masked
subseries from a small portion of unmasked subseries and their temporal context
in a pretraining manner, forcing the model to efficiently learn compressed and
contextual subseries temporal representations from long historical series.
Then, based on the learned representations, long-term trend is extracted by
using stacked 1D dilated convolution layers, and periodic features are
extracted by dynamic graph convolution layers. For the difficulties in making
time-step level prediction, LSTTN adopts a short-term trend extractor to learn
fine-grained short-term temporal features. Finally, LSTTN fuses the long-term
trend, periodic features and short-term features to obtain the prediction
results. Experiments on four real-world datasets show that in 60-minute-ahead
long-term forecasting, the LSTTN model achieves a minimum improvement of 5.63\%
and a maximum improvement of 16.78\% over baseline models. The source code is
available at https://github.com/GeoX-Lab/LSTTN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 10 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FedAC: A Adaptive Clustered Federated Learning Framework for
  Heterogeneous Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16460v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16460v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxin Zhang, Haoyu Chen, Zheng Lin, Zhe Chen, Jin Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Clustered federated learning (CFL) is proposed to mitigate the performance
deterioration stemming from data heterogeneity in federated learning (FL) by
grouping similar clients for cluster-wise model training. However, current CFL
methods struggle due to inadequate integration of global and intra-cluster
knowledge and the absence of an efficient online model similarity metric, while
treating the cluster count as a fixed hyperparameter limits flexibility and
robustness. In this paper, we propose an adaptive CFL framework, named FedAC,
which (1) efficiently integrates global knowledge into intra-cluster learning
by decoupling neural networks and utilizing distinct aggregation methods for
each submodule, significantly enhancing performance; (2) includes a
costeffective online model similarity metric based on dimensionality reduction;
(3) incorporates a cluster number fine-tuning module for improved adaptability
and scalability in complex, heterogeneous environments. Extensive experiments
show that FedAC achieves superior empirical performance, increasing the test
accuracy by around 1.82% and 12.67% on CIFAR-10 and CIFAR-100 datasets,
respectively, under different non-IID settings compared to SOTA methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DeepMachining: Online Prediction of Machining Errors of Lathe Machines 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16451v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16451v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang-Li Lu, Hwai-Jung Hsu, Che-Wei Chou, H. T. Kung, Chen-Hsin Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We describe DeepMachining, a deep learning-based AI system for online
prediction of machining errors of lathe machine operations. We have built and
evaluated DeepMachining based on manufacturing data from factories.
Specifically, we first pretrain a deep learning model for a given lathe
machine's operations to learn the salient features of machining states. Then,
we fine-tune the pretrained model to adapt to specific machining tasks. We
demonstrate that DeepMachining achieves high prediction accuracy for multiple
tasks that involve different workpieces and cutting tools. To the best of our
knowledge, this work is one of the first factory experiments using pre-trained
deep-learning models to predict machining errors of lathe machines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CodeS: Natural Language to Code Repository via Multi-Layer Sketch 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16443v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16443v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daoguang Zan, Ailun Yu, Wei Liu, Dong Chen, Bo Shen, Wei Li, Yafen Yao, Yongshun Gong, Xiaolin Chen, Bei Guan, Zhiguang Yang, Yongji Wang, Qianxiang Wang, Lizhen Cui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The impressive performance of large language models (LLMs) on code-related
tasks has shown the potential of fully automated software development. In light
of this, we introduce a new software engineering task, namely Natural Language
to code Repository (NL2Repo). This task aims to generate an entire code
repository from its natural language requirements. To address this task, we
propose a simple yet effective framework CodeS, which decomposes NL2Repo into
multiple sub-tasks by a multi-layer sketch. Specifically, CodeS includes three
modules: RepoSketcher, FileSketcher, and SketchFiller. RepoSketcher first
generates a repository's directory structure for given requirements;
FileSketcher then generates a file sketch for each file in the generated
structure; SketchFiller finally fills in the details for each function in the
generated file sketch. To rigorously assess CodeS on the NL2Repo task, we carry
out evaluations through both automated benchmarking and manual feedback
analysis. For benchmark-based evaluation, we craft a repository-oriented
benchmark, SketchEval, and design an evaluation metric, SketchBLEU. For
feedback-based evaluation, we develop a VSCode plugin for CodeS and engage 30
participants in conducting empirical studies. Extensive experiments prove the
effectiveness and practicality of CodeS on the NL2Repo task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://github.com/NL2Code/CodeS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ $\textit{Link<span class="highlight-title">Prompt</span>}$: Natural and Universal Adversarial Attacks on
  <span class="highlight-title">Prompt</span>-based Language Models <span class="chip">NAACL2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16432v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16432v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Xu, Wenjie Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prompt-based learning is a new language model training paradigm that adapts
the Pre-trained Language Models (PLMs) to downstream tasks, which revitalizes
the performance benchmarks across various natural language processing (NLP)
tasks. Instead of using a fixed prompt template to fine-tune the model, some
research demonstrates the effectiveness of searching for the prompt via
optimization. Such prompt optimization process of prompt-based learning on PLMs
also gives insight into generating adversarial prompts to mislead the model,
raising concerns about the adversarial vulnerability of this paradigm. Recent
studies have shown that universal adversarial triggers (UATs) can be generated
to alter not only the predictions of the target PLMs but also the prediction of
corresponding Prompt-based Fine-tuning Models (PFMs) under the prompt-based
learning paradigm. However, UATs found in previous works are often unreadable
tokens or characters and can be easily distinguished from natural texts with
adaptive defenses. In this work, we consider the naturalness of the UATs and
develop $\textit{LinkPrompt}$, an adversarial attack algorithm to generate UATs
by a gradient-based beam search algorithm that not only effectively attacks the
target PLMs and PFMs but also maintains the naturalness among the trigger
tokens. Extensive results demonstrate the effectiveness of
$\textit{LinkPrompt}$, as well as the transferability of UATs generated by
\textit{LinkPrompt} to open-sourced Large Language Model (LLM) Llama2 and
API-accessed LLM GPT-3.5-turbo.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the main conference of NAACL2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DOCTR: Disentangled Object-Centric <span class="highlight-title">Transformer</span> for Point Scene
  Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16431v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16431v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoxuan Yu, Hao Wang, Weiming Li, Qiang Wang, Soonyong Cho, Younghun Sung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Point scene understanding is a challenging task to process real-world scene
point cloud, which aims at segmenting each object, estimating its pose, and
reconstructing its mesh simultaneously. Recent state-of-the-art method first
segments each object and then processes them independently with multiple stages
for the different sub-tasks. This leads to a complex pipeline to optimize and
makes it hard to leverage the relationship constraints between multiple
objects. In this work, we propose a novel Disentangled Object-Centric
TRansformer (DOCTR) that explores object-centric representation to facilitate
learning with multiple objects for the multiple sub-tasks in a unified manner.
Each object is represented as a query, and a Transformer decoder is adapted to
iteratively optimize all the queries involving their relationship. In
particular, we introduce a semantic-geometry disentangled query (SGDQ) design
that enables the query features to attend separately to semantic information
and geometric information relevant to the corresponding sub-tasks. A hybrid
bipartite matching module is employed to well use the supervisions from all the
sub-tasks during training. Qualitative and quantitative experimental results
demonstrate that our method achieves state-of-the-art performance on the
challenging ScanNet dataset. Code is available at
https://github.com/SAITPublic/DOCTR.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Word4Per: Zero-shot Composed Person Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.16515v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.16515v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Delong Liu, Haiwen Li, Zhicheng Zhao, Fei Su, Yuan Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Searching for specific person has great social benefits and security value,
and it often involves a combination of visual and textual information.
Conventional person retrieval methods, whether image-based or text-based,
usually fall short in effectively harnessing both types of information, leading
to the loss of accuracy. In this paper, a whole new task called Composed Person
Retrieval (CPR) is proposed to jointly utilize both image and text information
for target person retrieval. However, the supervised CPR requires very costly
manual annotation dataset, while there are currently no available resources. To
mitigate this issue, we firstly introduce the Zero-shot Composed Person
Retrieval (ZS-CPR), which leverages existing domain-related data to resolve the
CPR problem without expensive annotations. Secondly, to learn ZS-CPR model, we
propose a two-stage learning framework, Word4Per, where a lightweight Textual
Inversion Network (TINet) and a text-based person retrieval model based on
fine-tuned Contrastive Language-Image Pre-training (CLIP) network are learned
without utilizing any CPR data. Thirdly, a finely annotated Image-Text Composed
Person Retrieval (ITCPR) dataset is built as the benchmark to assess the
performance of the proposed Word4Per framework. Extensive experiments under
both Rank-1 and mAP demonstrate the effectiveness of Word4Per for the ZS-CPR
task, surpassing the comparative methods by over 10\%. The code and ITCPR
dataset will be publicly available at
https://github.com/Delong-liu-bupt/Word4Per.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Design-Space Exploration of SNN Models using Application-Specific
  Multi-Core Architectures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12061v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12061v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                         Sanaullah, Shamini Koravuna, Ulrich Rückert, Thorsten Jungeblut
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the motivation and the difficulties that currently exist in
comprehending and utilizing the promising features of SNNs, we proposed a novel
run-time multi-core architecture-based simulator called "RAVSim" (Runtime
Analysis and Visualization Simulator), a cutting-edge SNN simulator, developed
using LabVIEW and it is publicly available on their website as an official
module. RAVSim is a runtime virtual simulation environment tool that enables
the user to interact with the model, observe its behavior of output
concentration, and modify the set of parametric values at any time while the
simulation is in execution. Recently some popular tools have been presented,
but we believe that none of the tools allow users to interact with the model
simulation in run time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Abstract Presentation in 2023 Neuro-Inspired Computing Elements
  (NICE) Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LongHeads: Multi-Head Attention is Secretly a Long Context Processor 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.10685v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.10685v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Lu, Xin Zhou, Wei He, Jun Zhao, Tao Ji, Tao Gui, Qi Zhang, Xuanjing Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have achieved impressive performance in numerous
domains but often struggle to process lengthy inputs effectively and
efficiently due to limited length generalization and attention's quadratic
computational demands. Many sought to mitigate this by restricting the
attention window within the pre-trained length. However, these methods
introduce new issues such as ignoring the middle context and requiring
additional training. To address these problems, we propose LongHeads, a
training-free framework that enhances LLM's long context ability by unlocking
multi-head attention's untapped potential. Instead of allowing each head to
attend to the full sentence, which struggles with generalizing to longer
sequences due to out-of-distribution (OOD) issues, we allow each head to
process in-distribution length by selecting and attending to important context
chunks. To this end, we propose a chunk selection strategy that relies on the
inherent correlation between the query and the key representations, efficiently
distributing context chunks to different heads. In this way, each head ensures
it can effectively process attended tokens within the trained length, while
different heads in different layers can collectively process longer contexts.
LongHeads works efficiently in linear time, fits seamlessly with many LLMs that
use relative positional encoding. LongHeads achieves 100% accuracy at the 128k
length on passkey retrieval task, verifying LongHeads's efficacy in extending
the usable context window for existing models. We release our code at
https://github.com/LuLuLuyi/LongHeads .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SVGDreamer: Text Guided SVG Generation with Diffusion Model <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.16476v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.16476v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ximing Xing, Haitao Zhou, Chuang Wang, Jing Zhang, Dong Xu, Qian Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, text-guided scalable vector graphics (SVGs) synthesis has shown
promise in domains such as iconography and sketch. However, existing
text-to-SVG generation methods lack editability and struggle with visual
quality and result diversity. To address these limitations, we propose a novel
text-guided vector graphics synthesis method called SVGDreamer. SVGDreamer
incorporates a semantic-driven image vectorization (SIVE) process that enables
the decomposition of synthesis into foreground objects and background, thereby
enhancing editability. Specifically, the SIVE process introduce attention-based
primitive control and an attention-mask loss function for effective control and
manipulation of individual elements. Additionally, we propose a Vectorized
Particle-based Score Distillation (VPSD) approach to tackle the challenges of
shape over-smoothing, color over-saturation, limited diversity in results, and
slow convergence in existing text-to-SVG generation methods. VPSD models SVGs
as distributions of control points and colors to counteract over-smoothing and
over-saturation. Furthermore, VPSD leverages a reward model to reweight vector
particles, which improves aesthetic appeal and accelerates convergence.
Extensive experiments have been conducted to validate the effectiveness of
SVGDreamer, demonstrating its superiority over baseline methods in terms of
editability, visual quality, and diversity. The code and demo of SVGDreamer can
be found at https://ximinng.github.io/SVGDreamer-project/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2024. project link:
  https://ximinng.github.io/SVGDreamer-project/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Implications of Decentralization in Blockchained Federated Learning:
  Evaluating the Impact of Model Staleness and Inconsistencies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07471v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07471v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francesc Wilhelmi, Nima Afraz, Elia Guerra, Paolo Dini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Blockchain promises to enhance distributed machine learning (ML) approaches
such as federated learning (FL) by providing further decentralization,
security, immutability, and trust, which are key properties for enabling
collaborative intelligence in next-generation applications. Nonetheless, the
intrinsic decentralized operation of peer-to-peer (P2P) blockchain nodes leads
to an uncharted setting for FL, whereby the concepts of FL round and global
model become meaningless, as devices' synchronization is lost without the
figure of a central orchestrating server. In this paper, we study the practical
implications of outsourcing the orchestration of FL to a democratic setting
such as in a blockchain. In particular, we focus on the effects that model
staleness and inconsistencies, endorsed by blockchains' modus operandi, have on
the training procedure held by FL devices asynchronously. Using simulation, we
evaluate the blockchained FL operation by applying two different ML models
(ranging from low to high complexity) on the well-known MNIST and CIFAR-10
datasets, respectively, and focus on the accuracy and timeliness of the
solutions. Our results show the high impact of model inconsistencies on the
accuracy of the models (up to a ~35% decrease in prediction accuracy), which
underscores the importance of properly designing blockchain systems based on
the characteristics of the underlying FL application.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sandra -- A Neuro-Symbolic Reasoner Based On Descriptions And Situations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.00591v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.00591v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicolas Lazzari, Stefano De Giorgis, Aldo Gangemi, Valentina Presutti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents sandra, a neuro-symbolic reasoner combining vectorial
representations with deductive reasoning. Sandra builds a vector space
constrained by an ontology and performs reasoning over it. The geometric nature
of the reasoner allows its combination with neural networks, bridging the gap
with symbolic knowledge representations. Sandra is based on the Description and
Situation (DnS) ontology design pattern, a formalization of frame semantics.
Given a set of facts (a situation) it allows to infer all possible perspectives
(descriptions) that can provide a plausible interpretation for it, even in
presence of incomplete information. We prove that our method is correct with
respect to the DnS model. We experiment with two different tasks and their
standard benchmarks, demonstrating that, without increasing complexity, sandra
(i) outperforms all the baselines (ii) provides interpretability in the
classification process, and (iii) allows control over the vector space, which
is designed a priori.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BatteryML:An Open-source platform for Machine Learning on Battery
  Degradation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.14714v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.14714v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Zhang, Xiaofan Gui, Shun Zheng, Ziheng Lu, Yuqi Li, Jiang Bian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Battery degradation remains a pivotal concern in the energy storage domain,
with machine learning emerging as a potent tool to drive forward insights and
solutions. However, this intersection of electrochemical science and machine
learning poses complex challenges. Machine learning experts often grapple with
the intricacies of battery science, while battery researchers face hurdles in
adapting intricate models tailored to specific datasets. Beyond this, a
cohesive standard for battery degradation modeling, inclusive of data formats
and evaluative benchmarks, is conspicuously absent. Recognizing these
impediments, we present BatteryML - a one-step, all-encompass, and open-source
platform designed to unify data preprocessing, feature extraction, and the
implementation of both traditional and state-of-the-art models. This
streamlined approach promises to enhance the practicality and efficiency of
research applications. BatteryML seeks to fill this void, fostering an
environment where experts from diverse specializations can collaboratively
contribute, thus elevating the collective understanding and advancement of
battery research.The code for our project is publicly available on GitHub at
https://github.com/microsoft/BatteryML.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Causal Question Answering with Reinforcement Learning <span class="chip">WWW 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.02760v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.02760v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lukas Blübaum, Stefan Heindorf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Causal questions inquire about causal relationships between different events
or phenomena. They are important for a variety of use cases, including virtual
assistants and search engines. However, many current approaches to causal
question answering cannot provide explanations or evidence for their answers.
Hence, in this paper, we aim to answer causal questions with a causality graph,
a large-scale dataset of causal relations between noun phrases along with the
relations' provenance data. Inspired by recent, successful applications of
reinforcement learning to knowledge graph tasks, such as link prediction and
fact-checking, we explore the application of reinforcement learning on a
causality graph for causal question answering. We introduce an
Actor-Critic-based agent which learns to search through the graph to answer
causal questions. We bootstrap the agent with a supervised learning procedure
to deal with large action spaces and sparse rewards. Our evaluation shows that
the agent successfully prunes the search space to answer binary causal
questions by visiting less than 30 nodes per question compared to over 3,000
nodes by a naive breadth-first search. Our ablation study indicates that our
supervised learning strategy provides a strong foundation upon which our
reinforcement learning agent improves. The paths returned by our agent explain
the mechanisms by which a cause produces an effect. Moreover, for each edge on
a path, our causality graph provides its original source allowing for easy
verification of paths.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at WWW 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Align-to-Distill: Trainable Attention Alignment for Knowledge
  Distillation in Neural Machine Translation <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.01479v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.01479v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Heegon Jin, Seonil Son, Jemin Park, Youngseok Kim, Hyungjong Noh, Yeonsoo Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of scalable deep models and large datasets has improved the
performance of Neural Machine Translation. Knowledge Distillation (KD) enhances
efficiency by transferring knowledge from a teacher model to a more compact
student model. However, KD approaches to Transformer architecture often rely on
heuristics, particularly when deciding which teacher layers to distill from. In
this paper, we introduce the 'Align-to-Distill' (A2D) strategy, designed to
address the feature mapping problem by adaptively aligning student attention
heads with their teacher counterparts during training. The Attention Alignment
Module in A2D performs a dense head-by-head comparison between student and
teacher attention heads across layers, turning the combinatorial mapping
heuristics into a learning problem. Our experiments show the efficacy of A2D,
demonstrating gains of up to +3.61 and +0.63 BLEU points for WMT-2022 De->Dsb
and WMT-2014 En->De, respectively, compared to Transformer baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring the Adversarial Capabilities of Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.09132v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.09132v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lukas Struppek, Minh Hieu Le, Dominik Hintersdorf, Kristian Kersting
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The proliferation of large language models (LLMs) has sparked widespread and
general interest due to their strong language generation capabilities, offering
great potential for both industry and research. While previous research delved
into the security and privacy issues of LLMs, the extent to which these models
can exhibit adversarial behavior remains largely unexplored. Addressing this
gap, we investigate whether common publicly available LLMs have inherent
capabilities to perturb text samples to fool safety measures, so-called
adversarial examples resp.~attacks. More specifically, we investigate whether
LLMs are inherently able to craft adversarial examples out of benign samples to
fool existing safe rails. Our experiments, which focus on hate speech
detection, reveal that LLMs succeed in finding adversarial perturbations,
effectively undermining hate speech detection systems. Our findings carry
significant implications for (semi-)autonomous systems relying on LLMs,
highlighting potential challenges in their interaction with existing systems
and safety measures.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HealthFC: Verifying Health Claims with Evidence-Based Medical
  Fact-Checking <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.08503v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.08503v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juraj Vladika, Phillip Schneider, Florian Matthes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the digital age, seeking health advice on the Internet has become a common
practice. At the same time, determining the trustworthiness of online medical
content is increasingly challenging. Fact-checking has emerged as an approach
to assess the veracity of factual claims using evidence from credible knowledge
sources. To help advance automated Natural Language Processing (NLP) solutions
for this task, in this paper we introduce a novel dataset HealthFC. It consists
of 750 health-related claims in German and English, labeled for veracity by
medical experts and backed with evidence from systematic reviews and clinical
trials. We provide an analysis of the dataset, highlighting its characteristics
and challenges. The dataset can be used for NLP tasks related to automated
fact-checking, such as evidence retrieval, claim verification, or explanation
generation. For testing purposes, we provide baseline systems based on
different approaches, examine their performance, and discuss the findings. We
show that the dataset is a challenging test bed with a high potential for
future use.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ With Greater Text Comes Greater Necessity: Inference-Time Training Helps
  Long Text Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.11504v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.11504v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Y. Wang, D. Ma, D. Cai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Long text generation, such as novel writing and discourse-level translation
with extremely long contexts, presents significant challenges to current
language models. Existing methods mainly focus on extending the model's context
window through strategies like length extrapolation. However, these approaches
demand substantial hardware resources during the training and/or inference
phases. Our proposed method, Temp-Lora, introduces an alternative concept.
Instead of relying on the KV cache to store all context information, we embeds
this information directly into a temporary Lora module. In the process of long
text generation, this module is progressively trained with text generated
previously. This approach not only efficiently preserves contextual knowledge
but also prevents any permanent alteration to the model's parameters given that
the module is discarded post-generation. Extensive experiments on the PG19
language modeling benchmark and the GuoFeng discourse-level translation
benchmark validate the effectiveness of Temp-Lora. Our results show that: 1)
Temp-Lora substantially enhances generation quality for long text, as indicated
by a 13.2% decrease in perplexity (PPL) on a subset of PG19, and a 29.3%
decrease in PPL along with a 113.2% increase in BLEU score on a subset of
GuoFeng, 2) Temp-Lora is compatible with and enhances most existing long text
generation methods, and 3) Temp-Lora can greatly reduce computational costs by
shortening the context window. For example, we can ensure a moderate
improvement in generation quality (a decrease of 3.8% in PPL) while enabling a
51.5% memory usage reduction and a 60.0% decrease in latency for inference.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-agent reinforcement learning using echo-state network and its
  application to pedestrian dynamics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.11834v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.11834v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hisato Komatsu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, simulations of pedestrians using the multi-agent
reinforcement learning (MARL) have been studied. This study considered the
roads on a grid-world environment, and implemented pedestrians as MARL agents
using an echo-state network and the least squares policy iteration method.
Under this environment, the ability of these agents to learn to move forward by
avoiding other agents was investigated. Specifically, we considered two types
of tasks: the choice between a narrow direct route and a broad detour, and the
bidirectional pedestrian flow in a corridor. The simulations results indicated
that the learning was successful when the density of the agents was not that
high.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 17 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dial-MAE: ConTextual Masked Auto-Encoder for Retrieval-based Dialogue
  Systems <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04357v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04357v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenpeng Su, Xing Wu, Wei Zhou, Guangyuan Ma, Songlin Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dialogue response selection aims to select an appropriate response from
several candidates based on a given user and system utterance history. Most
existing works primarily focus on post-training and fine-tuning tailored for
cross-encoders. However, there are no post-training methods tailored for dense
encoders in dialogue response selection. We argue that when the current
language model, based on dense dialogue systems (such as BERT), is employed as
a dense encoder, it separately encodes dialogue context and response, leading
to a struggle to achieve the alignment of both representations. Thus, we
propose Dial-MAE (Dialogue Contextual Masking Auto-Encoder), a straightforward
yet effective post-training technique tailored for dense encoders in dialogue
response selection. Dial-MAE uses an asymmetric encoder-decoder architecture to
compress the dialogue semantics into dense vectors, which achieves better
alignment between the features of the dialogue context and response. Our
experiments have demonstrated that Dial-MAE is highly effective, achieving
state-of-the-art performance on two commonly evaluated benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted by NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Preference as Reward, Maximum Preference Optimization with Importance
  Sampling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.16430v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.16430v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zaifan Jiang, Xing Huang, Chao Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Preference learning is a key technology for aligning language models with
human values. Reinforcement Learning from Human Feedback (RLHF) is a
model-based algorithm to optimize preference learning, which first fits a
reward model for preference scores and then optimizes the generating policy
with an on-policy PPO algorithm to maximize the reward. The processing of RLHF
is complex, time-consuming, and unstable. The Direct Preference Optimization
(DPO) algorithm uses an off-policy algorithm to directly optimize the
generating policy and eliminates the need for a reward model. DPO is more
data-efficient and stable. However, DPO has a drawback of overfitting to the
preference data and ignoring the KL-regularization term when the preference is
deterministic. Identity mapping Preference Optimization(IPO) uses a
root-finding MSE loss to incorporate KL-regularization. However, both DPO and
IPO fail to properly address the KL-regularization term because the support of
the preference distribution is not equal to the reference distribution. In this
paper, we propose a simple and intuitive off-policy preference optimization
algorithm from an importance sampling view, which we call Maximum Preference
Optimization (MPO). MPO incorporates the off-policy KL-regularization term,
making regularization truly effective. MPO achieves the best of both worlds by
combining the objectives of RLHF and IPO while being an off-policy algorithm.
Furthermore, MPO eliminates the need for a reward model and reference policy,
simplifying the learning process and reducing memory usage.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Survey</span> of Confidence Estimation and Calibration in Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.08298v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.08298v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahui Geng, Fengyu Cai, Yuxia Wang, Heinz Koeppl, Preslav Nakov, Iryna Gurevych
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated remarkable capabilities across
a wide range of tasks in various domains. Despite their impressive performance,
they can be unreliable due to factual errors in their generations. Assessing
their confidence and calibrating them across different tasks can help mitigate
risks and enable LLMs to produce better generations. There has been a lot of
recent research aiming to address this, but there has been no comprehensive
overview to organize it and outline the main lessons learned. The present
survey aims to bridge this gap. In particular, we outline the challenges and we
summarize recent technical advancements for LLM confidence estimation and
calibration. We further discuss their applications and suggest promising
directions for future work.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 1 page, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Knowledge Engineering Primer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.17196v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.17196v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Agnieszka Ławrynowicz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The aim of this primer is to introduce the subject of knowledge engineering
in a concise but synthetic way to develop the reader's intuition about the
area.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring Chat<span class="highlight-title">GPT</span> and its Impact on Society 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14643v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14643v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md. Asraful Haque, Shuai Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial intelligence has been around for a while, but suddenly it has
received more attention than ever before. Thanks to innovations from companies
like Google, Microsoft, Meta, and other major brands in technology. OpenAI,
though, has triggered the button with its ground-breaking invention ChatGPT.
ChatGPT is a Large Language Model (LLM) based on Transformer architecture that
has the ability to generate human-like responses in a conversational context.
It uses deep learning algorithms to generate natural language responses to
input text. Its large number of parameters, contextual generation, and
open-domain training make it a versatile and effective tool for a wide range of
applications, from chatbots to customer service to language translation. It has
the potential to revolutionize various industries and transform the way we
interact with technology. However, the use of ChatGPT has also raised several
concerns, including ethical, social, and employment challenges, which must be
carefully considered to ensure the responsible use of this technology. The
article provides an overview of ChatGPT, delving into its architecture and
training process. It highlights the potential impacts of ChatGPT on the
society. In this paper, we suggest some approaches involving technology,
regulation, education, and ethics in an effort to maximize ChatGPT's benefits
while minimizing its negative impacts. This study is expected to contribute to
a greater understanding of ChatGPT and aid in predicting the potential changes
it may bring about.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 Pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PIA: Your Personalized Image Animator via Plug-and-Play Modules in
  Text-to-Image Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.13964v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.13964v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiming Zhang, Zhening Xing, Yanhong Zeng, Youqing Fang, Kai Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in personalized text-to-image (T2I) models have
revolutionized content creation, empowering non-experts to generate stunning
images with unique styles. While promising, adding realistic motions into these
personalized images by text poses significant challenges in preserving distinct
styles, high-fidelity details, and achieving motion controllability by text. In
this paper, we present PIA, a Personalized Image Animator that excels in
aligning with condition images, achieving motion controllability by text, and
the compatibility with various personalized T2I models without specific tuning.
To achieve these goals, PIA builds upon a base T2I model with well-trained
temporal alignment layers, allowing for the seamless transformation of any
personalized T2I model into an image animation model. A key component of PIA is
the introduction of the condition module, which utilizes the condition frame
and inter-frame affinity as input to transfer appearance information guided by
the affinity hint for individual frame synthesis in the latent space. This
design mitigates the challenges of appearance-related image alignment within
and allows for a stronger focus on aligning with motion-related guidance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://pi-animator.github.io/</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Databases
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Chase Termination Beyond Polynomial Time 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16712v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16712v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philipp Hanisch, Markus Krötzsch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The chase is a widely implemented approach to reason with tuple-generating
dependencies (tgds), used in data exchange, data integration, and
ontology-based query answering. However, it is merely a semi-decision
procedure, which may fail to terminate. Many decidable conditions have been
proposed for tgds to ensure chase termination, typically by forbidding some
kind of "cycle" in the chase process. We propose a new criterion that
explicitly allows some such cycles, and yet ensures termination of the standard
chase under reasonable conditions. This leads to new decidable fragments of
tgds that are not only syntactically more general but also strictly more
expressive than the fragments defined by prior acyclicity conditions. Indeed,
while known terminating fragments are restricted to PTime data complexity, our
conditions yield decidable languages for any k-ExpTime. We further refine our
syntactic conditions to obtain fragments of tgds for which an optimised chase
procedure decides query entailment in PSpace or k-ExpSpace, respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Implementing and Evaluating E2LSH on Storage 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16404v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16404v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Nakanishi, Kazuhiro Hiwada, Yosuke Bando, Tomoya Suzuki, Hirotsugu Kajihara, Shintaro Sano, Tatsuro Endo, Tatsuo Shiozawa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Locality sensitive hashing (LSH) is one of the widely-used approaches to
approximate nearest neighbor search (ANNS) in high-dimensional spaces. The
first work on LSH for the Euclidean distance, E2LSH, showed how ANNS can be
solved efficiently at a sublinear query time in the database size with
theoretically-guaranteed accuracy, although it required a large hash index
size. Since then, several LSH variants having much smaller index sizes have
been proposed. Their query time is linear or superlinear, but they have been
shown to run effectively faster because they require fewer I/Os when the index
is stored on hard disk drives and because they also permit in-memory execution
with modern DRAM capacity.
  In this paper, we show that E2LSH is regaining the advantage in query speed
with the advent of modern flash storage devices such as solid-state drives
(SSDs). We evaluate E2LSH on a modern single-node computing environment and
analyze its computational cost and I/O cost, from which we derive storage
performance requirements for its external memory execution. Our analysis
indicates that E2LSH on a single consumer-grade SSD can run faster than the
state-of-the-art small-index methods executed in-memory. It also indicates that
E2LSH with emerging high-performance storage devices and interfaces can
approach in-memory E2LSH speeds. We implement a simple adaptation of E2LSH to
external memory, E2LSH-on-Storage (E2LSHoS), and evaluate it for practical
large datasets of up to one billion objects using different combinations of
modern storage devices and interfaces. We demonstrate that our E2LSHoS
implementation runs much faster than small-index methods and can approach
in-memory E2LSH speeds, and also that its query time scales sublinearly with
the database size beyond the index size limit of in-memory E2LSH.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Corra: Correlation-Aware Column Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17229v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17229v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanwen Liu, Mihail Stoian, Alexander van Renen, Andreas Kipf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Column encoding schemes have witnessed a spark of interest lately. This is
not surprising -- as data volume increases, being able to keep one's dataset in
main memory for fast processing is a coveted desideratum. However, it also
seems that single-column encoding schemes have reached a plateau in terms of
the compression size they can achieve.
  We argue that this is because they do not exploit correlations in the data.
Consider for instance the column pair ($\texttt{city}$, $\texttt{zip-code}$) of
the DMV dataset: a city has only a few dozen unique zip codes. Such
information, if properly exploited, can significantly reduce the space
consumption of the latter column.
  In this work, we depart from the established, well-trodden path of
compressing data using only single-column encoding schemes and introduce
$\textit{correlation-aware}$ encoding schemes. We demonstrate their advantages
compared to single-column encoding schemes on the well-known TPC-H's
$\texttt{lineitem}$, LDBC's $\texttt{message}$, DMV, and Taxi. For example, we
obtain a saving rate of 58.3% for $\texttt{lineitem}$'s $\texttt{shipdate}$,
while the dropoff timestamps in Taxi witness a saving rate of 30.6%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GGDMiner - Discovery of Graph Generating Dependencies for Graph Data
  Profiling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17082v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17082v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Larissa C. Shimomura, Nikolay Yakovets, George Fletcher
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the increasing use of graph-structured data, there is also increasing
interest in investigating graph data dependencies and their applications, e.g.,
in graph data profiling. Graph Generating Dependencies (GGDs) are a class of
dependencies for property graphs that can express the relation between
different graph patterns and constraints based on their attribute similarities.
Rich syntax and semantics of GGDs make them a good candidate for graph data
profiling. Nonetheless, GGDs are difficult to define manually, especially when
there are no data experts available. In this paper, we propose GGDMiner, a
framework for discovering approximate GGDs from graph data automatically, with
the intention of profiling graph data through GGDs for the user. GGDMiner has
three main steps: (1) pre-processing, (2) candidate generation, and, (3) GGD
extraction. To optimize memory consumption and execution time, GGDMiner uses a
factorized representation of each discovered graph pattern, called Answer
Graph. Our results show that the discovered set of GGDs can give an overview
about the input graph, both schema level information and also correlations
between the graph patterns and attributes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ProvDeploy: Provenance-oriented Containerization of High Performance
  Computing Scientific Workflows 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15324v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15324v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liliane Kunstmann, Débora Pina, Daniel de Oliveira, Marta Mattoso
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many existing scientific workflows require High Performance Computing
environments to produce results in a timely manner. These workflows have
several software library components and use different environments, making the
deployment and execution of the software stack not trivial. This complexity
increases if the user needs to add provenance data capture services to the
workflow. This manuscript introduces ProvDeploy to assist the user in
configuring containers for scientific workflows with integrated provenance data
capture. ProvDeploy was evaluated with a Scientific Machine Learning workflow,
exploring containerization strategies focused on provenance in two distinct HPC
environments
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ When is Shapley Value Computation a Matter of Counting? <span class="chip">PODS'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.14529v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.14529v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Meghyn Bienvenu, Diego Figueira, Pierre Lafourcade
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Shapley value provides a natural means of quantifying the contributions
of facts to database query answers. In this work, we seek to broaden our
understanding of Shapley value computation (SVC) in the database setting by
revealing how it relates to Fixed-size Generalized Model Counting (FGMC), which
is the problem of computing the number of sub-databases of a given size and
containing a given set of assumed facts that satisfy a fixed query. Our focus
will be on explaining the difficulty of SVC via FGMC, and to this end, we
identify general conditions on queries which enable reductions from FGMC to
SVC. As a byproduct, we not only obtain alternative explanations for most
existing results on SVC, but also new complexity results. In particular, we
establish FP-#P complexity dichotomies for constant-free connected UCQs and
homomorphism-closed connected graph queries. We further explore variants of
SVC, either in the absence of assumed facts, or where we measure the
contribution of constants rather than facts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at PODS'24</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ One-Shot Domain Incremental Learning <span class="chip">IJCNN</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16707v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16707v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yasushi Esaki, Satoshi Koide, Takuro Kutsuna
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Domain incremental learning (DIL) has been discussed in previous studies on
deep neural network models for classification. In DIL, we assume that samples
on new domains are observed over time. The models must classify inputs on all
domains. In practice, however, we may encounter a situation where we need to
perform DIL under the constraint that the samples on the new domain are
observed only infrequently. Therefore, in this study, we consider the extreme
case where we have only one sample from the new domain, which we call one-shot
DIL. We first empirically show that existing DIL methods do not work well in
one-shot DIL. We have analyzed the reason for this failure through various
investigations. According to our analysis, we clarify that the difficulty of
one-shot DIL is caused by the statistics in the batch normalization layers.
Therefore, we propose a technique regarding these statistics and demonstrate
the effectiveness of our technique through experiments on open datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted at IEEE International Joint Conference on Neural Networks
  (IJCNN) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Assessing the Performance of Deep Learning for Automated Gleason Grading
  in Prostate Cancer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16695v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16695v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dominik Müller, Philip Meyer, Lukas Rentschler, Robin Manz, Daniel Hieber, Jonas Bäcker, Samantha Cramer, Christoph Wengenmayr, Bruno Märkl, Ralf Huss, Frank Kramer, Iñaki Soto-Rey, Johannes Raffler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prostate cancer is a dominant health concern calling for advanced diagnostic
tools. Utilizing digital pathology and artificial intelligence, this study
explores the potential of 11 deep neural network architectures for automated
Gleason grading in prostate carcinoma focusing on comparing traditional and
recent architectures. A standardized image classification pipeline, based on
the AUCMEDI framework, facilitated robust evaluation using an in-house dataset
consisting of 34,264 annotated tissue tiles. The results indicated varying
sensitivity across architectures, with ConvNeXt demonstrating the strongest
performance. Notably, newer architectures achieved superior performance, even
though with challenges in differentiating closely related Gleason grades. The
ConvNeXt model was capable of learning a balance between complexity and
generalizability. Overall, this study lays the groundwork for enhanced Gleason
grading systems, potentially improving diagnostic efficiency for prostate
cancer.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Synapse: Learning Preferential Concepts from Visual Demonstrations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16689v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16689v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sadanand Modak, Noah Patton, Isil Dillig, Joydeep Biswas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses the problem of preference learning, which aims to learn
user-specific preferences (e.g., "good parking spot", "convenient drop-off
location") from visual input. Despite its similarity to learning factual
concepts (e.g., "red cube"), preference learning is a fundamentally harder
problem due to its subjective nature and the paucity of person-specific
training data. We address this problem using a new framework called Synapse,
which is a neuro-symbolic approach designed to efficiently learn preferential
concepts from limited demonstrations. Synapse represents preferences as
neuro-symbolic programs in a domain-specific language (DSL) that operates over
images, and leverages a novel combination of visual parsing, large language
models, and program synthesis to learn programs representing individual
preferences. We evaluate Synapse through extensive experimentation including a
user case study focusing on mobility-related concepts in mobile robotics and
autonomous driving. Our evaluation demonstrates that Synapse significantly
outperforms existing baselines as well as its own ablations. The code and other
details can be found on the project website https://amrl.cs.utexas.edu/synapse .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 7 figures; Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A note on generalization bounds for losses with finite moments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16681v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16681v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Borja Rodríguez-Gálvez, Omar Rivasplata, Ragnar Thobaben, Mikael Skoglund
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper studies the truncation method from Alquier [1] to derive
high-probability PAC-Bayes bounds for unbounded losses with heavy tails.
Assuming that the $p$-th moment is bounded, the resulting bounds interpolate
between a slow rate $1 / \sqrt{n}$ when $p=2$, and a fast rate $1 / n$ when $p
\to \infty$ and the loss is essentially bounded. Moreover, the paper derives a
high-probability PAC-Bayes bound for losses with a bounded variance. This bound
has an exponentially better dependence on the confidence parameter and the
dependency measure than previous bounds in the literature. Finally, the paper
extends all results to guarantees in expectation and single-draw PAC-Bayes. In
order to so, it obtains analogues of the PAC-Bayes fast rate bound for bounded
losses from [2] in these settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages: 5 of main text, 1 of references, and 3 of appendices</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Symmetric Basis Convolutions for Learning Lagrangian Fluid Mechanics <span class="chip">ICLR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16680v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16680v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rene Winchenbach, Nils Thuerey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning physical simulations has been an essential and central aspect of
many recent research efforts in machine learning, particularly for
Navier-Stokes-based fluid mechanics. Classic numerical solvers have
traditionally been computationally expensive and challenging to use in inverse
problems, whereas Neural solvers aim to address both concerns through machine
learning. We propose a general formulation for continuous convolutions using
separable basis functions as a superset of existing methods and evaluate a
large set of basis functions in the context of (a) a compressible 1D SPH
simulation, (b) a weakly compressible 2D SPH simulation, and (c) an
incompressible 2D SPH Simulation. We demonstrate that even and odd symmetries
included in the basis functions are key aspects of stability and accuracy. Our
broad evaluation shows that Fourier-based continuous convolutions outperform
all other architectures regarding accuracy and generalization. Finally, using
these Fourier-based networks, we show that prior inductive biases, such as
window functions, are no longer necessary. An implementation of our approach,
as well as complete datasets and solver implementations, is available at
https://github.com/tum-pbs/SFBC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at International Conference on Learning Representation
  (ICLR) 2024, 54 pages, 39 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DeepGleason: a System for Automated Gleason Grading of Prostate Cancer
  using Deep Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16678v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16678v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dominik Müller, Philip Meyer, Lukas Rentschler, Robin Manz, Jonas Bäcker, Samantha Cramer, Christoph Wengenmayr, Bruno Märkl, Ralf Huss, Iñaki Soto-Rey, Johannes Raffler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advances in digital pathology and artificial intelligence (AI) offer
promising opportunities for clinical decision support and enhancing diagnostic
workflows. Previous studies already demonstrated AI's potential for automated
Gleason grading, but lack state-of-the-art methodology and model reusability.
To address this issue, we propose DeepGleason: an open-source deep neural
network based image classification system for automated Gleason grading using
whole-slide histopathology images from prostate tissue sections. Implemented
with the standardized AUCMEDI framework, our tool employs a tile-wise
classification approach utilizing fine-tuned image preprocessing techniques in
combination with a ConvNeXt architecture which was compared to various
state-of-the-art architectures. The neural network model was trained and
validated on an in-house dataset of 34,264 annotated tiles from 369 prostate
carcinoma slides. We demonstrated that DeepGleason is capable of highly
accurate and reliable Gleason grading with a macro-averaged F1-score of 0.806,
AUC of 0.991, and Accuracy of 0.974. The internal architecture comparison
revealed that the ConvNeXt model was superior performance-wise on our dataset
to established and other modern architectures like transformers. Furthermore,
we were able to outperform the current state-of-the-art in tile-wise
fine-classification with a sensitivity and specificity of 0.94 and 0.98 for
benign vs malignant detection as well as of 0.91 and 0.75 for Gleason 3 vs
Gleason 4 & 5 classification, respectively. Our tool contributes to the wider
adoption of AI-based Gleason grading within the research community and paves
the way for broader clinical application of deep learning models in digital
pathology. DeepGleason is open-source and publicly available for research
application in the following Git repository:
https://github.com/frankkramer-lab/DeepGleason.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FOOL: Addressing the Downlink Bottleneck in Satellite Computing with
  Neural Feature Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16677v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16677v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alireza Furutanpey, Qiyang Zhang, Philipp Raith, Tobias Pfandzelter, Shangguang Wang, Schahram Dustdar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nanosatellite constellations equipped with sensors capturing large geographic
regions provide unprecedented opportunities for Earth observation. As
constellation sizes increase, network contention poses a downlink bottleneck.
Orbital Edge Computing (OEC) leverages limited onboard compute resources to
reduce transfer costs by processing the raw captures at the source. However,
current solutions have limited practicability due to reliance on crude
filtering methods or over-prioritizing particular downstream tasks.
  This work presents FOOL, an OEC-native and task-agnostic feature compression
method that preserves prediction performance. FOOL partitions high-resolution
satellite imagery to maximize throughput. Further, it embeds context and
leverages inter-tile dependencies to lower transfer costs with negligible
overhead. While FOOL is a feature compressor, it can recover images with
competitive scores on perceptual quality measures at lower bitrates. We
extensively evaluate transfer cost reduction by including the peculiarity of
intermittently available network connections in low earth orbit. Lastly, we
test the feasibility of our system for standardized nanosatellite form factors.
We demonstrate that FOOL permits downlinking over 100x the data volume without
relying on prior information on the downstream tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, double column, 19 figures, 7 tables, Initial Submission to
  IEEE Transactions on Mobile Computing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding the Functional Roles of Modelling Components in Spiking
  Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16674v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16674v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huifeng Yin, Hanle Zheng, Jiayi Mao, Siyuan Ding, Xing Liu, Mingkun Xu, Yifan Hu, Jing Pei, Lei Deng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spiking neural networks (SNNs), inspired by the neural circuits of the brain,
are promising in achieving high computational efficiency with biological
fidelity. Nevertheless, it is quite difficult to optimize SNNs because the
functional roles of their modelling components remain unclear. By designing and
evaluating several variants of the classic model, we systematically investigate
the functional roles of key modelling components, leakage, reset, and
recurrence, in leaky integrate-and-fire (LIF) based SNNs. Through extensive
experiments, we demonstrate how these components influence the accuracy,
generalization, and robustness of SNNs. Specifically, we find that the leakage
plays a crucial role in balancing memory retention and robustness, the reset
mechanism is essential for uninterrupted temporal processing and computational
efficiency, and the recurrence enriches the capability to model complex
dynamics at a cost of robustness degradation. With these interesting
observations, we provide optimization suggestions for enhancing the performance
of SNNs in different scenarios. This work deepens the understanding of how SNNs
work, which offers valuable guidance for the development of more effective and
robust neuromorphic models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Graph Augmentation for Recommendation <span class="chip">ICDE 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16656v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16656v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qianru Zhang, Lianghao Xia, Xuheng Cai, Siuming Yiu, Chao Huang, Christian S. Jensen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph augmentation with contrastive learning has gained significant attention
in the field of recommendation systems due to its ability to learn expressive
user representations, even when labeled data is limited. However, directly
applying existing GCL models to real-world recommendation environments poses
challenges. There are two primary issues to address. Firstly, the lack of
consideration for data noise in contrastive learning can result in noisy
self-supervised signals, leading to degraded performance. Secondly, many
existing GCL approaches rely on graph neural network (GNN) architectures, which
can suffer from over-smoothing problems due to non-adaptive message passing. To
address these challenges, we propose a principled framework called GraphAug.
This framework introduces a robust data augmentor that generates denoised
self-supervised signals, enhancing recommender systems. The GraphAug framework
incorporates a graph information bottleneck (GIB)-regularized augmentation
paradigm, which automatically distills informative self-supervision information
and adaptively adjusts contrastive view generation. Through rigorous
experimentation on real-world datasets, we thoroughly assessed the performance
of our novel GraphAug model. The outcomes consistently unveil its superiority
over existing baseline methods. The source code for our model is publicly
available at: https://github.com/HKUDS/GraphAug.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages and accepted by ICDE 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Novel Loss Function-based Support Vector Machine for Binary
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16654v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16654v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yan Li, Liping Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The previous support vector machine(SVM) including $0/1$ loss SVM, hinge loss
SVM, ramp loss SVM, truncated pinball loss SVM, and others, overlooked the
degree of penalty for the correctly classified samples within the margin. This
oversight affects the generalization ability of the SVM classifier to some
extent. To address this limitation, from the perspective of confidence margin,
we propose a novel Slide loss function ($\ell_s$) to construct the support
vector machine classifier($\ell_s$-SVM). By introducing the concept of proximal
stationary point, and utilizing the property of Lipschitz continuity, we derive
the first-order optimality conditions for $\ell_s$-SVM. Based on this, we
define the $\ell_s$ support vectors and working set of $\ell_s$-SVM. To
efficiently handle $\ell_s$-SVM, we devise a fast alternating direction method
of multipliers with the working set ($\ell_s$-ADMM), and provide the
convergence analysis. The numerical experiments on real world datasets confirm
the robustness and effectiveness of the proposed method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bridging the Sim-to-Real Gap with Bayesian Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16644v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16644v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonas Rothfuss, Bhavya Sukhija, Lenart Treven, Florian Dörfler, Stelian Coros, Andreas Krause
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present SIM-FSVGD for learning robot dynamics from data. As opposed to
traditional methods, SIM-FSVGD leverages low-fidelity physical priors, e.g., in
the form of simulators, to regularize the training of neural network models.
While learning accurate dynamics already in the low data regime, SIM-FSVGD
scales and excels also when more data is available. We empirically show that
learning with implicit physical priors results in accurate mean model
estimation as well as precise uncertainty quantification. We demonstrate the
effectiveness of SIM-FSVGD in bridging the sim-to-real gap on a
high-performance RC racecar system. Using model-based RL, we demonstrate a
highly dynamic parking maneuver with drifting, using less than half the data
compared to the state of the art.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Scale Texture Loss for CT denoising with GANs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16640v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16640v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francesco Di Feola, Lorenzo Tronchin, Valerio Guarrasi, Paolo Soda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative Adversarial Networks (GANs) have proved as a powerful framework
for denoising applications in medical imaging. However, GAN-based denoising
algorithms still suffer from limitations in capturing complex relationships
within the images. In this regard, the loss function plays a crucial role in
guiding the image generation process, encompassing how much a synthetic image
differs from a real image. To grasp highly complex and non-linear textural
relationships in the training process, this work presents a loss function that
leverages the intrinsic multi-scale nature of the Gray-Level-Co-occurrence
Matrix (GLCM). Although the recent advances in deep learning have demonstrated
superior performance in classification and detection tasks, we hypothesize that
its information content can be valuable when integrated into GANs' training. To
this end, we propose a differentiable implementation of the GLCM suited for
gradient-based optimization. Our approach also introduces a self-attention
layer that dynamically aggregates the multi-scale texture information extracted
from the images. We validate our approach by carrying out extensive experiments
in the context of low-dose CT denoising, a challenging application that aims to
enhance the quality of noisy CT scans. We utilize three publicly available
datasets, including one simulated and two real datasets. The results are
promising as compared to other well-established loss functions, being also
consistent across three different GAN architectures. The code is available at:
https://github.com/FrancescoDiFeola/DenoTextureLoss
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A comparative analysis of embedding models for patent similarity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16630v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16630v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Grazia Sveva Ascione, Valerio Sterzi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper makes two contributions to the field of text-based patent
similarity. First, it compares the performance of different kinds of
patent-specific pretrained embedding models, namely static word embeddings
(such as word2vec and doc2vec models) and contextual word embeddings (such as
transformers based models), on the task of patent similarity calculation.
Second, it compares specifically the performance of Sentence Transformers
(SBERT) architectures with different training phases on the patent similarity
task. To assess the models' performance, we use information about patent
interferences, a phenomenon in which two or more patent claims belonging to
different patent applications are proven to be overlapping by patent examiners.
Therefore, we use these interferences cases as a proxy for maximum similarity
between two patents, treating them as ground-truth to evaluate the performance
of the different embedding models. Our results point out that, first, Patent
SBERT-adapt-ub, the domain adaptation of the pretrained Sentence Transformer
architecture proposed in this research, outperforms the current
state-of-the-art in patent similarity. Second, they show that, in some cases,
large static models performances are still comparable to contextual ones when
trained on extensive data; thus, we believe that the superiority in the
performance of contextual embeddings may not be related to the actual
architecture but rather to the way the training phase is performed.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Calibrating Bayesian UNet++ for Sub-Seasonal Forecasting <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16612v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16612v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Busra Asan, Abdullah Akgul, Alper Unal, Melih Kandemir, Gozde Unal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Seasonal forecasting is a crucial task when it comes to detecting the extreme
heat and colds that occur due to climate change. Confidence in the predictions
should be reliable since a small increase in the temperatures in a year has a
big impact on the world. Calibration of the neural networks provides a way to
ensure our confidence in the predictions. However, calibrating regression
models is an under-researched topic, especially in forecasters. We calibrate a
UNet++ based architecture, which was shown to outperform physics-based models
in temperature anomalies. We show that with a slight trade-off between
prediction error and calibration error, it is possible to get more reliable and
sharper forecasts. We believe that calibration should be an important part of
safety-critical machine learning applications such as weather forecasters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as a workshop paper at "ICLR 2024 Tackling Climate Change
  with Machine Learning"</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Distributed collaborative anomalous sound detection by embedding sharing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16610v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16610v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kota Dohi, Yohei Kawaguchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To develop a machine sound monitoring system, a method for detecting
anomalous sound is proposed. In this paper, we explore a method for multiple
clients to collaboratively learn an anomalous sound detection model while
keeping their raw data private from each other. In the context of industrial
machine anomalous sound detection, each client possesses data from different
machines or different operational states, making it challenging to learn
through federated learning or split learning. In our proposed method, each
client calculates embeddings using a common pre-trained model developed for
sound data classification, and these calculated embeddings are aggregated on
the server to perform anomalous sound detection through outlier exposure.
Experiments showed that our proposed method improves the AUC of anomalous sound
detection by an average of 6.8%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Industrial Transfer Learning with Style Filter: Cost Reduction
  and Defect-Focus 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16607v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16607v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Li, Ruijie Ma, Xiang Qian, Xiaohao Wang, Xinghui Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Addressing the challenge of data scarcity in industrial domains, transfer
learning emerges as a pivotal paradigm. This work introduces Style Filter, a
tailored methodology for industrial contexts. By selectively filtering source
domain data before knowledge transfer, Style Filter reduces the quantity of
data while maintaining or even enhancing the performance of transfer learning
strategy. Offering label-free operation, minimal reliance on prior knowledge,
independence from specific models, and re-utilization, Style Filter is
evaluated on authentic industrial datasets, highlighting its effectiveness when
employed before conventional transfer strategies in the deep learning domain.
The results underscore the effectiveness of Style Filter in real-world
industrial applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 11 figures,4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EDUE: Expert Disagreement-Guided One-Pass Uncertainty Estimation for
  Medical Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16594v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16594v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kudaibergen Abutalip, Numan Saeed, Ikboljon Sobirov, Vincent Andrearczyk, Adrien Depeursinge, Mohammad Yaqub
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deploying deep learning (DL) models in medical applications relies on
predictive performance and other critical factors, such as conveying
trustworthy predictive uncertainty. Uncertainty estimation (UE) methods provide
potential solutions for evaluating prediction reliability and improving the
model confidence calibration. Despite increasing interest in UE, challenges
persist, such as the need for explicit methods to capture aleatoric uncertainty
and align uncertainty estimates with real-life disagreements among domain
experts. This paper proposes an Expert Disagreement-Guided Uncertainty
Estimation (EDUE) for medical image segmentation. By leveraging variability in
ground-truth annotations from multiple raters, we guide the model during
training and incorporate random sampling-based strategies to enhance
calibration confidence. Our method achieves 55% and 23% improvement in
correlation on average with expert disagreements at the image and pixel levels,
respectively, better calibration, and competitive segmentation performance
compared to the state-of-the-art deep ensembles, requiring only a single
forward pass.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deciphering the Interplay between Local Differential Privacy, Average
  Bayesian Privacy, and Maximum Bayesian Privacy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16591v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16591v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaojin Zhang, Yulin Fei, Wei Chen, Hai Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The swift evolution of machine learning has led to emergence of various
definitions of privacy due to the threats it poses to privacy, including the
concept of local differential privacy (LDP). Although widely embraced and
utilized across numerous domains, this conventional approach to measure privacy
still exhibits certain limitations, spanning from failure to prevent
inferential disclosure to lack of consideration for the adversary's background
knowledge. In this comprehensive study, we introduce Bayesian privacy and delve
into the intricate relationship between local differential privacy and its
Bayesian counterparts, unveiling novel insights into utility-privacy
trade-offs. We introduce a framework that encapsulates both attack and defense
strategies, highlighting their interplay and effectiveness. Our theoretical
contributions are anchored in the rigorous definitions and relationships
between Average Bayesian Privacy (ABP) and Maximum Bayesian Privacy (MBP),
encapsulated by equations $\epsilon_{p,a} \leq
\frac{1}{\sqrt{2}}\sqrt{(\epsilon_{p,m} + \epsilon)\cdot(e^{\epsilon_{p,m} +
\epsilon} - 1)}$ and the equivalence between $\xi$-MBP and $2\xi$-LDP
established under uniform prior distribution. These relationships fortify our
understanding of the privacy guarantees provided by various mechanisms, leading
to the realization that a mechanism satisfying $\xi$-LDP also confers
$\xi$-MBP, and vice versa. Our work not only lays the groundwork for future
empirical exploration but also promises to enhance the design of
privacy-preserving algorithms that do not compromise on utility, thereby
fostering the development of trustworthy machine learning solutions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ In the Search for Optimal Multi-view Learning Models for Crop
  Classification with Global Remote Sensing Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16582v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16582v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francisco Mena, Diego Arenas, Andreas Dengel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Crop classification is of critical importance due to its role in studying
crop pattern changes, resource management, and carbon sequestration. When
employing data-driven techniques for its prediction, utilizing various temporal
data sources is necessary. Deep learning models have proven to be effective for
this task by mapping time series data to high-level representation for
prediction. However, they face substantial challenges when dealing with
multiple input patterns. The literature offers limited guidance for Multi-View
Learning (MVL) scenarios, as it has primarily focused on exploring fusion
strategies with specific encoders and validating them in local regions. In
contrast, we investigate the impact of simultaneous selection of the fusion
strategy and the encoder architecture evaluated on a global-scale cropland and
crop-type classifications. We use a range of five fusion strategies (Input,
Feature, Decision, Ensemble, Hybrid) and five temporal encoder architectures
(LSTM, GRU, TempCNN, TAE, L-TAE) as possible MVL model configurations. The
validation is on the CropHarvest dataset that provides optical, radar, and
weather time series, and topographic information as input data. We found that
in scenarios with a limited number of labeled samples, a unique configuration
is insufficient for all the cases. Instead, a specialized combination,
including encoder and fusion strategy, should be meticulously sought. To
streamline this search process, we suggest initially identifying the optimal
encoder architecture tailored for a particular fusion strategy, and then
determining the most suitable fusion strategy for the classification task. We
provide a technical framework for researchers exploring crop classification or
related tasks through a MVL approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Antigen-Specific Antibody Design via Direct Energy-based Preference
  Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16576v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16576v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiangxin Zhou, Dongyu Xue, Ruizhe Chen, Zaixiang Zheng, Liang Wang, Quanquan Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Antibody design, a crucial task with significant implications across various
disciplines such as therapeutics and biology, presents considerable challenges
due to its intricate nature. In this paper, we tackle antigen-specific antibody
design as a protein sequence-structure co-design problem, considering both
rationality and functionality. Leveraging a pre-trained conditional diffusion
model that jointly models sequences and structures of
complementarity-determining regions (CDR) in antibodies with equivariant neural
networks, we propose direct energy-based preference optimization to guide the
generation of antibodies with both rational structures and considerable binding
affinities to given antigens. Our method involves fine-tuning the pre-trained
diffusion model using a residue-level decomposed energy preference.
Additionally, we employ gradient surgery to address conflicts between various
types of energy, such as attraction and repulsion. Experiments on RAbD
benchmark show that our approach effectively optimizes the energy of generated
antibodies and achieves state-of-the-art performance in designing high-quality
antibodies with low total energy and high binding affinity, demonstrating the
superiority of our approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NSINA: A News Corpus for Sinhala <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16571v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16571v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hansi Hettiarachchi, Damith Premasiri, Lasitha Uyangodage, Tharindu Ranasinghe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The introduction of large language models (LLMs) has advanced natural
language processing (NLP), but their effectiveness is largely dependent on
pre-training resources. This is especially evident in low-resource languages,
such as Sinhala, which face two primary challenges: the lack of substantial
training data and limited benchmarking datasets. In response, this study
introduces NSINA, a comprehensive news corpus of over 500,000 articles from
popular Sinhala news websites, along with three NLP tasks: news media
identification, news category prediction, and news headline generation. The
release of NSINA aims to provide a solution to challenges in adapting LLMs to
Sinhala, offering valuable resources and benchmarks for improving NLP in the
Sinhala language. NSINA is the largest news corpus for Sinhala, available up to
date.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to LREC-COLING 2024 (The 2024 Joint International Conference
  on Computational Linguistics, Language Resources and Evaluation)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Revealing Vulnerabilities of Neural Networks in Parameter Learning and
  Defense Against Explanation-Aware Backdoors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16569v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16569v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Abdul Kadir, GowthamKrishna Addluri, Daniel Sonntag
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Explainable Artificial Intelligence (XAI) strategies play a crucial part in
increasing the understanding and trustworthiness of neural networks.
Nonetheless, these techniques could potentially generate misleading
explanations. Blinding attacks can drastically alter a machine learning
algorithm's prediction and explanation, providing misleading information by
adding visually unnoticeable artifacts into the input, while maintaining the
model's accuracy. It poses a serious challenge in ensuring the reliability of
XAI methods. To ensure the reliability of XAI methods poses a real challenge,
we leverage statistical analysis to highlight the changes in CNN weights within
a CNN following blinding attacks. We introduce a method specifically designed
to limit the effectiveness of such attacks during the evaluation phase,
avoiding the need for extra training. The method we suggest defences against
most modern explanation-aware adversarial attacks, achieving an approximate
decrease of ~99\% in the Attack Success Rate (ASR) and a ~91\% reduction in the
Mean Square Error (MSE) between the original explanation and the defended
(post-attack) explanation across three unique types of attacks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FedFixer: Mitigating Heterogeneous Label Noise in Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16561v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16561v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyuan Ji, Zhaowei Zhu, Wei Xi, Olga Gadyatskaya, Zilong Song, Yong Cai, Yang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) heavily depends on label quality for its performance.
However, the label distribution among individual clients is always both noisy
and heterogeneous. The high loss incurred by client-specific samples in
heterogeneous label noise poses challenges for distinguishing between
client-specific and noisy label samples, impacting the effectiveness of
existing label noise learning approaches. To tackle this issue, we propose
FedFixer, where the personalized model is introduced to cooperate with the
global model to effectively select clean client-specific samples. In the dual
models, updating the personalized model solely at a local level can lead to
overfitting on noisy data due to limited samples, consequently affecting both
the local and global models' performance. To mitigate overfitting, we address
this concern from two perspectives. Firstly, we employ a confidence regularizer
to alleviate the impact of unconfident predictions caused by label noise.
Secondly, a distance regularizer is implemented to constrain the disparity
between the personalized and global models. We validate the effectiveness of
FedFixer through extensive experiments on benchmark datasets. The results
demonstrate that FedFixer can perform well in filtering noisy label samples on
different clients, especially in highly heterogeneous label noise scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by AAA24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Accelerating Federated Learning by Selecting Beneficial Herd of Local
  Gradients 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16557v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16557v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ping Luo, Xiaoge Deng, Ziqing Wen, Tao Sun, Dongsheng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) is a distributed machine learning framework in
communication network systems. However, the systems' Non-Independent and
Identically Distributed (Non-IID) data negatively affect the convergence
efficiency of the global model, since only a subset of these data samples are
beneficial for model convergence. In pursuit of this subset, a reliable
approach involves determining a measure of validity to rank the samples within
the dataset. In this paper, We propose the BHerd strategy which selects a
beneficial herd of local gradients to accelerate the convergence of the FL
model. Specifically, we map the distribution of the local dataset to the local
gradients and use the Herding strategy to obtain a permutation of the set of
gradients, where the more advanced gradients in the permutation are closer to
the average of the set of gradients. These top portion of the gradients will be
selected and sent to the server for global aggregation. We conduct experiments
on different datasets, models and scenarios by building a prototype system, and
experimental results demonstrate that our BHerd strategy is effective in
selecting beneficial local gradients to mitigate the effects brought by the
Non-IID dataset, thus accelerating model convergence.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Differentially Private Online Federated Learning with Correlated Noise 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16542v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16542v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaojiao Zhang, Linglingzhi Zhu, Mikael Johansson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel differentially private algorithm for online federated
learning that employs temporally correlated noise to improve the utility while
ensuring the privacy of the continuously released models. To address challenges
stemming from DP noise and local updates with streaming noniid data, we develop
a perturbed iterate analysis to control the impact of the DP noise on the
utility. Moreover, we demonstrate how the drift errors from local updates can
be effectively managed under a quasi-strong convexity condition. Subject to an
$(\epsilon, \delta)$-DP budget, we establish a dynamic regret bound over the
entire time horizon that quantifies the impact of key parameters and the
intensity of changes in dynamic environments. Numerical experiments validate
the efficacy of the proposed algorithm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Causal Discovery from Poisson Branching Structural Causal Model Using
  High-Order Cumulant with Path Analysis <span class="chip">AAAI-2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16523v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16523v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jie Qiao, Yu Xiang, Zhengming Chen, Ruichu Cai, Zhifeng Hao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Count data naturally arise in many fields, such as finance, neuroscience, and
epidemiology, and discovering causal structure among count data is a crucial
task in various scientific and industrial scenarios. One of the most common
characteristics of count data is the inherent branching structure described by
a binomial thinning operator and an independent Poisson distribution that
captures both branching and noise. For instance, in a population count
scenario, mortality and immigration contribute to the count, where survival
follows a Bernoulli distribution, and immigration follows a Poisson
distribution. However, causal discovery from such data is challenging due to
the non-identifiability issue: a single causal pair is Markov equivalent, i.e.,
$X\rightarrow Y$ and $Y\rightarrow X$ are distributed equivalent. Fortunately,
in this work, we found that the causal order from $X$ to its child $Y$ is
identifiable if $X$ is a root vertex and has at least two directed paths to
$Y$, or the ancestor of $X$ with the most directed path to $X$ has a directed
path to $Y$ without passing $X$. Specifically, we propose a Poisson Branching
Structure Causal Model (PB-SCM) and perform a path analysis on PB-SCM using
high-order cumulants. Theoretical results establish the connection between the
path and cumulant and demonstrate that the path information can be obtained
from the cumulant. With the path information, causal order is identifiable
under some graphical conditions. A practical algorithm for learning causal
structure under PB-SCM is proposed and the experiments demonstrate and verify
the effectiveness of the proposed method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI-2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Human Understanding AI Paper Challenge 2024 -- <span class="highlight-title">Dataset</span> Design 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16509v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16509v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Se Won Oh, Hyuntae Jeong, Jeong Mook Lim, Seungeun Chung, Kyoung Ju Noh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In 2024, we will hold a research paper competition (the third Human
Understanding AI Paper Challenge) for the research and development of
artificial intelligence technologies to understand human daily life. This
document introduces the datasets that will be provided to participants in the
competition, and summarizes the issues to consider in data processing and
learning model development.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PathoTune: Adapting Visual Foundation Model to Pathological Specialists <span class="chip">MICCAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16497v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16497v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaxuan Lu, Fang Yan, Xiaofan Zhang, Yue Gao, Shaoting Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As natural image understanding moves towards the pretrain-finetune era,
research in pathology imaging is concurrently evolving. Despite the predominant
focus on pretraining pathological foundation models, how to adapt foundation
models to downstream tasks is little explored. For downstream adaptation, we
propose the existence of two domain gaps, i.e., the Foundation-Task Gap and the
Task-Instance Gap. To mitigate these gaps, we introduce PathoTune, a framework
designed to efficiently adapt pathological or even visual foundation models to
pathology-specific tasks via multi-modal prompt tuning. The proposed framework
leverages Task-specific Visual Prompts and Task-specific Textual Prompts to
identify task-relevant features, along with Instance-specific Visual Prompts
for encoding single pathological image features. Results across multiple
datasets at both patch-level and WSI-level demonstrate its superior performance
over single-modality prompt tuning approaches. Significantly, PathoTune
facilitates the direct adaptation of natural visual foundation models to
pathological tasks, drastically outperforming pathological foundation models
with simple linear probing. The code will be available upon acceptance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to MICCAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LSTTN: A Long-Short Term <span class="highlight-title">Transformer</span>-based Spatio-temporal Neural
  Network for Traffic Flow Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16495v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16495v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qinyao Luo, Silu He, Xing Han, Yuhan Wang, Haifeng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate traffic forecasting is a fundamental problem in intelligent
transportation systems and learning long-range traffic representations with key
information through spatiotemporal graph neural networks (STGNNs) is a basic
assumption of current traffic flow prediction models. However, due to
structural limitations, existing STGNNs can only utilize short-range traffic
flow data; therefore, the models cannot adequately learn the complex trends and
periodic features in traffic flow. Besides, it is challenging to extract the
key temporal information from the long historical traffic series and obtain a
compact representation. To solve the above problems, we propose a novel LSTTN
(Long-Short Term Transformer-based Network) framework comprehensively
considering the long- and short-term features in historical traffic flow.
First, we employ a masked subseries Transformer to infer the content of masked
subseries from a small portion of unmasked subseries and their temporal context
in a pretraining manner, forcing the model to efficiently learn compressed and
contextual subseries temporal representations from long historical series.
Then, based on the learned representations, long-term trend is extracted by
using stacked 1D dilated convolution layers, and periodic features are
extracted by dynamic graph convolution layers. For the difficulties in making
time-step level prediction, LSTTN adopts a short-term trend extractor to learn
fine-grained short-term temporal features. Finally, LSTTN fuses the long-term
trend, periodic features and short-term features to obtain the prediction
results. Experiments on four real-world datasets show that in 60-minute-ahead
long-term forecasting, the LSTTN model achieves a minimum improvement of 5.63\%
and a maximum improvement of 16.78\% over baseline models. The source code is
available at https://github.com/GeoX-Lab/LSTTN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 10 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Determined Multi-Label Learning via Similarity-Based <span class="highlight-title">Prompt</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16482v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16482v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Meng Wei, Zhongnian Li, Peng Ying, Yong Zhou, Xinzheng Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In multi-label classification, each training instance is associated with
multiple class labels simultaneously. Unfortunately, collecting the fully
precise class labels for each training instance is time- and labor-consuming
for real-world applications. To alleviate this problem, a novel labeling
setting termed \textit{Determined Multi-Label Learning} (DMLL) is proposed,
aiming to effectively alleviate the labeling cost inherent in multi-label
tasks. In this novel labeling setting, each training instance is associated
with a \textit{determined label} (either "Yes" or "No"), which indicates
whether the training instance contains the provided class label. The provided
class label is randomly and uniformly selected from the whole candidate labels
set. Besides, each training instance only need to be determined once, which
significantly reduce the annotation cost of the labeling task for multi-label
datasets. In this paper, we theoretically derive an risk-consistent estimator
to learn a multi-label classifier from these determined-labeled training data.
Additionally, we introduce a similarity-based prompt learning method for the
first time, which minimizes the risk-consistent loss of large-scale pre-trained
models to learn a supplemental prompt with richer semantic information.
Extensive experimental validation underscores the efficacy of our approach,
demonstrating superior performance compared to existing state-of-the-art
methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning from Reduced Labels for Long-Tailed Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16469v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16469v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Meng Wei, Zhongnian Li, Yong Zhou, Xinzheng Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Long-tailed data is prevalent in real-world classification tasks and heavily
relies on supervised information, which makes the annotation process
exceptionally labor-intensive and time-consuming. Unfortunately, despite being
a common approach to mitigate labeling costs, existing weakly supervised
learning methods struggle to adequately preserve supervised information for
tail samples, resulting in a decline in accuracy for the tail classes. To
alleviate this problem, we introduce a novel weakly supervised labeling setting
called Reduced Label. The proposed labeling setting not only avoids the decline
of supervised information for the tail samples, but also decreases the labeling
costs associated with long-tailed data. Additionally, we propose an
straightforward and highly efficient unbiased framework with strong theoretical
guarantees to learn from these Reduced Labels. Extensive experiments conducted
on benchmark datasets including ImageNet validate the effectiveness of our
approach, surpassing the performance of state-of-the-art weakly supervised
methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Training Generative Adversarial Network-Based Vocoder with Limited Data
  Using Augmentation-Conditional Discriminator <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16464v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16464v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Takuhiro Kaneko, Hirokazu Kameoka, Kou Tanaka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A generative adversarial network (GAN)-based vocoder trained with an
adversarial discriminator is commonly used for speech synthesis because of its
fast, lightweight, and high-quality characteristics. However, this data-driven
model requires a large amount of training data incurring high data-collection
costs. This fact motivates us to train a GAN-based vocoder on limited data. A
promising solution is to augment the training data to avoid overfitting.
However, a standard discriminator is unconditional and insensitive to
distributional changes caused by data augmentation. Thus, augmented speech
(which can be extraordinary) may be considered real speech. To address this
issue, we propose an augmentation-conditional discriminator (AugCondD) that
receives the augmentation state as input in addition to speech, thereby
assessing the input speech according to the augmentation state, without
inhibiting the learning of the original non-augmented distribution.
Experimental results indicate that AugCondD improves speech quality under
limited data conditions while achieving comparable speech quality under
sufficient data conditions. Audio samples are available at
https://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/augcondd/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICASSP 2024. Project page:
  https://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/augcondd/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FedAC: A Adaptive Clustered Federated Learning Framework for
  Heterogeneous Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16460v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16460v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxin Zhang, Haoyu Chen, Zheng Lin, Zhe Chen, Jin Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Clustered federated learning (CFL) is proposed to mitigate the performance
deterioration stemming from data heterogeneity in federated learning (FL) by
grouping similar clients for cluster-wise model training. However, current CFL
methods struggle due to inadequate integration of global and intra-cluster
knowledge and the absence of an efficient online model similarity metric, while
treating the cluster count as a fixed hyperparameter limits flexibility and
robustness. In this paper, we propose an adaptive CFL framework, named FedAC,
which (1) efficiently integrates global knowledge into intra-cluster learning
by decoupling neural networks and utilizing distinct aggregation methods for
each submodule, significantly enhancing performance; (2) includes a
costeffective online model similarity metric based on dimensionality reduction;
(3) incorporates a cluster number fine-tuning module for improved adaptability
and scalability in complex, heterogeneous environments. Extensive experiments
show that FedAC achieves superior empirical performance, increasing the test
accuracy by around 1.82% and 12.67% on CIFAR-10 and CIFAR-100 datasets,
respectively, under different non-IID settings compared to SOTA methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the rates of convergence for learning with convolutional neural
  networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16459v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16459v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunfei Yang, Han Feng, Ding-Xuan Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the approximation and learning capacities of convolutional neural
networks (CNNs). Our first result proves a new approximation bound for CNNs
with certain constraint on the weights. Our second result gives a new analysis
on the covering number of feed-forward neural networks, which include CNNs as
special cases. The analysis carefully takes into account the size of the
weights and hence gives better bounds than existing literature in some
situations. Using these two results, we are able to derive rates of convergence
for estimators based on CNNs in many learning problems. In particular, we
establish minimax optimal convergence rates of the least squares based on CNNs
for learning smooth functions in the nonparametric regression setting. For
binary classification, we derive convergence rates for CNN classifiers with
hinge loss and logistic loss. It is also shown that the obtained rates are
minimax optimal in several settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DeepMachining: Online Prediction of Machining Errors of Lathe Machines 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16451v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16451v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang-Li Lu, Hwai-Jung Hsu, Che-Wei Chou, H. T. Kung, Chen-Hsin Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We describe DeepMachining, a deep learning-based AI system for online
prediction of machining errors of lathe machine operations. We have built and
evaluated DeepMachining based on manufacturing data from factories.
Specifically, we first pretrain a deep learning model for a given lathe
machine's operations to learn the salient features of machining states. Then,
we fine-tune the pretrained model to adapt to specific machining tasks. We
demonstrate that DeepMachining achieves high prediction accuracy for multiple
tasks that involve different workpieces and cutting tools. To the best of our
knowledge, this work is one of the first factory experiments using pre-trained
deep-learning models to predict machining errors of lathe machines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BioNeRF: Biologically Plausible Neural Radiance Fields for View
  Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.07310v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.07310v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leandro A. Passos, Douglas Rodrigues, Danilo Jodas, Kelton A. P. Costa, Ahsan Adeel, João Paulo Papa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents BioNeRF, a biologically plausible architecture that
models scenes in a 3D representation and synthesizes new views through radiance
fields. Since NeRF relies on the network weights to store the scene's
3-dimensional representation, BioNeRF implements a cognitive-inspired mechanism
that fuses inputs from multiple sources into a memory-like structure, improving
the storing capacity and extracting more intrinsic and correlated information.
BioNeRF also mimics a behavior observed in pyramidal cells concerning
contextual information, in which the memory is provided as the context and
combined with the inputs of two subsequent neural models, one responsible for
producing the volumetric densities and the other the colors used to render the
scene. Experimental results show that BioNeRF outperforms state-of-the-art
results concerning a quality measure that encodes human perception in two
datasets: real-world images and synthetic data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LOCOST: State-Space Models for Long Document Abstractive Summarization <span class="chip">EACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.17919v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.17919v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florian Le Bronnec, Song Duong, Mathieu Ravaut, Alexandre Allauzen, Nancy F. Chen, Vincent Guigue, Alberto Lumbreras, Laure Soulier, Patrick Gallinari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-space models are a low-complexity alternative to transformers for
encoding long sequences and capturing long-term dependencies. We propose
LOCOST: an encoder-decoder architecture based on state-space models for
conditional text generation with long context inputs. With a computational
complexity of $O(L \log L)$, this architecture can handle significantly longer
sequences than state-of-the-art models that are based on sparse attention
patterns. We evaluate our model on a series of long document abstractive
summarization tasks. The model reaches a performance level that is 93-96%
comparable to the top-performing sparse transformers of the same size while
saving up to 50% memory during training and up to 87% during inference.
Additionally, LOCOST effectively handles input texts exceeding 600K tokens at
inference time, setting new state-of-the-art results on full-book summarization
and opening new perspectives for long input processing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 5 figures, 7 tables, EACL 2024 conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Second Look on BASS -- Boosting Abstractive Summarization with Unified
  Semantic Graphs -- A Replication Study <span class="chip">ECIR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.02930v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.02930v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Osman Alperen Koraş, Jörg Schlötterer, Christin Seifert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a detailed replication study of the BASS framework, an abstractive
summarization system based on the notion of Unified Semantic Graphs. Our
investigation includes challenges in replicating key components and an ablation
study to systematically isolate error sources rooted in replicating novel
components. Our findings reveal discrepancies in performance compared to the
original work. We highlight the significance of paying careful attention even
to reasonably omitted details for replicating advanced frameworks like BASS,
and emphasize key practices for writing replicable papers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This preprint has not undergone peer review or any post-submission
  improvements or corrections. The Version of Record of this contribution is
  published in Advances in Information Retrieval, 46th European Conference on
  Information Retrieval, ECIR 2024. 16 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Analysis of Linear Time Series Forecasting Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14587v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14587v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        William Toner, Luke Darlow
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite their simplicity, linear models perform well at time series
forecasting, even when pitted against deeper and more expensive models. A
number of variations to the linear model have been proposed, often including
some form of feature normalisation that improves model generalisation. In this
paper we analyse the sets of functions expressible using these linear model
architectures. In so doing we show that several popular variants of linear
models for time series forecasting are equivalent and functionally
indistinguishable from standard, unconstrained linear regression. We
characterise the model classes for each linear variant. We demonstrate that
each model can be reinterpreted as unconstrained linear regression over a
suitably augmented feature set, and therefore admit closed-form solutions when
using a mean-squared loss function. We provide experimental evidence that the
models under inspection learn nearly identical solutions, and finally
demonstrate that the simpler closed form solutions are superior forecasters
across 72% of test settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Spatio-Temporal Few-Shot Learning via Diffusive Neural Network
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.11922v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.11922v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuan Yuan, Chenyang Shao, Jingtao Ding, Depeng Jin, Yong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spatio-temporal modeling is foundational for smart city applications, yet it
is often hindered by data scarcity in many cities and regions. To bridge this
gap, we propose a novel generative pre-training framework, GPD, for
spatio-temporal few-shot learning with urban knowledge transfer. Unlike
conventional approaches that heavily rely on common feature extraction or
intricate few-shot learning designs, our solution takes a novel approach by
performing generative pre-training on a collection of neural network parameters
optimized with data from source cities. We recast spatio-temporal few-shot
learning as pre-training a generative diffusion model, which generates tailored
neural networks guided by prompts, allowing for adaptability to diverse data
distributions and city-specific characteristics. GPD employs a
Transformer-based denoising diffusion model, which is model-agnostic to
integrate with powerful spatio-temporal neural networks. By addressing
challenges arising from data gaps and the complexity of generalizing knowledge
across cities, our framework consistently outperforms state-of-the-art
baselines on multiple real-world datasets for tasks such as traffic speed
prediction and crowd flow prediction. The implementation of our approach is
available: https://github.com/tsinghua-fib-lab/GPD.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving the forecast accuracy of wind power by leveraging multiple
  hierarchical structure 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.03472v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.03472v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lucas English, Mahdi Abolghasemi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Renewable energy generation is of utmost importance for global
decarbonization. Forecasting renewable energies, particularly wind energy, is
challenging due to the inherent uncertainty in wind energy generation, which
depends on weather conditions. Recent advances in hierarchical forecasting
through reconciliation have demonstrated a significant increase in the quality
of wind energy forecasts for short-term periods. We leverage the
cross-sectional and temporal hierarchical structure of turbines in wind farms
and build cross-temporal hierarchies to further investigate how integrated
cross-sectional and temporal dimensions can add value to forecast accuracy in
wind farms. We found that cross-temporal reconciliation was superior to
individual cross-sectional reconciliation at multiple temporal aggregations.
Additionally, machine learning based forecasts that were cross-temporally
reconciled demonstrated high accuracy at coarser temporal granularities, which
may encourage adoption for short-term wind forecasts. Empirically, we provide
insights for decision-makers on the best methods for forecasting high-frequency
wind data across different forecasting horizons and levels.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>41 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LightIt: Illumination Modeling and Control for Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10615v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10615v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peter Kocsis, Julien Philip, Kalyan Sunkavalli, Matthias Nießner, Yannick Hold-Geoffroy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce LightIt, a method for explicit illumination control for image
generation. Recent generative methods lack lighting control, which is crucial
to numerous artistic aspects of image generation such as setting the overall
mood or cinematic appearance. To overcome these limitations, we propose to
condition the generation on shading and normal maps. We model the lighting with
single bounce shading, which includes cast shadows. We first train a shading
estimation module to generate a dataset of real-world images and shading pairs.
Then, we train a control network using the estimated shading and normals as
input. Our method demonstrates high-quality image generation and lighting
control in numerous scenes. Additionally, we use our generated dataset to train
an identity-preserving relighting model, conditioned on an image and a target
shading. Our method is the first that enables the generation of images with
controllable, consistent lighting and performs on par with specialized
relighting state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://peter-kocsis.github.io/LightIt/ Video:
  https://youtu.be/cCfSBD5aPLI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BatteryML:An Open-source platform for Machine Learning on Battery
  Degradation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.14714v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.14714v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Zhang, Xiaofan Gui, Shun Zheng, Ziheng Lu, Yuqi Li, Jiang Bian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Battery degradation remains a pivotal concern in the energy storage domain,
with machine learning emerging as a potent tool to drive forward insights and
solutions. However, this intersection of electrochemical science and machine
learning poses complex challenges. Machine learning experts often grapple with
the intricacies of battery science, while battery researchers face hurdles in
adapting intricate models tailored to specific datasets. Beyond this, a
cohesive standard for battery degradation modeling, inclusive of data formats
and evaluative benchmarks, is conspicuously absent. Recognizing these
impediments, we present BatteryML - a one-step, all-encompass, and open-source
platform designed to unify data preprocessing, feature extraction, and the
implementation of both traditional and state-of-the-art models. This
streamlined approach promises to enhance the practicality and efficiency of
research applications. BatteryML seeks to fill this void, fostering an
environment where experts from diverse specializations can collaboratively
contribute, thus elevating the collective understanding and advancement of
battery research.The code for our project is publicly available on GitHub at
https://github.com/microsoft/BatteryML.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Causal Question Answering with Reinforcement Learning <span class="chip">WWW 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.02760v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.02760v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lukas Blübaum, Stefan Heindorf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Causal questions inquire about causal relationships between different events
or phenomena. They are important for a variety of use cases, including virtual
assistants and search engines. However, many current approaches to causal
question answering cannot provide explanations or evidence for their answers.
Hence, in this paper, we aim to answer causal questions with a causality graph,
a large-scale dataset of causal relations between noun phrases along with the
relations' provenance data. Inspired by recent, successful applications of
reinforcement learning to knowledge graph tasks, such as link prediction and
fact-checking, we explore the application of reinforcement learning on a
causality graph for causal question answering. We introduce an
Actor-Critic-based agent which learns to search through the graph to answer
causal questions. We bootstrap the agent with a supervised learning procedure
to deal with large action spaces and sparse rewards. Our evaluation shows that
the agent successfully prunes the search space to answer binary causal
questions by visiting less than 30 nodes per question compared to over 3,000
nodes by a naive breadth-first search. Our ablation study indicates that our
supervised learning strategy provides a strong foundation upon which our
reinforcement learning agent improves. The paths returned by our agent explain
the mechanisms by which a cause produces an effect. Moreover, for each edge on
a path, our causality graph provides its original source allowing for easy
verification of paths.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at WWW 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring the Adversarial Capabilities of Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.09132v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.09132v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lukas Struppek, Minh Hieu Le, Dominik Hintersdorf, Kristian Kersting
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The proliferation of large language models (LLMs) has sparked widespread and
general interest due to their strong language generation capabilities, offering
great potential for both industry and research. While previous research delved
into the security and privacy issues of LLMs, the extent to which these models
can exhibit adversarial behavior remains largely unexplored. Addressing this
gap, we investigate whether common publicly available LLMs have inherent
capabilities to perturb text samples to fool safety measures, so-called
adversarial examples resp.~attacks. More specifically, we investigate whether
LLMs are inherently able to craft adversarial examples out of benign samples to
fool existing safe rails. Our experiments, which focus on hate speech
detection, reveal that LLMs succeed in finding adversarial perturbations,
effectively undermining hate speech detection systems. Our findings carry
significant implications for (semi-)autonomous systems relying on LLMs,
highlighting potential challenges in their interaction with existing systems
and safety measures.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the resilience of Collaborative Learning-based Recommender Systems
  Against Community Detection Attack 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.08929v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.08929v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yacine Belal, Sonia Ben Mokhtar, Mohamed Maouche, Anthony Simonet-Boulogne
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Collaborative-learning-based recommender systems emerged following the
success of collaborative learning techniques such as Federated Learning (FL)
and Gossip Learning (GL). In these systems, users participate in the training
of a recommender system while maintaining their history of consumed items on
their devices. While these solutions seemed appealing for preserving the
privacy of the participants at first glance, recent studies have revealed that
collaborative learning can be vulnerable to various privacy attacks. In this
paper, we study the resilience of collaborative learning-based recommender
systems against a novel privacy attack called Community Detection Attack (CDA).
This attack enables an adversary to identify community members based on a
chosen set of items (eg., identifying users interested in specific
points-of-interest). Through experiments on three real recommendation datasets
using two state-of-the-art recommendation models, we evaluate the sensitivity
of an FL-based recommender system as well as two flavors of Gossip
Learning-based recommender systems to CDA. The results show that across all
models and datasets, the FL setting is more vulnerable to CDA compared to
Gossip settings. Furthermore, we assess two off-the-shelf mitigation
strategies, namely differential privacy (DP) and a \emph{Share less} policy,
which consists of sharing a subset of less sensitive model parameters. The
findings indicate a more favorable privacy-utility trade-off for the
\emph{Share less} strategy, particularly in FedRecs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-agent reinforcement learning using echo-state network and its
  application to pedestrian dynamics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.11834v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.11834v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hisato Komatsu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, simulations of pedestrians using the multi-agent
reinforcement learning (MARL) have been studied. This study considered the
roads on a grid-world environment, and implemented pedestrians as MARL agents
using an echo-state network and the least squares policy iteration method.
Under this environment, the ability of these agents to learn to move forward by
avoiding other agents was investigated. Specifically, we considered two types
of tasks: the choice between a narrow direct route and a broad detour, and the
bidirectional pedestrian flow in a corridor. The simulations results indicated
that the learning was successful when the density of the agents was not that
high.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 17 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Distributionally Generative Augmentation for Fair Facial Attribute
  Classification <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06606v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06606v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fengda Zhang, Qianpei He, Kun Kuang, Jiashuo Liu, Long Chen, Chao Wu, Jun Xiao, Hanwang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Facial Attribute Classification (FAC) holds substantial promise in widespread
applications. However, FAC models trained by traditional methodologies can be
unfair by exhibiting accuracy inconsistencies across varied data
subpopulations. This unfairness is largely attributed to bias in data, where
some spurious attributes (e.g., Male) statistically correlate with the target
attribute (e.g., Smiling). Most of existing fairness-aware methods rely on the
labels of spurious attributes, which may be unavailable in practice. This work
proposes a novel, generation-based two-stage framework to train a fair FAC
model on biased data without additional annotation. Initially, we identify the
potential spurious attributes based on generative models. Notably, it enhances
interpretability by explicitly showing the spurious attributes in image space.
Following this, for each image, we first edit the spurious attributes with a
random degree sampled from a uniform distribution, while keeping target
attribute unchanged. Then we train a fair FAC model by fostering model
invariance to these augmentation. Extensive experiments on three common
datasets demonstrate the effectiveness of our method in promoting fairness in
FAC without compromising accuracy. Codes are in
https://github.com/heqianpei/DiGA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Preference as Reward, Maximum Preference Optimization with Importance
  Sampling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.16430v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.16430v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zaifan Jiang, Xing Huang, Chao Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Preference learning is a key technology for aligning language models with
human values. Reinforcement Learning from Human Feedback (RLHF) is a
model-based algorithm to optimize preference learning, which first fits a
reward model for preference scores and then optimizes the generating policy
with an on-policy PPO algorithm to maximize the reward. The processing of RLHF
is complex, time-consuming, and unstable. The Direct Preference Optimization
(DPO) algorithm uses an off-policy algorithm to directly optimize the
generating policy and eliminates the need for a reward model. DPO is more
data-efficient and stable. However, DPO has a drawback of overfitting to the
preference data and ignoring the KL-regularization term when the preference is
deterministic. Identity mapping Preference Optimization(IPO) uses a
root-finding MSE loss to incorporate KL-regularization. However, both DPO and
IPO fail to properly address the KL-regularization term because the support of
the preference distribution is not equal to the reference distribution. In this
paper, we propose a simple and intuitive off-policy preference optimization
algorithm from an importance sampling view, which we call Maximum Preference
Optimization (MPO). MPO incorporates the off-policy KL-regularization term,
making regularization truly effective. MPO achieves the best of both worlds by
combining the objectives of RLHF and IPO while being an off-policy algorithm.
Furthermore, MPO eliminates the need for a reward model and reference policy,
simplifying the learning process and reducing memory usage.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Data driven modeling for self-similar dynamics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08282v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08282v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruyi Tao, Ningning Tao, Yi-zhuang You, Jiang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multiscale modeling of complex systems is crucial for understanding their
intricacies. Data-driven multiscale modeling has emerged as a promising
approach to tackle challenges associated with complex systems. On the other
hand, self-similarity is prevalent in complex systems, hinting that large-scale
complex systems can be modeled at a reduced cost. In this paper, we introduce a
multiscale neural network framework that incorporates self-similarity as prior
knowledge, facilitating the modeling of self-similar dynamical systems. For
deterministic dynamics, our framework can discern whether the dynamics are
self-similar. For uncertain dynamics, it can compare and determine which
parameter set is closer to self-similarity. The framework allows us to extract
scale-invariant kernels from the dynamics for modeling at any scale. Moreover,
our method can identify the power law exponents in self-similar systems.
Preliminary tests on the Ising model yielded critical exponents consistent with
theoretical expectations, providing valuable insights for addressing critical
phase transitions in non-equilibrium systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages,7 figures,1 table</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Science and Game Theory
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A new social welfare function with a number of desirable properties 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16373v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16373v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fujun Hou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  By relaxing the dominating set in three ways (e.g., from "each member beats
every non-member" to "each member beats or ties every non-member, with an
additional requirement that at least one member beat every non-member"), we
propose a new social welfare function, which satisfies a number of desirable
properties including Condorcet winner principle, Condorcet loser principle,
strong Gehrlein-stability (hence Smith set principle), anonymity, neutrality,
weak Pareto, strong Pareto, non-dictatorship, and [independence of irrelevant
alternatives (IIA) when the pairwise majority relation is an ordering on the
alternative set]. If the pairwise majority relation is complete and transitive,
the proposed method yields a collective preference relation that coincides with
the input majority relation. It thus shares the same collective preference
function on the dichotomous domain with the approval voting and the majority
voting. It runs in polynomial time and thus possesses a competitive advantage
over a number of computationally intractable voting rules such as the Dodgson's
rule, the Kemeny's rule, the Slater's rule, the Banks rule, and the Schwartz's
tournament equilibrium set (TEQ) rule. When it is used in tournaments, its
winner belongs to the uncovered set, the top cycle set, the Smith set, and the
Schwartz set. In addition, in a tournament where the number of alternatives is
not more than 4, its winner set is a subset, sometimes proper, of the Copeland
winner set. Whether this attractive argument is still valid in
four-more-alternative tournaments remains an open question.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A new social choice function (and a corresponding social welfare
  function) is proposed. It has a number of desirable properties. An open
  question is also posed</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Equilibrium Analysis of the Arad-Rubinstein Game 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17139v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17139v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christian Ewerhart, Stanisław Kaźmierowski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Colonel Blotto games with discrete strategy spaces effectively illus- trate
the intricate nature of multidimensional strategic reasoning. This paper
studies the equilibrium set of such games where, in line with prior
experimental work, the tie-breaking rule is allowed to be flexible. We begin by
pointing out that equilibrium constructions known from the literature extend to
our class of games. However, we also note that, irrespective of the
tie-breaking rule, the equilibrium set is excessively large. Specifically, any
pure strategy that allocates at most twice the fair share to each battlefield
is used with positive probability in some equilibrium. Furthermore, refinements
based on the elimination of weakly dominated strategies prove ineffective. To
derive specific predictions amid this multiplicity, we compute strategies
resulting from long-run adaptive learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Economic DAO Governance: A Contestable Control Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16980v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16980v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeff Strnad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this article, we propose a new form of DAO governance that uses a
sequential auction mechanism to overcome entrenched control issues that have
emerged for DAOs by creating a regime of temporary contestable control. The
mechanism avoids potential public choice problems inherent in voting approaches
but at the same time provides a vehicle that can enhance and secure value than
inheres to DAO voting and other DAO non-market governance procedures. It is
robust to empty voting and is code feasible. It facilitates the ability of DAOs
to meet their normative and operational goals in the face of diverse regulatory
approaches. Designed to shift control to the party with the most promising
business plan, at the same time it distributes surplus in a way that tends to
promote investment by other parties.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>84 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Do LLM Agents Have Regret? A Case Study in Online Learning and Games 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16843v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16843v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chanwoo Park, Xiangyu Liu, Asuman Ozdaglar, Kaiqing Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have been increasingly employed for
(interactive) decision-making, via the development of LLM-based autonomous
agents. Despite their emerging successes, the performance of LLM agents in
decision-making has not been fully investigated through quantitative metrics,
especially in the multi-agent setting when they interact with each other, a
typical scenario in real-world LLM-agent applications. To better understand the
limits of LLM agents in these interactive environments, we propose to study
their interactions in benchmark decision-making settings in online learning and
game theory, through the performance metric of \emph{regret}. We first
empirically study the {no-regret} behaviors of LLMs in canonical
(non-stationary) online learning problems, as well as the emergence of
equilibria when LLM agents interact through playing repeated games. We then
provide some theoretical insights into the no-regret behaviors of LLM agents,
under certain assumptions on the supervised pre-training and the rationality
model of human decision-makers who generate the data. Notably, we also identify
(simple) cases where advanced LLMs such as GPT-4 fail to be no-regret. To
promote the no-regret behaviors, we propose a novel \emph{unsupervised}
training loss of \emph{regret-loss}, which, in contrast to the supervised
pre-training loss, does not require the labels of (optimal) actions. We then
establish the statistical guarantee of generalization bound for regret-loss
minimization, followed by the optimization guarantee that minimizing such a
loss may automatically lead to known no-regret learning algorithms. Our further
experiments demonstrate the effectiveness of our regret-loss, especially in
addressing the above ``regrettable'' cases.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Method for Finding Optimal Strategies in Chopstick Auctions
  with Uniform Objects Values <span class="chip">AAMAS-24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16799v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16799v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stanisław Kaźmierowski, Marcin Dziubiński
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose an algorithm for computing Nash equilibria (NE) in a class of
conflicts with multiple battlefields with uniform battlefield values and a
non-linear aggregation function. By expanding the symmetrization idea of Hart
[9], proposed for the Colonel Blotto game, to the wider class of symmetric
conflicts with multiple battlefields, we reduce the number of strategies of the
players by an exponential factor. We propose a clash matrix algorithm which
allows for computing the payoffs in the symmetrized model in polynomial time.
Combining symmetrization and clash matrix algorithm with the double oracle
algorithm we obtain an algorithm for computing NE in the models in question
that achieves a significant speed-up as compared to the standard, LP-based,
approach. We also introduce a heuristic to further speed up the process.
Overall, our approach offers an efficient and novel method for computing NE in
a specific class of conflicts, with potential practical applications in various
fields.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for AAMAS-24 conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multilinear formulations for computing Nash equilibrium of multi-player
  matrix games 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.03406v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.03406v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Miriam Fischer, Akshay Gupte
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present multilinear and mixed-integer multilinear programs to find a Nash
equilibrium in multi-player noncooperative games. We compare the formulations
to common algorithms in Gambit, and conclude that a multilinear feasibility
program finds a Nash equilibrium faster than any of the methods we compare it
to, including the quantal response equilibrium method, which is recommended for
large games. Hence, the multilinear feasibility program is an alternative
method to find a Nash equilibrium in multi-player games, and outperforms many
common algorithms. The mixed-integer formulations are generalisations of known
mixed-integer programs for two-player games, however unlike two-player games,
these mixed-integer programs do not give better performance than existing
algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 page conference paper accepted</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Project-Fair and Truthful Mechanisms for Budget Aggregation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.02613v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.02613v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rupert Freeman, Ulrike Schmidt-Kraepelin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the budget aggregation problem in which a set of strategic voters
must split a finite divisible resource (such as money or time) among a set of
competing projects. Our goal is twofold: We seek truthful mechanisms that
provide fairness guarantees to the projects. For the first objective, we focus
on the class of moving phantom mechanisms [Freeman et al., 2021], which are --
to this day -- essentially the only known truthful mechanisms in this setting.
For project fairness, we consider the mean division as a fair baseline, and
bound the maximum difference between the funding received by any project and
this baseline. We propose a novel and simple moving phantom mechanism that
provides optimal project fairness guarantees. As a corollary of our results, we
show that our new mechanism minimizes the $\ell_1$ distance to the mean (a
measure suggested by Caragiannis et al. [2022]) for three projects and gives
the first non-trivial bounds on this quantity for more than three projects.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Theory
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Resonant Beam Communications: A New Design Paradigm and Challenges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16699v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16699v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanming Tian, Dongxu Li, Chuan Huang, Qingwen Liu, Shengli Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Resonant beam communications (RBCom), which adopt oscillating photons between
two separate retroreflectors for information transmission, exhibit potential
advantages over other types of wireless optical communications (WOC). However,
echo interference generated by the modulated beam reflected from the receiver
affects the transmission of the desired information. To tackle this challenge,
a synchronization-based point-to-point RBCom system is proposed to eliminate
the echo interference, and the design for the transmitter and receiver is
discussed. Subsequently, the performance of the proposed RBCom is evaluated and
compared with that of visible light communications (VLC) and free space optical
communications (FOC). Finally, future research directions are outlined and
several implementation challenges of RBCom systems are highlighted.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Design and Performance of Resonant Beam Communications -- Part II:
  Mobile Scenario 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16694v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16694v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongxu Li, Yuanming Tian, Chuan Huang, Qingwen Liu, Shengli Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This two-part paper focuses on the system design and performance analysis for
a point-to-point resonant beam communication (RBCom) system under both the
quasi-static and mobile scenarios. Part I of this paper proposes a
synchronization-based information transmission scheme and derives the capacity
upper and lower bounds for the quasi-static channel case. In Part II, we
address the mobile scenario, where the receiver is in relative motion to the
transmitter, and derive a mobile RBCom channel model that jointly considers the
Doppler effect, channel variation, and echo interference. With the obtained
channel model, we prove that the channel gain of the mobile RBCom decreases as
the number of transmitted frames increases, and thus show that the considered
mobile RBCom terminates after the transmitter sends a certain number of frames
without frequency compensation. By deriving an upper bound on the number of
successfully transmitted frames, we formulate the throughput maximization
problem for the considered mobile RBCom system, and solve it via a sequential
parametric convex approximation (SPCA) method. Finally, simulation results
validate the analysis of our proposed method in some typical scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Design and Performance of Resonant Beam Communications -- Part I:
  Quasi-Static Scenario 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16676v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16676v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongxu Li, Yuanming Tian, Chuan Huang, Qingwen Liu, Shengli Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This two-part paper studies a point-to-point resonant beam communication
(RBCom) system, where two separately deployed retroreflectors are adopted to
generate the resonant beam between the transmitter and the receiver, and
analyzes the transmission rate of the considered system under both the
quasi-static and mobile scenarios. Part I of this paper focuses on the
quasi-static scenario where the locations of the transmitter and the receiver
are relatively fixed. Specifically, we propose a new information-bearing scheme
which adopts a synchronization-based amplitude modulation method to mitigate
the echo interference caused by the reflected resonant beam. With this scheme,
we show that the quasi-static RBCom channel is equivalent to a Markov channel
and can be further simplified as an amplitude-constrained additive white
Gaussian noise channel. Moreover, we develop an algorithm that jointly employs
the bisection and exhaustive search to maximize its capacity upper and lower
bounds. Finally, numerical results validate our analysis. Part II of this paper
discusses the performance of the RBCom system under the mobile scenario.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Measure of Synergy based on Union Information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16575v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16575v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        André F. C. Gomes, Mário A. T. Figueiredo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The partial information decomposition (PID) framework is concerned with
decomposing the information that a set of (two or more) random variables (the
sources) has about another variable (the target) into three types of
information: unique, redundant, and synergistic. Classical information theory
alone does not provide a unique way to decompose information in this manner and
additional assumptions have to be made. One often overlooked way to achieve
this decomposition is using a so-called measure of union information - which
quantifies the information that is present in at least one of the sources -
from which a synergy measure stems. In this paper, we introduce a new measure
of union information based on adopting a communication channel perspective,
compare it with existing measures, and study some of its properties. We also
include a comprehensive critical review of characterizations of union
information and synergy measures that have been proposed in the literature.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BackCom Assisted Hybrid NOMA Uplink Transmission for Ambient IoT 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16498v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16498v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiguo Ding
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hybrid non-orthogonal multiple access (H-NOMA) has recently received
significant attention as a general framework of multiple access, where both
conventional orthogonal multiple access (OMA) and pure NOMA are its special
cases. This paper focuses on the application of H-NOMA to ambient Internet of
Things (IoT) with energy-constrained devices, where a new backscatter
communication (BackCom) assisted H-NOMA uplink scheme is developed. Resource
allocation for H-NOMA uplink transmission is also considered, where an overall
power minimization problem is formulated. Insightful understandings for the key
features of BackCom assisted H-NOMA and its difference from conventional H-NOMA
are illustrated by developing analytical results for the two-user special case.
For the general multi-user scenario, two algorithms, one based on the
branch-bound (BB) principle and the other based on successive convex
approximation (SCA), are developed to realize different tradeoffs between the
system performance and complexity. The numerical results are also provided to
verify the accuracy of the developed analytical results and demonstrate the
performance gain of H-NOMA over OMA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Safeguarding Next Generation Multiple Access Using Physical Layer
  Security Techniques: A Tutorial 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16477v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16477v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lu Lv, Dongyang Xu, Rose Qingyang Hu, Yinghui Ye, Long Yang, Xianfu Lei, Xianbin Wang, Dong In Kim, Arumugam Nallanathan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Driven by the ever-increasing requirements of ultra-high spectral efficiency,
ultra-low latency, and massive connectivity, the forefront of wireless research
calls for the design of advanced next generation multiple access schemes to
facilitate provisioning of these stringent demands. This inspires the embrace
of non-orthogonal multiple access (NOMA) in future wireless communication
networks. Nevertheless, the support of massive access via NOMA leads to
additional security threats, due to the open nature of the air interface, the
broadcast characteristic of radio propagation as well as intertwined
relationship among paired NOMA users. To address this specific challenge, the
superimposed transmission of NOMA can be explored as new opportunities for
security aware design, for example, multiuser interference inherent in NOMA can
be constructively engineered to benefit communication secrecy and privacy. The
purpose of this tutorial is to provide a comprehensive overview on the
state-of-the-art physical layer security techniques that guarantee wireless
security and privacy for NOMA networks, along with the opportunities, technical
challenges, and future research trends.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Invited paper by Proceedings of the IEEE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Power-Aware Sparse Reflect Beamforming in Active RIS-aided Interference
  Channels 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16472v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16472v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruizhe Long, Hu Zhou, Ying-Chang Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Active reconfigurable intelligent surface (RIS) has attracted significant
attention in wireless communications, due to its reflecting elements (REs)
capable of reflecting incident signals with not only phase shifts but also
amplitude amplifications. In this paper, we are interested in active RIS-aided
interference channels in which $K$ user pairs share the same time and frequency
resources with the aid of active RIS. Thanks to the promising amplitude
amplification capability, activating a moderate number of REs, rather than all
of them, is sufficient for the active RIS to mitigate cross-channel
interferences. Motivated by this, we propose a power-aware sparse reflect
beamforming design for the active RIS-aided interference channels, which allows
the active RIS to flexibly adjust the number of activated REs for the sake of
reducing hardware and power costs. Specifically, we establish the power
consumption model in which only those activated REs consume the biasing and
operation power that supports the amplitude amplification, yielding an
$\ell_0$-norm power consumption function. Based on the proposed model, we
investigate a sum-rate maximization problem and an active RIS power
minimization problem by carefully designing the sparse reflect beamforming
vector. To solve these problems, we first replace the nonconvex $\ell_0$-norm
function with an iterative reweighted $\ell_1$-norm function. Then, fractional
programming is used to solve the sum-rate maximization, while semidefinite
programming together with the difference-of-convex algorithm (DCA) is used to
solve the active RIS power minimization. Numerical results show that the
proposed sparse designs can notably increase the sum rate of user pairs and
decrease the power consumption of active RIS in interference channels.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Computation-Limited Signals: A Channel Capacity Regime Constrained by
  Computational Complexity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05794v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05794v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saulo Queiroz, João P. Vilela, Edmundo Monteiro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this letter, we introduce the computational-limited (comp-limited)
signals, a communication capacity regime in which the signal time computational
complexity overhead is the key constraint -- rather than power or bandwidth --
to the overall communication capacity. We present the Spectro-Computational
(SC) analysis, a novel mathematical framework that enhances classic concepts of
information theory -- such as throughput, spectral efficiency and capacity --
to account for the signal processing computational complexity overhead. We
consider a specific Shannon regime under which capacity is expected to get
arbitrarily large as channel resources grow. Under that regime, we identify the
conditions under which the time complexity overhead causes capacity to decrease
rather than increasing, thereby creating the case for the comp-limited regime.
We also provide examples of the SC analysis and show the OFDM waveform is
comp-limited unless the lower-bound computational complexity of the $N$-point
DFT problem verifies as $\Omega(N)$, which remains an open challenge.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Performance Analysis of In-Band-Full-Duplex Multi-Cell Wideband IAB
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.04615v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.04615v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junkai Zhang, Tharmalingam Ratnarajah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper analyzes the performance of the 3rd Generation Partnership Project
(3GPP)-inspired multi-cell wideband single-hop backhaul
millimeter-wave-in-band-full-duplex (IBFD)-integrated access and backhaul (IAB)
networks by using stochastic geometry. We model the wired-connected Next
Generation NodeBs (gNBs) as the Mat\'ern hard-core point process (MHCPP) to
meet the real-world deployment requirement and reduce the cost caused by wired
connection in the network. We first derive association probabilities that
reflect how likely the typical user-equipment is served by a gNB or an IAB-node
based on the maximum long-term averaged biased-received-desired-signal power
criteria. Further, by leveraging the composite Gamma-Lognormal distribution, we
derive the closed-form signal to interference plus noise ratio coverage,
capacity with outage, and ergodic capacity of the network. In order to avoid
underestimating the noise, we consider the sidelobe gain on inter-cell
interference links and the analog to digital converter quantization noise.
Compared with the half-duplex transmission, numerical results show an enhanced
capacity with outage and ergodic capacity provided by IBFD under successful
self-interference cancellation. We also study how the power bias and density
ratio of the IAB-node to gNB, and the hard-core distance can affect system
performances.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Probabilistic positioning via ray tracing with noisy angle of arrival
  measurements 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.00611v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.00611v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vincent Corlay, Viet-Hoa Nguyen, Nicolas Gresset
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates the problems of interference prediction and sensing
for efficient spectrum access and link adaptation. The considered approach for
interference prediction relies on a parametric model. However, we assume that
the number of observations available to learn theses parameters is limited.
This implies that they should be treated as random variables rather than fixed
values. We show how this can impact the spectrum access and link adaptation
strategies. We also introduce the notion of "interferer-coherence time" to
establish the number of independent interferer state realizations experienced
by a codeword. We explain how it can be computed taking into account the model
uncertainty and how this impacts the link adaptation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE Communications Letters</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-24T00:00:00Z">2024-03-24</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Distributed, Parallel, and Cluster Computing
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Q-adaptive: A Multi-Agent Reinforcement Learning Based Routing on
  Dragonfly Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16301v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16301v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yao Kang, Xin Wang, Zhiling Lan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  on adaptive routing to balance network traffic for optimum performance.
Ideally, adaptive routing attempts to forward packets between minimal and
non-minimal paths with the least congestion. In practice, current adaptive
routing algorithms estimate routing path congestion based on local information
such as output queue occupancy. Using local information to estimate global path
congestion is inevitably inaccurate because a router has no precise knowledge
of link states a few hops away. This inaccuracy could lead to interconnect
congestion. In this study, we present Q-adaptive routing, a multi-agent
reinforcement learning routing scheme for Dragonfly systems. Q-adaptive routing
enables routers to learn to route autonomously by leveraging advanced
reinforcement learning technology. The proposed Q-adaptive routing is highly
scalable thanks to its fully distributed nature without using any shared
information between routers. Furthermore, a new two-level Q-table is designed
for Q-adaptive to make it computational lightly and saves 50% of router memory
usage compared with the previous Q-routing. We implement the proposed
Q-adaptive routing in SST/Merlin simulator. Our evaluation results show that
Q-adaptive routing achieves up to 10.5% system throughput improvement and 5.2x
average packet latency reduction compared with adaptive routing algorithms.
Remarkably, Q-adaptive can even outperform the optimal VALn non-minimal routing
under the ADV+1 adversarial traffic pattern with up to 3% system throughput
improvement and 75% average packet latency reduction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MRSch: Multi-Resource Scheduling for HPC 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16298v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16298v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boyang Li, Yuping Fan, Matthew Dearing, Zhiling Lan, Paul Richy, William Allcocky, Michael Papka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Emerging workloads in high-performance computing (HPC) are embracing
significant changes, such as having diverse resource requirements instead of
being CPU-centric. This advancement forces cluster schedulers to consider
multiple schedulable resources during decision-making. Existing scheduling
studies rely on heuristic or optimization methods, which are limited by an
inability to adapt to new scenarios for ensuring long-term scheduling
performance. We present an intelligent scheduling agent named MRSch for
multi-resource scheduling in HPC that leverages direct future prediction (DFP),
an advanced multi-objective reinforcement learning algorithm. While DFP
demonstrated outstanding performance in a gaming competition, it has not been
previously explored in the context of HPC scheduling. Several key techniques
are developed in this study to tackle the challenges involved in multi-resource
scheduling. These techniques enable MRSch to learn an appropriate scheduling
policy automatically and dynamically adapt its policy in response to workload
changes via dynamic resource prioritizing. We compare MRSch with existing
scheduling methods through extensive tracebase simulations. Our results
demonstrate that MRSch improves scheduling performance by up to 48% compared to
the existing scheduling methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interpretable Modeling of Deep Reinforcement Learning Driven Scheduling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16293v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16293v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boyang Li, Zhiling Lan, Michael E. Papka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the field of high-performance computing (HPC), there has been recent
exploration into the use of deep reinforcement learning for cluster scheduling
(DRL scheduling), which has demonstrated promising outcomes. However, a
significant challenge arises from the lack of interpretability in deep neural
networks (DNN), rendering them as black-box models to system managers. This
lack of model interpretability hinders the practical deployment of DRL
scheduling. In this work, we present a framework called IRL (Interpretable
Reinforcement Learning) to address the issue of interpretability of DRL
scheduling. The core idea is to interpret DNN (i.e., the DRL policy) as a
decision tree by utilizing imitation learning. Unlike DNN, decision tree models
are non-parametric and easily comprehensible to humans. To extract an effective
and efficient decision tree, IRL incorporates the Dataset Aggregation (DAgger)
algorithm and introduces the notion of critical state to prune the derived
decision tree. Through trace-based experiments, we demonstrate that IRL is
capable of converting a black-box DNN policy into an interpretable rulebased
decision tree while maintaining comparable scheduling performance.
Additionally, IRL can contribute to the setting of rewards in DRL scheduling.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Study of Workload Interference with Intelligent Routing on Dragonfly 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16288v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16288v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yao Kang, Xin Wang, Zhiling Lan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dragonfly interconnect is a crucial network technology for supercomputers. To
support exascale systems, network resources are shared such that links and
routers are not dedicated to any node pair. While link utilization is
increased, workload performance is often offset by network contention.
Recently, intelligent routing built on reinforcement learning demonstrates
higher network throughput with lower packet latency. However, its effectiveness
in reducing workload interference is unknown. In this work, we present
extensive network simulations to study multi-workload contention under
different routing mechanisms, intelligent routing and adaptive routing, on a
large-scale Dragonfly system. We develop an enhanced network simulation
toolkit, along with a suite of workloads with distinctive communication
patterns. We also present two metrics to characterize application communication
intensity. Our analysis focuses on examining how different workloads interfere
with each other under different routing mechanisms by inspecting both
application-level and network-level metrics. Several key insights are made from
the analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Codesign of Scheduling and Parallelization for Large Model Training in
  Heterogeneous Clusters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16125v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16125v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chunyu Xue, Weihao Cui, Han Zhao, Quan Chen, Shulai Zhang, Pengyu Yang, Jing Yang, Shaobo Li, Minyi Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Joint consideration of scheduling and adaptive parallelism offers great
opportunities for improving the training efficiency of large models on
heterogeneous GPU clusters. However, integrating adaptive parallelism into a
cluster scheduler expands the cluster scheduling space. The new space is the
product of the original scheduling space and the parallelism exploration space
of adaptive parallelism (also a product of pipeline, data, and tensor
parallelism). The exponentially enlarged scheduling space and ever-changing
optimal parallelism plan from adaptive parallelism together result in the
contradiction between low-overhead and accurate performance data acquisition
for efficient cluster scheduling. This paper presents Crius, a training system
for efficiently scheduling multiple large models with adaptive parallelism in a
heterogeneous cluster. Crius proposes a novel scheduling granularity called
Cell. It represents a job with deterministic resources and pipeline stages. The
exploration space of Cell is shrunk to the product of only data and tensor
parallelism, thus exposing the potential for accurate and low-overhead
performance estimation. Crius then accurately estimates Cells and efficiently
schedules training jobs. When a Cell is selected as a scheduling choice, its
represented job runs with the optimal parallelism plan explored. Experimental
results show that Crius reduces job completion time by up to 48.9% and
schedules large models with up to 1.49x cluster throughput improvement.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FedNMUT -- Federated Noisy Model Update Tracking Convergence Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13247v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13247v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vishnu Pandi Chellapandi, Antesh Upadhyay, Abolfazl Hashemi, Stanislaw H. Żak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A novel Decentralized Noisy Model Update Tracking Federated Learning
algorithm (FedNMUT) is proposed that is tailored to function efficiently in the
presence of noisy communication channels that reflect imperfect information
exchange. This algorithm uses gradient tracking to minimize the impact of data
heterogeneity while minimizing communication overhead. The proposed algorithm
incorporates noise into its parameters to mimic the conditions of noisy
communication channels, thereby enabling consensus among clients through a
communication graph topology in such challenging environments. FedNMUT
prioritizes parameter sharing and noise incorporation to increase the
resilience of decentralized learning systems against noisy communications.
Theoretical results for the smooth non-convex objective function are provided
by us, and it is shown that the $\epsilon-$stationary solution is achieved by
our algorithm at the rate of $\mathcal{O}\left(\frac{1}{\sqrt{T}}\right)$,
where $T$ is the total number of communication rounds. Additionally, via
empirical validation, we demonstrated that the performance of FedNMUT is
superior to the existing state-of-the-art methods and conventional
parameter-mixing approaches in dealing with imperfect information sharing. This
proves the capability of the proposed algorithm to counteract the negative
effects of communication noise in a decentralized learning framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2303.10695</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Gradient-less Federated Gradient Boosting Trees with Learnable Learning
  Rates 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.07537v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.07537v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenyang Ma, Xinchi Qiu, Daniel J. Beutel, Nicholas D. Lane
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The privacy-sensitive nature of decentralized datasets and the robustness of
eXtreme Gradient Boosting (XGBoost) on tabular data raise the needs to train
XGBoost in the context of federated learning (FL). Existing works on federated
XGBoost in the horizontal setting rely on the sharing of gradients, which
induce per-node level communication frequency and serious privacy concerns. To
alleviate these problems, we develop an innovative framework for horizontal
federated XGBoost which does not depend on the sharing of gradients and
simultaneously boosts privacy and communication efficiency by making the
learning rates of the aggregated tree ensembles learnable. We conduct extensive
evaluations on various classification and regression datasets, showing our
approach achieves performance comparable to the state-of-the-art method and
effectively improves communication efficiency by lowering both communication
rounds and communication overhead by factors ranging from 25x to 700x. Project
Page: https://flower.ai/blog/2023-04-19-xgboost-with-flower/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the 3rd ACM Workshop on Machine Learning and Systems
  (EuroMLSys), May 8th 2023, Rome, Italy</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ How Does Stake Distribution Influence Consensus? Analyzing Blockchain
  Decentralization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.13938v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.13938v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shashank Motepalli, Hans-Arno Jacobsen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the PoS blockchain landscape, the challenge of achieving full
decentralization is often hindered by a disproportionate concentration of
staked tokens among a few validators. This study analyses this challenge by
first formalizing decentralization metrics for weighted consensus mechanisms.
An empirical analysis across ten permissionless blockchains uncovers
significant weight concentration among validators, underscoring the need for an
equitable approach. To counter this, we introduce the Square Root Stake Weight
(SRSW) model, which effectively recalibrates staking weight distribution. Our
examination of the SRSW model demonstrates notable improvements in the
decentralization metrics: the Gini index improves by 37.16% on average, while
Nakamoto coefficients for liveness and safety see mean enhancements of 101.04%
and 80.09%, respectively. This research is a pivotal step toward a more fair
and equitable distribution of staking weight, advancing the decentralization in
blockchain consensus mechanisms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in ICBC 2024</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Performance
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Explainable Port Mapping Inference with Sparse Performance Counters for
  AMD's Zen Architectures <span class="chip">ASPLOS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16063v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16063v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fabian Ritter, Sebastian Hack
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Performance models are instrumental for optimizing performance-sensitive
code. When modeling the use of functional units of out-of-order x86-64 CPUs,
data availability varies by the manufacturer: Instruction-to-port mappings for
Intel's processors are available, whereas information for AMD's designs are
lacking. The reason for this disparity is that standard techniques to infer
exact port mappings require hardware performance counters that AMD does not
provide.
  In this work, we modify the port mapping inference algorithm of the widely
used uops.info project to not rely on Intel's performance counters. The
modifications are based on a formal port mapping model with a
counter-example-guided algorithm powered by an SMT solver. We investigate in
how far AMD's processors comply with this model and where unexpected
performance characteristics prevent an accurate port mapping. Our results
provide valuable insights for creators of CPU performance models as well as for
software developers who want to achieve peak performance on recent AMD CPUs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ASPLOS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Performance evaluation of accelerated complex multiple-precision LU
  decomposition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16013v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16013v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tomonori Kouya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The direct method is one of the most important algorithms for solving linear
systems of equations, with LU decomposition comprising a significant portion of
its computation time. This study explores strategies to accelerate complex LU
decomposition using multiple-precision floating-point arithmetic of the
multiple-component type. Specifically, we explore the potential efficiency
gains using a combination of SIMDization and the 3M method for complex matrix
multiplication. Our benchmark tests compare this approach with the direct
method implementation in MPLAPACK, focusing on computation time and numerical
errors.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Databases
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Reporting Durable Patterns in Temporal Proximity Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16312v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16312v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pankaj K. Agarwal, Xiao Hu, Stavros Sintos, Jun Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Finding patterns in graphs is a fundamental problem in databases and data
mining. In many applications, graphs are temporal and evolve over time, so we
are interested in finding durable patterns, such as triangles and paths, which
persist over a long time. While there has been work on finding durable simple
patterns, existing algorithms do not have provable guarantees and run in
strictly super-linear time. The paper leverages the observation that many
graphs arising in practice are naturally proximity graphs or can be
approximated as such, where nodes are embedded as points in some
high-dimensional space, and two nodes are connected by an edge if they are
close to each other. We work with an implicit representation of the proximity
graph, where nodes are additionally annotated by time intervals, and design
near-linear-time algorithms for finding (approximately) durable patterns above
a given durability threshold. We also consider an interactive setting where a
client experiments with different durability thresholds in a sequence of
queries; we show how to compute incremental changes to result patterns
efficiently in time near-linear to the size of the changes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SQL-Encoder: Improving NL2SQL In-Context Learning Through a
  Context-Aware Encoder 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16204v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16204v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammadreza Pourreza, Davood Rafiei, Yuxi Feng, Raymond Li, Zhenan Fan, Weiwei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting structural similarity between queries is essential for selecting
examples in in-context learning models. However, assessing structural
similarity based solely on the natural language expressions of queries, without
considering SQL queries, presents a significant challenge. This paper explores
the significance of this similarity metric and proposes a model for accurately
estimating it. To achieve this, we leverage a dataset comprising 170k question
pairs, meticulously curated to train a similarity prediction model. Our
comprehensive evaluation demonstrates that the proposed model adeptly captures
the structural similarity between questions, as evidenced by improvements in
Kendall-Tau distance and precision@k metrics. Notably, our model outperforms
strong competitive embedding models from OpenAI and Cohere. Furthermore,
compared to these competitive models, our proposed encoder enhances the
downstream performance of NL2SQL models in 1-shot in-context learning scenarios
by 1-2\% for GPT-3.5-turbo, 4-8\% for CodeLlama-7B, and 2-3\% for
CodeLlama-13B.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ByteCard: Enhancing Data Warehousing with Learned Cardinality Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16110v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16110v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxing Han, Haoyu Wang, Lixiang Chen, Yifeng Dong, Xing Chen, Benquan Yu, Chengcheng Yang, Weining Qian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cardinality estimation is a critical component and a longstanding challenge
in modern data warehouses. ByteHouse, ByteDance's cloud-native engine for big
data analysis in exabyte-scale environments, serves numerous internal
decision-making business scenarios. With the increasing demand of ByteHouse,
cardinality estimation becomes the bottleneck for efficiently processing
queries. Specifically, the existing query optimizer of ByteHouse uses the
traditional Selinger-like cardinality estimator, which can produce huge
estimation errors, resulting in sub-optimal query plans. To improve cardinality
estimation accuracy while maintaining a practical inference overhead, we
develop ByteCard framework that enables efficient training/updating and
integration of cardinality estimators. Furthermore, ByteCard adapts recent
advances in cardinality estimation to build models that can balance accuracy
and practicality (e.g., inference latency, model size, training/updating
overhead). We observe significant query processing speed-up in ByteHouse after
replacing the system's existing cardinality estimation with ByteCard's
estimations for several optimization strategies. Evaluations on real-world
datasets show the integration of ByteCard leads to an improvement of up to 30%
in the 99th quantile of latency. At last, we share our valuable experience in
engineering advanced cardinality estimators. We believe this experience can
help other data warehouses integrate more accurate and sophisticated solutions
on the critical path of query execution.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Insert-Only versus Insert-Delete in Dynamic Query Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.09331v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.09331v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mahmoud Abo Khamis, Ahmet Kara, Dan Olteanu, Dan Suciu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the dynamic query evaluation problem: Given a join query Q and a
stream of updates, we would like to construct a data structure that supports
constant-delay enumeration of the query output after each update.
  We show that a stream of N insert-only updates (to an initially empty
database) can be executed in total time O(N^{w(Q)}), where w(Q) is the
fractional hypertree width of Q. This matches the complexity of the static
query evaluation problem for Q and a database of size N. One corollary is that
the average time per single-tuple insert is constant for acyclic joins.
  In contrast, we show that a stream of N insert-and-delete updates to Q can be
executed in total time O(N^{w(Q')}), where Q' is obtained from Q by extending
every relational atom with extra variables that represent the "lifespans" of
tuples in Q. We show that this reduction is optimal in the sense that the
static evaluation runtime of Q' provides a lower bound on the total update time
of Q. Our approach recovers the optimal single-tuple update time for known
queries such as the hierarchical and Loomis-Whitney join queries.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Grover's Search Algorithm: A Modified Approach to Increase the
  Probability of Good States 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.00082v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.00082v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ismael Abdulrahman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This article introduces an enhancement to the Grover search algorithm to
speed up computing the probability of finding good states. It suggests
incorporating a rotation phase angle determined mathematically from the
derivative of the model during the initial iteration. At each iteration, a new
phase angle is computed and used in a rotation gate around y+z axis in the
diffusion operator. The computed phase angles are optimized through an adaptive
adjustment based on the estimated increasing ratio of the consecutive
amplitudes. The findings indicate an average decrease of 28% in the required
number of iterations resulting in a faster overall process and fewer number of
quantum gates. For large search space, this improvement rises to 29.58%. Given
the computational capabilities of the computer utilized for the simulation, the
approach is applied to instances with up to 12 qubits or 4096 possible
combination of search entries.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Finding Smallest Witnesses for Conjunctive Queries 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.18157v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.18157v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiao Hu, Stavros Sintos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A witness is a sub-database that preserves the query results of the original
database but of much smaller size. It has wide applications in query rewriting
and debugging, query explanation, IoT analytics, multi-layer network routing,
etc. In this paper, we study the smallest witness problem (SWP) for the class
of conjunctive queries (CQs) without self-joins.
  We first establish the dichotomy that SWP for a CQ can be computed in
polynomial time if and only if it has {\em head-cluster property}, unless
$\texttt{P} = \texttt{NP}$. We next turn to the approximated version by
relaxing the size of a witness from being minimum. We surprisingly find that
the {\em head-domination} property - that has been identified for the deletion
propagation problem \cite{kimelfeld2012maximizing} - can also precisely capture
the hardness of the approximated smallest witness problem. In polynomial time,
SWP for any CQ with head-domination property can be approximated within a
constant factor, while SWP for any CQ without such a property cannot be
approximated within a logarithmic factor, unless $\texttt{P} = \texttt{NP}$.
  We further explore efficient approximation algorithms for CQs without
head-domination property: (1) we show a trivial algorithm which achieves a
polynomially large approximation ratio for general CQs; (2) for any CQ with
only one non-output attribute, such as star CQs, we show a greedy algorithm
with a logarithmic approximation ratio; (3) for line CQs, which contain at
least two non-output attributes, we relate SWP problem to the directed steiner
forest problem, whose algorithms can be applied to line CQs directly.
Meanwhile, we establish a much higher lower bound, exponentially larger than
the logarithmic lower bound obtained above. It remains open to close the gap
between the lower and upper bound of the approximated SWP for CQs without
head-domination property.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Science and Game Theory
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Coupled Optimization Framework for Correlated Equilibria in
  Normal-Form Game 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16223v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16223v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sarah H. Q. Li, Yue Yu, Florian Dörfler, John Lygeros
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In competitive multi-player interactions, simultaneous optimality is a key
requirement for establishing strategic equilibria. This property is explicit
when the game-theoretic equilibrium is the simultaneously optimal solution of
coupled optimization problems. However, no such optimization problems exist for
the correlated equilibrium, a strategic equilibrium where the players can
correlate their actions. We address the lack of a coupled optimization
framework for the correlated equilibrium by introducing an {unnormalized game}
-- an extension of normal-form games in which the player strategies are lifted
to unnormalized measures over the joint actions. We show that the set of fully
mixed generalized Nash equilibria of this unnormalized game is a subset of the
correlated equilibrium of the normal-form game. Furthermore, we introduce an
entropy regularization to the unnormalized game and prove that the
entropy-regularized generalized Nash equilibrium is a sub-optimal correlated
equilibrium of the normal form game where the degree of sub-optimality depends
on the magnitude of regularization. We prove that the entropy-regularized
unnormalized game has a closed-form solution, and empirically verify its
computational efficacy at approximating the correlated equilibrium of
normal-form games.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-23T00:00:00Z">2024-03-23</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Distributed, Parallel, and Cluster Computing
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LOAM: Low-latency Communication, Caching, and Computation Placement in
  Data-Intensive Computing Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15927v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15927v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinkun Zhang, Edmund Yeh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deploying data- and computation-intensive applications such as large-scale AI
into heterogeneous dispersed computing networks can significantly enhance
application performance by mitigating bottlenecks caused by limited network
resources, including bandwidth, storage, and computing power. However, current
resource allocation methods in dispersed computing do not provide a
comprehensive solution that considers arbitrary topology, elastic resource
amount, reuse of computation results, and nonlinear congestion-dependent
optimization objectives. In this paper, we propose LOAM, a low-latency joint
communication, caching, and computation placement framework with a rigorous
analytical foundation that incorporates the above aspects. We tackle the
NP-hard aggregated cost minimization problem with two methods: an offline
method with a 1/2 approximation and an online adaptive method with a bounded
gap from the optimum. Through extensive simulation, the proposed framework
outperforms multiple baselines in both synthesis and real-world network
scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Initialisation and Topology Effects in Decentralised Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15855v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15855v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arash Badie-Modiri, Chiara Boldrini, Lorenzo Valerio, János Kertész, Márton Karsai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fully decentralised federated learning enables collaborative training of
individual machine learning models on distributed devices on a network while
keeping the training data localised. This approach enhances data privacy and
eliminates both the single point of failure and the necessity for central
coordination. Our research highlights that the effectiveness of decentralised
federated learning is significantly influenced by the network topology of
connected devices. A simplified numerical model for studying the early
behaviour of these systems leads us to an improved artificial neural network
initialisation strategy, which leverages the distribution of eigenvector
centralities of the nodes of the underlying network, leading to a radically
improved training efficiency. Additionally, our study explores the scaling
behaviour and choice of environmental parameters under our proposed
initialisation strategy. This work paves the way for more efficient and
scalable artificial neural network training in a distributed and uncoordinated
environment, offering a deeper understanding of the intertwining roles of
network structure and learning dynamics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Resource-efficient Parallel Split Learning in Heterogeneous Edge
  Computing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15815v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15815v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingjin Zhang, Jiannong Cao, Yuvraj Sahni, Xiangchun Chen, Shan Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Edge AI has been recently proposed to facilitate the training and deployment
of Deep Neural Network (DNN) models in proximity to the sources of data. To
enable the training of large models on resource-constraint edge devices and
protect data privacy, parallel split learning is becoming a practical and
popular approach. However, current parallel split learning neglects the
resource heterogeneity of edge devices, which may lead to the straggler issue.
In this paper, we propose EdgeSplit, a novel parallel split learning framework
to better accelerate distributed model training on heterogeneous and
resource-constraint edge devices. EdgeSplit enhances the efficiency of model
training on less powerful edge devices by adaptively segmenting the model into
varying depths. Our approach focuses on reducing total training time by
formulating and solving a task scheduling problem, which determines the most
efficient model partition points and bandwidth allocation for each device. We
employ a straightforward yet effective alternating algorithm for this purpose.
Comprehensive tests conducted with a range of DNN models and datasets
demonstrate that EdgeSplit not only facilitates the training of large models on
resource-restricted edge devices but also surpasses existing baselines in
performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by International Conference on Computing, Networking and
  Communications (ICNC 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Upload-Efficient Scheme for Transferring Knowledge From a Server-Side
  <span class="highlight-title">Pre-train</span>ed Generator to Clients in Heterogeneous Federated Learning <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15760v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15760v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianqing Zhang, Yang Liu, Yang Hua, Jian Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Heterogeneous Federated Learning (HtFL) enables collaborative learning on
multiple clients with different model architectures while preserving privacy.
Despite recent research progress, knowledge sharing in HtFL is still difficult
due to data and model heterogeneity. To tackle this issue, we leverage the
knowledge stored in pre-trained generators and propose a new upload-efficient
knowledge transfer scheme called Federated Knowledge-Transfer Loop (FedKTL).
Our FedKTL can produce client-task-related prototypical image-vector pairs via
the generator's inference on the server. With these pairs, each client can
transfer pre-existing knowledge from the generator to its local model through
an additional supervised local task. We conduct extensive experiments on four
datasets under two types of data heterogeneity with 14 kinds of models
including CNNs and ViTs. Results show that our upload-efficient FedKTL
surpasses seven state-of-the-art methods by up to 7.31% in accuracy. Moreover,
our knowledge transfer scheme is applicable in scenarios with only one edge
client. Code: https://github.com/TsingZ0/FedKTL
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Radical-Cylon: A Heterogeneous Data Pipeline for Scientific Computing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15721v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15721v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arup Kumar Sarker, Aymen Alsaadi, Niranda Perera, Mills Staylor, Gregor von Laszewski, Matteo Turilli, Ozgur Ozan Kilic, Mikhail Titov, Andre Merzky, Shantenu Jha, Geoffrey Fox
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Managing and preparing complex data for deep learning, a prevalent approach
in large-scale data science can be challenging. Data transfer for model
training also presents difficulties, impacting scientific fields like genomics,
climate modeling, and astronomy. A large-scale solution like Google Pathways
with a distributed execution environment for deep learning models exists but is
proprietary. Integrating existing open-source, scalable runtime tools and data
frameworks on high-performance computing (HPC) platforms are crucial to address
these challenges. Our objective is to establish a smooth and unified method of
combining data engineering and deep learning frameworks with diverse execution
capabilities that can be deployed on various high-performance computing
platforms, including cloud and supercomputers. We aim to support heterogeneous
systems with accelerators, where Cylon and other data engineering and deep
learning frameworks can utilize heterogeneous execution. To achieve this, we
propose Radical-Cylon, a heterogeneous runtime system with a parallel and
distributed data framework to execute Cylon as a task of Radical Pilot. We
thoroughly explain Radical-Cylon's design and development and the execution
process of Cylon tasks using Radical Pilot. This approach enables the use of
heterogeneous MPI-communicators across multiple nodes. Radical-Cylon achieves
better performance than Bare-Metal Cylon with minimal and constant overhead.
Radical-Cylon achieves (4~15)% faster execution time than batch execution while
performing similar join and sort operations with 35 million and 3.5 billion
rows with the same resources. The approach aims to excel in both scientific and
engineering research HPC systems while demonstrating robust performance on
cloud infrastructures. This dual capability fosters collaboration and
innovation within the open-source scientific research community.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 16 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Navigating the Landscape of Distributed File Systems: Architectures,
  Implementations, and Considerations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15701v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15701v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xueting Pan, Ziqian Luo, Lisang Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Distributed File Systems (DFS) have emerged as sophisticated solutions for
efficient file storage and management across interconnected computer nodes. The
main objective of DFS is to achieve flexible, scalable, and resilient file
storage management by dispersing file data across multiple interconnected
computer nodes, enabling users to seamlessly access and manipulate files
distributed across diverse nodes. This article provides an overview of DFS, its
architecture, classification methods, design considerations, challenges, and
common implementations. Common DFS implementations discussed include NFS, AFS,
GFS, HDFS, and CephFS, each tailored to specific use cases and design goals.
Understanding the nuances of DFS architecture, classification, and design
considerations is crucial for developing efficient, stable, and secure
distributed file systems to meet diverse user and application needs in modern
computing environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improved Methods of Task Assignment and Resource Allocation with
  Preemption in Edge Computing Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15665v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15665v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Caroline Rublein, Fidan Mehmeti, Mark Mahon, Thomas F. La Porta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Edge computing has become a very popular service that enables mobile devices
to run complex tasks with the help of network-based computing resources.
However, edge clouds are often resource-constrained, which makes resource
allocation a challenging issue. In addition, edge cloud servers must make
allocation decisions with only limited information available, since the arrival
of future client tasks might be impossible to predict, and the states and
behavior of neighboring servers might be obscured. We focus on a distributed
resource allocation method in which servers operate independently and do not
communicate with each other, but interact with clients (tasks) to make
allocation decisions. We follow a two-round bidding approach to assign tasks to
edge cloud servers, and servers are allowed to preempt previous tasks to
allocate more useful ones. We evaluate the performance of our system using
realistic simulations and real-world trace data from a high-performance
computing cluster. Results show that our heuristic improves system-wide
performance by $20-25\%$ over previous work when accounting for the time taken
by each approach. In this way, an ideal trade-off between performance and speed
is achieved.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TablePuppet: A Generic Framework for Relational Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15839v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15839v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lijie Xu, Chulin Xie, Yiran Guo, Gustavo Alonso, Bo Li, Guoliang Li, Wei Wang, Wentao Wu, Ce Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current federated learning (FL) approaches view decentralized training data
as a single table, divided among participants either horizontally (by rows) or
vertically (by columns). However, these approaches are inadequate for handling
distributed relational tables across databases. This scenario requires
intricate SQL operations like joins and unions to obtain the training data,
which is either costly or restricted by privacy concerns. This raises the
question: can we directly run FL on distributed relational tables?
  In this paper, we formalize this problem as relational federated learning
(RFL). We propose TablePuppet, a generic framework for RFL that decomposes the
learning process into two steps: (1) learning over join (LoJ) followed by (2)
learning over union (LoU). In a nutshell, LoJ pushes learning down onto the
vertical tables being joined, and LoU further pushes learning down onto the
horizontal partitions of each vertical table. TablePuppet incorporates
computation/communication optimizations to deal with the duplicate tuples
introduced by joins, as well as differential privacy (DP) to protect against
both feature and label leakages. We demonstrate the efficiency of TablePuppet
in combination with two widely-used ML training algorithms, stochastic gradient
descent (SGD) and alternating direction method of multipliers (ADMM), and
compare their computation/communication complexity. We evaluate the SGD/ADMM
algorithms developed atop TablePuppet by training diverse ML models. Our
experimental results show that TablePuppet achieves model accuracy comparable
to the centralized baselines running directly atop the SQL results. Moreover,
ADMM takes less communication time than SGD to converge to similar model
accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ev-Edge: Efficient Execution of Event-based Vision Algorithms on
  Commodity Edge Platforms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15717v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15717v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shrihari Sridharan, Surya Selvam, Kaushik Roy, Anand Raghunathan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Event cameras have emerged as a promising sensing modality for autonomous
navigation systems, owing to their high temporal resolution, high dynamic range
and negligible motion blur. To process the asynchronous temporal event streams
from such sensors, recent research has shown that a mix of Artificial Neural
Networks (ANNs), Spiking Neural Networks (SNNs) as well as hybrid SNN-ANN
algorithms are necessary to achieve high accuracies across a range of
perception tasks. However, we observe that executing such workloads on
commodity edge platforms which feature heterogeneous processing elements such
as CPUs, GPUs and neural accelerators results in inferior performance. This is
due to the mismatch between the irregular nature of event streams and diverse
characteristics of algorithms on the one hand and the underlying hardware
platform on the other. We propose Ev-Edge, a framework that contains three key
optimizations to boost the performance of event-based vision systems on edge
platforms: (1) An Event2Sparse Frame converter directly transforms raw event
streams into sparse frames, enabling the use of sparse libraries with minimal
encoding overheads (2) A Dynamic Sparse Frame Aggregator merges sparse frames
at runtime by trading off the temporal granularity of events and computational
demand thereby improving hardware utilization (3) A Network Mapper maps
concurrently executing tasks to different processing elements while also
selecting layer precision by considering both compute and communication
overheads. On several state-of-art networks for a range of autonomous
navigation tasks, Ev-Edge achieves 1.28x-2.05x improvements in latency and
1.23x-2.15x in energy over an all-GPU implementation on the NVIDIA Jetson
Xavier AGX platform for single-task execution scenarios. Ev-Edge also achieves
1.43x-1.81x latency improvements over round-robin scheduling methods in
multi-task execution scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A learning-based solution approach to the application placement problem
  in mobile edge computing under uncertainty 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11259v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11259v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taha-Hossein Hejazi, Zahra Ghadimkhani, Arezoo Borji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Placing applications in mobile edge computing servers presents a complex
challenge involving many servers, users, and their requests. Existing
algorithms take a long time to solve high-dimensional problems with significant
uncertainty scenarios. Therefore, an efficient approach is required to maximize
the quality of service while considering all technical constraints. One of
these approaches is machine learning, which emulates optimal solutions for
application placement in edge servers. Machine learning models are expected to
learn how to allocate user requests to servers based on the spatial positions
of users and servers. In this study, the problem is formulated as a two-stage
stochastic programming. A sufficient amount of training records is generated by
varying parameters such as user locations, their request rates, and solving the
optimization model. Then, based on the distance features of each user from the
available servers and their request rates, machine learning models generate
decision variables for the first stage of the stochastic optimization model,
which is the user-to-server request allocation, and are employed as independent
decision agents that reliably mimic the optimization model. Support Vector
Machines (SVM) and Multi-layer Perceptron (MLP) are used in this research to
achieve practical decisions from the stochastic optimization models. The
performance of each model has shown an execution effectiveness of over 80%.
This research aims to provide a more efficient approach for tackling
high-dimensional problems and scenarios with uncertainties in mobile edge
computing by leveraging machine learning models for optimal decision-making in
request allocation to edge servers. These results suggest that machine-learning
models can significantly improve solution times compared to conventional
approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FrankenSplit: Efficient Neural Feature Compression with Shallow
  Variational Bottleneck Injection for Mobile Edge Computing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.10681v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.10681v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alireza Furutanpey, Philipp Raith, Schahram Dustdar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rise of mobile AI accelerators allows latency-sensitive applications to
execute lightweight Deep Neural Networks (DNNs) on the client side. However,
critical applications require powerful models that edge devices cannot host and
must therefore offload requests, where the high-dimensional data will compete
for limited bandwidth. This work proposes shifting away from focusing on
executing shallow layers of partitioned DNNs. Instead, it advocates
concentrating the local resources on variational compression optimized for
machine interpretability. We introduce a novel framework for resource-conscious
compression models and extensively evaluate our method in an environment
reflecting the asymmetric resource distribution between edge devices and
servers. Our method achieves 60% lower bitrate than a state-of-the-art SC
method without decreasing accuracy and is up to 16x faster than offloading with
existing codec standards.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submission to IEEE Transactions on Mobile Computing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Survey</span> for Federated Learning Evaluations: Goals and Measures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.11841v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.11841v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Di Chai, Leye Wang, Liu Yang, Junxue Zhang, Kai Chen, Qiang Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evaluation is a systematic approach to assessing how well a system achieves
its intended purpose. Federated learning (FL) is a novel paradigm for
privacy-preserving machine learning that allows multiple parties to
collaboratively train models without sharing sensitive data. However,
evaluating FL is challenging due to its interdisciplinary nature and diverse
goals, such as utility, efficiency, and security. In this survey, we first
review the major evaluation goals adopted in the existing studies and then
explore the evaluation metrics used for each goal. We also introduce FedEval,
an open-source platform that provides a standardized and comprehensive
evaluation framework for FL algorithms in terms of their utility, efficiency,
and security. Finally, we discuss several challenges and future research
directions for FL evaluation.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Databases
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TablePuppet: A Generic Framework for Relational Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15839v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15839v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lijie Xu, Chulin Xie, Yiran Guo, Gustavo Alonso, Bo Li, Guoliang Li, Wei Wang, Wentao Wu, Ce Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current federated learning (FL) approaches view decentralized training data
as a single table, divided among participants either horizontally (by rows) or
vertically (by columns). However, these approaches are inadequate for handling
distributed relational tables across databases. This scenario requires
intricate SQL operations like joins and unions to obtain the training data,
which is either costly or restricted by privacy concerns. This raises the
question: can we directly run FL on distributed relational tables?
  In this paper, we formalize this problem as relational federated learning
(RFL). We propose TablePuppet, a generic framework for RFL that decomposes the
learning process into two steps: (1) learning over join (LoJ) followed by (2)
learning over union (LoU). In a nutshell, LoJ pushes learning down onto the
vertical tables being joined, and LoU further pushes learning down onto the
horizontal partitions of each vertical table. TablePuppet incorporates
computation/communication optimizations to deal with the duplicate tuples
introduced by joins, as well as differential privacy (DP) to protect against
both feature and label leakages. We demonstrate the efficiency of TablePuppet
in combination with two widely-used ML training algorithms, stochastic gradient
descent (SGD) and alternating direction method of multipliers (ADMM), and
compare their computation/communication complexity. We evaluate the SGD/ADMM
algorithms developed atop TablePuppet by training diverse ML models. Our
experimental results show that TablePuppet achieves model accuracy comparable
to the centralized baselines running directly atop the SQL results. Moreover,
ADMM takes less communication time than SGD to converge to similar model
accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Data Access Paths for Mixed Vector-Relational Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15807v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15807v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Viktor Sanca, Anastasia Ailamaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid growth of machine learning capabilities and the adoption of data
processing methods using vector embeddings sparked a great interest in creating
systems for vector data management. While the predominant approach of vector
data management is to use specialized index structures for fast search over the
entirety of the vector embeddings, once combined with other (meta)data, the
search queries can also become selective on relational attributes - typical for
analytical queries. As using vector indexes differs from traditional relational
data access, we revisit and analyze alternative access paths for efficient
mixed vector-relational search.
  We first evaluate the accurate but exhaustive scan-based search and propose
hardware optimizations and alternative tensor-based formulation and batching to
offset the cost. We outline the complex access-path design space, primarily
driven by relational selectivity, and the decisions to consider when selecting
an exhaustive scan-based search against an approximate index-based approach.
Since the vector index primarily avoids expensive computation across the entire
dataset, contrary to the common relational knowledge, it is better to scan at
lower selectivity and probe at higher, with a cross-point between the two
approaches dictated by data dimensionality and the number of concurrent search
queries.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ No more optimization rules: LLM-enabled policy-based multi-modal query
  optimizer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13597v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13597v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Wang, Haodi Ma, Daisy Zhe Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language model (LLM) has marked a pivotal moment in the field of
machine learning and deep learning. Recently its capability for query planning
has been investigated, including both single-modal and multi-modal queries.
However, there is no work on the query optimization capability of LLM. As a
critical (or could even be the most important) step that significantly impacts
the execution performance of the query plan, such analysis and attempts should
not be missed. From another aspect, existing query optimizers are usually
rule-based or rule-based + cost-based, i.e., they are dependent on manually
created rules to complete the query plan rewrite/transformation. Given the fact
that modern optimizers include hundreds to thousands of rules, designing a
multi-modal query optimizer following a similar way is significantly
time-consuming since we will have to enumerate as many multi-modal optimization
rules as possible, which has not been well addressed today. In this paper, we
investigate the query optimization ability of LLM and use LLM to design LaPuda,
a novel LLM and Policy based multi-modal query optimizer. Instead of
enumerating specific and detailed rules, LaPuda only needs a few abstract
policies to guide LLM in the optimization, by which much time and human effort
are saved. Furthermore, to prevent LLM from making mistakes or negative
optimization, we borrow the idea of gradient descent and propose a guided cost
descent (GCD) algorithm to perform the optimization, such that the optimization
can be kept in the correct direction. In our evaluation, our methods
consistently outperform the baselines in most cases. For example, the optimized
plans generated by our methods result in 1~3x higher execution speed than those
by the baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Yifan and Haodi contribute equally to the work</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Performance
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Offloading and Quality Control for AI Generated Content Services in 6G
  Mobile Edge Computing Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.06203v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.06203v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yitong Wang, Chang Liu, Jun Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  AI-Generated Content (AIGC), as a novel manner of providing Metaverse
services in the forthcoming Internet paradigm, can resolve the obstacles of
immersion requirements. Concurrently, edge computing, as an evolutionary
paradigm of computing in communication systems, effectively augments real-time
interactive services. In pursuit of enhancing the accessibility of AIGC
services, the deployment of AIGC models (e.g., diffusion models) to edge
servers and local devices has become a prevailing trend. Nevertheless, this
approach faces constraints imposed by battery life and computational resources
when tasks are offloaded to local devices, limiting the capacity to deliver
high-quality content to users while adhering to stringent latency requirements.
So there will be a tradeoff between the utility of AIGC models and offloading
decisions in the edge computing paradigm. This paper proposes a joint
optimization algorithm for offloading decisions, computation time, and
diffusion steps of the diffusion models in the reverse diffusion stage.
Moreover, we take the average error into consideration as the metric for
evaluating the quality of the generated results. Experimental results
conclusively demonstrate that the proposed algorithm achieves superior joint
optimization performance compared to the baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper appears in the 2024 IEEE 99th Vehicular Technology
  Conference (VTC)</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Operating Systems
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UPSS: a User-centric Private Storage System with its applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15884v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15884v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arastoo Bozorgi, Mahya Soleimani Jadidi, Jonathan Anderson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Strong confidentiality, integrity, user control, reliability and performance
are critical requirements in privacy-sensitive applications. Such applications
would benefit from a data storage and sharing infrastructure that provides
these properties even in decentralized topologies with untrusted storage
backends, but users today are forced to choose between systemic security
properties and system reliability or performance. As an alternative to this
status quo we present UPSS: the user-centric private sharing system, a
cryptographic storage system that can be used as a conventional filesystem or
as the foundation for security-sensitive applications such as redaction with
integrity and private revision control. We demonstrate that both the security
and performance properties of UPSS exceed that of existing cryptographic
filesystems and that its performance is comparable to mature conventional
filesystems - in some cases, even superior. Whether used directly via its Rust
API or as a conventional filesystem, UPSS provides strong security and
practical performance on untrusted storage.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Science and Game Theory
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Stability of Learning in Network Games with Many Players <span class="chip">AAMAS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15848v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15848v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aamal Hussain, Dan Leonte, Francesco Belardinelli, Georgios Piliouras
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-agent learning algorithms have been shown to display complex, unstable
behaviours in a wide array of games. In fact, previous works indicate that
convergent behaviours are less likely to occur as the total number of agents
increases. This seemingly prohibits convergence to stable strategies, such as
Nash Equilibria, in games with many players.
  To make progress towards addressing this challenge we study the Q-Learning
Dynamics, a classical model for exploration and exploitation in multi-agent
learning. In particular, we study the behaviour of Q-Learning on games where
interactions between agents are constrained by a network. We determine a number
of sufficient conditions, depending on the game and network structure, which
guarantee that agent strategies converge to a unique stable strategy, called
the Quantal Response Equilibrium (QRE). Crucially, these sufficient conditions
are independent of the total number of agents, allowing for provable
convergence in arbitrarily large games.
  Next, we compare the learned QRE to the underlying NE of the game, by showing
that any QRE is an $\epsilon$-approximate Nash Equilibrium. We first provide
tight bounds on $\epsilon$ and show how these bounds lead naturally to a
centralised scheme for choosing exploration rates, which enables independent
learners to learn stable approximate Nash Equilibrium strategies. We validate
the method through experiments and demonstrate its effectiveness even in the
presence of numerous agents and actions. Through these results, we show that
independent learning dynamics may converge to approximate Nash Equilibria, even
in the presence of many agents.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAMAS 2024. arXiv admin note: text overlap with arXiv:2307.13922</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Non-convex potential games for finding global solutions to sensor
  network localization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.03326v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.03326v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gehui Xu, Guanpu Chen, Yiguang Hong, Baris Fidan, Thomas Parisini, Karl H. Johansson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sensor network localization (SNL) problems require determining the physical
coordinates of all sensors in a network. This process relies on the global
coordinates of anchors and the available measurements between non-anchor and
anchor nodes. Attributed to the intrinsic non-convexity, obtaining a globally
optimal solution to SNL is challenging, as well as implementing corresponding
algorithms. In this paper, we formulate a non-convex multi-player potential
game for a generic SNL problem to investigate the identification condition of
the global Nash equilibrium (NE) therein, where the global NE represents the
global solution of SNL. We employ canonical duality theory to transform the
non-convex game into a complementary dual problem. Then we develop a
conjugation-based algorithm to compute the stationary points of the
complementary dual problem. On this basis, we show an identification condition
of the global NE: the stationary point of the proposed algorithm satisfies a
duality relation. Finally, simulation results are provided to validate the
effectiveness of the theoretical results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cooperative networks and f-Shapley value 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.06860v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.06860v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tongseok Lim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lloyd Shapley's cooperative value allocation theory stands as a central
concept in game theory, extensively utilized across various domains to
distribute resources, evaluate individual contributions, and ensure fairness.
The Shapley value formula and his four axioms that characterize it form the
foundation of the theory.
  Traditionally, the Shapley value is assigned under the assumption that all
players in a cooperative game will ultimately form the grand coalition. In this
paper, we reinterpret the Shapley value as an expectation of a certain
stochastic path integral, with each path representing a general coalition
formation process. As a result, the value allocation is naturally extended to
all partial coalition states. In addition, we provide a set of five properties
that extend the Shapley axioms and characterize the stochastic path integral.
Finally, by integrating Hodge calculus, stochastic processes, and path
integration of edge flows on graphs, we expand the cooperative value allocation
theory beyond the standard coalition game structure to encompass a broader
range of cooperative network configurations.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-22T00:00:00Z">2024-03-22</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Distributed, Parallel, and Cluster Computing
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fourier Transform-based Estimators for Data Sketches 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15366v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15366v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seth Pettie, Dingyu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we consider the problem of estimating the $f$-moment
($\sum_{v\in [n]} (f(\mathbf{x}(v))-f(0))$) of a dynamic vector $\mathbf{x}\in
\mathbb{G}^n$ over some abelian group $(\mathbb{G},+)$, where the
$\|f\|_\infty$ norm is bounded. We propose a simple sketch and new estimation
framework based on the \emph{Fourier transform} of $f$. I.e., we decompose $f$
into a linear combination of homomorphisms $f_1,f_2,\ldots$ from
$(\mathbb{G},+)$ to $(\mathbb{C},\times)$, estimate the $f_k$-moment for each
$f_k$, and synthesize them to obtain an estimate of the $f$-moment. Our
estimators are asymptotically unbiased and have variance asymptotic to
$\|\mathbf{x}\|_0^2 (\|f\|_\infty^2 m^{-1} + \|\hat{f}\|_1^2 m^{-2})$, where
the size of the sketch is $O(m\log n\log|\mathbb{G}|)$ bits.
  When $\mathbb{G}=\mathbb{Z}$ this problem can also be solved using
off-the-shelf $\ell_0$-samplers with space $O(m\log^2 n)$ bits, which does not
obviously generalize to finite groups. As a concrete benchmark, we extend
Ganguly, Garofalakis, and Rastogi's singleton-detector-based sampler to work
over $\mathbb{G}$ using $O(m\log n\log|\mathbb{G}|\log(m\log n))$ bits.
  We give some experimental evidence that the Fourier-based estimation
framework is significantly more accurate than sampling-based approaches at the
same memory footprint.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ProvDeploy: Provenance-oriented Containerization of High Performance
  Computing Scientific Workflows 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15324v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15324v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liliane Kunstmann, Débora Pina, Daniel de Oliveira, Marta Mattoso
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many existing scientific workflows require High Performance Computing
environments to produce results in a timely manner. These workflows have
several software library components and use different environments, making the
deployment and execution of the software stack not trivial. This complexity
increases if the user needs to add provenance data capture services to the
workflow. This manuscript introduces ProvDeploy to assist the user in
configuring containers for scientific workflows with integrated provenance data
capture. ProvDeploy was evaluated with a Scientific Machine Learning workflow,
exploring containerization strategies focused on provenance in two distinct HPC
environments
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FSD-Inference: Fully Serverless Distributed Inference with Scalable
  Cloud Communication <span class="chip">ICDE</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15195v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15195v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joe Oakley, Hakan Ferhatosmanoglu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Serverless computing offers attractive scalability, elasticity and
cost-effectiveness. However, constraints on memory, CPU and function runtime
have hindered its adoption for data-intensive applications and machine learning
(ML) workloads. Traditional 'server-ful' platforms enable distributed
computation via fast networks and well-established inter-process communication
(IPC) mechanisms such as MPI and shared memory. In the absence of such
solutions in the serverless domain, parallel computation with significant IPC
requirements is challenging. We present FSD-Inference, the first fully
serverless and highly scalable system for distributed ML inference. We explore
potential communication channels, in conjunction with Function-as-a-Service
(FaaS) compute, to design a state-of-the-art solution for distributed ML within
the context of serverless data-intensive computing. We introduce novel fully
serverless communication schemes for ML inference workloads, leveraging both
cloud-based publish-subscribe/queueing and object storage offerings. We
demonstrate how publish-subscribe/queueing services can be adapted for FaaS IPC
with comparable performance to object storage, while offering significantly
reduced cost at high parallelism levels. We conduct in-depth experiments on
benchmark DNNs of various sizes. The results show that when compared to
server-based alternatives, FSD-Inference is significantly more cost-effective
and scalable, and can even achieve competitive performance against optimized
HPC solutions. Experiments also confirm that our serverless solution can handle
large distributed workloads and leverage high degrees of FaaS parallelism.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In Proceedings of 2024 IEEE 40th International Conference on Data
  Engineering (ICDE) (to appear)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ECHO: Efficient Off-Chain Payments and Cross-Chain Swaps for
  Cryptocurrencies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15191v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15191v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Di Wu, Jian Liu, Zhengwei Hou, Wu Wen, Kui Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present ECHO, a TEE-based layer-2 solution that tackles two
crucial challenges in the realm of cryptocurrencies: off-chain payments and
cross-chain swaps. It offers three notable features: - Channel-free off-chain
payments: it allows a payer to make direct payments to anyone without requiring
any on-chain relationship or intermediary channels. - Real-time yet
decentralized cross-chain swaps: it is the first known solution that enables
real-time cross-chain swaps without relying on a central server. This novel
feature is made possible through a ground-breaking fair exchange protocol. -
TEE crash-tolerance: it offers two solutions to handle TEE crashes, one of
which involves an innovative application of time-lock puzzles in this context.
We evaluate ECHO on a network consists of 1000 nodes and the evaluation results
show that ECHO can achieve 7000 TPS
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Modeling Distributed Computing Infrastructures for HEP Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14903v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14903v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maximilian Horzela, Henri Casanova, Manuel Giffels, Artur Gottmann, Robin Hofsaess, Günter Quast, Simone Rossi Tisbeni, Achim Streit, Frédéric Suter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Predicting the performance of various infrastructure design options in
complex federated infrastructures with computing sites distributed over a wide
area network that support a plethora of users and workflows, such as the
Worldwide LHC Computing Grid (WLCG), is not trivial. Due to the complexity and
size of these infrastructures, it is not feasible to deploy experimental
test-beds at large scales merely for the purpose of comparing and evaluating
alternate designs. An alternative is to study the behaviours of these systems
using simulation. This approach has been used successfully in the past to
identify efficient and practical infrastructure designs for High Energy Physics
(HEP). A prominent example is the Monarc simulation framework, which was used
to study the initial structure of the WLCG. New simulation capabilities are
needed to simulate large-scale heterogeneous computing systems with complex
networks, data access and caching patterns. A modern tool to simulate HEP
workloads that execute on distributed computing infrastructures based on the
SimGrid and WRENCH simulation frameworks is outlined. Studies of its accuracy
and scalability are presented using HEP as a case-study. Hypothetical
adjustments to prevailing computing architectures in HEP are studied providing
insights into the dynamics of a part of the WLCG and candidates for
improvements.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in Proceedings of the 26th International Conference on
  Computing in High Energy & Nuclear Physics (CHEP2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fast real-time arbitrary waveform generation using graphic processing
  units 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15582v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15582v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juntian Tu, Sarthak Subhankar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-time Arbitrary Waveform Generation (AWG) is essential in various
engineering and research applications, and often requires complex bespoke
hardware and software. This paper introduces an AWG framework using an NVIDIA
Graphics Processing Unit (GPU) and a commercially available high-speed
Digital-to-Analog Converter (DAC) card, both running on a desktop personal
computer (PC). The GPU accelerates the "embarrassingly" data parallel additive
waveform synthesis framework for AWG, and the DAC reconstructs the generated
waveform in the analog domain at high speed. The AWG framework is programmed
using the developer-friendly Compute Unified Device Architecture (CUDA) runtime
application programming interface from NVIDIA and is readily customizable, and
scalable with additional parallel hardware. We present and characterize two
different pathways for computing modulated radio-frequency (rf) waveforms: one
pathway offers high-complexity simultaneous chirping of 1000 individual
Nyquist-limited single-frequency tones for 35 ms at a sampling rate of 560
MB/s, and the other pathway allows simultaneous continuous chirping of 194
individual Nyquist-limited single-frequency tones at 100 MB/s, or 20 individual
tones at 560 MB/s. This AWG framework is designed for fast on-the-fly
rearrangement of a large stochastically-loaded optical tweezer array of single
atoms or molecules into a defect-free array needed for quantum simulation and
quantum computation applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LOOPer: A Learned Automatic Code Optimizer For Polyhedral Compilers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11522v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11522v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Massinissa Merouani, Khaled Afif Boudaoud, Iheb Nassim Aouadj, Nassim Tchoulak, Islem Kara Bernou, Hamza Benyamina, Fatima Benbouzid-Si Tayeb, Karima Benatchba, Hugh Leather, Riyadh Baghdadi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While polyhedral compilers have shown success in implementing advanced code
transformations, they still have challenges in selecting the most profitable
transformations that lead to the best speedups. This has motivated the use of
machine learning to build cost models to guide the search for polyhedral
optimizations. State-of-the-art polyhedral compilers have demonstrated a viable
proof-of-concept of this approach. While such a proof-of-concept has shown
promise, it still has significant limitations. State-of-the-art polyhedral
compilers that use a deep-learning cost model only support a small subset of
affine transformations, limiting their ability to apply complex code
transformations. They also only support simple programs that have a single loop
nest and a rectangular iteration domain, limiting their applicability to many
programs. These limitations significantly impact the generality of such
compilers and autoschedulers and put into question the whole approach. In this
paper, we introduce LOOPer, the first polyhedral autoscheduler that uses a
deep-learning based cost model and covers a large set of affine transformations
and programs. It supports the exploration of a large set of affine
transformations, allowing the application of complex sequences of polyhedral
transformations. It also supports the optimization of programs with multiple
loop nests and with rectangular and non-rectangular iteration domains, allowing
the optimization of an extensive set of programs. We implement and evaluate
LOOPer and show that it achieves speedups over the state-of-the-art. On the
Polybench benchmark, LOOPer achieves a geometric mean speedup of 1.59x over
Tiramisu. LOOPer also achieves competitive speedups with a geometric mean
speedup of 1.34x over Pluto, a state-of-the-art polyhedral compiler that does
not use a machine-learning based cost model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sparse Mean Field Load Balancing in Large Localized Queueing Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.12973v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.12973v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anam Tahir, Kai Cui, Heinz Koeppl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scalable load balancing algorithms are of great interest in cloud networks
and data centers, necessitating the use of tractable techniques to compute
optimal load balancing policies for good performance. However, most existing
scalable techniques, especially asymptotically scaling methods based on mean
field theory, have not been able to model large queueing networks with strong
locality. Meanwhile, general multi-agent reinforcement learning techniques can
be hard to scale and usually lack a theoretical foundation. In this work, we
address this challenge by leveraging recent advances in sparse mean field
theory to learn a near-optimal load balancing policy in sparsely connected
queueing networks in a tractable manner, which may be preferable to global
approaches in terms of wireless communication overhead. Importantly, we obtain
a general load balancing framework for a large class of sparse bounded-degree
wireless topologies. By formulating a novel mean field control problem in the
context of graphs with bounded degree, we reduce the otherwise difficult
multi-agent problem to a single-agent problem. Theoretically, the approach is
justified by approximation guarantees. Empirically, the proposed methodology
performs well on several realistic and scalable wireless network topologies as
compared to a number of well-known load balancing heuristics and existing
scalable multi-agent reinforcement learning methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ No distributed quantum advantage for approximate graph coloring <span class="chip">STOC 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.09444v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.09444v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xavier Coiteux-Roy, Francesco d'Amore, Rishikesh Gajjala, Fabian Kuhn, François Le Gall, Henrik Lievonen, Augusto Modanese, Marc-Olivier Renou, Gustav Schmid, Jukka Suomela
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We give an almost complete characterization of the hardness of $c$-coloring
$\chi$-chromatic graphs with distributed algorithms, for a wide range of models
of distributed computing. In particular, we show that these problems do not
admit any distributed quantum advantage. To do that: 1) We give a new
distributed algorithm that finds a $c$-coloring in $\chi$-chromatic graphs in
$\tilde{\mathcal{O}}(n^{\frac{1}{\alpha}})$ rounds, with $\alpha =
\bigl\lfloor\frac{c-1}{\chi - 1}\bigr\rfloor$. 2) We prove that any distributed
algorithm for this problem requires $\Omega(n^{\frac{1}{\alpha}})$ rounds.
  Our upper bound holds in the classical, deterministic LOCAL model, while the
near-matching lower bound holds in the non-signaling model. This model,
introduced by Arfaoui and Fraigniaud in 2014, captures all models of
distributed graph algorithms that obey physical causality; this includes not
only classical deterministic LOCAL and randomized LOCAL but also quantum-LOCAL,
even with a pre-shared quantum state.
  We also show that similar arguments can be used to prove that, e.g.,
3-coloring 2-dimensional grids or $c$-coloring trees remain hard problems even
for the non-signaling model, and in particular do not admit any quantum
advantage. Our lower-bound arguments are purely graph-theoretic at heart; no
background on quantum information theory is needed to establish the proofs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to STOC 2024</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Artificial Intelligence
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLaVA-PruMerge: Adaptive Token Reduction for Efficient Large Multimodal
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15388v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15388v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuzhang Shang, Mu Cai, Bingxin Xu, Yong Jae Lee, Yan Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Multimodal Models (LMMs) have shown significant reasoning capabilities
by connecting a visual encoder and a large language model. LMMs typically use a
fixed amount of visual tokens, such as the penultimate layer features in the
CLIP visual encoder, as the prefix content. Recent LMMs incorporate more
complex visual inputs, such as high-resolution images and videos, which
increase the number of visual tokens significantly. However, due to the design
of the Transformer architecture, computational costs associated with these
models tend to increase quadratically with the number of input tokens. To
tackle this problem, we explore a token reduction mechanism and find, similar
to prior work, that many visual tokens are spatially redundant. Based on this,
we propose PruMerge, a novel adaptive visual token reduction approach, which
largely reduces the number of visual tokens while maintaining comparable model
performance. We first select the unpruned visual tokens based on their
similarity to class tokens and spatial tokens. We then cluster the pruned
tokens based on key similarity and merge the clustered tokens with the unpruned
tokens to supplement their information. Empirically, when applied to LLaVA-1.5,
our approach can compress the visual tokens by 14.4 times on average, and
achieve comparable performance across diverse visual question-answering and
reasoning tasks. Code and checkpoints are at https://llava-prumerge.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://llava-prumerge.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LATTE3D: Large-scale Amortized Text-To-Enhanced3D Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15385v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15385v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kevin Xie, Jonathan Lorraine, Tianshi Cao, Jun Gao, James Lucas, Antonio Torralba, Sanja Fidler, Xiaohui Zeng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent text-to-3D generation approaches produce impressive 3D results but
require time-consuming optimization that can take up to an hour per prompt.
Amortized methods like ATT3D optimize multiple prompts simultaneously to
improve efficiency, enabling fast text-to-3D synthesis. However, they cannot
capture high-frequency geometry and texture details and struggle to scale to
large prompt sets, so they generalize poorly. We introduce LATTE3D, addressing
these limitations to achieve fast, high-quality generation on a significantly
larger prompt set. Key to our method is 1) building a scalable architecture and
2) leveraging 3D data during optimization through 3D-aware diffusion priors,
shape regularization, and model initialization to achieve robustness to diverse
and complex training prompts. LATTE3D amortizes both neural field and textured
surface generation to produce highly detailed textured meshes in a single
forward pass. LATTE3D generates 3D objects in 400ms, and can be further
enhanced with fast test-time optimization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>See the project website at
  https://research.nvidia.com/labs/toronto-ai/LATTE3D/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can large language models explore in-context? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15371v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15371v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akshay Krishnamurthy, Keegan Harris, Dylan J. Foster, Cyril Zhang, Aleksandrs Slivkins
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate the extent to which contemporary Large Language Models (LLMs)
can engage in exploration, a core capability in reinforcement learning and
decision making. We focus on native performance of existing LLMs, without
training interventions. We deploy LLMs as agents in simple multi-armed bandit
environments, specifying the environment description and interaction history
entirely in-context, i.e., within the LLM prompt. We experiment with GPT-3.5,
GPT-4, and Llama2, using a variety of prompt designs, and find that the models
do not robustly engage in exploration without substantial interventions: i)
Across all of our experiments, only one configuration resulted in satisfactory
exploratory behavior: GPT-4 with chain-of-thought reasoning and an externally
summarized interaction history, presented as sufficient statistics; ii) All
other configurations did not result in robust exploratory behavior, including
those with chain-of-thought reasoning but unsummarized history. Although these
findings can be interpreted positively, they suggest that external
summarization -- which may not be possible in more complex settings -- is
important for obtaining desirable behavior from LLM agents. We conclude that
non-trivial algorithmic interventions, such as fine-tuning or dataset curation,
may be required to empower LLM-based decision making agents in complex
settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CoLLEGe: Concept Embedding Generation for Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15362v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15362v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryan Teehan, Brenden Lake, Mengye Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current language models are unable to quickly learn new concepts on the fly,
often requiring a more involved finetuning process to learn robustly. Prompting
in-context is not robust to context distractions, and often fails to confer
much information about the new concepts. Classic methods for few-shot word
learning in NLP, relying on global word vectors, are less applicable to large
language models. In this paper, we introduce a novel approach named CoLLEGe
(Concept Learning with Language Embedding Generation) to modernize few-shot
concept learning. CoLLEGe is a meta-learning framework capable of generating
flexible embeddings for new concepts using a small number of example sentences
or definitions. Our primary meta-learning objective is simply to facilitate a
language model to make next word predictions in forthcoming sentences, making
it compatible with language model pretraining. We design a series of tasks to
test new concept learning in challenging real-world scenarios, including new
word acquisition, definition inference, and verbal reasoning, and demonstrate
that our method succeeds in each setting without task-specific training.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Collaborative AI Teaming in Unknown Environments via Active Goal
  Deduction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15341v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15341v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zuyuan Zhang, Hanhan Zhou, Mahdi Imani, Taeyoung Lee, Tian Lan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the advancements of artificial intelligence (AI), we're seeing more
scenarios that require AI to work closely with other agents, whose goals and
strategies might not be known beforehand. However, existing approaches for
training collaborative agents often require defined and known reward signals
and cannot address the problem of teaming with unknown agents that often have
latent objectives/rewards. In response to this challenge, we propose teaming
with unknown agents framework, which leverages kernel density Bayesian inverse
learning method for active goal deduction and utilizes pre-trained,
goal-conditioned policies to enable zero-shot policy adaptation. We prove that
unbiased reward estimates in our framework are sufficient for optimal teaming
with unknown agents. We further evaluate the framework of redesigned
multi-agent particle and StarCraft II micromanagement environments with diverse
unknown agents of different behaviors/rewards. Empirical results demonstrate
that our framework significantly advances the teaming performance of AI and
unknown agents in a wide range of collaborative scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Technological Perspective on Misuse of Available AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15325v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15325v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lukas Pöhler, Valentin Schrader, Alexander Ladwein, Florian von Keller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Potential malicious misuse of civilian artificial intelligence (AI) poses
serious threats to security on a national and international level. Besides
defining autonomous systems from a technological viewpoint and explaining how
AI development is characterized, we show how already existing and openly
available AI technology could be misused. To underline this, we developed three
exemplary use cases of potentially misused AI that threaten political, digital
and physical security. The use cases can be built from existing AI technologies
and components from academia, the private sector and the developer-community.
This shows how freely available AI can be combined into autonomous weapon
systems. Based on the use cases, we deduce points of control and further
measures to prevent the potential threat through misused AI. Further, we
promote the consideration of malicious misuse of civilian AI systems in the
discussion on autonomous weapon systems (AWS).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at the UN Meeting of the Group of Governmental Experts on
  Lethal Autonomous Weapons Systems, 30 August 2018</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Point-DETR3D: Leveraging Imagery Data with Spatial Point Prior for
  Weakly Semi-supervised 3D Object Detection <span class="chip">AAAI2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15317v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15317v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongzhi Gao, Zheng Chen, Zehui Chen, Lin Chen, Jiaming Liu, Shanghang Zhang, Feng Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training high-accuracy 3D detectors necessitates massive labeled 3D
annotations with 7 degree-of-freedom, which is laborious and time-consuming.
Therefore, the form of point annotations is proposed to offer significant
prospects for practical applications in 3D detection, which is not only more
accessible and less expensive but also provides strong spatial information for
object localization.In this paper, we empirically discover that it is
non-trivial to merely adapt Point-DETR to its 3D form, encountering two main
bottlenecks: 1) it fails to encode strong 3D prior into the model, and 2) it
generates low-quality pseudo labels in distant regions due to the extreme
sparsity of LiDAR points. To overcome these challenges, we introduce
Point-DETR3D, a teacher-student framework for weakly semi-supervised 3D
detection, designed to fully capitalize on point-wise supervision within a
constrained instance-wise annotation budget.Different from Point-DETR which
encodes 3D positional information solely through a point encoder, we propose an
explicit positional query initialization strategy to enhance the positional
prior. Considering the low quality of pseudo labels at distant regions produced
by the teacher model, we enhance the detector's perception by incorporating
dense imagery data through a novel Cross-Modal Deformable RoI Fusion
(D-RoI).Moreover, an innovative point-guided self-supervised learning technique
is proposed to allow for fully exploiting point priors, even in student
models.Extensive experiments on representative nuScenes dataset demonstrate our
Point-DETR3D obtains significant improvements compared to previous works.
Notably, with only 5% of labeled data, Point-DETR3D achieves over 90%
performance of its fully supervised counterpart.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CR3DT: Camera-RADAR Fusion for 3D Detection and Tracking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15313v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15313v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicolas Baumann, Michael Baumgartner, Edoardo Ghignone, Jonas Kühne, Tobias Fischer, Yung-Hsu Yang, Marc Pollefeys, Michele Magno
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate detection and tracking of surrounding objects is essential to enable
self-driving vehicles. While Light Detection and Ranging (LiDAR) sensors have
set the benchmark for high performance, the appeal of camera-only solutions
lies in their cost-effectiveness. Notably, despite the prevalent use of Radio
Detection and Ranging (RADAR) sensors in automotive systems, their potential in
3D detection and tracking has been largely disregarded due to data sparsity and
measurement noise. As a recent development, the combination of RADARs and
cameras is emerging as a promising solution. This paper presents Camera-RADAR
3D Detection and Tracking (CR3DT), a camera-RADAR fusion model for 3D object
detection, and Multi-Object Tracking (MOT). Building upon the foundations of
the State-of-the-Art (SotA) camera-only BEVDet architecture, CR3DT demonstrates
substantial improvements in both detection and tracking capabilities, by
incorporating the spatial and velocity information of the RADAR sensor.
Experimental results demonstrate an absolute improvement in detection
performance of 5.3% in mean Average Precision (mAP) and a 14.9% increase in
Average Multi-Object Tracking Accuracy (AMOTA) on the nuScenes dataset when
leveraging both modalities. CR3DT bridges the gap between high-performance and
cost-effective perception systems in autonomous driving, by capitalizing on the
ubiquitous presence of RADAR in automotive applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ KTbench: A Novel Data Leakage-Free Framework for Knowledge Tracing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15304v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15304v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yahya Badran, Christine Preisach
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge Tracing (KT) is concerned with predicting students' future
performance on learning items in intelligent tutoring systems. Learning items
are tagged with skill labels called knowledge concepts (KCs). Many KT models
expand the sequence of item-student interactions into KC-student interactions
by replacing learning items with their constituting KCs. This often results in
a longer sequence length. This approach addresses the issue of sparse
item-student interactions and minimises model parameters. However, two problems
have been identified with such models.
  The first problem is the model's ability to learn correlations between KCs
belonging to the same item, which can result in the leakage of ground truth
labels and hinder performance. This problem can lead to a significant decrease
in performance on datasets with a higher number of KCs per item. The second
problem is that the available benchmark implementations ignore accounting for
changes in sequence length when expanding KCs, leading to different models
being tested with varying sequence lengths but still compared against the same
benchmark.
  To address these problems, we introduce a general masking framework that
mitigates the first problem and enhances the performance of such KT models
while preserving the original model architecture without significant
alterations. Additionally, we introduce KTbench, an open-source benchmark
library designed to ensure the reproducibility of this work while mitigating
the second problem.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Planning with a Learned Policy Basis to Optimally Solve Complex Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15301v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15301v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guillermo Infante, David Kuric, Anders Jonsson, Vicenç Gómez, Herke van Hoof
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conventional reinforcement learning (RL) methods can successfully solve a
wide range of sequential decision problems. However, learning policies that can
generalize predictably across multiple tasks in a setting with non-Markovian
reward specifications is a challenging problem. We propose to use successor
features to learn a policy basis so that each (sub)policy in it solves a
well-defined subproblem. In a task described by a finite state automaton (FSA)
that involves the same set of subproblems, the combination of these
(sub)policies can then be used to generate an optimal solution without
additional learning. In contrast to other methods that combine (sub)policies
via planning, our method asymptotically attains global optimality, even in
stochastic environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sphere Neural-Networks for Rational Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15297v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15297v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tiansi Dong, Mateja Jamnik, Pietro Liò
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The success of Large Language Models (LLMs), e.g., ChatGPT, is witnessed by
their planetary popularity, their capability of human-like question-answering,
and also by their steadily improved reasoning performance. However, it remains
unclear whether LLMs reason. It is an open problem how traditional neural
networks can be qualitatively extended to go beyond the statistic paradigm and
achieve high-level cognition. Here, we present a minimalist qualitative
extension by generalising computational building blocks from vectors to
spheres. We propose Sphere Neural Networks (SphNNs) for human-like reasoning
through model construction and inspection, and develop SphNN for syllogistic
reasoning, a microcosm of human rationality. Instead of training data, SphNN
uses a neuro-symbolic transition map of neighbourhood spatial relations to
guide transformations from the current sphere configuration towards the target.
SphNN is the first neural model that can determine the validity of long-chained
syllogistic reasoning in one epoch by constructing sphere configurations as
Euler diagrams, with the worst computational complexity of O(N^2). SphNN can
evolve into various types of reasoning, such as spatio-temporal reasoning,
logical reasoning with negation and disjunction, event reasoning,
neuro-symbolic reasoning, and humour understanding (the highest level of
cognition). All these suggest a new kind of Herbert A. Simon's scissors with
two neural blades. SphNNs will tremendously enhance interdisciplinary
collaborations to develop the two neural blades and realise deterministic
neural reasoning and human-bounded rationality and elevate LLMs to reliable
psychological AI. This work suggests that the non-zero radii of spheres are the
missing components that prevent traditional deep-learning systems from reaching
the realm of rational reasoning and cause LLMs to be trapped in the swamp of
hallucination.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bioinformatics and Biomedical Informatics with Chat<span class="highlight-title">GPT</span>: Year One <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15274v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15274v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinge Wang, Zien Cheng, Qiuming Yao, Li Liu, Dong Xu, Gangqing Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The year 2023 marked a significant surge in the exploration of applying large
language model (LLM) chatbots, notably ChatGPT, across various disciplines. We
surveyed the applications of ChatGPT in various sectors of bioinformatics and
biomedical informatics throughout the year, covering omics, genetics,
biomedical text mining, drug discovery, biomedical image understanding,
bioinformatics programming, and bioinformatics education. Our survey delineates
the current strengths and limitations of this chatbot in bioinformatics and
offers insights into potential avenues for future development.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 3 Figures, 1 Table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hierarchical Information Enhancement Network for Cascade Prediction in
  Social Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15257v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15257v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fanrui Zhang, Jiawei Liu, Qiang Zhang, Xiaoling Zhu, Zheng-Jun Zha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding information cascades in networks is a fundamental issue in
numerous applications. Current researches often sample cascade information into
several independent paths or subgraphs to learn a simple cascade
representation. However, these approaches fail to exploit the hierarchical
semantic associations between different modalities, limiting their predictive
performance. In this work, we propose a novel Hierarchical Information
Enhancement Network (HIENet) for cascade prediction. Our approach integrates
fundamental cascade sequence, user social graphs, and sub-cascade graph into a
unified framework. Specifically, HIENet utilizes DeepWalk to sample cascades
information into a series of sequences. It then gathers path information
between users to extract the social relationships of propagators. Additionally,
we employ a time-stamped graph convolutional network to aggregate sub-cascade
graph information effectively. Ultimately, we introduce a Multi-modal Cascade
Transformer to powerfully fuse these clues, providing a comprehensive
understanding of cascading process. Extensive experiments have demonstrated the
effectiveness of the proposed method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Safe Learning of PDDL Domains with Conditional Effects -- Extended
  Version 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15251v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15251v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Argaman Mordoch, Enrico Scala, Roni Stern, Brendan Juba
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Powerful domain-independent planners have been developed to solve various
types of planning problems. These planners often require a model of the acting
agent's actions, given in some planning domain description language. Manually
designing such an action model is a notoriously challenging task. An
alternative is to automatically learn action models from observation. Such an
action model is called safe if every plan created with it is consistent with
the real, unknown action model. Algorithms for learning such safe action models
exist, yet they cannot handle domains with conditional or universal effects,
which are common constructs in many planning problems. We prove that learning
non-trivial safe action models with conditional effects may require an
exponential number of samples. Then, we identify reasonable assumptions under
which such learning is tractable and propose SAM Learning of Conditional
Effects (Conditional-SAM), the first algorithm capable of doing so. We analyze
Conditional-SAM theoretically and evaluate it experimentally. Our results show
that the action models learned by Conditional-SAM can be used to solve
perfectly most of the test set problems in most of the experimented domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Comprehensive Reassessment of Large-Scale Evaluation Outcomes in LLMs: A
  Multifaceted Statistical Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15250v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15250v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kun Sun, Rong Wang, Haitao Liu, Anders Søgaard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Amidst the rapid evolution of LLMs, the significance of evaluation in
comprehending and propelling these models forward is increasingly paramount.
Evaluations have revealed that factors such as scaling, training types,
architectures and other factors profoundly impact the performance of LLMs.
However, the extent and nature of these impacts continue to be subjects of
debate because most assessments have been restricted to a limited number of
models and data points. Clarifying the effects of these factors on performance
scores can be more effectively achieved through a statistical lens. Our study
embarks on a thorough re-examination of these LLMs, targeting the inadequacies
in current evaluation methods. With the advent of a uniform evaluation
framework, our research leverages an expansive dataset of evaluation results,
introducing a comprehensive statistical methodology. This includes the
application of ANOVA, Tukey HSD tests, GAMM, and clustering technique, offering
a robust and transparent approach to deciphering LLM performance data. Contrary
to prevailing findings, our results challenge assumptions about emergent
abilities and the influence of given training types and architectures in LLMs.
These findings furnish new perspectives on the characteristics, intrinsic
nature, and developmental trajectories of LLMs. By providing straightforward
and reliable methods to scrutinize and reassess LLM performance data, this
study contributes a nuanced perspective on LLM efficiency and potentials.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Spectral Motion Alignment for Video Motion Transfer using Diffusion
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15249v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15249v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Geon Yeong Park, Hyeonho Jeong, Sang Wan Lee, Jong Chul Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The evolution of diffusion models has greatly impacted video generation and
understanding. Particularly, text-to-video diffusion models (VDMs) have
significantly facilitated the customization of input video with target
appearance, motion, etc. Despite these advances, challenges persist in
accurately distilling motion information from video frames. While existing
works leverage the consecutive frame residual as the target motion vector, they
inherently lack global motion context and are vulnerable to frame-wise
distortions. To address this, we present Spectral Motion Alignment (SMA), a
novel framework that refines and aligns motion vectors using Fourier and
wavelet transforms. SMA learns motion patterns by incorporating
frequency-domain regularization, facilitating the learning of whole-frame
global motion dynamics, and mitigating spatial artifacts. Extensive experiments
demonstrate SMA's efficacy in improving motion transfer while maintaining
computational efficiency and compatibility across various video customization
frameworks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page:
  https://geonyeong-park.github.io/spectral-motion-alignment/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Self-Supervised</span> Backbone Framework for Diverse Agricultural Vision Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15248v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15248v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sudhir Sornapudi, Rajhans Singh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Computer vision in agriculture is game-changing with its ability to transform
farming into a data-driven, precise, and sustainable industry. Deep learning
has empowered agriculture vision to analyze vast, complex visual data, but
heavily rely on the availability of large annotated datasets. This remains a
bottleneck as manual labeling is error-prone, time-consuming, and expensive.
The lack of efficient labeling approaches inspired us to consider
self-supervised learning as a paradigm shift, learning meaningful feature
representations from raw agricultural image data. In this work, we explore how
self-supervised representation learning unlocks the potential applicability to
diverse agriculture vision tasks by eliminating the need for large-scale
annotated datasets. We propose a lightweight framework utilizing SimCLR, a
contrastive learning approach, to pre-train a ResNet-50 backbone on a large,
unannotated dataset of real-world agriculture field images. Our experimental
analysis and results indicate that the model learns robust features applicable
to a broad range of downstream agriculture tasks discussed in the paper.
Additionally, the reduced reliance on annotated data makes our approach more
cost-effective and accessible, paving the way for broader adoption of computer
vision in agriculture.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reasoning-Enhanced Object-Centric Learning for Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15245v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15245v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jian Li, Pu Ren, Yang Liu, Hao Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object-centric learning aims to break down complex visual scenes into more
manageable object representations, enhancing the understanding and reasoning
abilities of machine learning systems toward the physical world. Recently,
slot-based video models have demonstrated remarkable proficiency in segmenting
and tracking objects, but they overlook the importance of the effective
reasoning module. In the real world, reasoning and predictive abilities play a
crucial role in human perception and object tracking; in particular, these
abilities are closely related to human intuitive physics. Inspired by this, we
designed a novel reasoning module called the Slot-based Time-Space Transformer
with Memory buffer (STATM) to enhance the model's perception ability in complex
scenes. The memory buffer primarily serves as storage for slot information from
upstream modules, the Slot-based Time-Space Transformer makes predictions
through slot-based spatiotemporal attention computations and fusion. Our
experiment results on various datasets show that STATM can significantly
enhance object-centric learning capabilities of slot-based video models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-perspective Memory Enhanced Network for Identifying Key Nodes in
  Social Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15235v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15235v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiang Zhang, Jiawei Liu, Fanrui Zhang, Xiaoling Zhu, Zheng-Jun Zha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Identifying key nodes in social networks plays a crucial role in timely
blocking false information. Existing key node identification methods usually
consider node influence only from the propagation structure perspective and
have insufficient generalization ability to unknown scenarios. In this paper,
we propose a novel Multi-perspective Memory Enhanced Network (MMEN) for
identifying key nodes in social networks, which mines key nodes from multiple
perspectives and utilizes memory networks to store historical information.
Specifically, MMEN first constructs two propagation networks from the
perspectives of user attributes and propagation structure and updates node
feature representations using graph attention networks. Meanwhile, the memory
network is employed to store information of similar subgraphs, enhancing the
model's generalization performance in unknown scenarios. Finally, MMEN applies
adaptive weights to combine the node influence of the two propagation networks
to select the ultimate key nodes. Extensive experiments demonstrate that our
method significantly outperforms previous methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 1 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Anytime, Anywhere, Anyone: Investigating the Feasibility of Segment
  Anything Model for Crowd-Sourcing Medical Image Annotations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15218v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15218v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pranav Kulkarni, Adway Kanhere, Dharmam Savani, Andrew Chan, Devina Chatterjee, Paul H. Yi, Vishwa S. Parekh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Curating annotations for medical image segmentation is a labor-intensive and
time-consuming task that requires domain expertise, resulting in "narrowly"
focused deep learning (DL) models with limited translational utility. Recently,
foundation models like the Segment Anything Model (SAM) have revolutionized
semantic segmentation with exceptional zero-shot generalizability across
various domains, including medical imaging, and hold a lot of promise for
streamlining the annotation process. However, SAM has yet to be evaluated in a
crowd-sourced setting to curate annotations for training 3D DL segmentation
models. In this work, we explore the potential of SAM for crowd-sourcing
"sparse" annotations from non-experts to generate "dense" segmentation masks
for training 3D nnU-Net models, a state-of-the-art DL segmentation model. Our
results indicate that while SAM-generated annotations exhibit high mean Dice
scores compared to ground-truth annotations, nnU-Net models trained on
SAM-generated annotations perform significantly worse than nnU-Net models
trained on ground-truth annotations ($p<0.001$, all).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ (Un)making AI Magic: a Design Taxonomy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15216v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15216v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maria Luce Lupetti, Dave Murray-Rust
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper examines the role that enchantment plays in the design of AI
things by constructing a taxonomy of design approaches that increase or
decrease the perception of magic and enchantment. We start from the design
discourse surrounding recent developments in AI technologies, highlighting
specific interaction qualities such as algorithmic uncertainties and errors and
articulating relations to the rhetoric of magic and supernatural thinking.
Through analyzing and reflecting upon 52 students' design projects from two
editions of a Master course in design and AI, we identify seven design
principles and unpack the effects of each in terms of enchantment and
disenchantment. We conclude by articulating ways in which this taxonomy can be
approached and appropriated by design/HCI practitioners, especially to support
exploration and reflexivity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FSD-Inference: Fully Serverless Distributed Inference with Scalable
  Cloud Communication <span class="chip">ICDE</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15195v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15195v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joe Oakley, Hakan Ferhatosmanoglu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Serverless computing offers attractive scalability, elasticity and
cost-effectiveness. However, constraints on memory, CPU and function runtime
have hindered its adoption for data-intensive applications and machine learning
(ML) workloads. Traditional 'server-ful' platforms enable distributed
computation via fast networks and well-established inter-process communication
(IPC) mechanisms such as MPI and shared memory. In the absence of such
solutions in the serverless domain, parallel computation with significant IPC
requirements is challenging. We present FSD-Inference, the first fully
serverless and highly scalable system for distributed ML inference. We explore
potential communication channels, in conjunction with Function-as-a-Service
(FaaS) compute, to design a state-of-the-art solution for distributed ML within
the context of serverless data-intensive computing. We introduce novel fully
serverless communication schemes for ML inference workloads, leveraging both
cloud-based publish-subscribe/queueing and object storage offerings. We
demonstrate how publish-subscribe/queueing services can be adapted for FaaS IPC
with comparable performance to object storage, while offering significantly
reduced cost at high parallelism levels. We conduct in-depth experiments on
benchmark DNNs of various sizes. The results show that when compared to
server-based alternatives, FSD-Inference is significantly more cost-effective
and scalable, and can even achieve competitive performance against optimized
HPC solutions. Experiments also confirm that our serverless solution can handle
large distributed workloads and leverage high degrees of FaaS parallelism.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In Proceedings of 2024 IEEE 40th International Conference on Data
  Engineering (ICDE) (to appear)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SFOD: Spiking Fusion Object Detector <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15192v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15192v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yimeng Fan, Wei Zhang, Changsong Liu, Mingyang Li, Wenrui Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Event cameras, characterized by high temporal resolution, high dynamic range,
low power consumption, and high pixel bandwidth, offer unique capabilities for
object detection in specialized contexts. Despite these advantages, the
inherent sparsity and asynchrony of event data pose challenges to existing
object detection algorithms. Spiking Neural Networks (SNNs), inspired by the
way the human brain codes and processes information, offer a potential solution
to these difficulties. However, their performance in object detection using
event cameras is limited in current implementations. In this paper, we propose
the Spiking Fusion Object Detector (SFOD), a simple and efficient approach to
SNN-based object detection. Specifically, we design a Spiking Fusion Module,
achieving the first-time fusion of feature maps from different scales in SNNs
applied to event cameras. Additionally, through integrating our analysis and
experiments conducted during the pretraining of the backbone network on the
NCAR dataset, we delve deeply into the impact of spiking decoding strategies
and loss functions on model performance. Thereby, we establish state-of-the-art
classification results based on SNNs, achieving 93.7\% accuracy on the NCAR
dataset. Experimental results on the GEN1 detection dataset demonstrate that
the SFOD achieves a state-of-the-art mAP of 32.1\%, outperforming existing
SNN-based approaches. Our research not only underscores the potential of SNNs
in object detection with event cameras but also propels the advancement of
SNNs. Code is available at https://github.com/yimeng-fan/SFOD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Brain-grounding of semantic vectors improves neural decoding of visual
  stimuli 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15176v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15176v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shirin Vafaei, Ryohei Fukuma, Huixiang Yang, Haruhiko Kishima, Takufumi Yanagisawa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Developing algorithms for accurate and comprehensive neural decoding of
mental contents is one of the long-cherished goals in the field of neuroscience
and brain-machine interfaces. Previous studies have demonstrated the
feasibility of neural decoding by training machine learning models to map brain
activity patterns into a semantic vector representation of stimuli. These
vectors, hereafter referred as pretrained feature vectors, are usually derived
from semantic spaces based solely on image and/or text features and therefore
they might have a totally different characteristics than how visual stimuli is
represented in the human brain, resulting in limiting the capability of brain
decoders to learn this mapping. To address this issue, we propose a
representation learning framework, termed brain-grounding of semantic vectors,
which fine-tunes pretrained feature vectors to better align with the neural
representation of visual stimuli in the human brain. We trained this model this
model with functional magnetic resonance imaging (fMRI) of 150 different visual
stimuli categories, and then performed zero-shot brain decoding and
identification analyses on 1) fMRI and 2) magnetoencephalography (MEG).
Interestingly, we observed that by using the brain-grounded vectors, the brain
decoding and identification accuracy on brain data from different neuroimaging
modalities increases. These findings underscore the potential of incorporating
a richer array of brain-derived features to enhance performance of brain
decoding algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring the Task-agnostic Trait of <span class="highlight-title">Self-supervised</span> Learning in the
  Context of Detecting Mental Disorders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15170v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15170v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rohan Kumar Gupta, Rohit Sinha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised learning (SSL) has been investigated to generate
task-agnostic representations across various domains. However, such
investigation has not been conducted for detecting multiple mental disorders.
The rationale behind the existence of a task-agnostic representation lies in
the overlapping symptoms among multiple mental disorders. Consequently, the
behavioural data collected for mental health assessment may carry a mixed bag
of attributes related to multiple disorders. Motivated by that, in this study,
we explore a task-agnostic representation derived through SSL in the context of
detecting major depressive disorder (MDD) and post-traumatic stress disorder
(PTSD) using audio and video data collected during interactive sessions. This
study employs SSL models trained by predicting multiple fixed targets or masked
frames. We propose a list of fixed targets to make the generated representation
more efficient for detecting MDD and PTSD. Furthermore, we modify the
hyper-parameters of the SSL encoder predicting fixed targets to generate global
representations that capture varying temporal contexts. Both these innovations
are noted to yield improved detection performances for considered mental
disorders and exhibit task-agnostic traits. In the context of the SSL model
predicting masked frames, the generated global representations are also noted
to exhibit task-agnostic traits.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Transition Graph Properties of Target Class Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15167v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15167v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Levon Aslanyan, Hasmik Sahakyan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Target class classification is a mixed classification and transition model
whose integrated goal is to assign objects to a certain, so called target or
normal class. The classification process is iterative, and in each step an
object in a certain class undergoes an action attached to that class,
initiating the transition of the object to one of the classes. The sequence of
transitions, which we call class transitions, must be designed to provide the
final assignment of objects to the target class. The transition process can be
described in the form of a directed graph, and the success of the final
classification is mainly due to the properties of this graph. In our previous
research we showed that the desirable structure of the transition graph is an
oriented rooted tree with orientation towards the root vertex, which
corresponds to the normal class. It is clear that the transition graph of an
arbitrary algorithm (policy) may not have this property. In this paper we study
the structure of realistic transition graphs, which makes it possible to find
classification inconsistencies, helping to transfer it into the desired form.
The medical interpretation of dynamic treatment regime considered in the
article further clarifies the investigated framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Modular Deep Active Learning Framework for Image Annotation: A Technical
  Report for the Ophthalmo-AI Project 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15143v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15143v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Abdul Kadir, Hasan Md Tusfiqur Alam, Pascale Maul, Hans-Jürgen Profitlich, Moritz Wolf, Daniel Sonntag
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image annotation is one of the most essential tasks for guaranteeing proper
treatment for patients and tracking progress over the course of therapy in the
field of medical imaging and disease diagnosis. However, manually annotating a
lot of 2D and 3D imaging data can be extremely tedious. Deep Learning (DL)
based segmentation algorithms have completely transformed this process and made
it possible to automate image segmentation. By accurately segmenting medical
images, these algorithms can greatly minimize the time and effort necessary for
manual annotation. Additionally, by incorporating Active Learning (AL) methods,
these segmentation algorithms can perform far more effectively with a smaller
amount of ground truth data. We introduce MedDeepCyleAL, an end-to-end
framework implementing the complete AL cycle. It provides researchers with the
flexibility to choose the type of deep learning model they wish to employ and
includes an annotation tool that supports the classification and segmentation
of medical images. The user-friendly interface allows for easy alteration of
the AL and DL model settings through a configuration file, requiring no prior
programming experience. While MedDeepCyleAL can be applied to any kind of image
data, we have specifically applied it to ophthalmology data in this project.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>DFKI Technical Report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unimodal Multi-Task Fusion for Emotional Mimicry Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11879v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11879v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tobias Hallmen, Fabian Deuser, Norbert Oswald, Elisabeth André
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we propose a methodology for the Emotional Mimicry Intensity
(EMI) Estimation task within the context of the 6th Workshop and Competition on
Affective Behavior Analysis in-the-wild. Our approach leverages the Wav2Vec 2.0
framework, pre-trained on a comprehensive podcast dataset, to extract a broad
range of audio features encompassing both linguistic and paralinguistic
elements. We enhance feature representation through a fusion technique that
integrates individual features with a global mean vector, introducing global
contextual insights into our analysis. Additionally, we incorporate a
pre-trained valence-arousal-dominance (VAD) module from the Wav2Vec 2.0 model.
Our fusion employs a Long Short-Term Memory (LSTM) architecture for efficient
temporal analysis of audio data. Utilizing only the provided audio data, our
approach demonstrates significant improvements over the established baseline.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AnyV2V: A Plug-and-Play Framework For Any Video-to-Video Editing Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14468v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14468v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Max Ku, Cong Wei, Weiming Ren, Harry Yang, Wenhu Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video-to-video editing involves editing a source video along with additional
control (such as text prompts, subjects, or styles) to generate a new video
that aligns with the source video and the provided control. Traditional methods
have been constrained to certain editing types, limiting their ability to meet
the wide range of user demands. In this paper, we introduce AnyV2V, a novel
training-free framework designed to simplify video editing into two primary
steps: (1) employing an off-the-shelf image editing model (e.g.
InstructPix2Pix, InstantID, etc) to modify the first frame, (2) utilizing an
existing image-to-video generation model (e.g. I2VGen-XL) for DDIM inversion
and feature injection. In the first stage, AnyV2V can plug in any existing
image editing tools to support an extensive array of video editing tasks.
Beyond the traditional prompt-based editing methods, AnyV2V also can support
novel video editing tasks, including reference-based style transfer,
subject-driven editing, and identity manipulation, which were unattainable by
previous methods. In the second stage, AnyV2V can plug in any existing
image-to-video models to perform DDIM inversion and intermediate feature
injection to maintain the appearance and motion consistency with the source
video. On the prompt-based editing, we show that AnyV2V can outperform the
previous best approach by 35\% on prompt alignment, and 25\% on human
preference. On the three novel tasks, we show that AnyV2V also achieves a high
success rate. We believe AnyV2V will continue to thrive due to its ability to
seamlessly integrate the fast-evolving image editing methods. Such
compatibility can help AnyV2V to increase its versatility to cater to diverse
user demands.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Simple and Scalable Strategies to Continually <span class="highlight-title">Pre-train</span> Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08763v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08763v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adam Ibrahim, Benjamin Thérien, Kshitij Gupta, Mats L. Richter, Quentin Anthony, Timothée Lesort, Eugene Belilovsky, Irina Rish
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are routinely pre-trained on billions of tokens,
only to start the process over again once new data becomes available. A much
more efficient solution is to continually pre-train these models, saving
significant compute compared to re-training. However, the distribution shift
induced by new data typically results in degraded performance on previous data
or poor adaptation to the new data. In this work, we show that a simple and
scalable combination of learning rate (LR) re-warming, LR re-decaying, and
replay of previous data is sufficient to match the performance of fully
re-training from scratch on all available data, as measured by final loss and
language model (LM) evaluation benchmarks. Specifically, we show this for a
weak but realistic distribution shift between two commonly used LLM
pre-training datasets (English$\rightarrow$English) and a stronger distribution
shift (English$\rightarrow$German) at the $405$M parameter model scale with
large dataset sizes (hundreds of billions of tokens). Selecting the weak but
realistic shift for larger-scale experiments, we also find that our continual
learning strategies match the re-training baseline for a 10B parameter LLM. Our
results demonstrate that LLMs can be successfully updated via simple and
scalable continual learning strategies, matching the re-training baseline using
only a fraction of the compute. Finally, inspired by previous work, we propose
alternatives to the cosine learning rate schedule that help circumvent
forgetting induced by LR re-warming and that are not bound to a fixed token
budget.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Finding the right XAI method -- A Guide for the Evaluation and Ranking
  of Explainable AI Methods in Climate Science 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.00652v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.00652v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philine Bommer, Marlene Kretschmer, Anna Hedström, Dilyara Bareeva, Marina M. -C. Höhne
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Explainable artificial intelligence (XAI) methods shed light on the
predictions of machine learning algorithms. Several different approaches exist
and have already been applied in climate science. However, usually missing
ground truth explanations complicate their evaluation and comparison,
subsequently impeding the choice of the XAI method. Therefore, in this work, we
introduce XAI evaluation in the climate context and discuss different desired
explanation properties, namely robustness, faithfulness, randomization,
complexity, and localization. To this end, we chose previous work as a case
study where the decade of annual-mean temperature maps is predicted. After
training both a multi-layer perceptron (MLP) and a convolutional neural network
(CNN), multiple XAI methods are applied and their skill scores in reference to
a random uniform explanation are calculated for each property. Independent of
the network, we find that XAI methods Integrated Gradients, layer-wise
relevance propagation, and input times gradients exhibit considerable
robustness, faithfulness, and complexity while sacrificing randomization
performance. Sensitivity methods -- gradient, SmoothGrad, NoiseGrad, and
FusionGrad, match the robustness skill but sacrifice faithfulness and
complexity for randomization skill. We find architecture-dependent performance
differences regarding robustness, complexity and localization skills of
different XAI methods, highlighting the necessity for research task-specific
evaluation. Overall, our work offers an overview of different evaluation
properties in the climate science context and shows how to compare and
benchmark different explanation methods, assessing their suitability based on
strengths and weaknesses, for the specific research problem at hand. By that,
we aim to support climate researchers in the selection of a suitable XAI
method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 10 figure, accepted at AIES journal by AMS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Videoshop: Localized Semantic Video Editing with Noise-Extrapolated
  Diffusion Inversion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14617v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14617v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Fan, Anand Bhattad, Ranjay Krishna
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Videoshop, a training-free video editing algorithm for localized
semantic edits. Videoshop allows users to use any editing software, including
Photoshop and generative inpainting, to modify the first frame; it
automatically propagates those changes, with semantic, spatial, and temporally
consistent motion, to the remaining frames. Unlike existing methods that enable
edits only through imprecise textual instructions, Videoshop allows users to
add or remove objects, semantically change objects, insert stock photos into
videos, etc. with fine-grained control over locations and appearance. We
achieve this through image-based video editing by inverting latents with noise
extrapolation, from which we generate videos conditioned on the edited image.
Videoshop produces higher quality edits against 6 baselines on 2 editing
benchmarks using 10 evaluation metrics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page at https://videoshop-editing.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Empowering Autonomous Driving with Large Language Models: A Safety
  Perspective <span class="chip">ICLR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.00812v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.00812v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixuan Wang, Ruochen Jiao, Sinong Simon Zhan, Chengtian Lang, Chao Huang, Zhaoran Wang, Zhuoran Yang, Qi Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous Driving (AD) encounters significant safety hurdles in long-tail
unforeseen driving scenarios, largely stemming from the non-interpretability
and poor generalization of the deep neural networks within the AD system,
particularly in out-of-distribution and uncertain data. To this end, this paper
explores the integration of Large Language Models (LLMs) into AD systems,
leveraging their robust common-sense knowledge and reasoning abilities. The
proposed methodologies employ LLMs as intelligent decision-makers in behavioral
planning, augmented with a safety verifier shield for contextual safety
learning, for enhancing driving performance and safety. We present two key
studies in a simulated environment: an adaptive LLM-conditioned Model
Predictive Control (MPC) and an LLM-enabled interactive behavior planning
scheme with a state machine. Demonstrating superior performance and safety
metrics compared to state-of-the-art approaches, our approach shows the
promising potential for using LLMs for autonomous vehicles.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to LLMAgent workshop @ICLR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLMR: Real-time <span class="highlight-title">Prompt</span>ing of Interactive Worlds using Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.12276v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.12276v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fernanda De La Torre, Cathy Mengying Fang, Han Huang, Andrzej Banburski-Fahey, Judith Amores Fernandez, Jaron Lanier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Large Language Model for Mixed Reality (LLMR), a framework for the
real-time creation and modification of interactive Mixed Reality experiences
using LLMs. LLMR leverages novel strategies to tackle difficult cases where
ideal training data is scarce, or where the design goal requires the synthesis
of internal dynamics, intuitive analysis, or advanced interactivity. Our
framework relies on text interaction and the Unity game engine. By
incorporating techniques for scene understanding, task planning,
self-debugging, and memory management, LLMR outperforms the standard GPT-4 by
4x in average error rate. We demonstrate LLMR's cross-platform interoperability
with several example worlds, and evaluate it on a variety of creation and
modification tasks to show that it can produce and edit diverse objects, tools,
and scenes. Finally, we conducted a usability study (N=11) with a diverse set
that revealed participants had positive experiences with the system and would
use it again.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>46 pages, 18 figures; Matching version accepted at CHI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Complexity to Clarity: Analytical Expressions of Deep Neural
  Network Weights via Clifford's Geometric Algebra and Convexity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.16512v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.16512v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mert Pilanci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce a novel analysis of neural networks based on
geometric (Clifford) algebra and convex optimization. We show that optimal
weights of deep ReLU neural networks are given by the wedge product of training
samples when trained with standard regularized loss. Furthermore, the training
problem reduces to convex optimization over wedge product features, which
encode the geometric structure of the training dataset. This structure is given
in terms of signed volumes of triangles and parallelotopes generated by data
vectors. The convex problem finds a small subset of samples via $\ell_1$
regularization to discover only relevant wedge product features. Our analysis
provides a novel perspective on the inner workings of deep neural networks and
sheds light on the role of the hidden layers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Building Efficient Universal Classifiers with Natural Language Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.17543v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.17543v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Moritz Laurer, Wouter van Atteveldt, Andreu Casas, Kasper Welbers
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative Large Language Models (LLMs) have become the mainstream choice for
fewshot and zeroshot learning thanks to the universality of text generation.
Many users, however, do not need the broad capabilities of generative LLMs when
they only want to automate a classification task. Smaller BERT-like models can
also learn universal tasks, which allow them to do any text classification task
without requiring fine-tuning (zeroshot classification) or to learn new tasks
with only a few examples (fewshot), while being significantly more efficient
than generative LLMs. This paper (1) explains how Natural Language Inference
(NLI) can be used as a universal classification task that follows similar
principles as instruction fine-tuning of generative LLMs, (2) provides a
step-by-step guide with reusable Jupyter notebooks for building a universal
classifier, and (3) shares the resulting universal classifier that is trained
on 33 datasets with 389 diverse classes. Parts of the code we share has been
used to train our older zeroshot classifiers that have been downloaded more
than 55 million times via the Hugging Face Hub as of December 2023. Our new
classifier improves zeroshot performance by 9.4%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VideoPoet: A Large Language Model for Zero-Shot Video Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.14125v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.14125v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dan Kondratyuk, Lijun Yu, Xiuye Gu, José Lezama, Jonathan Huang, Grant Schindler, Rachel Hornung, Vighnesh Birodkar, Jimmy Yan, Ming-Chang Chiu, Krishna Somandepalli, Hassan Akbari, Yair Alon, Yong Cheng, Josh Dillon, Agrim Gupta, Meera Hahn, Anja Hauth, David Hendon, Alonso Martinez, David Minnen, Mikhail Sirotenko, Kihyuk Sohn, Xuan Yang, Hartwig Adam, Ming-Hsuan Yang, Irfan Essa, Huisheng Wang, David A. Ross, Bryan Seybold, Lu Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present VideoPoet, a language model capable of synthesizing high-quality
video, with matching audio, from a large variety of conditioning signals.
VideoPoet employs a decoder-only transformer architecture that processes
multimodal inputs -- including images, videos, text, and audio. The training
protocol follows that of Large Language Models (LLMs), consisting of two
stages: pretraining and task-specific adaptation. During pretraining, VideoPoet
incorporates a mixture of multimodal generative objectives within an
autoregressive Transformer framework. The pretrained LLM serves as a foundation
that can be adapted for a range of video generation tasks. We present empirical
results demonstrating the model's state-of-the-art capabilities in zero-shot
video generation, specifically highlighting VideoPoet's ability to generate
high-fidelity motions. Project page: http://sites.research.google/videopoet/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: http://sites.research.google/videopoet/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fast ODE-based Sampling for Diffusion Models in Around 5 Steps <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.00094v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.00094v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenyu Zhou, Defang Chen, Can Wang, Chun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sampling from diffusion models can be treated as solving the corresponding
ordinary differential equations (ODEs), with the aim of obtaining an accurate
solution with as few number of function evaluations (NFE) as possible.
Recently, various fast samplers utilizing higher-order ODE solvers have emerged
and achieved better performance than the initial first-order one. However,
these numerical methods inherently result in certain approximation errors,
which significantly degrades sample quality with extremely small NFE (e.g.,
around 5). In contrast, based on the geometric observation that each sampling
trajectory almost lies in a two-dimensional subspace embedded in the ambient
space, we propose Approximate MEan-Direction Solver (AMED-Solver) that
eliminates truncation errors by directly learning the mean direction for fast
diffusion sampling. Besides, our method can be easily used as a plugin to
further improve existing ODE-based samplers. Extensive experiments on image
synthesis with the resolution ranging from 32 to 512 demonstrate the
effectiveness of our method. With only 5 NFE, we achieve 6.61 FID on CIFAR-10,
10.74 FID on ImageNet 64$\times$64, and 13.20 FID on LSUN Bedroom. Our code is
available at https://github.com/zju-pi/diff-sampler.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Novelty Detection in Reinforcement Learning with World Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08731v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08731v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Geigh Zollicoffer, Kenneth Eaton, Jonathan Balloch, Julia Kim, Mark O. Riedl, Robert Wright
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning (RL) using world models has found significant recent
successes. However, when a sudden change to world mechanics or properties
occurs then agent performance and reliability can dramatically decline. We
refer to the sudden change in visual properties or state transitions as
novelties. Implementing novelty detection within generated world model
frameworks is a crucial task for protecting the agent when deployed. In this
paper, we propose straightforward bounding approaches to incorporate novelty
detection into world model RL agents, by utilizing the misalignment of the
world model's hallucinated states and the true observed states as an anomaly
score. We provide effective approaches to detecting novelties in a distribution
of transitions learned by an agent in a world model. Finally, we show the
advantage of our work in a novel environment compared to traditional machine
learning novelty detection methods as well as currently accepted RL focused
novelty detection algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Faster Neighborhood Attention: Reducing the O(n^2) Cost of Self
  Attention at the Threadblock Level 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.04690v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.04690v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Hassani, Wen-Mei Hwu, Humphrey Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neighborhood attention reduces the cost of self attention by restricting each
token's attention span to its nearest neighbors. This restriction,
parameterized by a window size and dilation factor, draws a spectrum of
possible attention patterns between linear projection and self attention.
Neighborhood attention, and more generally sliding window attention patterns,
have long been bounded by infrastructure, particularly in higher-rank spaces
(2-D and 3-D), calling for the development of custom kernels, which have been
limited in either functionality, or performance, if not both. In this work, we
first show that neighborhood attention can be represented as a batched GEMM
problem, similar to standard attention, and implement it for 1-D and 2-D
neighborhood attention. These kernels on average provide 895% and 272%
improvement in full precision latency compared to existing naive kernels for
1-D and 2-D neighborhood attention respectively. We find certain inherent
inefficiencies in all unfused neighborhood attention kernels that bound their
performance and lower-precision scalability. We also developed fused
neighborhood attention; an adaptation of fused dot-product attention kernels
that allow fine-grained control over attention across different spatial axes.
Known for reducing the quadratic time complexity of self attention to a linear
complexity, neighborhood attention can now enjoy a reduced and constant memory
footprint, and record-breaking half precision latency. We observe that our
fused kernels successfully circumvent some of the unavoidable inefficiencies in
unfused implementations. While our unfused GEMM-based kernels only improve half
precision performance compared to naive kernels by an average of 496% and 113%
in 1-D and 2-D problems respectively, our fused kernels improve naive kernels
by an average of 1607% and 581% in 1-D and 2-D problems respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://github.com/SHI-Labs/NATTEN</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large Language Model-informed ECG Dual Attention Network for Heart
  Failure Risk Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10581v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10581v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Chen, Lei Li, Marcel Beetz, Abhirup Banerjee, Ramneek Gupta, Vicente Grau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Heart failure (HF) poses a significant public health challenge, with a rising
global mortality rate. Early detection and prevention of HF could significantly
reduce its impact. We introduce a novel methodology for predicting HF risk
using 12-lead electrocardiograms (ECGs). We present a novel, lightweight
dual-attention ECG network designed to capture complex ECG features essential
for early HF risk prediction, despite the notable imbalance between low and
high-risk groups. This network incorporates a cross-lead attention module and
twelve lead-specific temporal attention modules, focusing on cross-lead
interactions and each lead's local dynamics. To further alleviate model
overfitting, we leverage a large language model (LLM) with a public ECG-Report
dataset for pretraining on an ECG-report alignment task. The network is then
fine-tuned for HF risk prediction using two specific cohorts from the UK
Biobank study, focusing on patients with hypertension (UKB-HYP) and those who
have had a myocardial infarction (UKB-MI).The results reveal that LLM-informed
pre-training substantially enhances HF risk prediction in these cohorts. The
dual-attention design not only improves interpretability but also predictive
accuracy, outperforming existing competitive methods with C-index scores of
0.6349 for UKB-HYP and 0.5805 for UKB-MI. This demonstrates our method's
potential in advancing HF risk assessment with clinical complex ECG data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under journal revision</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ "This is not a data problem": Algorithms and Power in Public Higher
  Education in Canada 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13969v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13969v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kelly McConvey, Shion Guha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Algorithmic decision-making is increasingly being adopted across public
higher education. The expansion of data-driven practices by post-secondary
institutions has occurred in parallel with the adoption of New Public
Management approaches by neoliberal administrations. In this study, we conduct
a qualitative analysis of an in-depth ethnographic case study of data and
algorithms in use at a public college in Ontario, Canada. We identify the data,
algorithms, and outcomes in use at the college. We assess how the college's
processes and relationships support those outcomes and the different
stakeholders' perceptions of the college's data-driven systems. In addition, we
find that the growing reliance on algorithmic decisions leads to increased
student surveillance, exacerbation of existing inequities, and the automation
of the faculty-student relationship. Finally, we identify a cycle of increased
institutional power perpetuated by algorithmic decision-making, and driven by a
push towards financial sustainability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In CHI '24 Proceedings of the CHI Conference on Human Factors in
  Computing Systems Honolulu, HI, USA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LogPrécis: Unleashing Language Models for Automated Malicious Log
  Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.08309v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.08309v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matteo Boffa, Rodolfo Vieira Valentim, Luca Vassio, Danilo Giordano, Idilio Drago, Marco Mellia, Zied Ben Houidi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The collection of security-related logs holds the key to understanding attack
behaviors and diagnosing vulnerabilities. Still, their analysis remains a
daunting challenge. Recently, Language Models (LMs) have demonstrated unmatched
potential in understanding natural and programming languages. The question
arises whether and how LMs could be also useful for security experts since
their logs contain intrinsically confused and obfuscated information. In this
paper, we systematically study how to benefit from the state-of-the-art in LM
to automatically analyze text-like Unix shell attack logs. We present a
thorough design methodology that leads to LogPr\'ecis. It receives as input raw
shell sessions and automatically identifies and assigns the attacker tactic to
each portion of the session, i.e., unveiling the sequence of the attacker's
goals. We demonstrate LogPr\'ecis capability to support the analysis of two
large datasets containing about 400,000 unique Unix shell attacks. LogPr\'ecis
reduces them into about 3,000 fingerprints, each grouping sessions with the
same sequence of tactics. The abstraction it provides lets the analyst better
understand attacks, identify fingerprints, detect novelty, link similar
attacks, and track families and mutations. Overall, LogPr\'ecis, released as
open source, paves the way for better and more responsive defense against
cyberattacks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, Computer&Security
  (https://www.sciencedirect.com/science/article/pii/S0167404824001068), code
  available at https://github.com/SmartData-Polito/logprecis, models available
  at https://huggingface.co/SmartDataPolito</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Vision<span class="highlight-title">GPT</span>-3D: A Generalized Multimodal Agent for Enhanced 3D Vision
  Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09530v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09530v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chris Kelly, Luhui Hu, Jiayin Hu, Yu Tian, Deshun Yang, Bang Yang, Cindy Yang, Zihao Li, Zaoshan Huang, Yuexian Zou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The evolution of text to visual components facilitates people's daily lives,
such as generating image, videos from text and identifying the desired elements
within the images. Computer vision models involving the multimodal abilities in
the previous days are focused on image detection, classification based on
well-defined objects. Large language models (LLMs) introduces the
transformation from nature language to visual objects, which present the visual
layout for text contexts. OpenAI GPT-4 has emerged as the pinnacle in LLMs,
while the computer vision (CV) domain boasts a plethora of state-of-the-art
(SOTA) models and algorithms to convert 2D images to their 3D representations.
However, the mismatching between the algorithms with the problem could lead to
undesired results. In response to this challenge, we propose an unified
VisionGPT-3D framework to consolidate the state-of-the-art vision models,
thereby facilitating the development of vision-oriented AI. VisionGPT-3D
provides a versatile multimodal framework building upon the strengths of
multimodal foundation models. It seamlessly integrates various SOTA vision
models and brings the automation in the selection of SOTA vision models,
identifies the suitable 3D mesh creation algorithms corresponding to 2D depth
maps analysis, generates optimal results based on diverse multimodal inputs
such as text prompts.
  Keywords: VisionGPT-3D, 3D vision understanding, Multimodal agent
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 7 figures, pending conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MSAC: Multiple Speech Attribute Control Method for Reliable Speech
  Emotion Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.04025v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.04025v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Pan, Yuguang Yang, Yuheng Huang, Jixun Yao, Jingjing Yin, Yanni Hu, Heng Lu, Lei Ma, Jianjun Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite notable progress, speech emotion recognition (SER) remains
challenging due to the intricate and ambiguous nature of speech emotion,
particularly in wild world. While current studies primarily focus on
recognition and generalization abilities, our research pioneers an
investigation into the reliability of SER methods in the presence of semantic
data shifts and explores how to exert fine-grained control over various
attributes inherent in speech signals to enhance speech emotion modeling. In
this paper, we first introduce MSAC-SERNet, a novel unified SER framework
capable of simultaneously handling both single-corpus and cross-corpus SER.
Specifically, concentrating exclusively on the speech emotion attribute, a
novel CNN-based SER model is presented to extract discriminative emotional
representations, guided by additive margin softmax loss. Considering
information overlap between various speech attributes, we propose a novel
learning paradigm based on correlations of different speech attributes, termed
Multiple Speech Attribute Control (MSAC), which empowers the proposed SER model
to simultaneously capture fine-grained emotion-related features while
mitigating the negative impact of emotion-agnostic representations.
Furthermore, we make a first attempt to examine the reliability of the
MSAC-SERNet framework using out-of-distribution detection methods. Experiments
on both single-corpus and cross-corpus SER scenarios indicate that MSAC-SERNet
not only consistently outperforms the baseline in all aspects, but achieves
superior performance compared to state-of-the-art SER approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Task-Oriented GNNs Training on Large Knowledge Graphs for Accurate and
  Efficient Modeling <span class="chip">ICDE</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.05752v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.05752v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hussein Abdallah, Waleed Afandi, Panos Kalnis, Essam Mansour
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A Knowledge Graph (KG) is a heterogeneous graph encompassing a diverse range
of node and edge types. Heterogeneous Graph Neural Networks (HGNNs) are popular
for training machine learning tasks like node classification and link
prediction on KGs. However, HGNN methods exhibit excessive complexity
influenced by the KG's size, density, and the number of node and edge types. AI
practitioners handcraft a subgraph of a KG G relevant to a specific task. We
refer to this subgraph as a task-oriented subgraph (TOSG), which contains a
subset of task-related node and edge types in G. Training the task using TOSG
instead of G alleviates the excessive computation required for a large KG.
Crafting the TOSG demands a deep understanding of the KG's structure and the
task's objectives. Hence, it is challenging and time-consuming. This paper
proposes KG-TOSA, an approach to automate the TOSG extraction for task-oriented
HGNN training on a large KG. In KG-TOSA, we define a generic graph pattern that
captures the KG's local and global structure relevant to a specific task. We
explore different techniques to extract subgraphs matching our graph pattern:
namely (i) two techniques sampling around targeted nodes using biased random
walk or influence scores, and (ii) a SPARQL-based extraction method leveraging
RDF engines' built-in indices. Hence, it achieves negligible preprocessing
overhead compared to the sampling techniques. We develop a benchmark of real
KGs of large sizes and various tasks for node classification and link
prediction. Our experiments show that KG-TOSA helps state-of-the-art HGNN
methods reduce training time and memory usage by up to 70% while improving the
model performance, e.g., accuracy and inference time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages,9 Figures, 3 Tables, ICDE:2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Interpretability Guarantees with Merlin-Arthur Classifiers <span class="chip">AISTATS24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.00759v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.00759v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stephan Wäldchen, Kartikey Sharma, Berkant Turan, Max Zimmer, Sebastian Pokutta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose an interactive multi-agent classifier that provides provable
interpretability guarantees even for complex agents such as neural networks.
These guarantees consist of lower bounds on the mutual information between
selected features and the classification decision. Our results are inspired by
the Merlin-Arthur protocol from Interactive Proof Systems and express these
bounds in terms of measurable metrics such as soundness and completeness.
Compared to existing interactive setups, we rely neither on optimal agents nor
on the assumption that features are distributed independently. Instead, we use
the relative strength of the agents as well as the new concept of Asymmetric
Feature Correlation which captures the precise kind of correlations that make
interpretability guarantees difficult. We evaluate our results on two
small-scale datasets where high mutual information can be verified explicitly.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AISTATS24 Camera-Ready Version, 34 pages total (9 pages main part, 3
  pages references, 22 pages appendix), 17 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can a <span class="highlight-title">GPT</span>4-Powered AI Agent Be a Good Enough Performance Attribution
  Analyst? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10482v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10482v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bruno de Melo, Jamiel Sheikh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Performance attribution analysis, defined as the process of explaining the
drivers of the excess performance of an investment portfolio against a
benchmark, stands as a significant feature of portfolio management and plays a
crucial role in the investment decision-making process, particularly within the
fund management industry. Rooted in a solid financial and mathematical
framework, the importance and methodologies of this analytical technique are
extensively documented across numerous academic research papers and books. The
integration of large language models (LLMs) and AI agents marks a
groundbreaking development in this field. These agents are designed to automate
and enhance the performance attribution analysis by accurately calculating and
analyzing portfolio performances against benchmarks. In this study, we
introduce the application of an AI Agent for a variety of essential performance
attribution tasks, including the analysis of performance drivers and utilizing
LLMs as calculation engine for multi-level attribution analysis and
question-answering (QA) tasks. Leveraging advanced prompt engineering
techniques such as Chain-of-Thought (CoT) and Plan and Solve (PS), and
employing a standard agent framework from LangChain, the research achieves
promising results: it achieves accuracy rates exceeding 93% in analyzing
performance drivers, attains 100% in multi-level attribution calculations, and
surpasses 84% accuracy in QA exercises that simulate official examination
standards. These findings affirm the impactful role of AI agents, prompt
engineering and evaluation in advancing portfolio management processes,
highlighting a significant development in the practical application and
evaluation of Generative AI technologies within the domain.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PIA: Your Personalized Image Animator via Plug-and-Play Modules in
  Text-to-Image Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.13964v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.13964v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiming Zhang, Zhening Xing, Yanhong Zeng, Youqing Fang, Kai Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in personalized text-to-image (T2I) models have
revolutionized content creation, empowering non-experts to generate stunning
images with unique styles. While promising, adding realistic motions into these
personalized images by text poses significant challenges in preserving distinct
styles, high-fidelity details, and achieving motion controllability by text. In
this paper, we present PIA, a Personalized Image Animator that excels in
aligning with condition images, achieving motion controllability by text, and
the compatibility with various personalized T2I models without specific tuning.
To achieve these goals, PIA builds upon a base T2I model with well-trained
temporal alignment layers, allowing for the seamless transformation of any
personalized T2I model into an image animation model. A key component of PIA is
the introduction of the condition module, which utilizes the condition frame
and inter-frame affinity as input to transfer appearance information guided by
the affinity hint for individual frame synthesis in the latent space. This
design mitigates the challenges of appearance-related image alignment within
and allows for a stronger focus on aligning with motion-related guidance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://pi-animator.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FunQA: Towards Surprising Video Comprehension 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.14899v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.14899v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Binzhu Xie, Sicheng Zhang, Zitang Zhou, Bo Li, Yuanhan Zhang, Jack Hessel, Jingkang Yang, Ziwei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Surprising videos, such as funny clips, creative performances, or visual
illusions, attract significant attention. Enjoyment of these videos is not
simply a response to visual stimuli; rather, it hinges on the human capacity to
understand (and appreciate) commonsense violations depicted in these videos. We
introduce FunQA, a challenging video question-answering (QA) dataset
specifically designed to evaluate and enhance the depth of video reasoning
based on counter-intuitive and fun videos. Unlike most video QA benchmarks
which focus on less surprising contexts, e.g., cooking or instructional videos,
FunQA covers three previously unexplored types of surprising videos: 1)
HumorQA, 2) CreativeQA, and 3) MagicQA. For each subset, we establish rigorous
QA tasks designed to assess the model's capability in counter-intuitive
timestamp localization, detailed video description, and reasoning around
counter-intuitiveness. We also pose higher-level tasks, such as attributing a
fitting and vivid title to the video and scoring the video creativity. In
total, the FunQA benchmark consists of 312K free-text QA pairs derived from
4.3K video clips, spanning a total of 24 video hours. Moreover, we propose
FunMentor, an agent designed for Vision-Language Models (VLMs) that uses
multi-turn dialogues to enhance models' understanding of counter-intuitiveness.
Extensive experiments with existing VLMs demonstrate the effectiveness of
FunMentor and reveal significant performance gaps for the FunQA videos across
spatial-temporal reasoning, visual-centered reasoning, and free-text
generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://funqa-benchmark.github.io/ Codebase:
  https://github.com/Jingkang50/FunQA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning to Embed Time Series Patches Independently <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.16427v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.16427v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seunghan Lee, Taeyoung Park, Kibok Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Masked time series modeling has recently gained much attention as a
self-supervised representation learning strategy for time series. Inspired by
masked image modeling in computer vision, recent works first patchify and
partially mask out time series, and then train Transformers to capture the
dependencies between patches by predicting masked patches from unmasked
patches. However, we argue that capturing such patch dependencies might not be
an optimal strategy for time series representation learning; rather, learning
to embed patches independently results in better time series representations.
Specifically, we propose to use 1) the simple patch reconstruction task, which
autoencode each patch without looking at other patches, and 2) the simple
patch-wise MLP that embeds each patch independently. In addition, we introduce
complementary contrastive learning to hierarchically capture adjacent time
series information efficiently. Our proposed method improves time series
forecasting and classification performance compared to state-of-the-art
Transformer-based models, while it is more efficient in terms of the number of
parameters and training/inference time. Code is available at this repository:
https://github.com/seunghan96/pits.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Soft Contrastive Learning for Time Series <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.16424v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.16424v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seunghan Lee, Taeyoung Park, Kibok Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive learning has shown to be effective to learn representations from
time series in a self-supervised way. However, contrasting similar time series
instances or values from adjacent timestamps within a time series leads to
ignore their inherent correlations, which results in deteriorating the quality
of learned representations. To address this issue, we propose SoftCLT, a simple
yet effective soft contrastive learning strategy for time series. This is
achieved by introducing instance-wise and temporal contrastive loss with soft
assignments ranging from zero to one. Specifically, we define soft assignments
for 1) instance-wise contrastive loss by the distance between time series on
the data space, and 2) temporal contrastive loss by the difference of
timestamps. SoftCLT is a plug-and-play method for time series contrastive
learning that improves the quality of learned representations without bells and
whistles. In experiments, we demonstrate that SoftCLT consistently improves the
performance in various downstream tasks including classification,
semi-supervised learning, transfer learning, and anomaly detection, showing
state-of-the-art performance. Code is available at this repository:
https://github.com/seunghan96/softclt.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024 Spotlight</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Performance
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Network Calculus Characterization of Congestion Control for Time-Varying
  Traffic 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15303v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15303v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Harvinder Lehal, Natchanon Luangsomboon, Jörg Liebeherr
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Models for the dynamics of congestion control generally involve systems of
coupled differential equations. Universally, these models assume that traffic
sources saturate the maximum transmissions allowed by the congestion control
method. This is not suitable for studying congestion control of intermittent
but bursty traffic sources. In this paper, we present a characterization of
congestion control for arbitrary time-varying traffic that applies to
rate-based as well as window-based congestion control. We leverage the
capability of network calculus to precisely describe the input-output
relationship at network elements for arbitrary source traffic. We show that our
characterization can closely track the dynamics of even complex congestion
control algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Operating Systems
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Introduction to the Compute Express Link (CXL) Interconnect 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.11227v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.11227v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Debendra Das Sharma, Robert Blankenship, Daniel S. Berger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Compute Express Link (CXL) is an open industry-standard interconnect
between processors and devices such as accelerators, memory buffers, smart
network interfaces, persistent memory, and solid-state drives. CXL offers
coherency and memory semantics with bandwidth that scales with PCIe bandwidth
while achieving significantly lower latency than PCIe. All major CPU vendors,
device vendors, and datacenter operators have adopted CXL as a common standard.
This enables an inter-operable ecosystem that supports key computing use cases
including highly efficient accelerators, server memory bandwidth and capacity
expansion, multi-server resource pooling and sharing, and efficient
peer-to-peer communication. This survey provides an introduction to CXL
covering the standards CXL 1.0, CXL 2.0, and CXL 3.0. We further survey CXL
implementations, discuss CXL's impact on the datacenter landscape, and future
directions.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Databases
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fourier Transform-based Estimators for Data Sketches 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15366v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15366v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seth Pettie, Dingyu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we consider the problem of estimating the $f$-moment
($\sum_{v\in [n]} (f(\mathbf{x}(v))-f(0))$) of a dynamic vector $\mathbf{x}\in
\mathbb{G}^n$ over some abelian group $(\mathbb{G},+)$, where the
$\|f\|_\infty$ norm is bounded. We propose a simple sketch and new estimation
framework based on the \emph{Fourier transform} of $f$. I.e., we decompose $f$
into a linear combination of homomorphisms $f_1,f_2,\ldots$ from
$(\mathbb{G},+)$ to $(\mathbb{C},\times)$, estimate the $f_k$-moment for each
$f_k$, and synthesize them to obtain an estimate of the $f$-moment. Our
estimators are asymptotically unbiased and have variance asymptotic to
$\|\mathbf{x}\|_0^2 (\|f\|_\infty^2 m^{-1} + \|\hat{f}\|_1^2 m^{-2})$, where
the size of the sketch is $O(m\log n\log|\mathbb{G}|)$ bits.
  When $\mathbb{G}=\mathbb{Z}$ this problem can also be solved using
off-the-shelf $\ell_0$-samplers with space $O(m\log^2 n)$ bits, which does not
obviously generalize to finite groups. As a concrete benchmark, we extend
Ganguly, Garofalakis, and Rastogi's singleton-detector-based sampler to work
over $\mathbb{G}$ using $O(m\log n\log|\mathbb{G}|\log(m\log n))$ bits.
  We give some experimental evidence that the Fourier-based estimation
framework is significantly more accurate than sampling-based approaches at the
same memory footprint.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ProvDeploy: Provenance-oriented Containerization of High Performance
  Computing Scientific Workflows 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15324v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15324v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liliane Kunstmann, Débora Pina, Daniel de Oliveira, Marta Mattoso
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many existing scientific workflows require High Performance Computing
environments to produce results in a timely manner. These workflows have
several software library components and use different environments, making the
deployment and execution of the software stack not trivial. This complexity
increases if the user needs to add provenance data capture services to the
workflow. This manuscript introduces ProvDeploy to assist the user in
configuring containers for scientific workflows with integrated provenance data
capture. ProvDeploy was evaluated with a Scientific Machine Learning workflow,
exploring containerization strategies focused on provenance in two distinct HPC
environments
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hydro: Adaptive Query Processing of ML Queries 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14902v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14902v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gaurav Tarlok Kakkar, Jiashen Cao, Aubhro Sengupta, Joy Arulraj, Hyesoon Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Query optimization in relational database management systems (DBMSs) is
critical for fast query processing. The query optimizer relies on precise
selectivity and cost estimates to effectively optimize queries prior to
execution. While this strategy is effective for relational DBMSs, it is not
sufficient for DBMSs tailored for processing machine learning (ML) queries. In
ML-centric DBMSs, query optimization is challenging for two reasons. First, the
performance bottleneck of the queries shifts to user-defined functions (UDFs)
that often wrap around deep learning models, making it difficult to accurately
estimate UDF statistics without profiling the query. This leads to inaccurate
statistics and sub-optimal query plans. Second, the optimal query plan for ML
queries is data-dependent, necessitating DBMSs to adapt the query plan on the
fly during execution. So, a static query plan is not sufficient for such
queries.
  In this paper, we present Hydro, an ML-centric DBMS that utilizes adaptive
query processing (AQP) for efficiently processing ML queries. Hydro is designed
to quickly evaluate UDF-based query predicates by ensuring optimal predicate
evaluation order and improving the scalability of UDF execution. By integrating
AQP, Hydro continuously monitors UDF statistics, routes data to predicates in
an optimal order, and dynamically allocates resources for evaluating
predicates. We demonstrate Hydro's efficacy through four illustrative use
cases, delivering up to 11.52x speedup over a baseline system.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficiently Estimating Mutual Information Between Attributes Across
  Tables <span class="chip">ICDE 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15553v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15553v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aécio Santos, Flip Korn, Juliana Freire
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Relational data augmentation is a powerful technique for enhancing data
analytics and improving machine learning models by incorporating columns from
external datasets. However, it is challenging to efficiently discover relevant
external tables to join with a given input table. Existing approaches rely on
data discovery systems to identify joinable tables from external sources,
typically based on overlap or containment. However, the sheer number of tables
obtained from these systems results in irrelevant joins that need to be
performed; this can be computationally expensive or even infeasible in
practice. We address this limitation by proposing the use of efficient mutual
information (MI) estimation for finding relevant joinable tables. We introduce
a new sketching method that enables efficient evaluation of relationship
discovery queries by estimating MI without materializing the joins and
returning a smaller set of tables that are more likely to be relevant. We also
demonstrate the effectiveness of our approach at approximating MI in extensive
experiments using synthetic and real-world datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IEEE ICDE 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Combining the Strengths of Dutch <span class="highlight-title">Survey</span> and Register Data in a Data
  Challenge to Predict Fertility (PreFer) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.00705v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.00705v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elizaveta Sivak, Paulina Pankowska, Adrienne Mendrik, Tom Emery, Javier Garcia-Bernardo, Seyit Hocuk, Kasia Karpinska, Angelica Maineri, Joris Mulder, Malvina Nissim, Gert Stulp
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The social sciences have produced an impressive body of research on
determinants of fertility outcomes, or whether and when people have children.
However, the strength of these determinants and underlying theories are rarely
evaluated on their predictive ability on new data. This prevents us from
systematically comparing studies, hindering the evaluation and accumulation of
knowledge. In this paper, we present two datasets which can be used to study
the predictability of fertility outcomes in the Netherlands. One dataset is
based on the LISS panel, a longitudinal survey which includes thousands of
variables on a wide range of topics, including individual preferences and
values. The other is based on the Dutch register data which lacks attitudinal
data but includes detailed information about the life courses of millions of
Dutch residents. We provide information about the datasets and the samples, and
describe the fertility outcome of interest. We also introduce the fertility
prediction data challenge PreFer which is based on these datasets and will
start in Spring 2024. We outline the ways in which measuring the predictability
of fertility outcomes using these datasets and combining their strengths in the
data challenge can advance our understanding of fertility behaviour and
computational social science. We further provide details for participants on
how to take part in the data challenge.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Gen-T: Table Reclamation in Data Lakes <span class="chip">ICDE 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14128v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14128v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Grace Fan, Roee Shraga, Renée J. Miller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce the problem of Table Reclamation. Given a Source Table and a
large table repository, reclamation finds a set of tables that, when
integrated, reproduce the source table as closely as possible. Unlike query
discovery problems like Query-by-Example or by-Target, Table Reclamation
focuses on reclaiming the data in the Source Table as fully as possible using
real tables that may be incomplete or inconsistent. To do this, we define a new
measure of table similarity, called error-aware instance similarity, to measure
how close a reclaimed table is to a Source Table, a measure grounded in
instance similarity used in data exchange. Our search covers not only
SELECT-PROJECT- JOIN queries, but integration queries with unions, outerjoins,
and the unary operators subsumption and complementation that have been shown to
be important in data integration and fusion. Using reclamation, a data
scientist can understand if any tables in a repository can be used to exactly
reclaim a tuple in the Source. If not, one can understand if this is due to
differences in values or to incompleteness in the data. Our solution, Gen-T,
performs table discovery to retrieve a set of candidate tables from the table
repository, filters these down to a set of originating tables, then integrates
these tables to reclaim the Source as closely as possible. We show that our
solution, while approximate, is accurate, efficient and scalable in the size of
the table repository with experiments on real data lakes containing up to 15K
tables, where the average number of tuples varies from small (web tables) to
extremely large (open data tables) up to 1M tuples.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>to appear at ICDE 2024</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DiffusionMTL: Learning Multi-Task Denoising Diffusion Model from
  Partially Annotated Data <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15389v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15389v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanrong Ye, Dan Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, there has been an increased interest in the practical problem of
learning multiple dense scene understanding tasks from partially annotated
data, where each training sample is only labeled for a subset of the tasks. The
missing of task labels in training leads to low-quality and noisy predictions,
as can be observed from state-of-the-art methods. To tackle this issue, we
reformulate the partially-labeled multi-task dense prediction as a pixel-level
denoising problem, and propose a novel multi-task denoising diffusion framework
coined as DiffusionMTL. It designs a joint diffusion and denoising paradigm to
model a potential noisy distribution in the task prediction or feature maps and
generate rectified outputs for different tasks. To exploit multi-task
consistency in denoising, we further introduce a Multi-Task Conditioning
strategy, which can implicitly utilize the complementary nature of the tasks to
help learn the unlabeled tasks, leading to an improvement in the denoising
performance of the different tasks. Extensive quantitative and qualitative
experiments demonstrate that the proposed multi-task denoising diffusion model
can significantly improve multi-task prediction maps, and outperform the
state-of-the-art methods on three challenging multi-task benchmarks, under two
different partial-labeling evaluation settings. The code is available at
https://prismformore.github.io/diffusionmtl/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The paper is accepted by CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LATTE3D: Large-scale Amortized Text-To-Enhanced3D Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15385v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15385v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kevin Xie, Jonathan Lorraine, Tianshi Cao, Jun Gao, James Lucas, Antonio Torralba, Sanja Fidler, Xiaohui Zeng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent text-to-3D generation approaches produce impressive 3D results but
require time-consuming optimization that can take up to an hour per prompt.
Amortized methods like ATT3D optimize multiple prompts simultaneously to
improve efficiency, enabling fast text-to-3D synthesis. However, they cannot
capture high-frequency geometry and texture details and struggle to scale to
large prompt sets, so they generalize poorly. We introduce LATTE3D, addressing
these limitations to achieve fast, high-quality generation on a significantly
larger prompt set. Key to our method is 1) building a scalable architecture and
2) leveraging 3D data during optimization through 3D-aware diffusion priors,
shape regularization, and model initialization to achieve robustness to diverse
and complex training prompts. LATTE3D amortizes both neural field and textured
surface generation to produce highly detailed textured meshes in a single
forward pass. LATTE3D generates 3D objects in 400ms, and can be further
enhanced with fast test-time optimization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>See the project website at
  https://research.nvidia.com/labs/toronto-ai/LATTE3D/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can large language models explore in-context? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15371v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15371v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akshay Krishnamurthy, Keegan Harris, Dylan J. Foster, Cyril Zhang, Aleksandrs Slivkins
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate the extent to which contemporary Large Language Models (LLMs)
can engage in exploration, a core capability in reinforcement learning and
decision making. We focus on native performance of existing LLMs, without
training interventions. We deploy LLMs as agents in simple multi-armed bandit
environments, specifying the environment description and interaction history
entirely in-context, i.e., within the LLM prompt. We experiment with GPT-3.5,
GPT-4, and Llama2, using a variety of prompt designs, and find that the models
do not robustly engage in exploration without substantial interventions: i)
Across all of our experiments, only one configuration resulted in satisfactory
exploratory behavior: GPT-4 with chain-of-thought reasoning and an externally
summarized interaction history, presented as sufficient statistics; ii) All
other configurations did not result in robust exploratory behavior, including
those with chain-of-thought reasoning but unsummarized history. Although these
findings can be interpreted positively, they suggest that external
summarization -- which may not be possible in more complex settings -- is
important for obtaining desirable behavior from LLM agents. We conclude that
non-trivial algorithmic interventions, such as fine-tuning or dataset curation,
may be required to empower LLM-based decision making agents in complex
settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Augmented Reality based Simulated Data (ARSim) with multi-view
  consistency for AV perception networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15370v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15370v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aqeel Anwar, Tae Eun Choe, Zian Wang, Sanja Fidler, Minwoo Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting a diverse range of objects under various driving scenarios is
essential for the effectiveness of autonomous driving systems. However, the
real-world data collected often lacks the necessary diversity presenting a
long-tail distribution. Although synthetic data has been utilized to overcome
this issue by generating virtual scenes, it faces hurdles such as a significant
domain gap and the substantial efforts required from 3D artists to create
realistic environments. To overcome these challenges, we present ARSim, a fully
automated, comprehensive, modular framework designed to enhance real multi-view
image data with 3D synthetic objects of interest. The proposed method
integrates domain adaptation and randomization strategies to address covariate
shift between real and simulated data by inferring essential domain attributes
from real data and employing simulation-based randomization for other
attributes. We construct a simplified virtual scene using real data and
strategically place 3D synthetic assets within it. Illumination is achieved by
estimating light distribution from multiple images capturing the surroundings
of the vehicle. Camera parameters from real data are employed to render
synthetic assets in each frame. The resulting augmented multi-view consistent
dataset is used to train a multi-camera perception network for autonomous
vehicles. Experimental results on various AV perception tasks demonstrate the
superior performance of networks trained on the augmented dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 15 figures, 7 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Transfer Attack to Image Watermarks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15365v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15365v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuepeng Hu, Zhengyuan Jiang, Moyang Guo, Neil Gong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Watermark has been widely deployed by industry to detect AI-generated images.
The robustness of such watermark-based detector against evasion attacks in the
white-box and black-box settings is well understood in the literature. However,
the robustness in the no-box setting is much less understood. In particular,
multiple studies claimed that image watermark is robust in such setting. In
this work, we propose a new transfer evasion attack to image watermark in the
no-box setting. Our transfer attack adds a perturbation to a watermarked image
to evade multiple surrogate watermarking models trained by the attacker itself,
and the perturbed watermarked image also evades the target watermarking model.
Our major contribution is to show that, both theoretically and empirically,
watermark-based AI-generated image detector is not robust to evasion attacks
even if the attacker does not have access to the watermarking model nor the
detection API.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cascading Blackout Severity Prediction with Statistically-Augmented
  Graph Neural Networks <span class="chip">SC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15363v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15363v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joe Gorka, Tim Hsu, Wenting Li, Yury Maximov, Line Roald
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Higher variability in grid conditions, resulting from growing renewable
penetration and increased incidence of extreme weather events, has increased
the difficulty of screening for scenarios that may lead to catastrophic
cascading failures. Traditional power-flow-based tools for assessing cascading
blackout risk are too slow to properly explore the space of possible failures
and load/generation patterns. We add to the growing literature of faster
graph-neural-network (GNN)-based techniques, developing two novel techniques
for the estimation of blackout magnitude from initial grid conditions. First we
propose several methods for employing an initial classification step to filter
out safe "non blackout" scenarios prior to magnitude estimation. Second, using
insights from the statistical properties of cascading blackouts, we propose a
method for facilitating non-local message passing in our GNN models. We
validate these two approaches on a large simulated dataset, and show the
potential of both to increase blackout size estimation performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Power Systems Computation Conference (PSCC) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Topological Representations for Deep Image Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15361v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15361v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoling Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In many scenarios, especially biomedical applications, the correct
delineation of complex fine-scaled structures such as neurons, tissues, and
vessels is critical for downstream analysis. Despite the strong predictive
power of deep learning methods, they do not provide a satisfactory
representation of these structures, thus creating significant barriers in
scalable annotation and downstream analysis. In this dissertation, we tackle
such challenges by proposing novel representations of these topological
structures in a deep learning framework. We leverage the mathematical tools
from topological data analysis, i.e., persistent homology and discrete Morse
theory, to develop principled methods for better segmentation and uncertainty
estimation, which will become powerful tools for scalable annotation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Ph.D. thesis from Stony Brook University. This thesis includes works
  arXiv:1906.05404, arXiv:2110.08335, arXiv:2112.07812, arXiv:2103.09992,
  arXiv:2206.01742</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SiMBA: Simplified Mamba-Based Architecture for Vision and Multivariate
  Time series 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15360v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15360v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Badri N. Patro, Vijay S. Agneeswaran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers have widely adopted attention networks for sequence mixing and
MLPs for channel mixing, playing a pivotal role in achieving breakthroughs
across domains. However, recent literature highlights issues with attention
networks, including low inductive bias and quadratic complexity concerning
input sequence length. State Space Models (SSMs) like S4 and others (Hippo,
Global Convolutions, liquid S4, LRU, Mega, and Mamba), have emerged to address
the above issues to help handle longer sequence lengths. Mamba, while being the
state-of-the-art SSM, has a stability issue when scaled to large networks for
computer vision datasets. We propose SiMBA, a new architecture that introduces
Einstein FFT (EinFFT) for channel modeling by specific eigenvalue computations
and uses the Mamba block for sequence modeling. Extensive performance studies
across image and time-series benchmarks demonstrate that SiMBA outperforms
existing SSMs, bridging the performance gap with state-of-the-art transformers.
Notably, SiMBA establishes itself as the new state-of-the-art SSM on ImageNet
and transfer learning benchmarks such as Stanford Car and Flower as well as
task learning benchmarks as well as seven time series benchmark datasets. The
project page is available on this website
~\url{https://github.com/badripatro/Simba}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ultrasound Imaging based on the Variance of a Diffusion Restoration
  Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15316v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15316v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxin Zhang, Clément Huneau, Jérôme Idier, Diana Mateus
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite today's prevalence of ultrasound imaging in medicine, ultrasound
signal-to-noise ratio is still affected by several sources of noise and
artefacts. Moreover, enhancing ultrasound image quality involves balancing
concurrent factors like contrast, resolution, and speckle preservation.
Recently, there has been progress in both model-based and learning-based
approaches addressing the problem of ultrasound image reconstruction. Bringing
the best from both worlds, we propose a hybrid reconstruction method combining
an ultrasound linear direct model with a learning-based prior coming from a
generative Denoising Diffusion model. More specifically, we rely on the
unsupervised fine-tuning of a pre-trained Denoising Diffusion Restoration Model
(DDRM). Given the nature of multiplicative noise inherent to ultrasound, this
paper proposes an empirical model to characterize the stochasticity of
diffusion reconstruction of ultrasound images, and shows the interest of its
variance as an echogenicity map estimator. We conduct experiments on synthetic,
in-vitro, and in-vivo data, demonstrating the efficacy of our variance imaging
approach in achieving high-quality image reconstructions from single plane-wave
acquisitions and in comparison to state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages; submitted to EUSIPCO 2024. arXiv admin note: text overlap
  with arXiv:2310.20618</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Wasserstein perspective of Vanilla GANs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15312v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15312v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lea Kunkel, Mathias Trabs
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The empirical success of Generative Adversarial Networks (GANs) caused an
increasing interest in theoretical research. The statistical literature is
mainly focused on Wasserstein GANs and generalizations thereof, which
especially allow for good dimension reduction properties. Statistical results
for Vanilla GANs, the original optimization problem, are still rather limited
and require assumptions such as smooth activation functions and equal
dimensions of the latent space and the ambient space. To bridge this gap, we
draw a connection from Vanilla GANs to the Wasserstein distance. By doing so,
existing results for Wasserstein GANs can be extended to Vanilla GANs. In
particular, we obtain an oracle inequality for Vanilla GANs in Wasserstein
distance. The assumptions of this oracle inequality are designed to be
satisfied by network architectures commonly used in practice, such as
feedforward ReLU networks. By providing a quantitative result for the
approximation of a Lipschitz function by a feedforward ReLU network with
bounded H\"older norm, we conclude a rate of convergence for Vanilla GANs as
well as Wasserstein GANs as estimators of the unknown probability distribution.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Controlled Training Data Generation with Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15309v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15309v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Teresa Yeo, Andrei Atanov, Harold Benoit, Aleksandr Alekseev, Ruchira Ray, Pooya Esmaeil Akhoondi, Amir Zamir
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we present a method to control a text-to-image generative model
to produce training data specifically "useful" for supervised learning. Unlike
previous works that employ an open-loop approach and pre-define prompts to
generate new data using either a language model or human expertise, we develop
an automated closed-loop system which involves two feedback mechanisms. The
first mechanism uses feedback from a given supervised model and finds
adversarial prompts that result in image generations that maximize the model
loss. While these adversarial prompts result in diverse data informed by the
model, they are not informed of the target distribution, which can be
inefficient. Therefore, we introduce the second feedback mechanism that guides
the generation process towards a certain target distribution. We call the
method combining these two mechanisms Guided Adversarial Prompts. We perform
our evaluations on different tasks, datasets and architectures, with different
types of distribution shifts (spuriously correlated data, unseen domains) and
demonstrate the efficiency of the proposed feedback mechanisms compared to
open-loop approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page at https://adversarial-prompts.epfl.ch/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ KTbench: A Novel Data Leakage-Free Framework for Knowledge Tracing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15304v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15304v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yahya Badran, Christine Preisach
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge Tracing (KT) is concerned with predicting students' future
performance on learning items in intelligent tutoring systems. Learning items
are tagged with skill labels called knowledge concepts (KCs). Many KT models
expand the sequence of item-student interactions into KC-student interactions
by replacing learning items with their constituting KCs. This often results in
a longer sequence length. This approach addresses the issue of sparse
item-student interactions and minimises model parameters. However, two problems
have been identified with such models.
  The first problem is the model's ability to learn correlations between KCs
belonging to the same item, which can result in the leakage of ground truth
labels and hinder performance. This problem can lead to a significant decrease
in performance on datasets with a higher number of KCs per item. The second
problem is that the available benchmark implementations ignore accounting for
changes in sequence length when expanding KCs, leading to different models
being tested with varying sequence lengths but still compared against the same
benchmark.
  To address these problems, we introduce a general masking framework that
mitigates the first problem and enhances the performance of such KT models
while preserving the original model architecture without significant
alterations. Additionally, we introduce KTbench, an open-source benchmark
library designed to ensure the reproducibility of this work while mitigating
the second problem.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Planning with a Learned Policy Basis to Optimally Solve Complex Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15301v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15301v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guillermo Infante, David Kuric, Anders Jonsson, Vicenç Gómez, Herke van Hoof
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conventional reinforcement learning (RL) methods can successfully solve a
wide range of sequential decision problems. However, learning policies that can
generalize predictably across multiple tasks in a setting with non-Markovian
reward specifications is a challenging problem. We propose to use successor
features to learn a policy basis so that each (sub)policy in it solves a
well-defined subproblem. In a task described by a finite state automaton (FSA)
that involves the same set of subproblems, the combination of these
(sub)policies can then be used to generate an optimal solution without
additional learning. In contrast to other methods that combine (sub)policies
via planning, our method asymptotically attains global optimality, even in
stochastic environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Blockchain-based Pseudonym Management for Vehicle Twin Migrations in
  Vehicular Edge Metaverse 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15285v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15285v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiawen Kang, Xiaofeng Luo, Jiangtian Nie, Tianhao Wu, Haibo Zhou, Yonghua Wang, Dusit Niyato, Shiwen Mao, Shengli Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Driven by the great advances in metaverse and edge computing technologies,
vehicular edge metaverses are expected to disrupt the current paradigm of
intelligent transportation systems. As highly computerized avatars of Vehicular
Metaverse Users (VMUs), the Vehicle Twins (VTs) deployed in edge servers can
provide valuable metaverse services to improve driving safety and on-board
satisfaction for their VMUs throughout journeys. To maintain uninterrupted
metaverse experiences, VTs must be migrated among edge servers following the
movements of vehicles. This can raise concerns about privacy breaches during
the dynamic communications among vehicular edge metaverses. To address these
concerns and safeguard location privacy, pseudonyms as temporary identifiers
can be leveraged by both VMUs and VTs to realize anonymous communications in
the physical space and virtual spaces. However, existing pseudonym management
methods fall short in meeting the extensive pseudonym demands in vehicular edge
metaverses, thus dramatically diminishing the performance of privacy
preservation. To this end, we present a cross-metaverse empowered dual
pseudonym management framework. We utilize cross-chain technology to enhance
management efficiency and data security for pseudonyms. Furthermore, we propose
a metric to assess the privacy level and employ a Multi-Agent Deep
Reinforcement Learning (MADRL) approach to obtain an optimal pseudonym
generating strategy. Numerical results demonstrate that our proposed schemes
are high-efficiency and cost-effective, showcasing their promising applications
in vehicular edge metaverses.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Parametric PDE Control with Deep Reinforcement Learning and
  Differentiable L0-Sparse Polynomial Policies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15267v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15267v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicolò Botteghi, Urban Fasel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Optimal control of parametric partial differential equations (PDEs) is
crucial in many applications in engineering and science. In recent years, the
progress in scientific machine learning has opened up new frontiers for the
control of parametric PDEs. In particular, deep reinforcement learning (DRL)
has the potential to solve high-dimensional and complex control problems in a
large variety of applications. Most DRL methods rely on deep neural network
(DNN) control policies. However, for many dynamical systems, DNN-based control
policies tend to be over-parametrized, which means they need large amounts of
training data, show limited robustness, and lack interpretability. In this
work, we leverage dictionary learning and differentiable L$_0$ regularization
to learn sparse, robust, and interpretable control policies for parametric
PDEs. Our sparse policy architecture is agnostic to the DRL method and can be
used in different policy-gradient and actor-critic DRL algorithms without
changing their policy-optimization procedure. We test our approach on the
challenging tasks of controlling parametric Kuramoto-Sivashinsky and
convection-diffusion-reaction PDEs. We show that our method (1) outperforms
baseline DNN-based DRL policies, (2) allows for the derivation of interpretable
equations of the learned optimal control laws, and (3) generalizes to unseen
parameters of the PDE without retraining the policies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Federated Bayesian Deep Learning: The Application of Statistical
  Aggregation Methods to Bayesian Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15263v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15263v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        John Fischer, Marko Orescanin, Justin Loomis, Patrick McClure
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning (FL) is an approach to training machine learning models
that takes advantage of multiple distributed datasets while maintaining data
privacy and reducing communication costs associated with sharing local
datasets. Aggregation strategies have been developed to pool or fuse the
weights and biases of distributed deterministic models; however, modern
deterministic deep learning (DL) models are often poorly calibrated and lack
the ability to communicate a measure of epistemic uncertainty in prediction,
which is desirable for remote sensing platforms and safety-critical
applications. Conversely, Bayesian DL models are often well calibrated and
capable of quantifying and communicating a measure of epistemic uncertainty
along with a competitive prediction accuracy. Unfortunately, because the
weights and biases in Bayesian DL models are defined by a probability
distribution, simple application of the aggregation methods associated with FL
schemes for deterministic models is either impossible or results in sub-optimal
performance. In this work, we use independent and identically distributed (IID)
and non-IID partitions of the CIFAR-10 dataset and a fully variational
ResNet-20 architecture to analyze six different aggregation strategies for
Bayesian DL models. Additionally, we analyze the traditional federated
averaging approach applied to an approximate Bayesian Monte Carlo dropout model
as a lightweight alternative to more complex variational inference methods in
FL. We show that aggregation strategy is a key hyperparameter in the design of
a Bayesian FL system with downstream effects on accuracy, calibration,
uncertainty quantification, training stability, and client compute
requirements.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Comprehensive Reassessment of Large-Scale Evaluation Outcomes in LLMs: A
  Multifaceted Statistical Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15250v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15250v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kun Sun, Rong Wang, Haitao Liu, Anders Søgaard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Amidst the rapid evolution of LLMs, the significance of evaluation in
comprehending and propelling these models forward is increasingly paramount.
Evaluations have revealed that factors such as scaling, training types,
architectures and other factors profoundly impact the performance of LLMs.
However, the extent and nature of these impacts continue to be subjects of
debate because most assessments have been restricted to a limited number of
models and data points. Clarifying the effects of these factors on performance
scores can be more effectively achieved through a statistical lens. Our study
embarks on a thorough re-examination of these LLMs, targeting the inadequacies
in current evaluation methods. With the advent of a uniform evaluation
framework, our research leverages an expansive dataset of evaluation results,
introducing a comprehensive statistical methodology. This includes the
application of ANOVA, Tukey HSD tests, GAMM, and clustering technique, offering
a robust and transparent approach to deciphering LLM performance data. Contrary
to prevailing findings, our results challenge assumptions about emergent
abilities and the influence of given training types and architectures in LLMs.
These findings furnish new perspectives on the characteristics, intrinsic
nature, and developmental trajectories of LLMs. By providing straightforward
and reliable methods to scrutinize and reassess LLM performance data, this
study contributes a nuanced perspective on LLM efficiency and potentials.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Spectral Motion Alignment for Video Motion Transfer using Diffusion
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15249v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15249v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Geon Yeong Park, Hyeonho Jeong, Sang Wan Lee, Jong Chul Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The evolution of diffusion models has greatly impacted video generation and
understanding. Particularly, text-to-video diffusion models (VDMs) have
significantly facilitated the customization of input video with target
appearance, motion, etc. Despite these advances, challenges persist in
accurately distilling motion information from video frames. While existing
works leverage the consecutive frame residual as the target motion vector, they
inherently lack global motion context and are vulnerable to frame-wise
distortions. To address this, we present Spectral Motion Alignment (SMA), a
novel framework that refines and aligns motion vectors using Fourier and
wavelet transforms. SMA learns motion patterns by incorporating
frequency-domain regularization, facilitating the learning of whole-frame
global motion dynamics, and mitigating spatial artifacts. Extensive experiments
demonstrate SMA's efficacy in improving motion transfer while maintaining
computational efficiency and compatibility across various video customization
frameworks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page:
  https://geonyeong-park.github.io/spectral-motion-alignment/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FollowIR: Evaluating and Teaching Information Retrieval Models to Follow
  Instructions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15246v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15246v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Orion Weller, Benjamin Chang, Sean MacAvaney, Kyle Lo, Arman Cohan, Benjamin Van Durme, Dawn Lawrie, Luca Soldaini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern Large Language Models (LLMs) are capable of following long and complex
instructions that enable a diverse amount of user tasks. However, despite
Information Retrieval (IR) models using LLMs as the backbone of their
architectures, nearly all of them still only take queries as input, with no
instructions. For the handful of recent models that do take instructions, it's
unclear how they use them. We introduce our dataset FollowIR, which contains a
rigorous instruction evaluation benchmark as well as a training set for helping
IR models learn to better follow real-world instructions. FollowIR builds off
the long history of the TREC conferences: as TREC provides human annotators
with instructions (also known as narratives) to determine document relevance,
so should IR models be able to understand and decide relevance based on these
detailed instructions. Our evaluation benchmark starts with three deeply judged
TREC collections and alters the annotator instructions, re-annotating relevant
documents. Through this process, we can measure how well IR models follow
instructions, through a new pairwise evaluation framework. Our results indicate
that existing retrieval models fail to correctly use instructions, using them
for basic keywords and struggling to understand long-form information. However,
we show that it is possible for IR models to learn to follow complex
instructions: our new FollowIR-7B model has significant improvements (over 13%)
after fine-tuning on our training set.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reasoning-Enhanced Object-Centric Learning for Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15245v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15245v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jian Li, Pu Ren, Yang Liu, Hao Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object-centric learning aims to break down complex visual scenes into more
manageable object representations, enhancing the understanding and reasoning
abilities of machine learning systems toward the physical world. Recently,
slot-based video models have demonstrated remarkable proficiency in segmenting
and tracking objects, but they overlook the importance of the effective
reasoning module. In the real world, reasoning and predictive abilities play a
crucial role in human perception and object tracking; in particular, these
abilities are closely related to human intuitive physics. Inspired by this, we
designed a novel reasoning module called the Slot-based Time-Space Transformer
with Memory buffer (STATM) to enhance the model's perception ability in complex
scenes. The memory buffer primarily serves as storage for slot information from
upstream modules, the Slot-based Time-Space Transformer makes predictions
through slot-based spatiotemporal attention computations and fusion. Our
experiment results on various datasets show that STATM can significantly
enhance object-centric learning capabilities of slot-based video models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Stochastic Quasi-Newton Method for Non-convex Optimization with
  Non-uniform Smoothness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15244v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15244v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenyu Sun, Ermin Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Classical convergence analyses for optimization algorithms rely on the
widely-adopted uniform smoothness assumption. However, recent experimental
studies have demonstrated that many machine learning problems exhibit
non-uniform smoothness, meaning the smoothness factor is a function of the
model parameter instead of a universal constant. In particular, it has been
observed that the smoothness grows with respect to the gradient norm along the
training trajectory. Motivated by this phenomenon, the recently introduced
$(L_0, L_1)$-smoothness is a more general notion, compared to traditional
$L$-smoothness, that captures such positive relationship between smoothness and
gradient norm. Under this type of non-uniform smoothness, existing literature
has designed stochastic first-order algorithms by utilizing gradient clipping
techniques to obtain the optimal $\mathcal{O}(\epsilon^{-3})$ sample complexity
for finding an $\epsilon$-approximate first-order stationary solution.
Nevertheless, the studies of quasi-Newton methods are still lacking.
Considering higher accuracy and more robustness for quasi-Newton methods, in
this paper we propose a fast stochastic quasi-Newton method when there exists
non-uniformity in smoothness. Leveraging gradient clipping and variance
reduction, our algorithm can achieve the best-known
$\mathcal{O}(\epsilon^{-3})$ sample complexity and enjoys convergence speedup
with simple hyperparameter tuning. Our numerical experiments show that our
proposed algorithm outperforms the state-of-the-art approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust Utility Optimization via a GAN Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15243v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15243v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florian Krach, Josef Teichmann, Hanna Wutte
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robust utility optimization enables an investor to deal with market
uncertainty in a structured way, with the goal of maximizing the worst-case
outcome. In this work, we propose a generative adversarial network (GAN)
approach to (approximately) solve robust utility optimization problems in
general and realistic settings. In particular, we model both the investor and
the market by neural networks (NN) and train them in a mini-max zero-sum game.
This approach is applicable for any continuous utility function and in
realistic market settings with trading costs, where only observable information
of the market can be used. A large empirical study shows the versatile
usability of our method. Whenever an optimal reference strategy is available,
our method performs on par with it and in the (many) settings without known
optimal strategy, our method outperforms all other reference strategies.
Moreover, we can conclude from our study that the trained path-dependent
strategies do not outperform Markovian ones. Lastly, we uncover that our
generative approach for learning optimal, (non-) robust investments under
trading costs generates universally applicable alternatives to well known
asymptotic strategies of idealized settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Guided Decoding for Robot Motion Generation and Adaption 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15239v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15239v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nutan Chen, Elie Aljalbout, Botond Cseke, Patrick van der Smagt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We address motion generation for high-DoF robot arms in complex settings with
obstacles, via points, etc. A significant advancement in this domain is
achieved by integrating Learning from Demonstration (LfD) into the motion
generation process. This integration facilitates rapid adaptation to new tasks
and optimizes the utilization of accumulated expertise by allowing robots to
learn and generalize from demonstrated trajectories.
  We train a transformer architecture on a large dataset of simulated
trajectories. This architecture, based on a conditional variational autoencoder
transformer, learns essential motion generation skills and adapts these to meet
auxiliary tasks and constraints. Our auto-regressive approach enables real-time
integration of feedback from the physical system, enhancing the adaptability
and efficiency of motion generation. We show that our model can generate motion
from initial and target points, but also that it can adapt trajectories in
navigating complex tasks, including obstacle avoidance, via points, and meeting
velocity and acceleration constraints, across platforms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Exploratory Investigation into Code License Infringements in Large
  Language Model Training <span class="highlight-title">Dataset</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15230v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15230v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonathan Katzy, Răzvan-Mihai Popescu, Arie van Deursen, Maliheh Izadi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Does the training of large language models potentially infringe upon code
licenses? Furthermore, are there any datasets available that can be safely used
for training these models without violating such licenses? In our study, we
assess the current trends in the field and the importance of incorporating code
into the training of large language models. Additionally, we examine publicly
available datasets to see whether these models can be trained on them without
the risk of legal issues in the future. To accomplish this, we compiled a list
of 53 large language models trained on file-level code. We then extracted their
datasets and analyzed how much they overlap with a dataset we created,
consisting exclusively of strong copyleft code.
  Our analysis revealed that every dataset we examined contained license
inconsistencies, despite being selected based on their associated repository
licenses. We analyzed a total of 514 million code files, discovering 38 million
exact duplicates present in our strong copyleft dataset. Additionally, we
examined 171 million file-leading comments, identifying 16 million with strong
copyleft licenses and another 11 million comments that discouraged copying
without explicitly mentioning a license. Based on the findings of our study,
which highlights the pervasive issue of license inconsistencies in large
language models trained on code, our recommendation for both researchers and
the community is to prioritize the development and adoption of best practices
for dataset creation and management.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to FORGE 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Anytime, Anywhere, Anyone: Investigating the Feasibility of Segment
  Anything Model for Crowd-Sourcing Medical Image Annotations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15218v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15218v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pranav Kulkarni, Adway Kanhere, Dharmam Savani, Andrew Chan, Devina Chatterjee, Paul H. Yi, Vishwa S. Parekh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Curating annotations for medical image segmentation is a labor-intensive and
time-consuming task that requires domain expertise, resulting in "narrowly"
focused deep learning (DL) models with limited translational utility. Recently,
foundation models like the Segment Anything Model (SAM) have revolutionized
semantic segmentation with exceptional zero-shot generalizability across
various domains, including medical imaging, and hold a lot of promise for
streamlining the annotation process. However, SAM has yet to be evaluated in a
crowd-sourced setting to curate annotations for training 3D DL segmentation
models. In this work, we explore the potential of SAM for crowd-sourcing
"sparse" annotations from non-experts to generate "dense" segmentation masks
for training 3D nnU-Net models, a state-of-the-art DL segmentation model. Our
results indicate that while SAM-generated annotations exhibit high mean Dice
scores compared to ground-truth annotations, nnU-Net models trained on
SAM-generated annotations perform significantly worse than nnU-Net models
trained on ground-truth annotations ($p<0.001$, all).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Early Period of Training Impacts Out-of-Distribution Generalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15210v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15210v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Cecilia Liu, Iryna Gurevych
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prior research has found that differences in the early period of neural
network training significantly impact the performance of in-distribution (ID)
tasks. However, neural networks are often sensitive to out-of-distribution
(OOD) data, making them less reliable in downstream applications. Yet, the
impact of the early training period on OOD generalization remains understudied
due to its complexity and lack of effective analytical methodologies. In this
work, we investigate the relationship between learning dynamics and OOD
generalization during the early period of neural network training. We utilize
the trace of Fisher Information and sharpness, with a focus on gradual
unfreezing (i.e. progressively unfreezing parameters during training) as the
methodology for investigation. Through a series of empirical experiments, we
show that 1) selecting the number of trainable parameters at different times
during training, i.e. realized by gradual unfreezing -- has a minuscule impact
on ID results, but greatly affects the generalization to OOD data; 2) the
absolute values of sharpness and trace of Fisher Information at the initial
period of training are not indicative for OOD generalization, but the relative
values could be; 3) the trace of Fisher Information and sharpness may be used
as indicators for the removal of interventions during early period of training
for better OOD generalization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>WIP</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Simple and Scalable Strategies to Continually <span class="highlight-title">Pre-train</span> Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08763v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08763v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adam Ibrahim, Benjamin Thérien, Kshitij Gupta, Mats L. Richter, Quentin Anthony, Timothée Lesort, Eugene Belilovsky, Irina Rish
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are routinely pre-trained on billions of tokens,
only to start the process over again once new data becomes available. A much
more efficient solution is to continually pre-train these models, saving
significant compute compared to re-training. However, the distribution shift
induced by new data typically results in degraded performance on previous data
or poor adaptation to the new data. In this work, we show that a simple and
scalable combination of learning rate (LR) re-warming, LR re-decaying, and
replay of previous data is sufficient to match the performance of fully
re-training from scratch on all available data, as measured by final loss and
language model (LM) evaluation benchmarks. Specifically, we show this for a
weak but realistic distribution shift between two commonly used LLM
pre-training datasets (English$\rightarrow$English) and a stronger distribution
shift (English$\rightarrow$German) at the $405$M parameter model scale with
large dataset sizes (hundreds of billions of tokens). Selecting the weak but
realistic shift for larger-scale experiments, we also find that our continual
learning strategies match the re-training baseline for a 10B parameter LLM. Our
results demonstrate that LLMs can be successfully updated via simple and
scalable continual learning strategies, matching the re-training baseline using
only a fraction of the compute. Finally, inspired by previous work, we propose
alternatives to the cosine learning rate schedule that help circumvent
forgetting induced by LR re-warming and that are not bound to a fixed token
budget.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Finding the right XAI method -- A Guide for the Evaluation and Ranking
  of Explainable AI Methods in Climate Science 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.00652v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.00652v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philine Bommer, Marlene Kretschmer, Anna Hedström, Dilyara Bareeva, Marina M. -C. Höhne
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Explainable artificial intelligence (XAI) methods shed light on the
predictions of machine learning algorithms. Several different approaches exist
and have already been applied in climate science. However, usually missing
ground truth explanations complicate their evaluation and comparison,
subsequently impeding the choice of the XAI method. Therefore, in this work, we
introduce XAI evaluation in the climate context and discuss different desired
explanation properties, namely robustness, faithfulness, randomization,
complexity, and localization. To this end, we chose previous work as a case
study where the decade of annual-mean temperature maps is predicted. After
training both a multi-layer perceptron (MLP) and a convolutional neural network
(CNN), multiple XAI methods are applied and their skill scores in reference to
a random uniform explanation are calculated for each property. Independent of
the network, we find that XAI methods Integrated Gradients, layer-wise
relevance propagation, and input times gradients exhibit considerable
robustness, faithfulness, and complexity while sacrificing randomization
performance. Sensitivity methods -- gradient, SmoothGrad, NoiseGrad, and
FusionGrad, match the robustness skill but sacrifice faithfulness and
complexity for randomization skill. We find architecture-dependent performance
differences regarding robustness, complexity and localization skills of
different XAI methods, highlighting the necessity for research task-specific
evaluation. Overall, our work offers an overview of different evaluation
properties in the climate science context and shows how to compare and
benchmark different explanation methods, assessing their suitability based on
strengths and weaknesses, for the specific research problem at hand. By that,
we aim to support climate researchers in the selection of a suitable XAI
method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 10 figure, accepted at AIES journal by AMS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Videoshop: Localized Semantic Video Editing with Noise-Extrapolated
  Diffusion Inversion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14617v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14617v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Fan, Anand Bhattad, Ranjay Krishna
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Videoshop, a training-free video editing algorithm for localized
semantic edits. Videoshop allows users to use any editing software, including
Photoshop and generative inpainting, to modify the first frame; it
automatically propagates those changes, with semantic, spatial, and temporally
consistent motion, to the remaining frames. Unlike existing methods that enable
edits only through imprecise textual instructions, Videoshop allows users to
add or remove objects, semantically change objects, insert stock photos into
videos, etc. with fine-grained control over locations and appearance. We
achieve this through image-based video editing by inverting latents with noise
extrapolation, from which we generate videos conditioned on the edited image.
Videoshop produces higher quality edits against 6 baselines on 2 editing
benchmarks using 10 evaluation metrics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page at https://videoshop-editing.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Empowering Autonomous Driving with Large Language Models: A Safety
  Perspective <span class="chip">ICLR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.00812v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.00812v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixuan Wang, Ruochen Jiao, Sinong Simon Zhan, Chengtian Lang, Chao Huang, Zhaoran Wang, Zhuoran Yang, Qi Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous Driving (AD) encounters significant safety hurdles in long-tail
unforeseen driving scenarios, largely stemming from the non-interpretability
and poor generalization of the deep neural networks within the AD system,
particularly in out-of-distribution and uncertain data. To this end, this paper
explores the integration of Large Language Models (LLMs) into AD systems,
leveraging their robust common-sense knowledge and reasoning abilities. The
proposed methodologies employ LLMs as intelligent decision-makers in behavioral
planning, augmented with a safety verifier shield for contextual safety
learning, for enhancing driving performance and safety. We present two key
studies in a simulated environment: an adaptive LLM-conditioned Model
Predictive Control (MPC) and an LLM-enabled interactive behavior planning
scheme with a state machine. Demonstrating superior performance and safety
metrics compared to state-of-the-art approaches, our approach shows the
promising potential for using LLMs for autonomous vehicles.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to LLMAgent workshop @ICLR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Complexity to Clarity: Analytical Expressions of Deep Neural
  Network Weights via Clifford's Geometric Algebra and Convexity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.16512v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.16512v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mert Pilanci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce a novel analysis of neural networks based on
geometric (Clifford) algebra and convex optimization. We show that optimal
weights of deep ReLU neural networks are given by the wedge product of training
samples when trained with standard regularized loss. Furthermore, the training
problem reduces to convex optimization over wedge product features, which
encode the geometric structure of the training dataset. This structure is given
in terms of signed volumes of triangles and parallelotopes generated by data
vectors. The convex problem finds a small subset of samples via $\ell_1$
regularization to discover only relevant wedge product features. Our analysis
provides a novel perspective on the inner workings of deep neural networks and
sheds light on the role of the hidden layers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MM1: Methods, Analysis & Insights from Multimodal LLM <span class="highlight-title">Pre-train</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09611v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09611v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brandon McKinzie, Zhe Gan, Jean-Philippe Fauconnier, Sam Dodge, Bowen Zhang, Philipp Dufter, Dhruti Shah, Xianzhi Du, Futang Peng, Floris Weers, Anton Belyi, Haotian Zhang, Karanjeet Singh, Doug Kang, Ankur Jain, Hongyu Hè, Max Schwarzer, Tom Gunter, Xiang Kong, Aonan Zhang, Jianyu Wang, Chong Wang, Nan Du, Tao Lei, Sam Wiseman, Guoli Yin, Mark Lee, Zirui Wang, Ruoming Pang, Peter Grasch, Alexander Toshev, Yinfei Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we discuss building performant Multimodal Large Language Models
(MLLMs). In particular, we study the importance of various architecture
components and data choices. Through careful and comprehensive ablations of the
image encoder, the vision language connector, and various pre-training data
choices, we identified several crucial design lessons. For example, we
demonstrate that for large-scale multimodal pre-training using a careful mix of
image-caption, interleaved image-text, and text-only data is crucial for
achieving state-of-the-art (SOTA) few-shot results across multiple benchmarks,
compared to other published pre-training results. Further, we show that the
image encoder together with image resolution and the image token count has
substantial impact, while the vision-language connector design is of
comparatively negligible importance. By scaling up the presented recipe, we
build MM1, a family of multimodal models up to 30B parameters, including both
dense models and mixture-of-experts (MoE) variants, that are SOTA in
pre-training metrics and achieve competitive performance after supervised
fine-tuning on a range of established multimodal benchmarks. Thanks to
large-scale pre-training, MM1 enjoys appealing properties such as enhanced
in-context learning, and multi-image reasoning, enabling few-shot
chain-of-thought prompting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Attractor reconstruction with reservoir computers: The effect of the
  reservoir's conditional Lyapunov exponents on faithful attractor
  reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00885v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00885v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joseph D. Hart
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reservoir computing is a machine learning framework that has been shown to be
able to replicate the chaotic attractor, including the fractal dimension and
the entire Lyapunov spectrum, of the dynamical system on which it is trained.
We quantitatively relate the generalized synchronization dynamics of a driven
reservoir during the training stage to the performance of the trained reservoir
computer at the attractor reconstruction task. We show that, in order to obtain
successful attractor reconstruction and Lyapunov spectrum estimation, the
largest conditional Lyapunov exponent of the driven reservoir must be
significantly more negative than the most negative Lyapunov exponent of the
target system. We also find that the maximal conditional Lyapunov exponent of
the reservoir depends strongly on the spectral radius of the reservoir
adjacency matrix, and therefore, for attractor reconstruction and Lyapunov
spectrum estimation, small spectral radius reservoir computers perform better
in general. Our arguments are supported by numerical examples on well-known
chaotic systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning to Predict Structural Vibrations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05469v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05469v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jan van Delden, Julius Schultz, Christopher Blech, Sabine C. Langer, Timo Lüddecke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In mechanical structures like airplanes, cars and houses, noise is generated
and transmitted through vibrations. To take measures to reduce this noise,
vibrations need to be simulated with expensive numerical computations.
Surrogate deep learning models present a promising alternative to classical
numerical simulations as they can be evaluated magnitudes faster, while
trading-off accuracy. To quantify such trade-offs systematically and foster the
development of methods, we present a benchmark on the task of predicting the
vibration of harmonically excited plates. The benchmark features a total of
12000 plate geometries with varying forms of beadings, material and sizes with
associated numerical solutions. To address the benchmark task, we propose a new
network architecture, named Frequency-Query Operator, which is trained to map
plate geometries to their vibration pattern given a specific excitation
frequency. Applying principles from operator learning and implicit models for
shape encoding, our approach effectively addresses the prediction of highly
variable frequency response functions occurring in dynamic systems. To quantify
the prediction quality, we introduce a set of evaluation metrics and evaluate
the method on our vibrating-plates benchmark. Our method outperforms DeepONets,
Fourier Neural Operators and more traditional neural network architectures.
Code, dataset and visualizations: https://eckerlab.org/code/delden2023_plate
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning High-level Semantic-Relational Concepts for SLAM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.00401v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.00401v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jose Andres Millan-Romera, Hriday Bavle, Muhammad Shaheer, Martin R. Oswald, Holger Voos, Jose Luis Sanchez-Lopez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent works on SLAM extend their pose graphs with higher-level semantic
concepts like Rooms exploiting relationships between them, to provide, not only
a richer representation of the situation/environment but also to improve the
accuracy of its estimation. Concretely, our previous work, Situational Graphs
(S-Graphs+), a pioneer in jointly leveraging semantic relationships in the
factor optimization process, relies on semantic entities such as Planes and
Rooms, whose relationship is mathematically defined. Nevertheless, there is no
unique approach to finding all the hidden patterns in lower-level factor-graphs
that correspond to high-level concepts of different natures. It is currently
tackled with ad-hoc algorithms, which limits its graph expressiveness.
  To overcome this limitation, in this work, we propose an algorithm based on
Graph Neural Networks for learning high-level semantic-relational concepts that
can be inferred from the low-level factor graph. Given a set of mapped Planes
our algorithm is capable of inferring Room entities relating to the Planes.
Additionally, to demonstrate the versatility of our method, our algorithm can
infer an additional semantic-relational concept, i.e. Wall, and its
relationship with its Planes. We validate our method in both simulated and real
datasets demonstrating improved performance over two baseline approaches.
Furthermore, we integrate our method into the S-Graphs+ algorithm providing
improved pose and map accuracy compared to the baseline while further enhancing
the scene representation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Novelty Detection in Reinforcement Learning with World Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08731v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08731v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Geigh Zollicoffer, Kenneth Eaton, Jonathan Balloch, Julia Kim, Mark O. Riedl, Robert Wright
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning (RL) using world models has found significant recent
successes. However, when a sudden change to world mechanics or properties
occurs then agent performance and reliability can dramatically decline. We
refer to the sudden change in visual properties or state transitions as
novelties. Implementing novelty detection within generated world model
frameworks is a crucial task for protecting the agent when deployed. In this
paper, we propose straightforward bounding approaches to incorporate novelty
detection into world model RL agents, by utilizing the misalignment of the
world model's hallucinated states and the true observed states as an anomaly
score. We provide effective approaches to detecting novelties in a distribution
of transitions learned by an agent in a world model. Finally, we show the
advantage of our work in a novel environment compared to traditional machine
learning novelty detection methods as well as currently accepted RL focused
novelty detection algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Faster Neighborhood Attention: Reducing the O(n^2) Cost of Self
  Attention at the Threadblock Level 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.04690v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.04690v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Hassani, Wen-Mei Hwu, Humphrey Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neighborhood attention reduces the cost of self attention by restricting each
token's attention span to its nearest neighbors. This restriction,
parameterized by a window size and dilation factor, draws a spectrum of
possible attention patterns between linear projection and self attention.
Neighborhood attention, and more generally sliding window attention patterns,
have long been bounded by infrastructure, particularly in higher-rank spaces
(2-D and 3-D), calling for the development of custom kernels, which have been
limited in either functionality, or performance, if not both. In this work, we
first show that neighborhood attention can be represented as a batched GEMM
problem, similar to standard attention, and implement it for 1-D and 2-D
neighborhood attention. These kernels on average provide 895% and 272%
improvement in full precision latency compared to existing naive kernels for
1-D and 2-D neighborhood attention respectively. We find certain inherent
inefficiencies in all unfused neighborhood attention kernels that bound their
performance and lower-precision scalability. We also developed fused
neighborhood attention; an adaptation of fused dot-product attention kernels
that allow fine-grained control over attention across different spatial axes.
Known for reducing the quadratic time complexity of self attention to a linear
complexity, neighborhood attention can now enjoy a reduced and constant memory
footprint, and record-breaking half precision latency. We observe that our
fused kernels successfully circumvent some of the unavoidable inefficiencies in
unfused implementations. While our unfused GEMM-based kernels only improve half
precision performance compared to naive kernels by an average of 496% and 113%
in 1-D and 2-D problems respectively, our fused kernels improve naive kernels
by an average of 1607% and 581% in 1-D and 2-D problems respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://github.com/SHI-Labs/NATTEN</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unveiling Group-Specific Distributed Concept Drift: A Fairness
  Imperative in Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.07586v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.07586v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Teresa Salazar, João Gama, Helder Araújo, Pedro Henriques Abreu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the evolving field of machine learning, ensuring fairness has become a
critical concern, prompting the development of algorithms designed to mitigate
discriminatory outcomes in decision-making processes. However, achieving
fairness in the presence of group-specific concept drift remains an unexplored
frontier, and our research represents pioneering efforts in this regard.
Group-specific concept drift refers to situations where one group experiences
concept drift over time while another does not, leading to a decrease in
fairness even if accuracy remains fairly stable. Within the framework of
federated learning, where clients collaboratively train models, its distributed
nature further amplifies these challenges since each client can experience
group-specific concept drift independently while still sharing the same
underlying concept, creating a complex and dynamic environment for maintaining
fairness. One of the significant contributions of our research is the
formalization and introduction of the problem of group-specific concept drift
and its distributed counterpart, shedding light on its critical importance in
the realm of fairness. In addition, leveraging insights from prior research, we
adapt an existing distributed concept drift adaptation algorithm to tackle
group-specific distributed concept drift which utilizes a multi-model approach,
a local group-specific drift detection mechanism, and continuous clustering of
models over time. The findings from our experiments highlight the importance of
addressing group-specific concept drift and its distributed counterpart to
advance fairness in machine learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Recurrent Drafter for Fast Speculative Decoding in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09919v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09919v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aonan Zhang, Chong Wang, Yi Wang, Xuanyu Zhang, Yunfei Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce an improved approach of speculative decoding
aimed at enhancing the efficiency of serving large language models. Our method
capitalizes on the strengths of two established techniques: the classic
two-model speculative decoding approach, and the more recent single-model
approach, Medusa. Drawing inspiration from Medusa, our approach adopts a
single-model strategy for speculative decoding. However, our method
distinguishes itself by employing a single, lightweight draft head with a
recurrent dependency design, akin in essence to the small, draft model uses in
classic speculative decoding, but without the complexities of the full
transformer architecture. And because of the recurrent dependency, we can use
beam search to swiftly filter out undesired candidates with the draft head. The
outcome is a method that combines the simplicity of single-model design and
avoids the need to create a data-dependent tree attention structure only for
inference in Medusa. We empirically demonstrate the effectiveness of the
proposed method on several popular open source language models, along with a
comprehensive analysis of the trade-offs involved in adopting this approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large Language Model-informed ECG Dual Attention Network for Heart
  Failure Risk Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10581v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10581v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Chen, Lei Li, Marcel Beetz, Abhirup Banerjee, Ramneek Gupta, Vicente Grau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Heart failure (HF) poses a significant public health challenge, with a rising
global mortality rate. Early detection and prevention of HF could significantly
reduce its impact. We introduce a novel methodology for predicting HF risk
using 12-lead electrocardiograms (ECGs). We present a novel, lightweight
dual-attention ECG network designed to capture complex ECG features essential
for early HF risk prediction, despite the notable imbalance between low and
high-risk groups. This network incorporates a cross-lead attention module and
twelve lead-specific temporal attention modules, focusing on cross-lead
interactions and each lead's local dynamics. To further alleviate model
overfitting, we leverage a large language model (LLM) with a public ECG-Report
dataset for pretraining on an ECG-report alignment task. The network is then
fine-tuned for HF risk prediction using two specific cohorts from the UK
Biobank study, focusing on patients with hypertension (UKB-HYP) and those who
have had a myocardial infarction (UKB-MI).The results reveal that LLM-informed
pre-training substantially enhances HF risk prediction in these cohorts. The
dual-attention design not only improves interpretability but also predictive
accuracy, outperforming existing competitive methods with C-index scores of
0.6349 for UKB-HYP and 0.5805 for UKB-MI. This demonstrates our method's
potential in advancing HF risk assessment with clinical complex ECG data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under journal revision</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-resolution Time-Series <span class="highlight-title">Transformer</span> for Long-term Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.04147v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.04147v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yitian Zhang, Liheng Ma, Soumyasundar Pal, Yingxue Zhang, Mark Coates
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The performance of transformers for time-series forecasting has improved
significantly. Recent architectures learn complex temporal patterns by
segmenting a time-series into patches and using the patches as tokens. The
patch size controls the ability of transformers to learn the temporal patterns
at different frequencies: shorter patches are effective for learning localized,
high-frequency patterns, whereas mining long-term seasonalities and trends
requires longer patches. Inspired by this observation, we propose a novel
framework, Multi-resolution Time-Series Transformer (MTST), which consists of a
multi-branch architecture for simultaneous modeling of diverse temporal
patterns at different resolutions. In contrast to many existing time-series
transformers, we employ relative positional encoding, which is better suited
for extracting periodic components at different scales. Extensive experiments
on several real-world datasets demonstrate the effectiveness of MTST in
comparison to state-of-the-art forecasting techniques.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Residual Denoising Diffusion Models <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.13712v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.13712v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiawei Liu, Qiang Wang, Huijie Fan, Yinong Wang, Yandong Tang, Liangqiong Qu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose residual denoising diffusion models (RDDM), a novel dual diffusion
process that decouples the traditional single denoising diffusion process into
residual diffusion and noise diffusion. This dual diffusion framework expands
the denoising-based diffusion models, initially uninterpretable for image
restoration, into a unified and interpretable model for both image generation
and restoration by introducing residuals. Specifically, our residual diffusion
represents directional diffusion from the target image to the degraded input
image and explicitly guides the reverse generation process for image
restoration, while noise diffusion represents random perturbations in the
diffusion process. The residual prioritizes certainty, while the noise
emphasizes diversity, enabling RDDM to effectively unify tasks with varying
certainty or diversity requirements, such as image generation and restoration.
We demonstrate that our sampling process is consistent with that of DDPM and
DDIM through coefficient transformation, and propose a partially
path-independent generation process to better understand the reverse process.
Notably, our RDDM enables a generic UNet, trained with only an L1 loss and a
batch size of 1, to compete with state-of-the-art image restoration methods. We
provide code and pre-trained models to encourage further exploration,
application, and development of our innovative framework
(https://github.com/nachifur/RDDM).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dynamic-SUPERB: Towards A Dynamic, Collaborative, and Comprehensive
  Instruction-Tuning Benchmark for Speech <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.09510v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.09510v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chien-yu Huang, Ke-Han Lu, Shih-Heng Wang, Chi-Yuan Hsiao, Chun-Yi Kuan, Haibin Wu, Siddhant Arora, Kai-Wei Chang, Jiatong Shi, Yifan Peng, Roshan Sharma, Shinji Watanabe, Bhiksha Ramakrishnan, Shady Shehata, Hung-yi Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text language models have shown remarkable zero-shot capability in
generalizing to unseen tasks when provided with well-formulated instructions.
However, existing studies in speech processing primarily focus on limited or
specific tasks. Moreover, the lack of standardized benchmarks hinders a fair
comparison across different approaches. Thus, we present Dynamic-SUPERB, a
benchmark designed for building universal speech models capable of leveraging
instruction tuning to perform multiple tasks in a zero-shot fashion. To achieve
comprehensive coverage of diverse speech tasks and harness instruction tuning,
we invite the community to collaborate and contribute, facilitating the dynamic
growth of the benchmark. To initiate, Dynamic-SUPERB features 55 evaluation
instances by combining 33 tasks and 22 datasets. This spans a broad spectrum of
dimensions, providing a comprehensive platform for evaluation. Additionally, we
propose several approaches to establish benchmark baselines. These include the
utilization of speech models, text language models, and the multimodal encoder.
Evaluation results indicate that while these baselines perform reasonably on
seen tasks, they struggle with unseen ones. We release all materials to the
public and welcome researchers to collaborate on the project, advancing
technologies in the field together.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in the proceedings of ICASSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Combining the Strengths of Dutch <span class="highlight-title">Survey</span> and Register Data in a Data
  Challenge to Predict Fertility (PreFer) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.00705v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.00705v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elizaveta Sivak, Paulina Pankowska, Adrienne Mendrik, Tom Emery, Javier Garcia-Bernardo, Seyit Hocuk, Kasia Karpinska, Angelica Maineri, Joris Mulder, Malvina Nissim, Gert Stulp
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The social sciences have produced an impressive body of research on
determinants of fertility outcomes, or whether and when people have children.
However, the strength of these determinants and underlying theories are rarely
evaluated on their predictive ability on new data. This prevents us from
systematically comparing studies, hindering the evaluation and accumulation of
knowledge. In this paper, we present two datasets which can be used to study
the predictability of fertility outcomes in the Netherlands. One dataset is
based on the LISS panel, a longitudinal survey which includes thousands of
variables on a wide range of topics, including individual preferences and
values. The other is based on the Dutch register data which lacks attitudinal
data but includes detailed information about the life courses of millions of
Dutch residents. We provide information about the datasets and the samples, and
describe the fertility outcome of interest. We also introduce the fertility
prediction data challenge PreFer which is based on these datasets and will
start in Spring 2024. We outline the ways in which measuring the predictability
of fertility outcomes using these datasets and combining their strengths in the
data challenge can advance our understanding of fertility behaviour and
computational social science. We further provide details for participants on
how to take part in the data challenge.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sparse Mean Field Load Balancing in Large Localized Queueing Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.12973v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.12973v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anam Tahir, Kai Cui, Heinz Koeppl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scalable load balancing algorithms are of great interest in cloud networks
and data centers, necessitating the use of tractable techniques to compute
optimal load balancing policies for good performance. However, most existing
scalable techniques, especially asymptotically scaling methods based on mean
field theory, have not been able to model large queueing networks with strong
locality. Meanwhile, general multi-agent reinforcement learning techniques can
be hard to scale and usually lack a theoretical foundation. In this work, we
address this challenge by leveraging recent advances in sparse mean field
theory to learn a near-optimal load balancing policy in sparsely connected
queueing networks in a tractable manner, which may be preferable to global
approaches in terms of wireless communication overhead. Importantly, we obtain
a general load balancing framework for a large class of sparse bounded-degree
wireless topologies. By formulating a novel mean field control problem in the
context of graphs with bounded degree, we reduce the otherwise difficult
multi-agent problem to a single-agent problem. Theoretically, the approach is
justified by approximation guarantees. Empirically, the proposed methodology
performs well on several realistic and scalable wireless network topologies as
compared to a number of well-known load balancing heuristics and existing
scalable multi-agent reinforcement learning methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Task-Oriented GNNs Training on Large Knowledge Graphs for Accurate and
  Efficient Modeling <span class="chip">ICDE</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.05752v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.05752v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hussein Abdallah, Waleed Afandi, Panos Kalnis, Essam Mansour
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A Knowledge Graph (KG) is a heterogeneous graph encompassing a diverse range
of node and edge types. Heterogeneous Graph Neural Networks (HGNNs) are popular
for training machine learning tasks like node classification and link
prediction on KGs. However, HGNN methods exhibit excessive complexity
influenced by the KG's size, density, and the number of node and edge types. AI
practitioners handcraft a subgraph of a KG G relevant to a specific task. We
refer to this subgraph as a task-oriented subgraph (TOSG), which contains a
subset of task-related node and edge types in G. Training the task using TOSG
instead of G alleviates the excessive computation required for a large KG.
Crafting the TOSG demands a deep understanding of the KG's structure and the
task's objectives. Hence, it is challenging and time-consuming. This paper
proposes KG-TOSA, an approach to automate the TOSG extraction for task-oriented
HGNN training on a large KG. In KG-TOSA, we define a generic graph pattern that
captures the KG's local and global structure relevant to a specific task. We
explore different techniques to extract subgraphs matching our graph pattern:
namely (i) two techniques sampling around targeted nodes using biased random
walk or influence scores, and (ii) a SPARQL-based extraction method leveraging
RDF engines' built-in indices. Hence, it achieves negligible preprocessing
overhead compared to the sampling techniques. We develop a benchmark of real
KGs of large sizes and various tasks for node classification and link
prediction. Our experiments show that KG-TOSA helps state-of-the-art HGNN
methods reduce training time and memory usage by up to 70% while improving the
model performance, e.g., accuracy and inference time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages,9 Figures, 3 Tables, ICDE:2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An axiomatized PDE model of deep neural networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.12333v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.12333v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tangjun Wang, Wenqi Tao, Chenglong Bao, Zuoqiang Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inspired by the relation between deep neural network (DNN) and partial
differential equations (PDEs), we study the general form of the PDE models of
deep neural networks. To achieve this goal, we formulate DNN as an evolution
operator from a simple base model. Based on several reasonable assumptions, we
prove that the evolution operator is actually determined by
convection-diffusion equation. This convection-diffusion equation model gives
mathematical explanation for several effective networks. Moreover, we show that
the convection-diffusion model improves the robustness and reduces the
Rademacher complexity. Based on the convection-diffusion equation, we design a
new training method for ResNets. Experiments validate the performance of the
proposed method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The experiment design in the paper lacks careful thought and may be
  misleading in demonstrating our contribution</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fast Nonlinear Two-Time-Scale Stochastic Approximation: Achieving
  $O(1/k)$ Finite-Sample Complexity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.12764v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.12764v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thinh T. Doan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes to develop a new variant of the two-time-scale stochastic
approximation to find the roots of two coupled nonlinear operators, assuming
only noisy samples of these operators can be observed. Our key idea is to
leverage the classic Ruppert-Polyak averaging technique to dynamically estimate
the operators through their samples. The estimated values of these averaging
steps will then be used in the two-time-scale stochastic approximation updates
to find the desired solution. Our main theoretical result is to show that under
the strongly monotone condition of the underlying nonlinear operators the
mean-squared errors of the iterates generated by the proposed method converge
to zero at an optimal rate $O(1/k)$, where $k$ is the number of iterations. Our
result significantly improves the existing result of two-time-scale stochastic
approximation, where the best known finite-time convergence rate is
$O(1/k^{2/3})$. We illustrate this result by applying the proposed method to
develop new reinforcement learning algorithms with improved performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ KGLiDS: A Platform for Semantic Abstraction, Linking, and Automation of
  Data Science 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.02204v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.02204v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mossad Helali, Niki Monjazeb, Shubham Vashisth, Philippe Carrier, Ahmed Helal, Antonio Cavalcante, Khaled Ammar, Katja Hose, Essam Mansour
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, we have witnessed the growing interest from academia and
industry in applying data science technologies to analyze large amounts of
data. In this process, a myriad of artifacts (datasets, pipeline scripts, etc.)
are created. However, there has been no systematic attempt to holistically
collect and exploit all the knowledge and experiences that are implicitly
contained in those artifacts. Instead, data scientists recover information and
expertise from colleagues or learn via trial and error. Hence, this paper
presents a scalable platform, KGLiDS, that employs machine learning and
knowledge graph technologies to abstract and capture the semantics of data
science artifacts and their connections. Based on this information, KGLiDS
enables various downstream applications, such as data discovery and pipeline
automation. Our comprehensive evaluation covers use cases in data discovery,
data cleaning, transformation, and AutoML. It shows that KGLiDS is
significantly faster with a lower memory footprint than the state-of-the-art
systems while achieving comparable or better accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Interpretability Guarantees with Merlin-Arthur Classifiers <span class="chip">AISTATS24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.00759v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.00759v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stephan Wäldchen, Kartikey Sharma, Berkant Turan, Max Zimmer, Sebastian Pokutta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose an interactive multi-agent classifier that provides provable
interpretability guarantees even for complex agents such as neural networks.
These guarantees consist of lower bounds on the mutual information between
selected features and the classification decision. Our results are inspired by
the Merlin-Arthur protocol from Interactive Proof Systems and express these
bounds in terms of measurable metrics such as soundness and completeness.
Compared to existing interactive setups, we rely neither on optimal agents nor
on the assumption that features are distributed independently. Instead, we use
the relative strength of the agents as well as the new concept of Asymmetric
Feature Correlation which captures the precise kind of correlations that make
interpretability guarantees difficult. We evaluate our results on two
small-scale datasets where high mutual information can be verified explicitly.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AISTATS24 Camera-Ready Version, 34 pages total (9 pages main part, 3
  pages references, 22 pages appendix), 17 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Science and Game Theory
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Strategic Network Creation for Enabling Greedy Routing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15307v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15307v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julian Berger, Tobias Friedrich, Pascal Lenzner, Paraskevi Machaira, Janosch Ruff
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present the first game-theoretic network creation model
that incorporates greedy routing, i.e., the agents in our model are embedded in
some metric space and strive for creating a network where all-pairs greedy
routing is enabled. In contrast to graph-theoretic shortest paths, our agents
route their traffic along greedy paths, which are sequences of nodes where the
distance in the metric space to the respective target node gets strictly
smaller by each hop. Besides enabling greedy routing, the agents also optimize
their connection quality within the created network by constructing greedy
paths with low stretch. This ensures that greedy routing is always possible in
equilibrium networks, while realistically modeling the agents' incentives for
local structural changes to the network. With this we augment the elegant
network creation model by Moscibroda, Schmidt, and Wattenhofer (PODC'06) with
the feature of greedy routing.
  For our model, we analyze the existence of (approximate)-equilibria and the
computational hardness in different underlying metric spaces. E.g., we
characterize the set of equilibria in 1-2-metrics and tree metrics, we show
that in both metrics Nash equilibria always exist, and we prove that the
well-known $\Theta$-graph construction yields constant-approximate Nash
equilibria in Euclidean space. The latter justifies distributed network
construction via $\Theta$-graphs from a new point-of-view, since it shows that
this powerful technique not only guarantees networks having a low stretch but
also networks that are almost stable.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Human behaviour through a LENS: How Linguistic content triggers Emotions
  and Norms and determines Strategy choices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15293v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15293v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Valerio Capraro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Over the last two decades, a growing body of experimental research has
provided evidence that linguistic frames influence human behaviour in economic
games, beyond the economic consequences of the available actions. This article
proposes a novel framework that transcends the traditional confines of
outcome-based preference models. According to the LENS model, the Linguistic
description of the decision problem triggers Emotional responses and suggests
potential Norms of behaviour, which then interact to shape an individual's
Strategic choice. The article reviews experimental evidence that supports each
path of the LENS model. Furthermore, it identifies and discusses several
critical research questions that arise from this model, pointing towards
avenues for future inquiry.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Weighted Top-Difference Distance: Axioms, Aggregation, and
  Approximation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15198v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15198v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrea Aveni, Ludovico Crippa, Giulio Principi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study a family of distance functions on rankings that allow for asymmetric
treatments of alternatives and consider the distinct relevance of the top and
bottom positions for ordered lists. We provide a full axiomatic
characterization of our distance. In doing so, we retrieve new
characterizations of existing axioms and show how to effectively weaken them
for our purposes. This analysis highlights the generality of our distance as it
embeds many (semi)metrics previously proposed in the literature. Subsequently,
we show that, notwithstanding its level of generality, our distance is still
readily applicable. We apply it to preference aggregation, studying the
features of the associated median voting rule. It is shown how the derived
preference function satisfies many desirable features in the context of voting
rules, ranging from fairness to majority and Pareto-related properties. We show
how to compute consensus rankings exactly, and provide generalized
Diaconis-Graham inequalities that can be leveraged to obtain approximation
algorithms. Finally, we propose some truncation ideas for our distances
inspired by Lu and Boutilier (2010). These can be leveraged to devise a
Polynomial-Time-Approximation Scheme for the corresponding rank aggregation
problem.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>64 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Variational Interpretation of Mirror Play in Monotone Games 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15636v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15636v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunian Pan, Tao Li, Quanyan Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mirror play (MP) is a well-accepted primal-dual multi-agent learning
algorithm where all agents simultaneously implement mirror descent in a
distributed fashion. The advantage of MP over vanilla gradient play lies in its
usage of mirror maps that better exploit the geometry of decision domains.
Despite extensive literature dedicated to the asymptotic convergence of MP to
equilibrium, the understanding of the finite-time behavior of MP before
reaching equilibrium is still rudimentary. To facilitate the study of MP's
non-equilibrium performance, this work establishes an equivalence between MP's
finite-time primal-dual path (mirror path) in monotone games and the
closed-loop Nash equilibrium path of a finite-horizon differential game,
referred to as mirror differential game (MDG). Our construction of MDG rests on
the Brezis-Ekeland variational principle, and the stage cost functional for MDG
is Fenchel coupling between MP's iterates and associated gradient updates. The
variational interpretation of mirror path in static games as the equilibrium
path in MDG holds in deterministic and stochastic cases. Such a variational
interpretation translates the non-equilibrium studies of learning dynamics into
a more tractable equilibrium analysis of dynamic games, as demonstrated in a
case study on the Cournot game, where MP dynamics corresponds to a linear
quadratic game.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Balancing Fairness and Efficiency in Energy Resource Allocations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15616v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15616v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiayi Li, Matthew Motoki, Baosen Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bringing fairness to energy resource allocation remains a challenge, due to
the complexity of system structures and economic interdependencies among users
and system operators' decision-making. The rise of distributed energy resources
has introduced more diverse heterogeneous user groups, surpassing the
capabilities of traditional efficiency-oriented allocation schemes. Without
explicitly bringing fairness to user-system interaction, this disparity often
leads to disproportionate payments for certain user groups due to their utility
formats or group sizes.
  Our paper addresses this challenge by formalizing the problem of fair energy
resource allocation and introducing the framework for aggregators. This
framework enables optimal fairness-efficiency trade-offs by selecting
appropriate objectives in a principled way. By jointly optimizing over the
total resources to allocate and individual allocations, our approach reveals
optimized allocation schemes that lie on the Pareto front, balancing fairness
and efficiency in resource allocation strategies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PPA-Game: Characterizing and Learning Competitive Dynamics Among Online
  Content Creators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15524v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15524v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Renzhe Xu, Haotian Wang, Xingxuan Zhang, Bo Li, Peng Cui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce the Proportional Payoff Allocation Game (PPA-Game) to model how
agents, akin to content creators on platforms like YouTube and TikTok, compete
for divisible resources and consumers' attention. Payoffs are allocated to
agents based on heterogeneous weights, reflecting the diversity in content
quality among creators. Our analysis reveals that although a pure Nash
equilibrium (PNE) is not guaranteed in every scenario, it is commonly observed,
with its absence being rare in our simulations. Beyond analyzing static
payoffs, we further discuss the agents' online learning about resource payoffs
by integrating a multi-player multi-armed bandit framework. We propose an
online algorithm facilitating each agent's maximization of cumulative payoffs
over $T$ rounds. Theoretically, we establish that the regret of any agent is
bounded by $O(\log^{1 + \eta} T)$ for any $\eta > 0$. Empirical results further
validate the effectiveness of our approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A-PSRO: A Unified Strategy Learning Method with Advantage Function for
  Normal-form Games 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.12520v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.12520v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yudong Hu, Haoran Li, Congying Han, Tiande Guo, Mingqiang Li, Bonan Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Solving Nash equilibrium is the key challenge in normal-form games with large
strategy spaces, where open-ended learning frameworks offer an efficient
approach. In this work, we propose an innovative unified open-ended learning
framework A-PSRO, i.e., Advantage Policy Space Response Oracle, as a
comprehensive framework for both zero-sum and general-sum games. In particular,
we introduce the advantage function as an enhanced evaluation metric for
strategies, enabling a unified learning objective for agents engaged in
normal-form games. We prove that the advantage function exhibits favorable
properties and is connected with the Nash equilibrium, which can be used as an
objective to guide agents to learn strategies efficiently. Our experiments
reveal that A-PSRO achieves a considerable decrease in exploitability in
zero-sum games and an escalation in rewards in general-sum games, significantly
outperforming previous PSRO algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Theory
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Information Rates of Successive Interference Cancellation for Optical
  Fiber 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15240v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15240v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alex Jäger, Gerhard Kramer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Successive interference cancellation (SIC) is used to approach the achievable
information rates (AIRs) of joint detection and decoding for long-haul optical
fiber links. The AIRs of memoryless ring constellations are compared to those
of circularly symmetric complex Gaussian modulation for surrogate channel
models with correlated phase noise. Simulations are performed for 1000 km of
standard single-mode fiber with ideal Raman amplification. In this setup, 32
rings and 16 SIC-stages with Gaussian message-passing receivers achieve the AIR
peaks of previous work. The computational complexity scales in proportion to
the number of SIC-stages, where one stage has the complexity of separate
detection and decoding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mutual Information of a class of Poisson-type Channels using Markov
  Renewal Theory 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15221v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15221v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maximilian Gehri, Nicolai Engelmann, Heinz Koeppl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The mutual information (MI) of Poisson-type channels has been linked to a
filtering problem since the 70s, but its evaluation for specific
continuous-time, discrete-state systems remains a demanding task. As an
advantage, Markov renewal processes (MrP) retain their renewal property under
state space filtering. This offers a way to solve the filtering problem
analytically for small systems. We consider a class of communication systems $X
\to Y$ that can be derived from a MrP by a custom filtering procedure. For the
subclasses, where (i) $Y$ is a renewal process or (ii) $(X,Y)$ belongs to a
class of MrPs, we provide an evolution equation for finite transmission
duration $T>0$ and limit theorems for $T \to \infty$ that facilitate
simulation-free evaluation of the MI $\mathbb{I}(X_{[0,T]}; Y_{[0,T]})$ and its
associated mutual information rate (MIR). In other cases, simulation cost is
significantly reduced. We show that systems with an additional $X$-modulating
level $C$, which statically chooses between different processes $X_{[0,T]}(c)$,
can naturally be included in our framework thereby giving an expression for
$\mathbb{I}(C; Y_{[0,T]})$. The theoretical framework is showcased in an
application to bacterial gene expression, where filtering is analytically
tractable.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 main pages, 1 main figure, 4 appendix pages, 2 appendix figures,
  conference submission</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Channel Orthogonalization with Reconfigurable Surfaces: General Models,
  Theoretical Limits, and Effective Configuration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15165v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15165v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juan Vidal Alegría, Johan Thunberg, Ove Edfors
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We envision a future in which multi-antenna technology effectively exploits
the spatial domain as a set of non-interfering orthogonal resources, allowing
for flexible resource allocation and efficient modulation/demodulation.
Reconfigurable intelligent surface (RIS) has emerged as a promising technology
which allows shaping the propagation environment for improved performance. This
paper studies the ability of three extended types of reconfigurable surface
(RS), including the recently proposed beyond diagonal RIS (BD-RIS), to achieve
perfectly orthogonal channels in a general multi-user multiple-input
multiple-output (MU-MIMO) scenario. We propose practical implementations for
the three types of RS consisting of passive components, and obtain the
corresponding restrictions on their reconfigurability. We then use these
restrictions to derive closed-form conditions for achieving arbitrary
(orthogonal) channels. We also study the problem of optimal orthogonal channel
selection for achieving high channel gain without active amplification at the
RS, and we propose some methods with satisfying performance. Finally, we
provide efficient channel estimation and RS configuration techniques such that
all the computation, including the channel selection, may be performed at the
base station (BS). The numerical results showcase the potential and
practicality of RS channel orthogonalization, thus taking a step towards
orthogonal spatial domain multiplexing (OSDM).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 4 figures. This work has been submitted to the IEEE for
  possible publication, copyright information may be affected upon publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust Resource Allocation for STAR-RIS Assisted SWIPT Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15145v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15145v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangyu Zhu, Xidong Mu, Li Guo, Ao Huang, Shibiao Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A simultaneously transmitting and reflecting reconfigurable intelligent
surface (STAR-RIS) assisted simultaneous wireless information and power
transfer (SWIPT) system is proposed. More particularly, an STAR-RIS is deployed
to assist in the information/power transfer from a multi-antenna access point
(AP) to multiple single-antenna information users (IUs) and energy users (EUs),
where two practical STAR-RIS operating protocols, namely energy splitting (ES)
and time switching (TS), are employed. Under the imperfect channel state
information (CSI) condition, a multi-objective optimization problem (MOOP)
framework, that simultaneously maximizes the minimum data rate and minimum
harvested power, is employed to investigate the fundamental rate-energy
trade-off between IUs and EUs. To obtain the optimal robust resource allocation
strategy, the MOOP is first transformed into a single-objective optimization
problem (SOOP) via the {\epsilon}-constraint method, which is then reformulated
by approximating semi-infinite inequality constraints with the S-procedure. For
ES, an alternating optimization (AO)-based algorithm is proposed to jointly
design AP active beamforming and STAR-RIS passive beamforming, where a penalty
method is leveraged in STAR-RIS beamforming design. Furthermore, the developed
algorithm is extended to optimize the time allocation policy and beamforming
vectors in a two-layer iterative manner for TS. Numerical results reveal that:
1) deploying STAR-RISs achieves a significant performance gain over
conventional RISs, especially in terms of harvested power for EUs; 2) the ES
protocol obtains a better user fairness performance when focusing only on IUs
or EUs, while the TS protocol yields a better balance between IUs and EUs; 3)
the imperfect CSI affects IUs more significantly than EUs, whereas TS can
confer a more robust design to attenuate these effects.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uplink soft handover for LEO constellations: how strong the
  inter-satellite link should be 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15131v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15131v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Houcem Ben Salem, Alberto Tarable, Alessandro Nordio, Behrooz Makki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider a constellation of low-earth-orbit (LEO) satellites connected to
a handheld device on the ground. Due to the very large orbital speed, an
effective handover strategy becomes of paramount importance. In particular, we
study the benefits of soft handover in the uplink from the physical-layer point
of view. We give a realistic model for both the ground-to-satellite and the
inter-satellite links, following the 3GPP channel model for the former. We
suppose that, during handover from a serving satellite to a target satellite,
one of the two satellites forwards the received signal from the ground user to
the other, thus acting as a relay. We quantify through simulations the loss of
hard handover, compared to soft handover. For the latter, we test both
amplify-and-forward (AF) and decode-and-forward (DF) relaying techniques and
verify that, at least in the simulated conditions, DF does not repay, in terms
of block error rate (BLER), the increase of complexity with respect to AF.
Also, we study the effect of the LEO constellation size on the network BLER.
Finally, we show that, with soft handover, the impact of misalignment on the
inter-satellite link is severe, especially at optical frequencies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Coexisting Passive RIS and Active Relay Assisted NOMA Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15130v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15130v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ao Huang, Li Guo, Xidong Mu, Chao Dong, Yuanwei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A novel coexisting passive reconfigurable intelligent surface (RIS) and
active decode-and-forward (DF) relay assisted non-orthogonal multiple access
(NOMA) transmission framework is proposed. In particular, two communication
protocols are conceived, namely Hybrid NOMA (H-NOMA) and Full NOMA (F-NOMA).
Based on the proposed two protocols, both the sum rate maximization and max-min
rate fairness problems are formulated for jointly optimizing the power
allocation at the access point and relay as well as the passive beamforming
design at the RIS. To tackle the non-convex problems, an alternating
optimization (AO) based algorithm is first developed, where the transmit power
and the RIS phase-shift are alternatingly optimized by leveraging the
two-dimensional search and rank-relaxed difference-of-convex (DC) programming,
respectively. Then, a two-layer penalty based joint optimization (JO) algorithm
is developed to jointly optimize the resource allocation coefficients within
each iteration. Finally, numerical results demonstrate that: i) the proposed
coexisting RIS and relay assisted transmission framework is capable of
achieving a significant user performance improvement than conventional schemes
without RIS or relay; ii) compared with the AO algorithm, the JO algorithm
requires less execution time at the cost of a slight performance loss; and iii)
the H-NOMA and F-NOMA protocols are generally preferable for ensuring user rate
fairness and enhancing user sum rate, respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ STAR-RIS Assisted Downlink Active and Uplink Backscatter Communications
  with NOMA 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15120v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15120v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ao Huang, Xidong Mu, Li Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A simultaneously transmitting and reflecting reconfigurable intelligent
surface (STAR-RIS) assisted downlink (DL) active and uplink (UL) backscatter
communication (BackCom) framework is proposed. More particularly, a full-duplex
(FD) base station (BS) communicates with the DL users via the STAR-RIS's
transmission link, while exciting and receiving the information from the UL
BackCom devices with the aid of the STAR-RIS's reflection link. Non-orthogonal
multiple access (NOMA) is exploited in both DL and UL communications for
improving the spectrum efficiency. The system weighted sum rate maximization
problem is formulated for jointly optimizing the FD BS active receive and
transmit beamforming, the STAR- RIS passive beamforming, and the DL NOMA
decoding orders, subject to the DL user's individual rate constraint. To tackle
this challenging non-convex problem, we propose an alternating optimization
(AO) based algorithm for the joint active and passive beamforming design with a
given DL NOMA decoding order. To address the potential high computational
complexity required for exhaustive searching all the NOMA decoding orders, an
efficient NOMA user ordering scheme is further developed. Finally, numerical
results demonstrate that: i) compared with the baseline schemes employing
conventional RISs or space division multiple access, the proposed scheme
achieves higher performance gains; and ii) higher UL rate gain is obtained at a
cost of DL performance degradation, as a remedy, a more flexible performance
tradeoff can be achieved by introducing the STAR-RIS.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Range-Angle Estimation for FDA-MIMO System With Frequency Offset 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14978v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14978v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengjiang Sun, Peng Chen, Zhenxin Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Frequency diverse array multiple-input multiple-output (FDA-MIMO) radar
differs from the traditional phased array (PA) radar, and can form
range-angle-dependent beampattern and differentiate between closely spaced
targets sharing the same angle but occupying distinct range cells. In the
FDA-MIMO radar, target range estimation is achieved by employing a subtle
frequency variation between adjacent array antennas, so the estimation
performance is degraded severely in a practical scenario with frequency offset.
In this paper, the range-angle estimation problem for FDA-MIMO radar is
considered with frequency offsets in both transmitting and receiving arrays.
First, we build a system model for the FDA-MIMO radar with transmitting and
receiving frequency offsets. Then, the frequency offset is transferred into an
equalized additional noise. The noise characteristics are analyzed in detail
theoretically, together with the influence on the range-angle estimation.
Moreover, since the effect of the transmitting frequency offset is similar to
additional colored noise, denoising algorithms are introduced to mitigate the
performance deterioration caused by the frequency offset. Finally,
Cram\'{e}r-Rao lower bounds (CRLB) for the range-angle estimation are derived
in the scenario with the frequency offsets. Simulation results show the
analysis of frequency offset and the corresponding estimation performance using
different algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Primary Rate Maximization in Movable Antennas Empowered Symbiotic Radio
  Communications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14943v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14943v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bin Lyu, Hao Liu, Wenqing Hong, Shimin Gong, Feng Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a movable antenna (MA) empowered scheme for
symbiotic radio (SR) communication systems. Specifically, multiple antennas at
the primary transmitter (PT) can be flexibly moved to favorable locations to
boost the channel conditions of the primary and secondary transmissions. The
primary transmission is achieved by the active transmission from the PT to the
primary user (PU), while the backscatter device (BD) takes a ride over the
incident signal from the PT to passively send the secondary signal to the PU.
Under this setup, we consider a primary rate maximization problem by jointly
optimizing the transmit beamforming and the positions of MAs at the PT under a
practical bit error rate constraint on the secondary transmission. Then, an
alternating optimization framework with the utilization of the successive
convex approximation, semi-definite processing and simulated annealing (SA)
modified particle swarm optimization (SA-PSO) methods is proposed to find the
solution of the transmit beamforming and MAs' positions. Finally, numerical
results are provided to demonstrate the performance improvement provided by the
proposed MA empowered scheme and the proposed algorithm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in IEEE VTC-Spring 2024. 6 Pages,5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Coverage and Rate Analysis for Integrated Sensing and Communication
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08343v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08343v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xu Gan, Chongwen Huang, Zhaohui Yang, Xiaoming Chen, Jiguang He, Zhaoyang Zhang, Chau Yuen, Yong Liang Guan, Mérouane Debbah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Integrated sensing and communication (ISAC) is increasingly recognized as a
pivotal technology for next-generation cellular networks, offering mutual
benefits in both sensing and communication capabilities. This advancement
necessitates a re-examination of the fundamental limits within networks where
these two functions coexist via shared spectrum and infrastructures. However,
traditional stochastic geometry-based performance analyses are confined to
either communication or sensing networks separately. This paper bridges this
gap by introducing a generalized stochastic geometry framework in ISAC
networks. Based on this framework, we define and calculate the coverage and
ergodic rate of sensing and communication performance under resource
constraints. Then, we shed light on the fundamental limits of ISAC networks by
presenting theoretical results for the coverage rate of the unified
performance, taking into account the coupling effects of dual functions in
coexistence networks. Further, we obtain the analytical formulations for
evaluating the ergodic sensing rate constrained by the maximum communication
rate, and the ergodic communication rate constrained by the maximum sensing
rate. Extensive numerical results validate the accuracy of all theoretical
derivations, and also indicate that denser networks significantly enhance ISAC
coverage. Specifically, increasing the base station density from $1$
$\text{km}^{-2}$ to $10$ $\text{km}^{-2}$ can boost the ISAC coverage rate from
$1.4\%$ to $39.8\%$. Further, results also reveal that with the increase of the
constrained sensing rate, the ergodic communication rate improves
significantly, but the reverse is not obvious.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-21T00:00:00Z">2024-03-21</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Distributed, Parallel, and Cluster Computing
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ History-Independent Concurrent Objects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14445v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14445v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hagit Attiya, Michael A. Bender, Martin Farach-Colton, Rotem Oshman, Noa Schiller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A data structure is called history independent if its internal memory
representation does not reveal the history of operations applied to it, only
its current state. In this paper we study history independence for concurrent
data structures, and establish foundational possibility and impossibility
results. We show that a large class of concurrent objects cannot be implemented
from smaller base objects in a manner that is both wait-free and history
independent; but if we settle for either lock-freedom instead of wait-freedom
or for a weak notion of history independence, then at least one object in the
class, multi-valued single-reader single-writer registers, can be implemented
from smaller base objects, binary registers.
  On the other hand, using large base objects, we give a strong possibility
result in the form of a universal construction: an object with $s$ possible
states can be implemented in a wait-free, history-independent manner from
compare-and-swap base objects that each have $O(s + 2^n)$ possible memory
states, where $n$ is the number of processes in the system.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Accelerating Time-to-Science by Streaming Detector Data Directly into
  Perlmutter Compute Nodes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14352v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14352v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samuel S. Welborn, Bjoern Enders, Chris Harris, Peter Ercius, Deborah J. Bard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in detector technology have significantly increased the
size and complexity of experimental data, and high-performance computing (HPC)
provides a path towards more efficient and timely data processing. However,
movement of large data sets from acquisition systems to HPC centers introduces
bottlenecks owing to storage I/O at both ends. This manuscript introduces a
streaming workflow designed for an high data rate electron detector that
streams data directly to compute node memory at the National Energy Research
Scientific Computing Center (NERSC), thereby avoiding storage I/O. The new
workflow deploys ZeroMQ-based services for data production, aggregation, and
distribution for on-the-fly processing, all coordinated through a distributed
key-value store. The system is integrated with the detector's science gateway
and utilizes the NERSC Superfacility API to initiate streaming jobs through a
web-based frontend. Our approach achieves up to a 14-fold increase in data
throughput and enhances predictability and reliability compared to a I/O-heavy
file-based transfer workflow. Our work highlights the transformative potential
of streaming workflows to expedite data analysis for time-sensitive
experiments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adversary-Augmented Simulation to evaluate client-fairness on
  HyperLedger Fabric 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14342v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14342v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Erwan Mahe, Rouwaida Abdallah, Sara Tucci-Piergiovanni, Pierre-Yves Piriou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a novel adversary model specifically tailored to
distributed systems, with the aim to asses the security of blockchain
technologies. Building upon literature on adversarial assumptions and
capabilities, we include classical notions of failure and communication models
to classify and bind the use of adversarial actions. We focus on the effect of
these actions on properties of distributed protocols. A significant effort of
our research is the integration of this model into the Multi-Agent eXperimenter
(MAX) framework. This integration enables realistic simulations of adversarial
attacks on blockchain systems. In particular, we have simulated attacks
violating a form of client-fairness on HyperLedger Fabric.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages (2 pages of references), 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Power of Quantum Distributed Proofs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14108v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14108v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Atsuya Hasegawa, Srijita Kundu, Harumichi Nishimura
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantum nondeterministic distributed computing was recently introduced as
dQMA (distributed quantum Merlin-Arthur) protocols by Fraigniaud, Le Gall,
Nishimura and Paz (ITCS 2021). In dQMA protocols, with the help of quantum
proofs and local communication, nodes on a network verify a global property of
the network. Fraigniaud et al. showed that, when the network size is small,
there exists an exponential separation in proof size between distributed
classical and quantum verification protocols, for the equality problem, where
the verifiers check if all the data owned by a subset of them are identical. In
this paper, we further investigate and characterize the power of the dQMA
protocols for various decision problems.
  First, we give a more efficient dQMA protocol for the equality problem with a
simpler analysis. This is done by adding a symmetrization step on each node and
exploiting properties of the permutation test, which is a generalization of the
SWAP test. We also show a quantum advantage for the equality problem on path
networks still persists even when the network size is large, by considering
``relay points'' between extreme nodes.
  Second, we show that even in a general network, there exist efficient dQMA
protocols for the ranking verification problem, the Hamming distance problem,
and more problems that derive from efficient quantum one-way communication
protocols. Third, in a line network, we construct an efficient dQMA protocol
for a problem that has an efficient two-party QMA communication protocol.
  Finally, we obtain the first lower bounds on the proof and communication cost
of dQMA protocols. To prove a lower bound on the equality problem, we show any
dQMA protocol with an entangled proof between nodes can be simulated with a
dQMA protocol with a separable proof between nodes by using a QMA
communication-complete problem introduced by Raz and Shpilka (CCC 2004).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>49 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Parcae: Proactive, Liveput-Optimized DNN Training on Preemptible
  Instances 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14097v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14097v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiangfei Duan, Ziang Song, Xupeng Miao, Xiaoli Xi, Dahua Lin, Harry Xu, Minjia Zhang, Zhihao Jia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks (DNNs) are becoming progressively large and costly to
train. This paper aims to reduce DNN training costs by leveraging preemptible
instances on modern clouds, which can be allocated at a much lower price when
idle but may be preempted by the cloud provider at any time. Prior work that
supports DNN training on preemptive instances employs a reactive approach to
handling instance preemptions and allocations after their occurrence, which
only achieves limited performance and scalability.
  We present Parcae, a system that enables cheap, fast, and scalable DNN
training on preemptible instances by proactively adjusting the parallelization
strategy of a DNN training job to adapt to predicted resource changes before
instance preemptions and allocations really happen, which significantly reduces
the cost of handling these events. Parcae optimizes liveput, a novel metric
that measures the expected training throughput of a DNN job under various
possible preemption scenarios. Compared to existing reactive,
throughput-optimized systems, Parcae's proactive, live-optimized solution
considers both the throughput of a job and its robustness under preemptions. To
optimize liveput, Parcae supports lightweight instance migration and uses an
availability predictor to forecast future preemptions. It then uses a liveput
optimizer to discover an optimal strategy to parallelize DNN training under
predicted preemptions. We evaluate Parcae on a variety of DNNs and preemption
traces and show that Parcae outperforms existing spot-instance DNN training
systems by up to 10$\times$. More importantly, Parcae achieves near-optimal
performance for training large DNNs under frequent preemptions, in which case
existing approaches cannot make any progress.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NSDI '24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Accelerating ViT Inference on FPGA through Static and Dynamic Pruning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14047v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14047v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dhruv Parikh, Shouyi Li, Bingyi Zhang, Rajgopal Kannan, Carl Busart, Viktor Prasanna
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision Transformers (ViTs) have achieved state-of-the-art accuracy on various
computer vision tasks. However, their high computational complexity prevents
them from being applied to many real-world applications. Weight and token
pruning are two well-known methods for reducing complexity: weight pruning
reduces the model size and associated computational demands, while token
pruning further dynamically reduces the computation based on the input.
Combining these two techniques should significantly reduce computation
complexity and model size; however, naively integrating them results in
irregular computation patterns, leading to significant accuracy drops and
difficulties in hardware acceleration.
  Addressing the above challenges, we propose a comprehensive
algorithm-hardware codesign for accelerating ViT on FPGA through simultaneous
pruning -combining static weight pruning and dynamic token pruning. For
algorithm design, we systematically combine a hardware-aware structured
block-pruning method for pruning model parameters and a dynamic token pruning
method for removing unimportant token vectors. Moreover, we design a novel
training algorithm to recover the model's accuracy. For hardware design, we
develop a novel hardware accelerator for executing the pruned model. The
proposed hardware design employs multi-level parallelism with load balancing
strategy to efficiently deal with the irregular computation pattern led by the
two pruning approaches. Moreover, we develop an efficient hardware mechanism
for efficiently executing the on-the-fly token pruning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>FCCM 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Loop Improvement: An Efficient Approach for Extracting Shared Features
  from Heterogeneous Data without Central Server 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14371v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14371v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fei Li, Chu Kiong Loo, Wei Shiung Liew, Xiaofeng Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In federated learning, data heterogeneity significantly impacts performance.
A typical solution involves segregating these parameters into shared and
personalized components, a concept also relevant in multi-task learning.
Addressing this, we propose "Loop Improvement" (LI), a novel method enhancing
this separation and feature extraction without necessitating a central server
or data interchange among participants. Our experiments reveal LI's superiority
in several aspects: In personalized federated learning environments, LI
consistently outperforms the advanced FedALA algorithm in accuracy across
diverse scenarios. Additionally, LI's feature extractor closely matches the
performance achieved when aggregating data from all clients. In global model
contexts, employing LI with stacked personalized layers and an additional
network also yields comparable results to combined client data scenarios.
Furthermore, LI's adaptability extends to multi-task learning, streamlining the
extraction of common features across tasks and obviating the need for
simultaneous training. This approach not only enhances individual task
performance but also achieves accuracy levels on par with classic multi-task
learning methods where all tasks are trained simultaneously. LI integrates a
loop topology with layer-wise and end-to-end training, compatible with various
neural network models. This paper also delves into the theoretical
underpinnings of LI's effectiveness, offering insights into its potential
applications. The code is on https://github.com/axedge1983/LI
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AI and Memory Wall 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14123v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14123v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amir Gholami, Zhewei Yao, Sehoon Kim, Coleman Hooper, Michael W. Mahoney, Kurt Keutzer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The availability of unprecedented unsupervised training data, along with
neural scaling laws, has resulted in an unprecedented surge in model size and
compute requirements for serving/training LLMs. However, the main performance
bottleneck is increasingly shifting to memory bandwidth. Over the past 20
years, peak server hardware FLOPS has been scaling at 3.0x/2yrs, outpacing the
growth of DRAM and interconnect bandwidth, which have only scaled at 1.6 and
1.4 times every 2 years, respectively. This disparity has made memory, rather
than compute, the primary bottleneck in AI applications, particularly in
serving. Here, we analyze encoder and decoder Transformer models and show how
memory bandwidth can become the dominant bottleneck for decoder models. We
argue for a redesign in model architecture, training, and deployment strategies
to overcome this memory limitation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in IEEE Micro Journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Blockchain e Sistemas Distribuídos: conceitos básicos e
  implicações 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14854v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14854v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        M. Witter, A. Rodrigo De Vit
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Blockchain technology has emerged as a necessity for the decentralization of
payment methods and transactions, but it has brought with it many properties of
distributed systems that have made it a crucial technology for overcoming some
of society's challenges, especially in the context of decentralizing services,
transparency of information, availability, and security. Its architecture and
communication methods, although possessing some complex nuances to understand,
particularly for the lay audience in the field of distributed systems,
protocols, and computer networks. In this article, we will explore some topics
of distributed systems related to blockchain technology.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>in Portuguese language</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ iSpLib: A Library for Accelerating Graph Neural Networks using
  Auto-tuned Sparse Operations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14853v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14853v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Saidul Hoque Anik, Pranav Badhe, Rohit Gampa, Ariful Azad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Core computations in Graph Neural Network (GNN) training and inference are
often mapped to sparse matrix operations such as sparse-dense matrix
multiplication (SpMM). These sparse operations are harder to optimize by manual
tuning because their performance depends significantly on the sparsity of input
graphs, GNN models, and computing platforms. To address this challenge, we
present iSpLib, a PyTorch-based C++ library equipped with auto-tuned sparse
operations. iSpLib expedites GNN training with a cache-enabled backpropagation
that stores intermediate matrices in local caches. The library offers a
user-friendly Python plug-in that allows users to take advantage of our
optimized PyTorch operations out-of-the-box for any existing linear
algebra-based PyTorch implementation of popular GNNs (Graph Convolution
Network, GraphSAGE, Graph Inference Network, etc.) with only two lines of
additional code. We demonstrate that iSpLib obtains up to 27x overall training
speedup compared to the equivalent PyTorch 2.1.0 and PyTorch Geometric 2.4.0
implementations on the CPU. Our library is publicly available at
https://github.com/HipGraph/iSpLib (https://doi.org/10.5281/zenodo.10806511).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CASPER: Carbon-Aware Scheduling and Provisioning for Distributed Web
  Services 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14792v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14792v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abel Souza, Shruti Jasoria, Basundhara Chakrabarty, Alexander Bridgwater, Axel Lundberg, Filip Skogh, Ahmed Ali-Eldin, David Irwin, Prashant Shenoy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There has been a significant societal push towards sustainable practices,
including in computing. Modern interactive workloads such as geo-distributed
web-services exhibit various spatiotemporal and performance flexibility,
enabling the possibility to adapt the location, time, and intensity of
processing to align with the availability of renewable and low-carbon energy.
An example is a web application hosted across multiple cloud regions, each with
varying carbon intensity based on their local electricity mix. Distributed
load-balancing enables the exploitation of low-carbon energy through load
migration across regions, reducing web applications carbon footprint. In this
paper, we present CASPER, a carbon-aware scheduling and provisioning system
that primarily minimizes the carbon footprint of distributed web services while
also respecting their Service Level Objectives (SLO). We formulate CASPER as an
multi-objective optimization problem that considers both the variable carbon
intensity and latency constraints of the network. Our evaluation reveals the
significant potential of CASPER in achieving substantial reductions in carbon
emissions. Compared to baseline methods, CASPER demonstrates improvements of up
to 70% with no latency performance degradation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FedMef: Towards Memory-efficient Federated Dynamic Pruning <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14737v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14737v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hong Huang, Weiming Zhuang, Chen Chen, Lingjuan Lyu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning (FL) promotes decentralized training while prioritizing
data confidentiality. However, its application on resource-constrained devices
is challenging due to the high demand for computation and memory resources to
train deep learning models. Neural network pruning techniques, such as dynamic
pruning, could enhance model efficiency, but directly adopting them in FL still
poses substantial challenges, including post-pruning performance degradation,
high activation memory usage, etc. To address these challenges, we propose
FedMef, a novel and memory-efficient federated dynamic pruning framework.
FedMef comprises two key components. First, we introduce the budget-aware
extrusion that maintains pruning efficiency while preserving post-pruning
performance by salvaging crucial information from parameters marked for pruning
within a given budget. Second, we propose scaled activation pruning to
effectively reduce activation memory footprints, which is particularly
beneficial for deploying FL to memory-limited devices. Extensive experiments
demonstrate the effectiveness of our proposed FedMef. In particular, it
achieves a significant reduction of 28.5% in memory footprint compared to
state-of-the-art methods while obtaining superior accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Task Graph offloading via Deep Reinforcement Learning in Mobile Edge
  Computing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.10569v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.10569v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiagang Liu, Yun Mi, Xinyu Zhang, Xiaocui Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Various mobile applications that comprise dependent tasks are gaining
widespread popularity and are increasingly complex. These applications often
have low-latency requirements, resulting in a significant surge in demand for
computing resources. With the emergence of mobile edge computing (MEC), it
becomes the most significant issue to offload the application tasks onto
small-scale devices deployed at the edge of the mobile network for obtaining a
high-quality user experience. However, since the environment of MEC is dynamic,
most existing works focusing on task graph offloading, which rely heavily on
expert knowledge or accurate analytical models, fail to fully adapt to such
environmental changes, resulting in the reduction of user experience. This
paper investigates the task graph offloading in MEC, considering the
time-varying computation capabilities of edge computing devices. To adapt to
environmental changes, we model the task graph scheduling for computation
offloading as a Markov Decision Process (MDP). Then, we design a deep
reinforcement learning algorithm (SATA-DRL) to learn the task scheduling
strategy from the interaction with the environment, to improve user experience.
Extensive simulations validate that SATA-DRL is superior to existing strategies
in terms of reducing average makespan and deadline violation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A new open source framework for multiscale modeling of fibrous materials
  on heterogeneous supercomputers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.09427v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.09427v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jacob Merson, Catalin Picu, Mark S. Shephard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This article presents MuMFiM, an open source application for multiscale
modeling of fibrous materials on massively parallel computers. MuMFiM uses two
scales to represent fibrous materials such as biological network materials
(extracellular matrix, connective tissue, etc.). It is designed to make use of
multiple levels of parallelism, including distributed parallelism of the macro
and microscales as well as GPU accelerated data-parallelism of the microscale.
Scaling results of the GPU accelerated microscale show that solving microscale
problems concurrently on the GPU can lead to a 1000x speedup over the solution
of a single RVE on the GPU. In addition, we show nearly optimal strong and weak
scaling results of MuMFiM on up to 128 nodes of AiMOS (Rensselaer Polytechnic
Institute) which is composed of IBM AC922 nodes with 6 Volta V100 GPU and 2 20
core Power 9 CPUs each. We also show how MuMFiM can be used to solve problems
of interest to the broader engineering community, in particular providing an
example of the facet capsule ligament (FCL) of the human spine undergoing
uniaxial extension.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Artificial Intelligence
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MathVerse: Does Your Multi-modal LLM Truly See the Diagrams in Visual
  Math Problems? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14624v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14624v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Peng Gao, Hongsheng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The remarkable progress of Multi-modal Large Language Models (MLLMs) has
garnered unparalleled attention, due to their superior performance in visual
contexts. However, their capabilities in visual math problem-solving remain
insufficiently evaluated and understood. We investigate current benchmarks to
incorporate excessive visual content within textual questions, which
potentially assist MLLMs in deducing answers without truly interpreting the
input diagrams. To this end, we introduce MathVerse, an all-around visual math
benchmark designed for an equitable and in-depth evaluation of MLLMs. We
meticulously collect 2,612 high-quality, multi-subject math problems with
diagrams from publicly available sources. Each problem is then transformed by
human annotators into six distinct versions, each offering varying degrees of
information content in multi-modality, contributing to 15K test samples in
total. This approach allows MathVerse to comprehensively assess whether and how
much MLLMs can truly understand the visual diagrams for mathematical reasoning.
In addition, we propose a Chain-of-Thought (CoT) evaluation strategy for a
fine-grained assessment of the output answers. Rather than naively judging True
or False, we employ GPT-4(V) to adaptively extract crucial reasoning steps, and
then score each step with detailed error analysis, which can reveal the
intermediate CoT reasoning quality by MLLMs. We hope the MathVerse benchmark
may provide unique insights to guide the future development of MLLMs. Project
page: https://mathverse-cuhk.github.io
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>46 Pages, Work in Progress, Benchmark Project Page:
  https://mathverse-cuhk.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Videoshop: Localized Semantic Video Editing with Noise-Extrapolated
  Diffusion Inversion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14617v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14617v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Fan, Anand Bhattad, Ranjay Krishna
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Videoshop, a training-free video editing algorithm for localized
semantic edits. Videoshop allows users to use any editing software, including
Photoshop and generative inpainting, to modify the first frame; it
automatically propagates those changes, with semantic, spatial, and temporally
consistent motion, to the remaining frames. Unlike existing methods that enable
edits only through imprecise textual instructions, Videoshop allows users to
add or remove objects, semantically change objects, insert stock photos into
videos, etc. with fine-grained control over locations and appearance. We
achieve this through image-based video editing by inverting latents with noise
extrapolation, from which we generate videos conditioned on the edited image.
Videoshop produces higher quality edits against 6 baselines on 2 editing
benchmarks using 10 evaluation metrics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Envisioning the Next-Generation AI Coding Assistants: Insights &
  Proposals 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14592v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14592v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Khanh Nghiem, Anh Minh Nguyen, Nghi D. Q. Bui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As a research-product hybrid group in AI for Software Engineering (AI4SE), we
present four key takeaways from our experience developing in-IDE AI coding
assistants. AI coding assistants should set clear expectations for usage,
integrate with advanced IDE capabilities and existing extensions, use
extendable backend designs, and collect app data responsibly for downstream
analyses. We propose open questions and challenges that academia and industry
should address to realize the vision of next-generation AI coding assistants.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ReAct Meets ActRe: Autonomous Annotations of Agent Trajectories for
  Contrastive Self-Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14589v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14589v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zonghan Yang, Peng Li, Ming Yan, Ji Zhang, Fei Huang, Yang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language agents have demonstrated autonomous decision-making abilities by
reasoning with foundation models. Recently, efforts have been made to train
language agents for performance improvement, with multi-step reasoning and
action trajectories as the training data. However, collecting such trajectories
still requires considerable human effort, by either artificial annotations or
implementations of diverse prompting frameworks. In this work, we propose
A$^3$T, a framework that enables the Autonomous Annotation of Agent
Trajectories in the style of ReAct. The central role is an ActRe prompting
agent, which explains the reason for an arbitrary action. When randomly
sampling an external action, the ReAct-style agent could query the ActRe agent
with the action to obtain its textual rationales. Novel trajectories are then
synthesized by prepending the posterior reasoning from ActRe to the sampled
action. In this way, the ReAct-style agent executes multiple trajectories for
the failed tasks, and selects the successful ones to supplement its failed
trajectory for contrastive self-training. Realized by policy gradient methods
with binarized rewards, the contrastive self-training with accumulated
trajectories facilitates a closed loop for multiple rounds of language agent
self-improvement. We conduct experiments using QLoRA fine-tuning with the
open-sourced Mistral-7B-Instruct-v0.2. In AlfWorld, the agent trained with
A$^3$T obtains a 1-shot success rate of 96%, and 100% success with 4 iterative
rounds. In WebShop, the 1-shot performance of the A$^3$T agent matches human
average, and 4 rounds of iterative refinement lead to the performance
approaching human experts. A$^3$T agents significantly outperform existing
techniques, including prompting with GPT-4, advanced agent frameworks, and
fully fine-tuned LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Language Models for Multi-Choice Question Classification of
  Medical Subjects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14582v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14582v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Víctor Ponce-López
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The aim of this paper is to evaluate whether large language models trained on
multi-choice question data can be used to discriminate between medical
subjects. This is an important and challenging task for automatic question
answering. To achieve this goal, we train deep neural networks for multi-class
classification of questions into the inferred medical subjects. Using our
Multi-Question (MQ) Sequence-BERT method, we outperform the state-of-the-art
results on the MedMCQA dataset with an accuracy of 0.68 and 0.60 on their
development and test sets, respectively. In this sense, we show the capability
of AI and LLMs in particular for multi-classification tasks in the Healthcare
domain.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">survey</span> on Concept-based Approaches For Model Improvement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14566v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14566v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Avani Gupta, P J Narayanan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The focus of recent research has shifted from merely increasing the Deep
Neural Networks (DNNs) performance in various tasks to DNNs, which are more
interpretable to humans. The field of eXplainable Artificial Intelligence (XAI)
has observed various techniques, including saliency-based and concept-based
approaches. Concept-based approaches explain the model's decisions in simple
human understandable terms called Concepts. Concepts are human interpretable
units of data and are the thinking ground of humans. Explanations in terms of
concepts enable detecting spurious correlations, inherent biases, or
clever-hans. With the advent of concept-based explanations, there have been
various concept representation methods and automatic concept discovery
algorithms. Some recent methods use concepts for post-hoc model disentanglement
evaluation, while others use them for ante-hoc training. The concept-based
approaches are new, with many representations coming up, and there is very
limited work on Concept-based Model improvement. We provide a systematic review
and taxonomy of various concept representations and their discovery algorithms
in DNNs, specifically in vision. We also provide details on concept-based model
improvement literature, which is the first to survey concept-based model
improvement methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Era of Semantic Decoding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14562v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14562v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maxime Peyrard, Martin Josifoski, Robert West
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work demonstrated great promise in the idea of orchestrating
collaborations between LLMs, human input, and various tools to address the
inherent limitations of LLMs. We propose a novel perspective called semantic
decoding, which frames these collaborative processes as optimization procedures
in semantic space. Specifically, we conceptualize LLMs as semantic processors
that manipulate meaningful pieces of information that we call semantic tokens
(known thoughts). LLMs are among a large pool of other semantic processors,
including humans and tools, such as search engines or code executors.
Collectively, semantic processors engage in dynamic exchanges of semantic
tokens to progressively construct high-utility outputs. We refer to these
orchestrated interactions among semantic processors, optimizing and searching
in semantic space, as semantic decoding algorithms. This concept draws a direct
parallel to the well-studied problem of syntactic decoding, which involves
crafting algorithms to best exploit auto-regressive language models for
extracting high-utility sequences of syntactic tokens. By focusing on the
semantic level and disregarding syntactic details, we gain a fresh perspective
on the engineering of AI systems, enabling us to imagine systems with much
greater complexity and capabilities. In this position paper, we formalize the
transition from syntactic to semantic tokens as well as the analogy between
syntactic and semantic decoding. Subsequently, we explore the possibilities of
optimizing within the space of semantic tokens via semantic decoding
algorithms. We conclude with a list of research opportunities and questions
arising from this fresh perspective. The semantic decoding perspective offers a
powerful abstraction for search and optimization directly in the space of
meaningful concepts, with semantic tokens as the fundamental units of a new
type of computation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lexicon-Level Contrastive Visual-Grounding Improves Language Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14551v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14551v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengxu Zhuang, Evelina Fedorenko, Jacob Andreas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Today's most accurate language models are trained on orders of magnitude more
language data than human language learners receive - but with no supervision
from other sensory modalities that play a crucial role in human learning. Can
we make LMs' representations and predictions more accurate (and more
human-like) with more ecologically plausible supervision? This paper describes
LexiContrastive Grounding (LCG), a grounded language learning procedure that
leverages visual supervision to improve textual representations.
LexiContrastive Grounding combines a next token prediction strategy with a
contrastive visual grounding objective, focusing on early-layer representations
that encode lexical information. Across multiple word-learning and
sentence-understanding benchmarks, LexiContrastive Grounding not only
outperforms standard language-only models in learning efficiency, but also
improves upon vision-and-language learning procedures including CLIP, GIT,
Flamingo, and Vokenization. Moreover, LexiContrastive Grounding improves
perplexity by around 5% on multiple language modeling tasks. This work
underscores the potential of incorporating visual grounding into language
models, aligning more closely with the multimodal nature of human language
acquisition.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dynamic Explanation Emphasis in Human-XAI Interaction with Communication
  Robot 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14550v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14550v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yosuke Fukuchi, Seiji Yamada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Communication robots have the potential to contribute to effective human-XAI
interaction as an interface that goes beyond textual or graphical explanations.
One of their strengths is that they can use physical and vocal expressions to
add detailed nuances to explanations. However, it is not clear how a robot can
apply such expressions, or in particular, how we can develop a strategy to
adaptively use such expressions depending on the task and user in dynamic
interactions. To address this question, this paper proposes DynEmph, a method
for a communication robot to decide where to emphasize XAI-generated
explanations with physical expressions. It predicts the effect of emphasizing
certain points on a user and aims to minimize the expected difference between
predicted user decisions and AI-suggested ones. DynEmph features a strategy for
deciding where to emphasize in a data-driven manner, relieving engineers from
the need to manually design a strategy. We further conducted experiments to
investigate how emphasis selection strategies affect the performance of user
decisions. The results suggest that, while a naive strategy (emphasizing
explanations for an AI's most probable class) does not necessarily work better,
DynEmph effectively guides users to better decisions under the condition that
the performance of the AI suggestion is high.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Object-Centric Domain Randomization for 3D Shape Reconstruction in the
  Wild 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14539v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14539v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junhyeong Cho, Kim Youwang, Hunmin Yang, Tae-Hyun Oh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One of the biggest challenges in single-view 3D shape reconstruction in the
wild is the scarcity of <3D shape, 2D image>-paired data from real-world
environments. Inspired by remarkable achievements via domain randomization, we
propose ObjectDR which synthesizes such paired data via a random simulation of
visual variations in object appearances and backgrounds. Our data synthesis
framework exploits a conditional generative model (e.g., ControlNet) to
generate images conforming to spatial conditions such as 2.5D sketches, which
are obtainable through a rendering process of 3D shapes from object collections
(e.g., Objaverse-XL). To simulate diverse variations while preserving object
silhouettes embedded in spatial conditions, we also introduce a disentangled
framework which leverages an initial object guidance. After synthesizing a wide
range of data, we pre-train a model on them so that it learns to capture a
domain-invariant geometry prior which is consistent across various domains. We
validate its effectiveness by substantially improving 3D shape reconstruction
models on a real-world benchmark. In a scale-up evaluation, our pre-training
achieves 23.6% superior results compared with the pre-training on high-quality
computer graphics renderings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://ObjectDR.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Click to Grasp: Zero-Shot Precise Manipulation via Visual Diffusion
  Descriptors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14526v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14526v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikolaos Tsagkas, Jack Rome, Subramanian Ramamoorthy, Oisin Mac Aodha, Chris Xiaoxuan Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Precise manipulation that is generalizable across scenes and objects remains
a persistent challenge in robotics. Current approaches for this task heavily
depend on having a significant number of training instances to handle objects
with pronounced visual and/or geometric part ambiguities. Our work explores the
grounding of fine-grained part descriptors for precise manipulation in a
zero-shot setting by utilizing web-trained text-to-image diffusion-based
generative models. We tackle the problem by framing it as a dense semantic part
correspondence task. Our model returns a gripper pose for manipulating a
specific part, using as reference a user-defined click from a source image of a
visually different instance of the same object. We require no manual grasping
demonstrations as we leverage the intrinsic object geometry and features.
Practical experiments in a real-world tabletop scenario validate the efficacy
of our approach, demonstrating its potential for advancing semantic-aware
robotics manipulation. Web page: https://tsagkas.github.io/click2grasp
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Constrained Reinforcement Learning with Smoothed Log Barrier Function 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14508v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14508v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Baohe Zhang, Yuan Zhang, Lilli Frison, Thomas Brox, Joschka Bödecker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning (RL) has been widely applied to many control tasks and
substantially improved the performances compared to conventional control
methods in many domains where the reward function is well defined. However, for
many real-world problems, it is often more convenient to formulate optimization
problems in terms of rewards and constraints simultaneously. Optimizing such
constrained problems via reward shaping can be difficult as it requires tedious
manual tuning of reward functions with several interacting terms. Recent
formulations which include constraints mostly require a pre-training phase,
which often needs human expertise to collect data or assumes having a
sub-optimal policy readily available. We propose a new constrained RL method
called CSAC-LB (Constrained Soft Actor-Critic with Log Barrier Function), which
achieves competitive performance without any pre-training by applying a linear
smoothed log barrier function to an additional safety critic. It implements an
adaptive penalty for policy learning and alleviates the numerical issues that
are known to complicate the application of the log barrier function method. As
a result, we show that with CSAC-LB, we achieve state-of-the-art performance on
several constrained control tasks with different levels of difficulty and
evaluate our methods in a locomotion task on a real quadruped robot platform.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Soft Learning Probabilistic Circuits 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14504v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14504v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Soroush Ghandi, Benjamin Quost, Cassio de Campos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Probabilistic Circuits (PCs) are prominent tractable probabilistic models,
allowing for a range of exact inferences. This paper focuses on the main
algorithm for training PCs, LearnSPN, a gold standard due to its efficiency,
performance, and ease of use, in particular for tabular data. We show that
LearnSPN is a greedy likelihood maximizer under mild assumptions. While
inferences in PCs may use the entire circuit structure for processing queries,
LearnSPN applies a hard method for learning them, propagating at each sum node
a data point through one and only one of the children/edges as in a hard
clustering process. We propose a new learning procedure named SoftLearn, that
induces a PC using a soft clustering process. We investigate the effect of this
learning-inference compatibility in PCs. Our experiments show that SoftLearn
outperforms LearnSPN in many situations, yielding better likelihoods and
arguably better samples. We also analyze comparable tractable models to
highlight the differences between soft/hard learning and model querying.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How Human-Centered Explainable AI Interface Are Designed and Evaluated:
  A Systematic <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14496v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14496v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thu Nguyen, Alessandro Canossa, Jichen Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite its technological breakthroughs, eXplainable Artificial Intelligence
(XAI) research has limited success in producing the {\em effective
explanations} needed by users. In order to improve XAI systems' usability,
practical interpretability, and efficacy for real users, the emerging area of
{\em Explainable Interfaces} (EIs) focuses on the user interface and user
experience design aspects of XAI. This paper presents a systematic survey of 53
publications to identify current trends in human-XAI interaction and promising
directions for EI design and development. This is among the first systematic
survey of EI research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning to Project for Cross-Task Knowledge Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14494v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14494v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dylan Auty, Roy Miles, Benedikt Kolbeinsson, Krystian Mikolajczyk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional knowledge distillation (KD) relies on a proficient teacher
trained on the target task, which is not always available. In this setting,
cross-task distillation can be used, enabling the use of any teacher model
trained on a different task. However, many KD methods prove ineffective when
applied to this cross-task setting. To address this limitation, we propose a
simple modification: the use of an inverted projection. We show that this
drop-in replacement for a standard projector is effective by learning to
disregard any task-specific features which might degrade the student's
performance. We find that this simple modification is sufficient for extending
many KD methods to the cross-task setting, where the teacher and student tasks
can be very different. In doing so, we obtain up to a 1.9% improvement in the
cross-task setting compared to the traditional projection, at no additional
cost. Our method can obtain significant performance improvements (up to 7%)
when using even a randomly-initialised teacher on various tasks such as depth
estimation, image translation, and semantic segmentation, despite the lack of
any learned knowledge to transfer. To provide conceptual and analytical
insights into this result, we show that using an inverted projection allows the
distillation loss to be decomposed into a knowledge transfer and a spectral
regularisation component. Through this analysis we are additionally able to
propose a novel regularisation loss that allows teacher-free distillation,
enabling performance improvements of up to 8.57% on ImageNet with no additional
training costs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Physics-Based Causal Reasoning for Safe & Robust Next-Best Action
  Selection in Robot Manipulation Tasks <span class="chip">IROS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14488v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14488v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ricardo Cannizzaro, Michael Groom, Jonathan Routley, Robert Osazuwa Ness, Lars Kunze
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Safe and efficient object manipulation is a key enabler of many real-world
robot applications. However, this is challenging because robot operation must
be robust to a range of sensor and actuator uncertainties. In this paper, we
present a physics-informed causal-inference-based framework for a robot to
probabilistically reason about candidate actions in a block stacking task in a
partially observable setting. We integrate a physics-based simulation of the
rigid-body system dynamics with a causal Bayesian network (CBN) formulation to
define a causal generative probabilistic model of the robot decision-making
process. Using simulation-based Monte Carlo experiments, we demonstrate our
framework's ability to successfully: (1) predict block tower stability with
high accuracy (Pred Acc: 88.6%); and, (2) select an approximate next-best
action for the block stacking task, for execution by an integrated robot
system, achieving 94.2% task success rate. We also demonstrate our framework's
suitability for real-world robot systems by demonstrating successful task
executions with a domestic support robot, with perception and manipulation
sub-system integration. Hence, we show that by embedding physics-based causal
reasoning into robots' decision-making processes, we can make robot task
execution safer, more reliable, and more robust to various types of
uncertainty.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 9 figures, submitted to 2024 IEEE/RSJ International
  Conference on Intelligent Robots and Systems (IROS)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HyperGALE: ASD Classification via Hypergraph Gated Attention with
  Learnable Hyperedges <span class="chip">IJCNN 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14484v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14484v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mehul Arora, Chirag Shantilal Jain, Lalith Bharadwaj Baru, Kamalaker Dadi, Bapi Raju Surampudi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autism Spectrum Disorder (ASD) is a neurodevelopmental condition
characterized by varied social cognitive challenges and repetitive behavioral
patterns. Identifying reliable brain imaging-based biomarkers for ASD has been
a persistent challenge due to the spectrum's diverse symptomatology. Existing
baselines in the field have made significant strides in this direction, yet
there remains room for improvement in both performance and interpretability. We
propose \emph{HyperGALE}, which builds upon the hypergraph by incorporating
learned hyperedges and gated attention mechanisms. This approach has led to
substantial improvements in the model's ability to interpret complex brain
graph data, offering deeper insights into ASD biomarker characterization.
Evaluated on the extensive ABIDE II dataset, \emph{HyperGALE} not only improves
interpretability but also demonstrates statistically significant enhancements
in key performance metrics compared to both previous baselines and the
foundational hypergraph model. The advancement \emph{HyperGALE} brings to ASD
research highlights the potential of sophisticated graph-based techniques in
neurodevelopmental studies. The source code and implementation instructions are
available at GitHub:https://github.com/mehular0ra/HyperGALE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IJCNN 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Utilizing the LightGBM Algorithm for Operator User Credit Assessment
  Research 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14483v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14483v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaojie Li, Xinqi Dong, Danqing Ma, Bo Dang, Hengyi Zang, Yulu Gong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mobile Internet user credit assessment is an important way for communication
operators to establish decisions and formulate measures, and it is also a
guarantee for operators to obtain expected benefits. However, credit evaluation
methods have long been monopolized by financial industries such as banks and
credit. As supporters and providers of platform network technology and network
resources, communication operators are also builders and maintainers of
communication networks. Internet data improves the user's credit evaluation
strategy. This paper uses the massive data provided by communication operators
to carry out research on the operator's user credit evaluation model based on
the fusion LightGBM algorithm. First, for the massive data related to user
evaluation provided by operators, key features are extracted by data
preprocessing and feature engineering methods, and a multi-dimensional feature
set with statistical significance is constructed; then, linear regression,
decision tree, LightGBM, and other machine learning algorithms build multiple
basic models to find the best basic model; finally, integrates Averaging,
Voting, Blending, Stacking and other integrated algorithms to refine multiple
fusion models, and finally establish the most suitable fusion model for
operator user evaluation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Detoxifying Large Language Models via Knowledge Editing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14472v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14472v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengru Wang, Ningyu Zhang, Ziwen Xu, Zekun Xi, Shumin Deng, Yunzhi Yao, Qishen Zhang, Linyi Yang, Jindong Wang, Huajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates using knowledge editing techniques to detoxify Large
Language Models (LLMs). We construct a benchmark, SafeEdit, which covers nine
unsafe categories with various powerful attack prompts and equips comprehensive
metrics for systematic evaluation. We conduct experiments to compare knowledge
editing approaches with previous baselines, indicating that knowledge editing
has the potential to efficiently detoxify LLMs with limited impact on general
performance. Then, we propose a simple yet effective baseline, dubbed
Detoxifying with Intraoperative Neural Monitoring (DINM), to diminish the
toxicity of LLMs within a few tuning steps via only one instance. We further
provide an in-depth analysis of the internal mechanism for various detoxify
approaches, demonstrating that previous methods like SFT and DPO may merely
suppress the activations of toxic parameters, while DINM mitigates the toxicity
of the toxic parameters to a certain extent, making permanent adjustments. We
hope that these insights could shed light on future work of developing
detoxifying approaches and the underlying knowledge mechanisms of LLMs. Code
and benchmark are available at https://github.com/zjunlp/EasyEdit.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Ongoing work. Project website:
  https://zjunlp.github.io/project/SafeEdit Benchmark:
  https://huggingface.co/datasets/zjunlp/SafeEdit Code:
  https://github.com/zjunlp/EasyEdit</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Chat<span class="highlight-title">GPT</span> Alternative Solutions: Large Language Models <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14469v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14469v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanieh Alipour, Nick Pendar, Kohinoor Roy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent times, the grandeur of Large Language Models (LLMs) has not only
shone in the realm of natural language processing but has also cast its
brilliance across a vast array of applications. This remarkable display of LLM
capabilities has ignited a surge in research contributions within this domain,
spanning a diverse spectrum of topics. These contributions encompass
advancements in neural network architecture, context length enhancements, model
alignment, training datasets, benchmarking, efficiency improvements, and more.
Recent years have witnessed a dynamic synergy between academia and industry,
propelling the field of LLM research to new heights. A notable milestone in
this journey is the introduction of ChatGPT, a powerful AI chatbot grounded in
LLMs, which has garnered widespread societal attention. The evolving technology
of LLMs has begun to reshape the landscape of the entire AI community,
promising a revolutionary shift in the way we create and employ AI algorithms.
Given this swift-paced technical evolution, our survey embarks on a journey to
encapsulate the recent strides made in the world of LLMs. Through an
exploration of the background, key discoveries, and prevailing methodologies,
we offer an up-to-the-minute review of the literature. By examining multiple
LLM models, our paper not only presents a comprehensive overview but also
charts a course that identifies existing challenges and points toward potential
future research trajectories. This survey furnishes a well-rounded perspective
on the current state of generative AI, shedding light on opportunities for
further exploration, enhancement, and innovation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AnyV2V: A Plug-and-Play Framework For Any Video-to-Video Editing Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14468v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14468v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Max Ku, Cong Wei, Weiming Ren, Huan Yang, Wenhu Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video-to-video editing involves editing a source video along with additional
control (such as text prompts, subjects, or styles) to generate a new video
that aligns with the source video and the provided control. Traditional methods
have been constrained to certain editing types, limiting their ability to meet
the wide range of user demands. In this paper, we introduce AnyV2V, a novel
training-free framework designed to simplify video editing into two primary
steps: (1) employing an off-the-shelf image editing model (e.g.
InstructPix2Pix, InstantID, etc) to modify the first frame, (2) utilizing an
existing image-to-video generation model (e.g. I2VGen-XL) for DDIM inversion
and feature injection. In the first stage, AnyV2V can plug in any existing
image editing tools to support an extensive array of video editing tasks.
Beyond the traditional prompt-based editing methods, AnyV2V also can support
novel video editing tasks, including reference-based style transfer,
subject-driven editing, and identity manipulation, which were unattainable by
previous methods. In the second stage, AnyV2V can plug in any existing
image-to-video models to perform DDIM inversion and intermediate feature
injection to maintain the appearance and motion consistency with the source
video. On the prompt-based editing, we show that AnyV2V can outperform the
previous best approach by 35\% on prompt alignment, and 25\% on human
preference. On the three novel tasks, we show that AnyV2V also achieves a high
success rate. We believe AnyV2V will continue to thrive due to its ability to
seamlessly integrate the fast-evolving image editing methods. Such
compatibility can help AnyV2V to increase its versatility to cater to diverse
user demands.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Single-System Illusion in Software-Defined Vehicles --
  Automated, AI-Powered Workflow 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14460v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14460v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Krzysztof Lebioda, Viktor Vorobev, Nenad Petrovic, Fengjunjie Pan, Vahid Zolfaghari, Alois Knoll
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel model- and feature-based approach to development of
vehicle software systems, where the end architecture is not explicitly defined.
Instead, it emerges from an iterative process of search and optimization given
certain constraints, requirements and hardware architecture, while retaining
the property of single-system illusion, where applications run in a logically
uniform environment. One of the key points of the presented approach is the
inclusion of modern generative AI, specifically Large Language Models (LLMs),
in the loop. With the recent advances in the field, we expect that the LLMs
will be able to assist in processing of requirements, generation of formal
system models, as well as generation of software deployment specification and
test code. The resulting pipeline is automated to a large extent, with feedback
being generated at each step.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Level Explanations for Generative Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14459v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14459v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lucas Monteiro Paes, Dennis Wei, Hyo Jin Do, Hendrik Strobelt, Ronny Luss, Amit Dhurandhar, Manish Nagireddy, Karthikeyan Natesan Ramamurthy, Prasanna Sattigeri, Werner Geyer, Soumya Ghosh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Perturbation-based explanation methods such as LIME and SHAP are commonly
applied to text classification. This work focuses on their extension to
generative language models. To address the challenges of text as output and
long text inputs, we propose a general framework called MExGen that can be
instantiated with different attribution algorithms. To handle text output, we
introduce the notion of scalarizers for mapping text to real numbers and
investigate multiple possibilities. To handle long inputs, we take a
multi-level approach, proceeding from coarser levels of granularity to finer
ones, and focus on algorithms with linear scaling in model queries. We conduct
a systematic evaluation, both automated and human, of perturbation-based
attribution methods for summarization and context-grounded question answering.
The results show that our framework can provide more locally faithful
explanations of generated outputs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Language Models Can Reduce Asymmetry in Information Markets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14443v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14443v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nasim Rahaman, Martin Weiss, Manuel Wüthrich, <span class="highlight-author">Yoshua Bengio</span>, Li Erran Li, Chris Pal, Bernhard Schölkopf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work addresses the buyer's inspection paradox for information markets.
The paradox is that buyers need to access information to determine its value,
while sellers need to limit access to prevent theft. To study this, we
introduce an open-source simulated digital marketplace where intelligent
agents, powered by language models, buy and sell information on behalf of
external participants. The central mechanism enabling this marketplace is the
agents' dual capabilities: they not only have the capacity to assess the
quality of privileged information but also come equipped with the ability to
forget. This ability to induce amnesia allows vendors to grant temporary access
to proprietary information, significantly reducing the risk of unauthorized
retention while enabling agents to accurately gauge the information's relevance
to specific queries or tasks. To perform well, agents must make rational
decisions, strategically explore the marketplace through generated sub-queries,
and synthesize answers from purchased information. Concretely, our experiments
(a) uncover biases in language models leading to irrational behavior and
evaluate techniques to mitigate these biases, (b) investigate how price affects
demand in the context of informational goods, and (c) show that inspection and
higher budgets both lead to higher quality outcomes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Analysing Diffusion Segmentation for Medical Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14440v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14440v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mathias Öttl, Siyuan Mei, Frauke Wilm, Jana Steenpass, Matthias Rübner, Arndt Hartmann, Matthias Beckmann, Peter Fasching, Andreas Maier, Ramona Erber, Katharina Breininger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Denoising Diffusion Probabilistic models have become increasingly popular due
to their ability to offer probabilistic modeling and generate diverse outputs.
This versatility inspired their adaptation for image segmentation, where
multiple predictions of the model can produce segmentation results that not
only achieve high quality but also capture the uncertainty inherent in the
model. Here, powerful architectures were proposed for improving diffusion
segmentation performance. However, there is a notable lack of analysis and
discussions on the differences between diffusion segmentation and image
generation, and thorough evaluations are missing that distinguish the
improvements these architectures provide for segmentation in general from their
benefit for diffusion segmentation specifically. In this work, we critically
analyse and discuss how diffusion segmentation for medical images differs from
diffusion image generation, with a particular focus on the training behavior.
Furthermore, we conduct an assessment how proposed diffusion segmentation
architectures perform when trained directly for segmentation. Lastly, we
explore how different medical segmentation tasks influence the diffusion
segmentation behavior and the diffusion process could be adapted accordingly.
With these analyses, we aim to provide in-depth insights into the behavior of
diffusion segmentation that allow for a better design and evaluation of
diffusion segmentation methods in the future.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Biased Binary Attribute Classifiers Ignore the Majority Classes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14435v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14435v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyi Zhang, Johanna Sophie Bieri, Manuel Günther
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To visualize the regions of interest that classifiers base their decisions
on, different Class Activation Mapping (CAM) methods have been developed.
However, all of these techniques target categorical classifiers only, though
most real-world tasks are binary classification. In this paper, we extend
gradient-based CAM techniques to work with binary classifiers and visualize the
active regions for binary facial attribute classifiers. When training an
unbalanced binary classifier on an imbalanced dataset, it is well-known that
the majority class, i.e. the class with many training samples, is mostly
predicted much better than minority class with few training instances. In our
experiments on the CelebA dataset, we verify these results, when training an
unbalanced classifier to extract 40 facial attributes simultaneously. One would
expect that the biased classifier has learned to extract features mainly for
the majority classes and that the proportional energy of the activations mainly
reside in certain specific regions of the image where the attribute is located.
However, we find very little regular activation for samples of majority
classes, while the active regions for minority classes seem mostly reasonable
and overlap with our expectations. These results suggest that biased
classifiers mainly rely on bias activation for majority classes. When training
a balanced classifier on the imbalanced data by employing attribute-specific
class weights, majority and minority classes are classified similarly well and
show expected activations for almost all attributes
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the continuity and smoothness of the value function in reinforcement
  learning and optimal control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14432v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14432v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hans Harder, Sebastian Peitz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The value function plays a crucial role as a measure for the cumulative
future reward an agent receives in both reinforcement learning and optimal
control. It is therefore of interest to study how similar the values of
neighboring states are, i.e., to investigate the continuity of the value
function. We do so by providing and verifying upper bounds on the value
function's modulus of continuity. Additionally, we show that the value function
is always H\"older continuous under relatively weak assumptions on the
underlying system and that non-differentiable value functions can be made
differentiable by slightly "disturbing" the system.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Style-Extracting Diffusion Models for Semi-Supervised Histopathology
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14429v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14429v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mathias Öttl, Frauke Wilm, Jana Steenpass, Jingna Qiu, Matthias Rübner, Arndt Hartmann, Matthias Beckmann, Peter Fasching, Andreas Maier, Ramona Erber, Bernhard Kainz, Katharina Breininger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning-based image generation has seen significant advancements with
diffusion models, notably improving the quality of generated images. Despite
these developments, generating images with unseen characteristics beneficial
for downstream tasks has received limited attention. To bridge this gap, we
propose Style-Extracting Diffusion Models, featuring two conditioning
mechanisms. Specifically, we utilize 1) a style conditioning mechanism which
allows to inject style information of previously unseen images during image
generation and 2) a content conditioning which can be targeted to a downstream
task, e.g., layout for segmentation. We introduce a trainable style encoder to
extract style information from images, and an aggregation block that merges
style information from multiple style inputs. This architecture enables the
generation of images with unseen styles in a zero-shot manner, by leveraging
styles from unseen images, resulting in more diverse generations. In this work,
we use the image layout as target condition and first show the capability of
our method on a natural image dataset as a proof-of-concept. We further
demonstrate its versatility in histopathology, where we combine prior knowledge
about tissue composition and unannotated data to create diverse synthetic
images with known layouts. This allows us to generate additional synthetic data
to train a segmentation network in a semi-supervised fashion. We verify the
added value of the generated images by showing improved segmentation results
and lower performance variability between patients when synthetic images are
included during segmentation training. Our code will be made publicly available
at [LINK].
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GLC++: Source-Free Universal Domain Adaptation through Global-Local
  Clustering and Contrastive Affinity Learning <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14410v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14410v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sanqing Qu, Tianpei Zou, Florian Röhrbein, Cewu Lu, Guang Chen, Dacheng Tao, Changjun Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks often exhibit sub-optimal performance under covariate
and category shifts. Source-Free Domain Adaptation (SFDA) presents a promising
solution to this dilemma, yet most SFDA approaches are restricted to closed-set
scenarios. In this paper, we explore Source-Free Universal Domain Adaptation
(SF-UniDA) aiming to accurately classify "known" data belonging to common
categories and segregate them from target-private "unknown" data. We propose a
novel Global and Local Clustering (GLC) technique, which comprises an adaptive
one-vs-all global clustering algorithm to discern between target classes,
complemented by a local k-NN clustering strategy to mitigate negative transfer.
Despite the effectiveness, the inherent closed-set source architecture leads to
uniform treatment of "unknown" data, impeding the identification of distinct
"unknown" categories. To address this, we evolve GLC to GLC++, integrating a
contrastive affinity learning strategy. We examine the superiority of GLC and
GLC++ across multiple benchmarks and category shift scenarios. Remarkably, in
the most challenging open-partial-set scenarios, GLC and GLC++ surpass GATE by
16.7% and 18.6% in H-score on VisDA, respectively. GLC++ enhances the novel
category clustering accuracy of GLC by 4.3% in open-set scenarios on
Office-Home. Furthermore, the introduced contrastive learning strategy not only
enhances GLC but also significantly facilitates existing methodologies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This is a substantial extension of the CVPR 2023 paper "Upcycling
  Models under Domain and Category Shift"</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Locating and Mitigating Gender Bias in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14409v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14409v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuchen Cai, Ding Cao, Rongxi Guo, Yaqin Wen, Guiquan Liu, Enhong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models(LLM) are pre-trained on extensive corpora to learn
facts and human cognition which contain human preferences. However, this
process can inadvertently lead to these models acquiring biases and stereotypes
prevalent in society. Prior research has typically tackled the issue of bias
through a one-dimensional perspective, concentrating either on locating or
mitigating it. This limited perspective has created obstacles in facilitating
research on bias to synergistically complement and progressively build upon one
another. In this study, we integrate the processes of locating and mitigating
bias within a unified framework. Initially, we use causal mediation analysis to
trace the causal effects of different components' activation within a large
language model. Building on this, we propose the LSDM (Least Square Debias
Method), a knowledge-editing based method for mitigating gender bias in
occupational pronouns, and compare it against two baselines on three gender
bias datasets and seven knowledge competency test datasets. The experimental
results indicate that the primary contributors to gender bias are the bottom
MLP modules acting on the last token of occupational pronouns and the top
attention module acting on the final word in the sentence. Furthermore, LSDM
mitigates gender bias in the model more effectively than the other baselines,
while fully preserving the model's capabilities in all other aspects.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive-RAG: Learning to Adapt Retrieval-Augmented Large Language
  Models through Question Complexity <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14403v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14403v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Soyeong Jeong, Jinheon Baek, Sukmin Cho, Sung Ju Hwang, Jong C. Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Large Language Models (LLMs), which incorporate the
non-parametric knowledge from external knowledge bases into LLMs, have emerged
as a promising approach to enhancing response accuracy in several tasks, such
as Question-Answering (QA). However, even though there are various approaches
dealing with queries of different complexities, they either handle simple
queries with unnecessary computational overhead or fail to adequately address
complex multi-step queries; yet, not all user requests fall into only one of
the simple or complex categories. In this work, we propose a novel adaptive QA
framework, that can dynamically select the most suitable strategy for
(retrieval-augmented) LLMs from the simplest to the most sophisticated ones
based on the query complexity. Also, this selection process is operationalized
with a classifier, which is a smaller LM trained to predict the complexity
level of incoming queries with automatically collected labels, obtained from
actual predicted outcomes of models and inherent inductive biases in datasets.
This approach offers a balanced strategy, seamlessly adapting between the
iterative and single-step retrieval-augmented LLMs, as well as the no-retrieval
methods, in response to a range of query complexities. We validate our model on
a set of open-domain QA datasets, covering multiple query complexities, and
show that ours enhances the overall efficiency and accuracy of QA systems,
compared to relevant baselines including the adaptive retrieval approaches.
Code is available at: https://github.com/starsuzi/Adaptive-RAG.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Building Accurate Translation-Tailored LLMs with Language Aware
  Instruction Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14399v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14399v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changtong Zan, Liang Ding, Li Shen, Yibing Zhen, Weifeng Liu, Dacheng Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Translation-tailored Large language models (LLMs) exhibit remarkable
translation capabilities, even competing with supervised-trained commercial
translation systems. However, off-target translation remains an unsolved
problem, especially for low-resource languages, hindering us from developing
accurate LLMs-based translation models. To mitigate the off-target translation
problem and enhance the performance of LLMs on translation, recent works have
either designed advanced prompting strategies to highlight the functionality of
translation instructions or exploited the in-context learning ability of LLMs
by feeding few-shot demonstrations. However, these methods essentially do not
improve LLM's ability to follow translation instructions, especially the
language direction information. In this work, we design a two-stage fine-tuning
algorithm to improve the instruction-following ability (especially the
translation direction) of LLMs. Specifically, we first tune LLMs with the
maximum likelihood estimation loss on the translation dataset to elicit the
basic translation capabilities. In the second stage, we construct
instruction-conflicting samples by randomly replacing the translation
directions with a wrong one within the instruction, and then introduce an extra
unlikelihood loss to learn those samples. Experiments on IWSLT and WMT
benchmarks upon the LLaMA model spanning 16 zero-shot directions show that,
compared to the competitive baseline -- translation-finetuned LLama, our method
could effectively reduce the off-target translation ratio (averagely -53.3\%),
thus improving translation quality with average +5.7 SacreBLEU and +16.4
BLEURT. Analysis shows that our method could preserve the model's general task
performance on AlpacaEval. Code and models will be released at
\url{https://github.com/alphadl/LanguageAware_Tuning}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Editing Knowledge Representation of Language Lodel via Rephrased Prefix
  <span class="highlight-title">Prompt</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14381v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14381v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuchen Cai, Ding Cao, Rongxi Guo, Yaqin Wen, Guiquan Liu, Enhong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural language models (LMs) have been extensively trained on vast corpora to
store factual knowledge about various aspects of the world described in texts.
Current technologies typically employ knowledge editing methods or specific
prompts to modify LM outputs. However, existing knowledge editing methods are
costly and inefficient, struggling to produce appropriate text. Additionally,
prompt engineering is opaque and requires significant effort to find suitable
prompts. To address these issues, we introduce a new method called PSPEM
(Prefix Soft Prompt Editing Method), that can be used for a lifetime with just
one training. It resolves the inefficiencies and generalizability issues in
knowledge editing methods and overcomes the opacity of prompt engineering by
automatically seeking optimal soft prompts. Specifically, PSPEM utilizes a
prompt encoder and an encoding converter to refine key information in prompts
and uses prompt alignment techniques to guide model generation, ensuring text
consistency and adherence to the intended structure and content, thereby
maintaining an optimal balance between efficiency and accuracy. We have
validated the effectiveness of PSPEM through knowledge editing and attribute
inserting. On the COUNTERFACT dataset, PSPEM achieved nearly 100\% editing
accuracy and demonstrated the highest level of fluency. We further analyzed the
similarities between PSPEM and original prompts and their impact on the model's
internals. The results indicate that PSPEM can serve as an alternative to
original prompts, supporting the model in effective editing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19pages,3figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Loop Improvement: An Efficient Approach for Extracting Shared Features
  from Heterogeneous Data without Central Server 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14371v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14371v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fei Li, Chu Kiong Loo, Wei Shiung Liew, Xiaofeng Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In federated learning, data heterogeneity significantly impacts performance.
A typical solution involves segregating these parameters into shared and
personalized components, a concept also relevant in multi-task learning.
Addressing this, we propose "Loop Improvement" (LI), a novel method enhancing
this separation and feature extraction without necessitating a central server
or data interchange among participants. Our experiments reveal LI's superiority
in several aspects: In personalized federated learning environments, LI
consistently outperforms the advanced FedALA algorithm in accuracy across
diverse scenarios. Additionally, LI's feature extractor closely matches the
performance achieved when aggregating data from all clients. In global model
contexts, employing LI with stacked personalized layers and an additional
network also yields comparable results to combined client data scenarios.
Furthermore, LI's adaptability extends to multi-task learning, streamlining the
extraction of common features across tasks and obviating the need for
simultaneous training. This approach not only enhances individual task
performance but also achieves accuracy levels on par with classic multi-task
learning methods where all tasks are trained simultaneously. LI integrates a
loop topology with layer-wise and end-to-end training, compatible with various
neural network models. This paper also delves into the theoretical
underpinnings of LI's effectiveness, offering insights into its potential
applications. The code is on https://github.com/axedge1983/LI
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring the Potential of Large Language Models in Graph Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14358v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14358v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Yao, Xin Wang, Zeyang Zhang, Yijian Qin, Ziwei Zhang, Xu Chu, Yuekui Yang, Wenwu Zhu, Hong Mei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have achieved great success in many fields, and
recent works have studied exploring LLMs for graph discriminative tasks such as
node classification. However, the abilities of LLMs for graph generation remain
unexplored in the literature. Graph generation requires the LLM to generate
graphs with given properties, which has valuable real-world applications such
as drug discovery, while tends to be more challenging. In this paper, we
propose LLM4GraphGen to explore the ability of LLMs for graph generation with
systematical task designs and extensive experiments. Specifically, we propose
several tasks tailored with comprehensive experiments to address key questions
regarding LLMs' understanding of different graph structure rules, their ability
to capture structural type distributions, and their utilization of domain
knowledge for property-based graph generation. Our evaluations demonstrate that
LLMs, particularly GPT-4, exhibit preliminary abilities in graph generation
tasks, including rule-based and distribution-based generation. We also observe
that popular prompting methods, such as few-shot and chain-of-thought
prompting, do not consistently enhance performance. Besides, LLMs show
potential in generating molecules with specific properties. These findings may
serve as foundations for designing good LLMs based models for graph generation
and provide valuable insights and further research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Elements of Differentiable Programming 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14606v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14606v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mathieu Blondel, Vincent Roulet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial intelligence has recently experienced remarkable advances, fueled
by large models, vast datasets, accelerated hardware, and, last but not least,
the transformative power of differentiable programming. This new programming
paradigm enables end-to-end differentiation of complex computer programs
(including those with control flows and data structures), making gradient-based
optimization of program parameters possible. As an emerging paradigm,
differentiable programming builds upon several areas of computer science and
applied mathematics, including automatic differentiation, graphical models,
optimization and statistics. This book presents a comprehensive review of the
fundamental concepts useful for differentiable programming. We adopt two main
perspectives, that of optimization and that of probability, with clear
analogies between the two. Differentiable programming is not merely the
differentiation of programs, but also the thoughtful design of programs
intended for differentiation. By making programs differentiable, we inherently
introduce probability distributions over their execution, providing a means to
quantify the uncertainty associated with program outputs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Draft version 1</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RAmBLA: A Framework for Evaluating the Reliability of LLMs as Assistants
  in the Biomedical Domain <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14578v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14578v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        William James Bolton, Rafael Poyiadzi, Edward R. Morrell, Gabriela van Bergen Gonzalez Bueno, Lea Goetz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) increasingly support applications in a wide
range of domains, some with potential high societal impact such as biomedicine,
yet their reliability in realistic use cases is under-researched. In this work
we introduce the Reliability AssesMent for Biomedical LLM Assistants (RAmBLA)
framework and evaluate whether four state-of-the-art foundation LLMs can serve
as reliable assistants in the biomedical domain. We identify prompt robustness,
high recall, and a lack of hallucinations as necessary criteria for this use
case. We design shortform tasks and tasks requiring LLM freeform responses
mimicking real-world user interactions. We evaluate LLM performance using
semantic similarity with a ground truth response, through an evaluator LLM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at ICLR 2024 Workshop on Reliable and Responsible
  Foundation Models</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Knowledge-Enhanced Recommendation with User-Centric Subgraph Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14377v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14377v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangyi Liu, Quanming Yao, Yongqi Zhang, Lei Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommendation systems, as widely implemented nowadays on various platforms,
recommend relevant items to users based on their preferences. The classical
methods which rely on user-item interaction matrices has limitations,
especially in scenarios where there is a lack of interaction data for new
items. Knowledge graph (KG)-based recommendation systems have emerged as a
promising solution. However, most KG-based methods adopt node embeddings, which
do not provide personalized recommendations for different users and cannot
generalize well to the new items. To address these limitations, we propose
Knowledge-enhanced User-Centric subgraph Network (KUCNet), a subgraph learning
approach with graph neural network (GNN) for effective recommendation. KUCNet
constructs a U-I subgraph for each user-item pair that captures both the
historical information of user-item interactions and the side information
provided in KG. An attention-based GNN is designed to encode the U-I subgraphs
for recommendation. Considering efficiency, the pruned user-centric computation
graph is further introduced such that multiple U-I subgraphs can be
simultaneously computed and that the size can be pruned by Personalized
PageRank. Our proposed method achieves accurate, efficient, and interpretable
recommendations especially for new items. Experimental results demonstrate the
superiority of KUCNet over state-of-the-art KG-based and collaborative
filtering (CF)-based methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TD-MPC2: Scalable, Robust World Models for Continuous Control <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.16828v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.16828v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicklas Hansen, Hao Su, Xiaolong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  TD-MPC is a model-based reinforcement learning (RL) algorithm that performs
local trajectory optimization in the latent space of a learned implicit
(decoder-free) world model. In this work, we present TD-MPC2: a series of
improvements upon the TD-MPC algorithm. We demonstrate that TD-MPC2 improves
significantly over baselines across 104 online RL tasks spanning 4 diverse task
domains, achieving consistently strong results with a single set of
hyperparameters. We further show that agent capabilities increase with model
and data size, and successfully train a single 317M parameter agent to perform
80 tasks across multiple task domains, embodiments, and action spaces. We
conclude with an account of lessons, opportunities, and risks associated with
large TD-MPC2 agents. Explore videos, models, data, code, and more at
https://tdmpc2.com
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024. Explore videos, models, data, code, and more at
  https://tdmpc2.com</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Emergent Dominance Hierarchies in Reinforcement Learning Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.12258v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.12258v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ram Rachum, Yonatan Nakar, Bill Tomlinson, Nitay Alon, Reuth Mirsky
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern Reinforcement Learning (RL) algorithms are able to outperform humans
in a wide variety of tasks. Multi-agent reinforcement learning (MARL) settings
present additional challenges, and successful cooperation in mixed-motive
groups of agents depends on a delicate balancing act between individual and
group objectives. Social conventions and norms, often inspired by human
institutions, are used as tools for striking this balance.
  In this paper, we examine a fundamental, well-studied social convention that
underlies cooperation in both animal and human societies: dominance
hierarchies.
  We adapt the ethological theory of dominance hierarchies to artificial
agents, borrowing the established terminology and definitions with as few
amendments as possible. We demonstrate that populations of RL agents, operating
without explicit programming or intrinsic rewards, can invent, learn, enforce,
and transmit a dominance hierarchy to new populations. The dominance
hierarchies that emerge have a similar structure to those studied in chickens,
mice, fish, and other species.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Intelligent Canvas: Enabling Design-Like Exploratory Visual Data
  Analysis with Generative AI through Rapid Prototyping, Iteration and Curation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.08812v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.08812v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zijian Ding, Joel Chan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Complex data analysis inherently seeks unexpected insights through
exploratory visual analysis methods, transcending logical, step-by-step
processing. However, existing interfaces such as notebooks and dashboards have
limitations in exploration and comparison for visual data analysis. Addressing
these limitations, we introduce a "design-like" intelligent canvas environment
integrating generative AI into data analysis, offering rapid prototyping,
iteration, and comparative visualization management. Our dual contributions
include the integration of generative AI components into a canvas interface,
and empirical findings from a user study (N=10) evaluating the effectiveness of
the canvas interface.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unimodal Multi-Task Fusion for Emotional Mimicry Prediciton 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11879v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11879v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tobias Hallmen, Fabian Deuser, Norbert Oswald, Elisabeth André
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we propose a methodology for the Emotional Mimicry Intensity
(EMI) Estimation task within the context of the 6th Workshop and Competition on
Affective Behavior Analysis in-the-wild. Our approach leverages the Wav2Vec 2.0
framework, pre-trained on a comprehensive podcast dataset, to extract a broad
range of audio features encompassing both linguistic and paralinguistic
elements. We enhance feature representation through a fusion technique that
integrates individual features with a global mean vector, introducing global
contextual insights into our analysis. Additionally, we incorporate a
pre-trained valence-arousal-dominance (VAD) module from the Wav2Vec 2.0 model.
Our fusion employs a Long Short-Term Memory (LSTM) architecture for efficient
temporal analysis of audio data. Utilizing only the provided audio data, our
approach demonstrates significant improvements over the established baseline.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EasyInstruct: An Easy-to-use Instruction Processing Framework for Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.03049v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.03049v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixin Ou, Ningyu Zhang, Honghao Gui, Ziwen Xu, Shuofei Qiao, Yida Xue, Runnan Fang, Kangwei Liu, Lei Li, Zhen Bi, Guozhou Zheng, Huajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, instruction tuning has gained increasing attention and
emerged as a crucial technique to enhance the capabilities of Large Language
Models (LLMs). To construct high-quality instruction datasets, many instruction
processing approaches have been proposed, aiming to achieve a delicate balance
between data quantity and data quality. Nevertheless, due to inconsistencies
that persist among various instruction processing methods, there is no standard
open-source instruction processing implementation framework available for the
community, which hinders practitioners from further developing and advancing.
To facilitate instruction processing research and development, we present
EasyInstruct, an easy-to-use instruction processing framework for LLMs, which
modularizes instruction generation, selection, and prompting, while also
considering their combination and interaction. EasyInstruct is publicly
released and actively maintained at https://github.com/zjunlp/EasyInstruct,
along with an online demo app and a demo video for quick-start, calling for
broader research centered on instruction data and synthetic data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project website: https://zjunlp.github.io/project/EasyInstruct Code:
  https://github.com/zjunlp/EasyInstruct Video: https://youtu.be/rfQOWYfziFo
  Demo: https://huggingface.co/spaces/zjunlp/EasyInstruct</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An explainable three dimension framework to uncover learning patterns: A
  unified look in variable sulci recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.00903v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.00903v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michail Mamalakis, Heloise de Vareilles, Atheer AI-Manea, Samantha C. Mitchell, Ingrid Arartz, Lynn Egeland Morch-Johnsen, Jane Garrison, Jon Simons, Pietro Lio, John Suckling, Graham Murray
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Explainable AI is crucial in medical imaging. In the challenging field of
neuroscience, visual topics present a high level of complexity, particularly
within three-dimensional space. The application of neuroscience, which involves
identifying brain sulcal features from MRI, faces significant hurdles due to
varying annotation protocols among experts and the intricate three-dimension
functionality of the brain. Consequently, traditional explainability approaches
fall short in effectively validating and evaluating these networks. To address
this, we first present a mathematical formulation delineating various
categories of explanation needs across diverse computer vision tasks,
categorized into self-explanatory, semi-explanatory, non-explanatory, and
new-pattern learning applications based on the reliability of the validation
protocol. With respect to this mathematical formulation, we propose a 3D
explainability framework aimed at validating the outputs of deep learning
networks in detecting the paracingulate sulcus an essential brain anatomical
feature. The framework integrates local 3D explanations, global explanations
through dimensionality reduction, concatenated global explanations, and
statistical shape features, unveiling new insights into pattern learning. We
trained and tested two advanced 3D deep learning networks on the challenging
TOP-OSLO dataset, significantly improving sulcus detection accuracy,
particularly on the left hemisphere. During evaluation with diverse annotation
protocols for this dataset, we highlighted the crucial role of an unbiased
annotation process in achieving precise predictions and effective pattern
learning within our proposed 3D framework. The proposed framework not only
annotates the variable sulcus but also uncovers hidden AI knowledge, promising
to advance our understanding of brain anatomy and function.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimizing Language Augmentation for Multilingual Large Language Models:
  A Case Study on Korean 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10882v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10882v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        ChangSu Choi, Yongbin Jeong, Seoyoon Park, InHo Won, HyeonSeok Lim, SangMin Kim, Yejee Kang, Chanhyuk Yoon, Jaewan Park, Yiseul Lee, HyeJin Lee, Younggyun Hahm, Hansaem Kim, KyungTae Lim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) use pretraining to predict the subsequent word;
however, their expansion requires significant computing resources. Numerous big
tech companies and research institutes have developed multilingual LLMs (MLLMs)
to meet current demands, overlooking less-resourced languages (LRLs). This
study proposed three strategies to enhance the performance of LRLs based on the
publicly available MLLMs. First, the MLLM vocabularies of LRLs were expanded to
enhance expressiveness. Second, bilingual data were used for pretraining to
align the high- and less-resourced languages. Third, a high-quality small-scale
instruction dataset was constructed and instruction-tuning was performed to
augment the LRL. The experiments employed the Llama2 model and Korean was used
as the LRL, which was quantitatively evaluated against other developed LLMs
across eight tasks. Furthermore, a qualitative assessment was performed based
on human evaluation and GPT4. Experimental results showed that our proposed
Bllossom model exhibited superior performance in qualitative analyses compared
to previously proposed Korean monolingual models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sequence-to-Sequence Spanish <span class="highlight-title">Pre-train</span>ed Language Models <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.11259v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.11259v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vladimir Araujo, Maria Mihaela Trusca, Rodrigo Tufiño, Marie-Francine Moens
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, significant advancements in pre-trained language models have
driven the creation of numerous non-English language variants, with a
particular emphasis on encoder-only and decoder-only architectures. While
Spanish language models based on BERT and GPT have demonstrated proficiency in
natural language understanding and generation, there remains a noticeable
scarcity of encoder-decoder models explicitly designed for sequence-to-sequence
tasks, which aim to map input sequences to generate output sequences
conditionally. This paper breaks new ground by introducing the implementation
and evaluation of renowned encoder-decoder architectures exclusively
pre-trained on Spanish corpora. Specifically, we present Spanish versions of
BART, T5, and BERT2BERT-style models and subject them to a comprehensive
assessment across various sequence-to-sequence tasks, including summarization,
question answering, split-and-rephrase, dialogue, and translation. Our findings
underscore the competitive performance of all models, with the BART- and
T5-based models emerging as top performers across all tasks. We have made all
models publicly available to the research community to foster future
explorations and advancements in Spanish NLP:
https://github.com/vgaraujov/Seq2Seq-Spanish-PLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted paper at LREC-Coling2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Effective Structured <span class="highlight-title">Prompt</span>ing by Meta-Learning and Representative
  Verbalizer <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.00618v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.00618v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weisen Jiang, Yu Zhang, James T. Kwok
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prompt tuning for pre-trained masked language models (MLM) has shown
promising performance in natural language processing tasks with few labeled
examples. It tunes a prompt for the downstream task, and a verbalizer is used
to bridge the predicted token and label prediction. Due to the limited training
data, prompt initialization is crucial for prompt tuning. Recently,
MetaPrompting (Hou et al., 2022) uses meta-learning to learn a shared
initialization for all task-specific prompts. However, a single initialization
is insufficient to obtain good prompts for all tasks and samples when the tasks
are complex. Moreover, MetaPrompting requires tuning the whole MLM, causing a
heavy burden on computation and memory as the MLM is usually large. To address
these issues, we use a prompt pool to extract more task knowledge and construct
instance-dependent prompts via attention. We further propose a novel soft
verbalizer (RepVerb) which constructs label embedding from feature embeddings
directly. Combining meta-learning the prompt pool and RepVerb, we propose
MetaPrompter for effective structured prompting. MetaPrompter is
parameter-efficient as only the pool is required to be tuned. Experimental
results demonstrate that MetaPrompter performs better than the recent
state-of-the-arts and RepVerb outperforms existing soft verbalizers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICML 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Toward a Theory of Causation for Interpreting Neural Code Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.03788v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.03788v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David N. Palacio, Alejandro Velasco, Nathan Cooper, Alvaro Rodriguez, Kevin Moran, Denys Poshyvanyk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural Language Models of Code, or Neural Code Models (NCMs), are rapidly
progressing from research prototypes to commercial developer tools. As such,
understanding the capabilities and limitations of such models is becoming
critical. However, the abilities of these models are typically measured using
automated metrics that often only reveal a portion of their real-world
performance. While, in general, the performance of NCMs appears promising,
currently much is unknown about how such models arrive at decisions. To this
end, this paper introduces $do_{code}$, a post hoc interpretability method
specific to NCMs that is capable of explaining model predictions. $do_{code}$
is based upon causal inference to enable programming language-oriented
explanations. While the theoretical underpinnings of $do_{code}$ are extensible
to exploring different model properties, we provide a concrete instantiation
that aims to mitigate the impact of spurious correlations by grounding
explanations of model behavior in properties of programming languages. To
demonstrate the practical benefit of $do_{code}$, we illustrate the insights
that our framework can provide by performing a case study on two popular deep
learning architectures and ten NCMs. The results of this case study illustrate
that our studied NCMs are sensitive to changes in code syntax. All our NCMs,
except for the BERT-like model, statistically learn to predict tokens related
to blocks of code (\eg brackets, parenthesis, semicolon) with less confounding
bias as compared to other programming language constructs. These insights
demonstrate the potential of $do_{code}$ as a useful method to detect and
facilitate the elimination of confounding bias in NCMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLM4SGG: Large Language Model for Weakly Supervised Scene Graph
  Generation <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.10404v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.10404v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kibum Kim, Kanghoon Yoon, Jaehyeong Jeon, Yeonjun In, Jinyoung Moon, Donghyun Kim, Chanyoung Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Weakly-Supervised Scene Graph Generation (WSSGG) research has recently
emerged as an alternative to the fully-supervised approach that heavily relies
on costly annotations. In this regard, studies on WSSGG have utilized image
captions to obtain unlocalized triplets while primarily focusing on grounding
the unlocalized triplets over image regions. However, they have overlooked the
two issues involved in the triplet formation process from the captions: 1)
Semantic over-simplification issue arises when extracting triplets from
captions, where fine-grained predicates in captions are undesirably converted
into coarse-grained predicates, resulting in a long-tailed predicate
distribution, and 2) Low-density scene graph issue arises when aligning the
triplets in the caption with entity/predicate classes of interest, where many
triplets are discarded and not used in training, leading to insufficient
supervision. To tackle the two issues, we propose a new approach, i.e., Large
Language Model for weakly-supervised SGG (LLM4SGG), where we mitigate the two
issues by leveraging the LLM's in-depth understanding of language and reasoning
ability during the extraction of triplets from captions and alignment of
entity/predicate classes with target data. To further engage the LLM in these
processes, we adopt the idea of Chain-of-Thought and the in-context few-shot
learning strategy. To validate the effectiveness of LLM4SGG, we conduct
extensive experiments on Visual Genome and GQA datasets, showing significant
improvements in both Recall@K and mean Recall@K compared to the
state-of-the-art WSSGG methods. A further appeal is that LLM4SGG is
data-efficient, enabling effective model training with a small amount of
training images.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages; CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Intrinsic Image Diffusion for Indoor Single-view Material Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.12274v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.12274v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peter Kocsis, Vincent Sitzmann, Matthias Nießner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Intrinsic Image Diffusion, a generative model for appearance
decomposition of indoor scenes. Given a single input view, we sample multiple
possible material explanations represented as albedo, roughness, and metallic
maps. Appearance decomposition poses a considerable challenge in computer
vision due to the inherent ambiguity between lighting and material properties
and the lack of real datasets. To address this issue, we advocate for a
probabilistic formulation, where instead of attempting to directly predict the
true material properties, we employ a conditional generative model to sample
from the solution space. Furthermore, we show that utilizing the strong learned
prior of recent diffusion models trained on large-scale real-world images can
be adapted to material estimation and highly improves the generalization to
real images. Our method produces significantly sharper, more consistent, and
more detailed materials, outperforming state-of-the-art methods by $1.5dB$ on
PSNR and by $45\%$ better FID score on albedo prediction. We demonstrate the
effectiveness of our approach through experiments on both synthetic and
real-world datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://peter-kocsis.github.io/IntrinsicImageDiffusion/
  Video: https://youtu.be/lz0meJlj5cA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Point2RBox: Combine Knowledge from Synthetic Visual Patterns for
  End-to-end Oriented Object Detection with Single Point Supervision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.14758v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.14758v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Yu, Xue Yang, Qingyun Li, Feipeng Da, Jifeng Dai, Yu Qiao, Junchi Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapidly increasing demand for oriented object detection (OOD),
recent research involving weakly-supervised detectors for learning rotated box
(RBox) from the horizontal box (HBox) has attracted more and more attention. In
this paper, we explore a more challenging yet label-efficient setting, namely
single point-supervised OOD, and present our approach called Point2RBox.
Specifically, we propose to leverage two principles: 1) Synthetic pattern
knowledge combination: By sampling around each labeled point on the image, we
spread the object feature to synthetic visual patterns with known boxes to
provide the knowledge for box regression. 2) Transform self-supervision: With a
transformed input image (e.g. scaled/rotated), the output RBoxes are trained to
follow the same transformation so that the network can perceive the relative
size/rotation between objects. The detector is further enhanced by a few
devised techniques to cope with peripheral issues, e.g. the anchor/layer
assignment as the size of the object is not available in our point supervision
setting. To our best knowledge, Point2RBox is the first end-to-end solution for
point-supervised OOD. In particular, our method uses a lightweight paradigm,
yet it achieves a competitive performance among point-supervised alternatives,
41.05%/27.62%/80.01% on DOTA/DIOR/HRSC datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 3 figures, 5 tables, code:
  https://github.com/yuyi1005/point2rbox-mmrotate</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CPSDBench: A Large Language Model Evaluation Benchmark and Baseline for
  Chinese Public Security Domain 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.07234v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.07234v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Tong, Bo Jin, Zhi Lin, Binjun Wang, Ting Yu, Qiang Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated significant potential and
effectiveness across multiple application domains. To assess the performance of
mainstream LLMs in public security tasks, this study aims to construct a
specialized evaluation benchmark tailored to the Chinese public security
domain--CPSDbench. CPSDbench integrates datasets related to public security
collected from real-world scenarios, supporting a comprehensive assessment of
LLMs across four key dimensions: text classification, information extraction,
question answering, and text generation. Furthermore, this study introduces a
set of innovative evaluation metrics designed to more precisely quantify the
efficacy of LLMs in executing tasks related to public security. Through the
in-depth analysis and evaluation conducted in this research, we not only
enhance our understanding of the performance strengths and limitations of
existing models in addressing public security issues but also provide
references for the future development of more accurate and customized LLM
models targeted at applications in this field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the convergence of loss and uncertainty-based active learning
  algorithms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.13927v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.13927v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Haimovich, Dima Karamshuk, Fridolin Linder, Niek Tax, Milan Vojnovic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the convergence rates of loss and uncertainty-based active
learning algorithms under various assumptions. Firstly, we establish a set of
conditions that ensure convergence rates when applied to linear classifiers and
linearly separable datasets. This includes demonstrating convergence rate
guarantees for loss-based sampling with various loss functions. Secondly, we
introduce a framework that allows us to derive convergence rate bounds for
loss-based sampling by leveraging known convergence rate bounds for stochastic
gradient descent algorithms. Lastly, we propose a new algorithm that combines
point sampling and stochastic Polyak's step size. We establish a condition on
the sampling process, ensuring a convergence rate guarantee for this algorithm,
particularly in the case of smooth convex loss functions. Our numerical results
showcase the efficiency of the proposed algorithm.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RiskQ: Risk-sensitive Multi-Agent Reinforcement Learning Value
  Factorization <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.01753v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.01753v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siqi Shen, Chennan Ma, Chao Li, Weiquan Liu, Yongquan Fu, Songzhu Mei, Xinwang Liu, Cheng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-agent systems are characterized by environmental uncertainty, varying
policies of agents, and partial observability, which result in significant
risks. In the context of Multi-Agent Reinforcement Learning (MARL), learning
coordinated and decentralized policies that are sensitive to risk is
challenging. To formulate the coordination requirements in risk-sensitive MARL,
we introduce the Risk-sensitive Individual-Global-Max (RIGM) principle as a
generalization of the Individual-Global-Max (IGM) and Distributional IGM (DIGM)
principles. This principle requires that the collection of risk-sensitive
action selections of each agent should be equivalent to the risk-sensitive
action selection of the central policy. Current MARL value factorization
methods do not satisfy the RIGM principle for common risk metrics such as the
Value at Risk (VaR) metric or distorted risk measurements. Therefore, we
propose RiskQ to address this limitation, which models the joint return
distribution by modeling quantiles of it as weighted quantile mixtures of
per-agent return distribution utilities. RiskQ satisfies the RIGM principle for
the VaR and distorted risk metrics. We show that RiskQ can obtain promising
performance through extensive experiments. The source code of RiskQ is
available in https://github.com/xmu-rl-3dv/RiskQ.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CryCeleb: A Speaker Verification <span class="highlight-title">Dataset</span> Based on Infant Cry Sounds <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.00969v7">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.00969v7.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Budaghyan, Charles C. Onu, Arsenii Gorin, Cem Subakan, Doina Precup
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper describes the Ubenwa CryCeleb dataset - a labeled collection of
infant cries - and the accompanying CryCeleb 2023 task, which is a public
speaker verification challenge based on cry sounds. We released more than 6
hours of manually segmented cry sounds from 786 newborns for academic use,
aiming to encourage research in infant cry analysis. The inaugural public
competition attracted 59 participants, 11 of whom improved the baseline
performance. The top-performing system achieved a significant improvement
scoring 25.8% equal error rate, which is still far from the performance of
state-of-the-art adult speaker verification systems. Therefore, we believe
there is room for further research on this dataset, potentially extending
beyond the verification task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICASSP 2024</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Databases
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quantifying Semantic Query Similarity for Automated Linear SQL Grading:
  A Graph-based Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14441v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14441v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leo Köberlein, Dominik Probst, Richard Lenz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantifying the semantic similarity between database queries is a critical
challenge with broad applications, ranging from query log analysis to automated
educational assessment of SQL skills. Traditional methods often rely solely on
syntactic comparisons or are limited to checking for semantic equivalence.
  This paper introduces a novel graph-based approach to measure the semantic
dissimilarity between SQL queries. Queries are represented as nodes in an
implicit graph, while the transitions between nodes are called edits, which are
weighted by semantic dissimilarity. We employ shortest path algorithms to
identify the lowest-cost edit sequence between two given queries, thereby
defining a quantifiable measure of semantic distance.
  A prototype implementation of this technique has been evaluated through an
empirical study, which strongly suggests that our method provides more accurate
and comprehensible grading compared to existing techniques. Moreover, the
results indicate that our approach comes close to the quality of manual
grading, making it a robust tool for diverse database query comparison tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Space-Efficient Indexes for Uncertain Strings <span class="chip">ICDE 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14256v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14256v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Esteban Gabory, Chang Liu, Grigorios Loukides, Solon P. Pissis, Wiktor Zuba
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Strings in the real world are often encoded with some level of uncertainty.
In the character-level uncertainty model, an uncertain string $X$ of length $n$
on an alphabet $\Sigma$ is a sequence of $n$ probability distributions over
$\Sigma$. Given an uncertain string $X$ and a weight threshold
$\frac{1}{z}\in(0,1]$, we say that pattern $P$ occurs in $X$ at position $i$,
if the product of probabilities of the letters of $P$ at positions
$i,\ldots,i+|P|-1$ is at least $\frac{1}{z}$. While indexing standard strings
for online pattern searches can be performed in linear time and space, indexing
uncertain strings is much more challenging. Specifically, the state-of-the-art
index for uncertain strings has $\mathcal{O}(nz)$ size, requires
$\mathcal{O}(nz)$ time and $\mathcal{O}(nz)$ space to be constructed, and
answers pattern matching queries in the optimal $\mathcal{O}(m+|\text{Occ}|)$
time, where $m$ is the length of $P$ and $|\text{Occ}|$ is the total number of
occurrences of $P$ in $X$. For large $n$ and (moderate) $z$ values, this index
is completely impractical to construct, which outweighs the benefit of the
supported optimal pattern matching queries. We were thus motivated to design a
space-efficient index at the expense of slower yet competitive pattern matching
queries. We propose an index of $\mathcal{O}(\frac{nz}{\ell}\log z)$ expected
size, which can be constructed using $\mathcal{O}(\frac{nz}{\ell}\log z)$
expected space, and supports very fast pattern matching queries in expectation,
for patterns of length $m\geq \ell$. We have implemented and evaluated several
versions of our index. The best-performing version of our index is up to two
orders of magnitude smaller than the state of the art in terms of both index
size and construction space, while offering faster or very competitive query
and construction times.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICDE 2024. Abstract abridged to satisfy arXiv
  requirements</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Learning for Trajectory Data Management and Mining: A <span class="highlight-title">Survey</span> and
  Beyond 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14151v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14151v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Chen, Yuxuan Liang, Yuanshao Zhu, Yanchuan Chang, Kang Luo, Haomin Wen, Lei Li, Yanwei Yu, Qingsong Wen, Chao Chen, Kai Zheng, Yunjun Gao, Xiaofang Zhou, Yu Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Trajectory computing is a pivotal domain encompassing trajectory data
management and mining, garnering widespread attention due to its crucial role
in various practical applications such as location services, urban traffic, and
public safety. Traditional methods, focusing on simplistic spatio-temporal
features, face challenges of complex calculations, limited scalability, and
inadequate adaptability to real-world complexities. In this paper, we present a
comprehensive review of the development and recent advances in deep learning
for trajectory computing (DL4Traj). We first define trajectory data and provide
a brief overview of widely-used deep learning models. Systematically, we
explore deep learning applications in trajectory management (pre-processing,
storage, analysis, and visualization) and mining (trajectory-related
forecasting, trajectory-related recommendation, trajectory classification,
travel time estimation, anomaly detection, and mobility generation). Notably,
we encapsulate recent advancements in Large Language Models (LLMs) that hold
the potential to augment trajectory computing. Additionally, we summarize
application scenarios, public datasets, and toolkits. Finally, we outline
current challenges in DL4Traj research and propose future directions. Relevant
papers and open-source resources have been collated and are continuously
updated at:
\href{https://github.com/yoshall/Awesome-Trajectory-Computing}{DL4Traj Repo}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 12 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Gen-T: Table Reclamation in Data Lakes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14128v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14128v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Grace Fan, Roee Shraga, Renée J. Miller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce the problem of Table Reclamation. Given a Source Table and a
large table repository, reclamation finds a set of tables that, when
integrated, reproduce the source table as closely as possible. Unlike query
discovery problems like Query-by-Example or by-Target, Table Reclamation
focuses on reclaiming the data in the Source Table as fully as possible using
real tables that may be incomplete or inconsistent. To do this, we define a new
measure of table similarity, called error-aware instance similarity, to measure
how close a reclaimed table is to a Source Table, a measure grounded in
instance similarity used in data exchange. Our search covers not only
SELECT-PROJECT- JOIN queries, but integration queries with unions, outerjoins,
and the unary operators subsumption and complementation that have been shown to
be important in data integration and fusion. Using reclamation, a data
scientist can understand if any tables in a repository can be used to exactly
reclaim a tuple in the Source. If not, one can understand if this is due to
differences in values or to incompleteness in the data. Our solution, Gen-T,
performs table discovery to retrieve a set of candidate tables from the table
repository, filters these down to a set of originating tables, then integrates
these tables to reclaim the Source as closely as possible. We show that our
solution, while approximate, is accurate, efficient and scalable in the size of
the table repository with experiments on real data lakes containing up to 15K
tables, where the average number of tuples varies from small (web tables) to
extremely large (open data tables) up to 1M tuples.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TensorBank: Tensor Lakehouse for Foundation Model Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.02094v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.02094v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Romeo Kienzler, Leonardo Pondian Tizzei, Benedikt Blumenstiel, Zoltan Arnold Nagy, S. Karthik Mukkavilli, Johannes Schmude, Marcus Freitag, Michael Behrendt, Daniel Salles Civitarese, Naomi Simumba, Daiki Kimura, Hendrik Hamann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Storing and streaming high dimensional data for foundation model training
became a critical requirement with the rise of foundation models beyond natural
language. In this paper we introduce TensorBank, a petabyte scale tensor
lakehouse capable of streaming tensors from Cloud Object Store (COS) to GPU
memory at wire speed based on complex relational queries. We use Hierarchical
Statistical Indices (HSI) for query acceleration. Our architecture allows to
directly address tensors on block level using HTTP range reads. Once in GPU
memory, data can be transformed using PyTorch transforms. We provide a generic
PyTorch dataset type with a corresponding dataset factory translating
relational queries and requested transformations as an instance. By making use
of the HSI, irrelevant blocks can be skipped without reading them as those
indices contain statistics on their content at different hierarchical
resolution levels. This is an opinionated architecture powered by open
standards and making heavy use of open-source technology. Although, hardened
for production use using geospatial-temporal data, this architecture
generalizes to other use case like computer vision, computational neuroscience,
biological sequence analysis and more.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generating Explanations to Understand and Repair Embedding-based Entity
  Alignment <span class="chip">ICDE 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.04877v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.04877v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaobin Tian, Zequn Sun, Wei Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Entity alignment (EA) seeks identical entities in different knowledge graphs,
which is a long-standing task in the database research. Recent work leverages
deep learning to embed entities in vector space and align them via nearest
neighbor search. Although embedding-based EA has gained marked success in
recent years, it lacks explanations for alignment decisions. In this paper, we
present the first framework that can generate explanations for understanding
and repairing embedding-based EA results. Given an EA pair produced by an
embedding model, we first compare its neighbor entities and relations to build
a matching subgraph as a local explanation. We then construct an alignment
dependency graph to understand the pair from an abstract perspective. Finally,
we repair the pair by resolving three types of alignment conflicts based on
dependency graphs. Experiments on a variety of EA datasets demonstrate the
effectiveness, generalization, and robustness of our framework in explaining
and repairing embedding-based EA results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in the 40th IEEE International Conference on Data
  Engineering (ICDE 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On enforcing dyadic-type homogeneous binary function product constraints
  in MatBase 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.06502v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.06502v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christian Mancas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Homogeneous binary function products are often encountered in the
sub-universes modeled by databases, from genealogical trees to sports, from
education to healthcare, etc. Their properties must be discovered and enforced
by the software applications managing such data to guarantee plausibility. The
(Elementary) Mathematical Data Model provides 18 dyadic-type homogeneous binary
function product constraint types. MatBase, an intelligent data and knowledge
base management system prototype, allows database designers to simply declare
them by only clicking corresponding checkboxes and automatically generates code
for enforcing them. This paper describes the algorithms that MatBase uses for
enforcing all these 18 homogeneous binary function product constraint types,
which may also be used by developers not having access to MatBase.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted on Dec. 7, 2023, to the Journal of Data Science and
  Intelligent Systems (JDSIS), on Dec. 20, 2023, to the Journal of
  Computational and Cognitive Engineering, both of Bon View Publishing,
  Singapore, on Dec. 30 to the Journal of Current Research and Studies, and on
  Jan. 25, 2024, to the Journal of Computer Science Research, Bilingual
  Publishing Group, Singapore</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MathVerse: Does Your Multi-modal LLM Truly See the Diagrams in Visual
  Math Problems? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14624v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14624v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Peng Gao, Hongsheng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The remarkable progress of Multi-modal Large Language Models (MLLMs) has
garnered unparalleled attention, due to their superior performance in visual
contexts. However, their capabilities in visual math problem-solving remain
insufficiently evaluated and understood. We investigate current benchmarks to
incorporate excessive visual content within textual questions, which
potentially assist MLLMs in deducing answers without truly interpreting the
input diagrams. To this end, we introduce MathVerse, an all-around visual math
benchmark designed for an equitable and in-depth evaluation of MLLMs. We
meticulously collect 2,612 high-quality, multi-subject math problems with
diagrams from publicly available sources. Each problem is then transformed by
human annotators into six distinct versions, each offering varying degrees of
information content in multi-modality, contributing to 15K test samples in
total. This approach allows MathVerse to comprehensively assess whether and how
much MLLMs can truly understand the visual diagrams for mathematical reasoning.
In addition, we propose a Chain-of-Thought (CoT) evaluation strategy for a
fine-grained assessment of the output answers. Rather than naively judging True
or False, we employ GPT-4(V) to adaptively extract crucial reasoning steps, and
then score each step with detailed error analysis, which can reveal the
intermediate CoT reasoning quality by MLLMs. We hope the MathVerse benchmark
may provide unique insights to guide the future development of MLLMs. Project
page: https://mathverse-cuhk.github.io
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>46 Pages, Work in Progress, Benchmark Project Page:
  https://mathverse-cuhk.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Simplified Diffusion Schrödinger Bridge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14623v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14623v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhicong Tang, Tiankai Hang, Shuyang Gu, Dong Chen, Baining Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a novel theoretical simplification of the Diffusion
Schr\"odinger Bridge (DSB) that facilitates its unification with Score-based
Generative Models (SGMs), addressing the limitations of DSB in complex data
generation and enabling faster convergence and enhanced performance. By
employing SGMs as an initial solution for DSB, our approach capitalizes on the
strengths of both frameworks, ensuring a more efficient training process and
improving the performance of SGM. We also propose a reparameterization
technique that, despite theoretical approximations, practically improves the
network's fitting capabilities. Our extensive experimental evaluations confirm
the effectiveness of the simplified DSB, demonstrating its significant
improvements. We believe the contributions of this work pave the way for
advanced generative modeling. The code is available at
https://github.com/tzco/Simplified-Diffusion-Schrodinger-Bridge.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Videoshop: Localized Semantic Video Editing with Noise-Extrapolated
  Diffusion Inversion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14617v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14617v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Fan, Anand Bhattad, Ranjay Krishna
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Videoshop, a training-free video editing algorithm for localized
semantic edits. Videoshop allows users to use any editing software, including
Photoshop and generative inpainting, to modify the first frame; it
automatically propagates those changes, with semantic, spatial, and temporally
consistent motion, to the remaining frames. Unlike existing methods that enable
edits only through imprecise textual instructions, Videoshop allows users to
add or remove objects, semantically change objects, insert stock photos into
videos, etc. with fine-grained control over locations and appearance. We
achieve this through image-based video editing by inverting latents with noise
extrapolation, from which we generate videos conditioned on the edited image.
Videoshop produces higher quality edits against 6 baselines on 2 editing
benchmarks using 10 evaluation metrics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DreamReward: Text-to-3D Generation with Human Preference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14613v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14613v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junliang Ye, Fangfu Liu, Qixiu Li, Zhengyi Wang, Yikai Wang, Xinzhou Wang, Yueqi Duan, Jun Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D content creation from text prompts has shown remarkable success recently.
However, current text-to-3D methods often generate 3D results that do not align
well with human preferences. In this paper, we present a comprehensive
framework, coined DreamReward, to learn and improve text-to-3D models from
human preference feedback. To begin with, we collect 25k expert comparisons
based on a systematic annotation pipeline including rating and ranking. Then,
we build Reward3D -- the first general-purpose text-to-3D human preference
reward model to effectively encode human preferences. Building upon the 3D
reward model, we finally perform theoretical analysis and present the Reward3D
Feedback Learning (DreamFL), a direct tuning algorithm to optimize the
multi-view diffusion models with a redefined scorer. Grounded by theoretical
proof and extensive experiment comparisons, our DreamReward successfully
generates high-fidelity and 3D consistent results with significant boosts in
prompt alignment with human intention. Our results demonstrate the great
potential for learning from human feedback to improve text-to-3D models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://jamesyjl.github.io/DreamReward</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14608v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14608v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeyu Han, Chao Gao, Jinyang Liu,  Jeff,  Zhang, Sai Qian Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large models represent a groundbreaking advancement in multiple application
fields, enabling remarkable achievements across various tasks. However, their
unprecedented scale comes with significant computational costs. These models,
often consisting of billions of parameters, require vast amounts of
computational resources for execution. Especially, the expansive scale and
computational demands pose considerable challenges when customizing them for
particular downstream tasks, particularly over the hardware platforms
constrained by computational capabilities. Parameter Efficient Fine-Tuning
(PEFT) provides a practical solution by efficiently adapt the large models over
the various downstream tasks. In particular, PEFT refers to the process of
adjusting the parameters of a pre-trained large models to adapt it to a
specific task while minimizing the number of additional parameters introduced
or computational resources required. This approach is particularly important
when dealing with large language models with high parameter counts, as
fine-tuning these models from scratch can be computationally expensive and
resource-intensive, posing considerable challenges in the supporting system
platform design. In this survey, we present comprehensive studies of various
PEFT algorithms, examining their performance and computational overhead.
Moreover, we provide an overview of applications developed using different PEFT
algorithms and discuss common techniques employed to mitigate computation costs
for PEFT. In addition to the algorithmic perspective, we overview various
real-world system designs to investigate the implementation costs associated
with different PEFT algorithms. This survey serves as an indispensable resource
for researchers aiming to understand both the PEFT algorithm and its system
implementation, offering detailed insights into recent advancements and
practical applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Elements of Differentiable Programming 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14606v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14606v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mathieu Blondel, Vincent Roulet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial intelligence has recently experienced remarkable advances, fueled
by large models, vast datasets, accelerated hardware, and, last but not least,
the transformative power of differentiable programming. This new programming
paradigm enables end-to-end differentiation of complex computer programs
(including those with control flows and data structures), making gradient-based
optimization of program parameters possible. As an emerging paradigm,
differentiable programming builds upon several areas of computer science and
applied mathematics, including automatic differentiation, graphical models,
optimization and statistics. This book presents a comprehensive review of the
fundamental concepts useful for differentiable programming. We adopt two main
perspectives, that of optimization and that of probability, with clear
analogies between the two. Differentiable programming is not merely the
differentiation of programs, but also the thoughtful design of programs
intended for differentiation. By making programs differentiable, we inherently
introduce probability distributions over their execution, providing a means to
quantify the uncertainty associated with program outputs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Draft version 1</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ReNoise: Real Image Inversion Through Iterative Noising 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14602v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14602v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Garibi, Or Patashnik, Andrey Voynov, Hadar Averbuch-Elor, Daniel Cohen-Or
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in text-guided diffusion models have unlocked powerful
image manipulation capabilities. However, applying these methods to real images
necessitates the inversion of the images into the domain of the pretrained
diffusion model. Achieving faithful inversion remains a challenge, particularly
for more recent models trained to generate images with a small number of
denoising steps. In this work, we introduce an inversion method with a high
quality-to-operation ratio, enhancing reconstruction accuracy without
increasing the number of operations. Building on reversing the diffusion
sampling process, our method employs an iterative renoising mechanism at each
inversion sampling step. This mechanism refines the approximation of a
predicted point along the forward diffusion trajectory, by iteratively applying
the pretrained diffusion model, and averaging these predictions. We evaluate
the performance of our ReNoise technique using various sampling algorithms and
models, including recent accelerated diffusion models. Through comprehensive
evaluations and comparisons, we show its effectiveness in terms of both
accuracy and speed. Furthermore, we confirm that our method preserves
editability by demonstrating text-driven image editing on real images.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>project page at: https://garibida.github.io/ReNoise-Inversion/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Extended Reality for Enhanced Human-Robot Collaboration: a
  Human-in-the-Loop Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14597v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14597v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yehor Karpichev, Todd Charter, Homayoun Najjaran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rise of automation has provided an opportunity to achieve higher
efficiency in manufacturing processes, yet it often compromises the flexibility
required to promptly respond to evolving market needs and meet the demand for
customization. Human-robot collaboration attempts to tackle these challenges by
combining the strength and precision of machines with human ingenuity and
perceptual understanding. In this paper, we conceptualize and propose an
implementation framework for an autonomous, machine learning-based manipulator
that incorporates human-in-the-loop principles and leverages Extended Reality
(XR) to facilitate intuitive communication and programming between humans and
robots. Furthermore, the conceptual framework foresees human involvement
directly in the robot learning process, resulting in higher adaptability and
task generalization. The paper highlights key technologies enabling the
proposed framework, emphasizing the importance of developing the digital
ecosystem as a whole. Additionally, we review the existent implementation
approaches of XR in human-robot collaboration, showcasing diverse perspectives
and methodologies. The challenges and future outlooks are discussed, delving
into the major obstacles and potential research avenues of XR for more natural
human-robot interaction and integration in the industrial landscape.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rethinking Adversarial Inverse Reinforcement Learning: From the Angles
  of Policy Imitation and Transferable Reward Recovery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14593v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14593v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yangchun Zhang, Yirui Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adversarial inverse reinforcement learning (AIRL) stands as a cornerstone
approach in imitation learning. This paper rethinks the two different angles of
AIRL: policy imitation and transferable reward recovery. We begin with
substituting the built-in algorithm in AIRL with soft actor-critic (SAC) during
the policy optimization process to enhance sample efficiency, thanks to the
off-policy formulation of SAC and identifiable Markov decision process (MDP)
models with respect to AIRL. It indeed exhibits a significant improvement in
policy imitation but accidentally brings drawbacks to transferable reward
recovery. To learn this issue, we illustrate that the SAC algorithm itself is
not feasible to disentangle the reward function comprehensively during the AIRL
training process, and propose a hybrid framework, PPO-AIRL + SAC, for
satisfactory transfer effect. Additionally, we analyze the capability of
environments to extract disentangled rewards from an algebraic theory
perspective.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ReAct Meets ActRe: Autonomous Annotations of Agent Trajectories for
  Contrastive Self-Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14589v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14589v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zonghan Yang, Peng Li, Ming Yan, Ji Zhang, Fei Huang, Yang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language agents have demonstrated autonomous decision-making abilities by
reasoning with foundation models. Recently, efforts have been made to train
language agents for performance improvement, with multi-step reasoning and
action trajectories as the training data. However, collecting such trajectories
still requires considerable human effort, by either artificial annotations or
implementations of diverse prompting frameworks. In this work, we propose
A$^3$T, a framework that enables the Autonomous Annotation of Agent
Trajectories in the style of ReAct. The central role is an ActRe prompting
agent, which explains the reason for an arbitrary action. When randomly
sampling an external action, the ReAct-style agent could query the ActRe agent
with the action to obtain its textual rationales. Novel trajectories are then
synthesized by prepending the posterior reasoning from ActRe to the sampled
action. In this way, the ReAct-style agent executes multiple trajectories for
the failed tasks, and selects the successful ones to supplement its failed
trajectory for contrastive self-training. Realized by policy gradient methods
with binarized rewards, the contrastive self-training with accumulated
trajectories facilitates a closed loop for multiple rounds of language agent
self-improvement. We conduct experiments using QLoRA fine-tuning with the
open-sourced Mistral-7B-Instruct-v0.2. In AlfWorld, the agent trained with
A$^3$T obtains a 1-shot success rate of 96%, and 100% success with 4 iterative
rounds. In WebShop, the 1-shot performance of the A$^3$T agent matches human
average, and 4 rounds of iterative refinement lead to the performance
approaching human experts. A$^3$T agents significantly outperform existing
techniques, including prompting with GPT-4, advanced agent frameworks, and
fully fine-tuned LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Analysis of Linear Time Series Forecasting Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14587v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14587v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        William Toner, Luke Darlow
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite their simplicity, linear models perform well at time series
forecasting, even when pitted against deeper and more expensive models. A
number of variations to the linear model have been proposed, often including
some form of feature normalisation that improves model generalisation. In this
paper we analyse the sets of functions expressible using these linear model
architectures. In so doing we show that several popular variants of linear
models for time series forecasting are equivalent and functionally
indistinguishable from standard, unconstrained linear regression. We
characterise the model classes for each linear variant. We demonstrate that
each model can be reinterpreted as unconstrained linear regression over a
suitably augmented feature set, and therefore admit closed-form solutions when
using a mean-squared loss function. We provide experimental evidence that the
models under inspection learn nearly identical solutions, and finally
demonstrate that the simpler closed form solutions are superior forecasters
across 72% of test settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Co-Optimization of Environment and Policies for Decentralized
  Multi-Agent Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14583v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14583v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhan Gao, Guang Yang, Amanda Prorok
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work views the multi-agent system and its surrounding environment as a
co-evolving system, where the behavior of one affects the other. The goal is to
take both agent actions and environment configurations as decision variables,
and optimize these two components in a coordinated manner to improve some
measure of interest. Towards this end, we consider the problem of decentralized
multi-agent navigation in cluttered environments. By introducing two
sub-objectives of multi-agent navigation and environment optimization, we
propose an $\textit{agent-environment co-optimization}$ problem and develop a
$\textit{coordinated algorithm}$ that alternates between these sub-objectives
to search for an optimal synthesis of agent actions and obstacle configurations
in the environment; ultimately, improving the navigation performance. Due to
the challenge of explicitly modeling the relation between agents, environment
and performance, we leverage policy gradient to formulate a model-free learning
mechanism within the coordinated framework. A formal convergence analysis shows
that our coordinated algorithm tracks the local minimum trajectory of an
associated time-varying non-convex optimization problem. Extensive numerical
results corroborate theoretical findings and show the benefits of
co-optimization over baselines. Interestingly, the results also indicate that
optimized environment configurations are able to offer structural guidance that
is key to de-conflicting agents in motion.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RAmBLA: A Framework for Evaluating the Reliability of LLMs as Assistants
  in the Biomedical Domain <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14578v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14578v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        William James Bolton, Rafael Poyiadzi, Edward R. Morrell, Gabriela van Bergen Gonzalez Bueno, Lea Goetz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) increasingly support applications in a wide
range of domains, some with potential high societal impact such as biomedicine,
yet their reliability in realistic use cases is under-researched. In this work
we introduce the Reliability AssesMent for Biomedical LLM Assistants (RAmBLA)
framework and evaluate whether four state-of-the-art foundation LLMs can serve
as reliable assistants in the biomedical domain. We identify prompt robustness,
high recall, and a lack of hallucinations as necessary criteria for this use
case. We design shortform tasks and tasks requiring LLM freeform responses
mimicking real-world user interactions. We evaluate LLM performance using
semantic similarity with a ground truth response, through an evaluator LLM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at ICLR 2024 Workshop on Reliable and Responsible
  Foundation Models</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">survey</span> on Concept-based Approaches For Model Improvement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14566v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14566v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Avani Gupta, P J Narayanan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The focus of recent research has shifted from merely increasing the Deep
Neural Networks (DNNs) performance in various tasks to DNNs, which are more
interpretable to humans. The field of eXplainable Artificial Intelligence (XAI)
has observed various techniques, including saliency-based and concept-based
approaches. Concept-based approaches explain the model's decisions in simple
human understandable terms called Concepts. Concepts are human interpretable
units of data and are the thinking ground of humans. Explanations in terms of
concepts enable detecting spurious correlations, inherent biases, or
clever-hans. With the advent of concept-based explanations, there have been
various concept representation methods and automatic concept discovery
algorithms. Some recent methods use concepts for post-hoc model disentanglement
evaluation, while others use them for ante-hoc training. The concept-based
approaches are new, with many representations coming up, and there is very
limited work on Concept-based Model improvement. We provide a systematic review
and taxonomy of various concept representations and their discovery algorithms
in DNNs, specifically in vision. We also provide details on concept-based model
improvement literature, which is the first to survey concept-based model
improvement methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lexicon-Level Contrastive Visual-Grounding Improves Language Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14551v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14551v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengxu Zhuang, Evelina Fedorenko, Jacob Andreas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Today's most accurate language models are trained on orders of magnitude more
language data than human language learners receive - but with no supervision
from other sensory modalities that play a crucial role in human learning. Can
we make LMs' representations and predictions more accurate (and more
human-like) with more ecologically plausible supervision? This paper describes
LexiContrastive Grounding (LCG), a grounded language learning procedure that
leverages visual supervision to improve textual representations.
LexiContrastive Grounding combines a next token prediction strategy with a
contrastive visual grounding objective, focusing on early-layer representations
that encode lexical information. Across multiple word-learning and
sentence-understanding benchmarks, LexiContrastive Grounding not only
outperforms standard language-only models in learning efficiency, but also
improves upon vision-and-language learning procedures including CLIP, GIT,
Flamingo, and Vokenization. Moreover, LexiContrastive Grounding improves
perplexity by around 5% on multiple language modeling tasks. This work
underscores the potential of incorporating visual grounding into language
models, aligning more closely with the multimodal nature of human language
acquisition.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Estimating Physical Information Consistency of Channel Data Augmentation
  for Remote Sensing Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14547v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14547v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tom Burgert, Begüm Demir
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The application of data augmentation for deep learning (DL) methods plays an
important role in achieving state-of-the-art results in supervised,
semi-supervised, and self-supervised image classification. In particular,
channel transformations (e.g., solarize, grayscale, brightness adjustments) are
integrated into data augmentation pipelines for remote sensing (RS) image
classification tasks. However, contradicting beliefs exist about their proper
applications to RS images. A common point of critique is that the application
of channel augmentation techniques may lead to physically inconsistent spectral
data (i.e., pixel signatures). To shed light on the open debate, we propose an
approach to estimate whether a channel augmentation technique affects the
physical information of RS images. To this end, the proposed approach estimates
a score that measures the alignment of a pixel signature within a time series
that can be naturally subject to deviations caused by factors such as
acquisition conditions or phenological states of vegetation. We compare the
scores associated with original and augmented pixel signatures to evaluate the
physical consistency. Experimental results on a multi-label image
classification task show that channel augmentations yielding a score that
exceeds the expected deviation of original pixel signatures can not improve the
performance of a baseline model trained without augmentation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the IEEE International Geoscience and Remote Sensing
  Symposium</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Object-Centric Domain Randomization for 3D Shape Reconstruction in the
  Wild 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14539v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14539v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junhyeong Cho, Kim Youwang, Hunmin Yang, Tae-Hyun Oh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One of the biggest challenges in single-view 3D shape reconstruction in the
wild is the scarcity of <3D shape, 2D image>-paired data from real-world
environments. Inspired by remarkable achievements via domain randomization, we
propose ObjectDR which synthesizes such paired data via a random simulation of
visual variations in object appearances and backgrounds. Our data synthesis
framework exploits a conditional generative model (e.g., ControlNet) to
generate images conforming to spatial conditions such as 2.5D sketches, which
are obtainable through a rendering process of 3D shapes from object collections
(e.g., Objaverse-XL). To simulate diverse variations while preserving object
silhouettes embedded in spatial conditions, we also introduce a disentangled
framework which leverages an initial object guidance. After synthesizing a wide
range of data, we pre-train a model on them so that it learns to capture a
domain-invariant geometry prior which is consistent across various domains. We
validate its effectiveness by substantially improving 3D shape reconstruction
models on a real-world benchmark. In a scale-up evaluation, our pre-training
achieves 23.6% superior results compared with the pre-training on high-quality
computer graphics renderings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://ObjectDR.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Machine-learning invariant foliations in forced systems for reduced
  order modelling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14514v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14514v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Robert Szalai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We identify reduced order models (ROM) of forced systems from data using
invariant foliations. The forcing can be external, parametric, periodic or
quasi-periodic. The process has four steps: 1. identify an approximate
invariant torus and the linear dynamics about the torus; 2. identify a globally
defined invariant foliation about the torus; 3. identify a local foliation
about an invariant manifold that complements the global foliation 4. extract
the invariant manifold as the leaf going through the torus and interpret the
result. We combine steps 2 and 3, so that we can track the location of the
invariant torus and scale the invariance equations appropriately. We highlight
some fundamental limitations of invariant manifolds and foliations when fitting
them to data, that require further mathematics to resolve.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Constrained Reinforcement Learning with Smoothed Log Barrier Function 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14508v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14508v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Baohe Zhang, Yuan Zhang, Lilli Frison, Thomas Brox, Joschka Bödecker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning (RL) has been widely applied to many control tasks and
substantially improved the performances compared to conventional control
methods in many domains where the reward function is well defined. However, for
many real-world problems, it is often more convenient to formulate optimization
problems in terms of rewards and constraints simultaneously. Optimizing such
constrained problems via reward shaping can be difficult as it requires tedious
manual tuning of reward functions with several interacting terms. Recent
formulations which include constraints mostly require a pre-training phase,
which often needs human expertise to collect data or assumes having a
sub-optimal policy readily available. We propose a new constrained RL method
called CSAC-LB (Constrained Soft Actor-Critic with Log Barrier Function), which
achieves competitive performance without any pre-training by applying a linear
smoothed log barrier function to an additional safety critic. It implements an
adaptive penalty for policy learning and alleviates the numerical issues that
are known to complicate the application of the log barrier function method. As
a result, we show that with CSAC-LB, we achieve state-of-the-art performance on
several constrained control tasks with different levels of difficulty and
evaluate our methods in a locomotion task on a real quadruped robot platform.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Soft Learning Probabilistic Circuits 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14504v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14504v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Soroush Ghandi, Benjamin Quost, Cassio de Campos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Probabilistic Circuits (PCs) are prominent tractable probabilistic models,
allowing for a range of exact inferences. This paper focuses on the main
algorithm for training PCs, LearnSPN, a gold standard due to its efficiency,
performance, and ease of use, in particular for tabular data. We show that
LearnSPN is a greedy likelihood maximizer under mild assumptions. While
inferences in PCs may use the entire circuit structure for processing queries,
LearnSPN applies a hard method for learning them, propagating at each sum node
a data point through one and only one of the children/edges as in a hard
clustering process. We propose a new learning procedure named SoftLearn, that
induces a PC using a soft clustering process. We investigate the effect of this
learning-inference compatibility in PCs. Our experiments show that SoftLearn
outperforms LearnSPN in many situations, yielding better likelihoods and
arguably better samples. We also analyze comparable tractable models to
highlight the differences between soft/hard learning and model querying.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Physics-Based Causal Reasoning for Safe & Robust Next-Best Action
  Selection in Robot Manipulation Tasks <span class="chip">IROS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14488v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14488v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ricardo Cannizzaro, Michael Groom, Jonathan Routley, Robert Osazuwa Ness, Lars Kunze
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Safe and efficient object manipulation is a key enabler of many real-world
robot applications. However, this is challenging because robot operation must
be robust to a range of sensor and actuator uncertainties. In this paper, we
present a physics-informed causal-inference-based framework for a robot to
probabilistically reason about candidate actions in a block stacking task in a
partially observable setting. We integrate a physics-based simulation of the
rigid-body system dynamics with a causal Bayesian network (CBN) formulation to
define a causal generative probabilistic model of the robot decision-making
process. Using simulation-based Monte Carlo experiments, we demonstrate our
framework's ability to successfully: (1) predict block tower stability with
high accuracy (Pred Acc: 88.6%); and, (2) select an approximate next-best
action for the block stacking task, for execution by an integrated robot
system, achieving 94.2% task success rate. We also demonstrate our framework's
suitability for real-world robot systems by demonstrating successful task
executions with a domestic support robot, with perception and manipulation
sub-system integration. Hence, we show that by embedding physics-based causal
reasoning into robots' decision-making processes, we can make robot task
execution safer, more reliable, and more robust to various types of
uncertainty.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 9 figures, submitted to 2024 IEEE/RSJ International
  Conference on Intelligent Robots and Systems (IROS)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HyperGALE: ASD Classification via Hypergraph Gated Attention with
  Learnable Hyperedges <span class="chip">IJCNN 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14484v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14484v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mehul Arora, Chirag Shantilal Jain, Lalith Bharadwaj Baru, Kamalaker Dadi, Bapi Raju Surampudi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autism Spectrum Disorder (ASD) is a neurodevelopmental condition
characterized by varied social cognitive challenges and repetitive behavioral
patterns. Identifying reliable brain imaging-based biomarkers for ASD has been
a persistent challenge due to the spectrum's diverse symptomatology. Existing
baselines in the field have made significant strides in this direction, yet
there remains room for improvement in both performance and interpretability. We
propose \emph{HyperGALE}, which builds upon the hypergraph by incorporating
learned hyperedges and gated attention mechanisms. This approach has led to
substantial improvements in the model's ability to interpret complex brain
graph data, offering deeper insights into ASD biomarker characterization.
Evaluated on the extensive ABIDE II dataset, \emph{HyperGALE} not only improves
interpretability but also demonstrates statistically significant enhancements
in key performance metrics compared to both previous baselines and the
foundational hypergraph model. The advancement \emph{HyperGALE} brings to ASD
research highlights the potential of sophisticated graph-based techniques in
neurodevelopmental studies. The source code and implementation instructions are
available at GitHub:https://github.com/mehular0ra/HyperGALE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IJCNN 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Utilizing the LightGBM Algorithm for Operator User Credit Assessment
  Research 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14483v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14483v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaojie Li, Xinqi Dong, Danqing Ma, Bo Dang, Hengyi Zang, Yulu Gong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mobile Internet user credit assessment is an important way for communication
operators to establish decisions and formulate measures, and it is also a
guarantee for operators to obtain expected benefits. However, credit evaluation
methods have long been monopolized by financial industries such as banks and
credit. As supporters and providers of platform network technology and network
resources, communication operators are also builders and maintainers of
communication networks. Internet data improves the user's credit evaluation
strategy. This paper uses the massive data provided by communication operators
to carry out research on the operator's user credit evaluation model based on
the fusion LightGBM algorithm. First, for the massive data related to user
evaluation provided by operators, key features are extracted by data
preprocessing and feature engineering methods, and a multi-dimensional feature
set with statistical significance is constructed; then, linear regression,
decision tree, LightGBM, and other machine learning algorithms build multiple
basic models to find the best basic model; finally, integrates Averaging,
Voting, Blending, Stacking and other integrated algorithms to refine multiple
fusion models, and finally establish the most suitable fusion model for
operator user evaluation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Detoxifying Large Language Models via Knowledge Editing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14472v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14472v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengru Wang, Ningyu Zhang, Ziwen Xu, Zekun Xi, Shumin Deng, Yunzhi Yao, Qishen Zhang, Linyi Yang, Jindong Wang, Huajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates using knowledge editing techniques to detoxify Large
Language Models (LLMs). We construct a benchmark, SafeEdit, which covers nine
unsafe categories with various powerful attack prompts and equips comprehensive
metrics for systematic evaluation. We conduct experiments to compare knowledge
editing approaches with previous baselines, indicating that knowledge editing
has the potential to efficiently detoxify LLMs with limited impact on general
performance. Then, we propose a simple yet effective baseline, dubbed
Detoxifying with Intraoperative Neural Monitoring (DINM), to diminish the
toxicity of LLMs within a few tuning steps via only one instance. We further
provide an in-depth analysis of the internal mechanism for various detoxify
approaches, demonstrating that previous methods like SFT and DPO may merely
suppress the activations of toxic parameters, while DINM mitigates the toxicity
of the toxic parameters to a certain extent, making permanent adjustments. We
hope that these insights could shed light on future work of developing
detoxifying approaches and the underlying knowledge mechanisms of LLMs. Code
and benchmark are available at https://github.com/zjunlp/EasyEdit.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Ongoing work. Project website:
  https://zjunlp.github.io/project/SafeEdit Benchmark:
  https://huggingface.co/datasets/zjunlp/SafeEdit Code:
  https://github.com/zjunlp/EasyEdit</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Universal Feature Selection for Simultaneous Interpretability of
  Multitask <span class="highlight-title">Dataset</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14466v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14466v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matt Raymond, Jacob Charles Saldinger, Paolo Elvati, Clayton Scott, Angela Violi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Extracting meaningful features from complex, high-dimensional datasets across
scientific domains remains challenging. Current methods often struggle with
scalability, limiting their applicability to large datasets, or make
restrictive assumptions about feature-property relationships, hindering their
ability to capture complex interactions. BoUTS's general and scalable feature
selection algorithm surpasses these limitations to identify both universal
features relevant to all datasets and task-specific features predictive for
specific subsets. Evaluated on seven diverse chemical regression datasets,
BoUTS achieves state-of-the-art feature sparsity while maintaining prediction
accuracy comparable to specialized methods. Notably, BoUTS's universal features
enable domain-specific knowledge transfer between datasets, and suggest deep
connections in seemingly-disparate chemical datasets. We expect these results
to have important repercussions in manually-guided inverse problems. Beyond its
current application, BoUTS holds immense potential for elucidating data-poor
systems by leveraging information from similar data-rich systems. BoUTS
represents a significant leap in cross-domain feature selection, potentially
leading to advancements in various scientific fields.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Main text: 14 pages, 3 figures, 1 table; SI: 7 pages, 1 figure, 4
  tables, 3 algorithms</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ gTBLS: Generating Tables from Text by Conditional Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14457v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14457v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anirudh Sundar, Christopher Richardson, Larry Heck
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Distilling large, unstructured text into a structured, condensed form such as
tables is an open research problem. One of the primary challenges in
automatically generating tables is ensuring their syntactic validity. Prior
approaches address this challenge by including additional parameters in the
Transformer's attention mechanism to attend to specific rows and column
headers. In contrast to this single-stage method, this paper presents a
two-stage approach called Generative Tables (gTBLS). The first stage infers
table structure (row and column headers) from the text. The second stage
formulates questions using these headers and fine-tunes a causal language model
to answer them. Furthermore, the gTBLS approach is amenable to the utilization
of pre-trained Large Language Models in a zero-shot configuration, presenting a
solution for table generation in situations where fine-tuning is not feasible.
gTBLS improves prior approaches by up to 10% in BERTScore on the table
construction task and up to 20% on the table content generation task of the
E2E, WikiTableText, WikiBio, and RotoWire datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Language Models Can Reduce Asymmetry in Information Markets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14443v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14443v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nasim Rahaman, Martin Weiss, Manuel Wüthrich, <span class="highlight-author">Yoshua Bengio</span>, Li Erran Li, Chris Pal, Bernhard Schölkopf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work addresses the buyer's inspection paradox for information markets.
The paradox is that buyers need to access information to determine its value,
while sellers need to limit access to prevent theft. To study this, we
introduce an open-source simulated digital marketplace where intelligent
agents, powered by language models, buy and sell information on behalf of
external participants. The central mechanism enabling this marketplace is the
agents' dual capabilities: they not only have the capacity to assess the
quality of privileged information but also come equipped with the ability to
forget. This ability to induce amnesia allows vendors to grant temporary access
to proprietary information, significantly reducing the risk of unauthorized
retention while enabling agents to accurately gauge the information's relevance
to specific queries or tasks. To perform well, agents must make rational
decisions, strategically explore the marketplace through generated sub-queries,
and synthesize answers from purchased information. Concretely, our experiments
(a) uncover biases in language models leading to irrational behavior and
evaluate techniques to mitigate these biases, (b) investigate how price affects
demand in the context of informational goods, and (c) show that inspection and
higher budgets both lead to higher quality outcomes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Analysing Diffusion Segmentation for Medical Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14440v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14440v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mathias Öttl, Siyuan Mei, Frauke Wilm, Jana Steenpass, Matthias Rübner, Arndt Hartmann, Matthias Beckmann, Peter Fasching, Andreas Maier, Ramona Erber, Katharina Breininger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Denoising Diffusion Probabilistic models have become increasingly popular due
to their ability to offer probabilistic modeling and generate diverse outputs.
This versatility inspired their adaptation for image segmentation, where
multiple predictions of the model can produce segmentation results that not
only achieve high quality but also capture the uncertainty inherent in the
model. Here, powerful architectures were proposed for improving diffusion
segmentation performance. However, there is a notable lack of analysis and
discussions on the differences between diffusion segmentation and image
generation, and thorough evaluations are missing that distinguish the
improvements these architectures provide for segmentation in general from their
benefit for diffusion segmentation specifically. In this work, we critically
analyse and discuss how diffusion segmentation for medical images differs from
diffusion image generation, with a particular focus on the training behavior.
Furthermore, we conduct an assessment how proposed diffusion segmentation
architectures perform when trained directly for segmentation. Lastly, we
explore how different medical segmentation tasks influence the diffusion
segmentation behavior and the diffusion process could be adapted accordingly.
With these analyses, we aim to provide in-depth insights into the behavior of
diffusion segmentation that allow for a better design and evaluation of
diffusion segmentation methods in the future.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Multimodal Approach to Device-Directed Speech Detection with Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14438v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14438v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dominik Wager, Alexander Churchill, Siddharth Sigtia, Panayiotis Georgiou, Matt Mirsamadi, Aarshee Mishra, Erik Marchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Interactions with virtual assistants typically start with a predefined
trigger phrase followed by the user command. To make interactions with the
assistant more intuitive, we explore whether it is feasible to drop the
requirement that users must begin each command with a trigger phrase. We
explore this task in three ways: First, we train classifiers using only
acoustic information obtained from the audio waveform. Second, we take the
decoder outputs of an automatic speech recognition (ASR) system, such as 1-best
hypotheses, as input features to a large language model (LLM). Finally, we
explore a multimodal system that combines acoustic and lexical features, as
well as ASR decoder signals in an LLM. Using multimodal information yields
relative equal-error-rate improvements over text-only and audio-only models of
up to 39% and 61%. Increasing the size of the LLM and training with low-rank
adaption leads to further relative EER reductions of up to 18% on our dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2312.03632</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Biased Binary Attribute Classifiers Ignore the Majority Classes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14435v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14435v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyi Zhang, Johanna Sophie Bieri, Manuel Günther
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To visualize the regions of interest that classifiers base their decisions
on, different Class Activation Mapping (CAM) methods have been developed.
However, all of these techniques target categorical classifiers only, though
most real-world tasks are binary classification. In this paper, we extend
gradient-based CAM techniques to work with binary classifiers and visualize the
active regions for binary facial attribute classifiers. When training an
unbalanced binary classifier on an imbalanced dataset, it is well-known that
the majority class, i.e. the class with many training samples, is mostly
predicted much better than minority class with few training instances. In our
experiments on the CelebA dataset, we verify these results, when training an
unbalanced classifier to extract 40 facial attributes simultaneously. One would
expect that the biased classifier has learned to extract features mainly for
the majority classes and that the proportional energy of the activations mainly
reside in certain specific regions of the image where the attribute is located.
However, we find very little regular activation for samples of majority
classes, while the active regions for minority classes seem mostly reasonable
and overlap with our expectations. These results suggest that biased
classifiers mainly rely on bias activation for majority classes. When training
a balanced classifier on the imbalanced data by employing attribute-specific
class weights, majority and minority classes are classified similarly well and
show expected activations for almost all attributes
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Style-Extracting Diffusion Models for Semi-Supervised Histopathology
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14429v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14429v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mathias Öttl, Frauke Wilm, Jana Steenpass, Jingna Qiu, Matthias Rübner, Arndt Hartmann, Matthias Beckmann, Peter Fasching, Andreas Maier, Ramona Erber, Bernhard Kainz, Katharina Breininger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning-based image generation has seen significant advancements with
diffusion models, notably improving the quality of generated images. Despite
these developments, generating images with unseen characteristics beneficial
for downstream tasks has received limited attention. To bridge this gap, we
propose Style-Extracting Diffusion Models, featuring two conditioning
mechanisms. Specifically, we utilize 1) a style conditioning mechanism which
allows to inject style information of previously unseen images during image
generation and 2) a content conditioning which can be targeted to a downstream
task, e.g., layout for segmentation. We introduce a trainable style encoder to
extract style information from images, and an aggregation block that merges
style information from multiple style inputs. This architecture enables the
generation of images with unseen styles in a zero-shot manner, by leveraging
styles from unseen images, resulting in more diverse generations. In this work,
we use the image layout as target condition and first show the capability of
our method on a natural image dataset as a proof-of-concept. We further
demonstrate its versatility in histopathology, where we combine prior knowledge
about tissue composition and unannotated data to create diverse synthetic
images with known layouts. This allows us to generate additional synthetic data
to train a segmentation network in a semi-supervised fashion. We verify the
added value of the generated images by showing improved segmentation results
and lower performance variability between patients when synthetic images are
included during segmentation training. Our code will be made publicly available
at [LINK].
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Task-optimal data-driven surrogate models for eNMPC via differentiable
  simulation and optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14425v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14425v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Mayfrank, Na Young Ahn, Alexander Mitsos, Manuel Dahmen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a method for end-to-end learning of Koopman surrogate models for
optimal performance in control. In contrast to previous contributions that
employ standard reinforcement learning (RL) algorithms, we use a training
algorithm that exploits the potential differentiability of environments based
on mechanistic simulation models. We evaluate the performance of our method by
comparing it to that of other controller type and training algorithm
combinations on a literature known eNMPC case study. Our method exhibits
superior performance on this problem, thereby constituting a promising avenue
towards more capable controllers that employ dynamic surrogate models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 4 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DP-RDM: Adapting Diffusion Models to Private Domains Without Fine-Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14421v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14421v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonathan Lebensold, Maziar Sanjabi, Pietro Astolfi, Adriana Romero-Soriano, Kamalika Chaudhuri, Mike Rabbat, Chuan Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-image diffusion models have been shown to suffer from sample-level
memorization, possibly reproducing near-perfect replica of images that they are
trained on, which may be undesirable. To remedy this issue, we develop the
first differentially private (DP) retrieval-augmented generation algorithm that
is capable of generating high-quality image samples while providing provable
privacy guarantees. Specifically, we assume access to a text-to-image diffusion
model trained on a small amount of public data, and design a DP retrieval
mechanism to augment the text prompt with samples retrieved from a private
retrieval dataset. Our \emph{differentially private retrieval-augmented
diffusion model} (DP-RDM) requires no fine-tuning on the retrieval dataset to
adapt to another domain, and can use state-of-the-art generative models to
generate high-quality image samples while satisfying rigorous DP guarantees.
For instance, when evaluated on MS-COCO, our DP-RDM can generate samples with a
privacy budget of $\epsilon=10$, while providing a $3.5$ point improvement in
FID compared to public-only retrieval for up to $10,000$ queries.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Model Uncertainty in Evolutionary Optimization and Bayesian
  Optimization: A Comparative Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14413v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14413v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Hao, Xiaoqun Zhang, Aimin Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Black-box optimization problems, which are common in many real-world
applications, require optimization through input-output interactions without
access to internal workings. This often leads to significant computational
resources being consumed for simulations. Bayesian Optimization (BO) and
Surrogate-Assisted Evolutionary Algorithm (SAEA) are two widely used
gradient-free optimization techniques employed to address such challenges. Both
approaches follow a similar iterative procedure that relies on surrogate models
to guide the search process. This paper aims to elucidate the similarities and
differences in the utilization of model uncertainty between these two methods,
as well as the impact of model inaccuracies on algorithmic performance. A novel
model-assisted strategy is introduced, which utilizes unevaluated solutions to
generate offspring, leveraging the population-based search capabilities of
evolutionary algorithm to enhance the effectiveness of model-assisted
optimization. Experimental results demonstrate that the proposed approach
outperforms mainstream Bayesian optimization algorithms in terms of accuracy
and efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GLC++: Source-Free Universal Domain Adaptation through Global-Local
  Clustering and Contrastive Affinity Learning <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14410v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14410v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sanqing Qu, Tianpei Zou, Florian Röhrbein, Cewu Lu, Guang Chen, Dacheng Tao, Changjun Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks often exhibit sub-optimal performance under covariate
and category shifts. Source-Free Domain Adaptation (SFDA) presents a promising
solution to this dilemma, yet most SFDA approaches are restricted to closed-set
scenarios. In this paper, we explore Source-Free Universal Domain Adaptation
(SF-UniDA) aiming to accurately classify "known" data belonging to common
categories and segregate them from target-private "unknown" data. We propose a
novel Global and Local Clustering (GLC) technique, which comprises an adaptive
one-vs-all global clustering algorithm to discern between target classes,
complemented by a local k-NN clustering strategy to mitigate negative transfer.
Despite the effectiveness, the inherent closed-set source architecture leads to
uniform treatment of "unknown" data, impeding the identification of distinct
"unknown" categories. To address this, we evolve GLC to GLC++, integrating a
contrastive affinity learning strategy. We examine the superiority of GLC and
GLC++ across multiple benchmarks and category shift scenarios. Remarkably, in
the most challenging open-partial-set scenarios, GLC and GLC++ surpass GATE by
16.7% and 18.6% in H-score on VisDA, respectively. GLC++ enhances the novel
category clustering accuracy of GLC by 4.3% in open-set scenarios on
Office-Home. Furthermore, the introduced contrastive learning strategy not only
enhances GLC but also significantly facilitates existing methodologies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This is a substantial extension of the CVPR 2023 paper "Upcycling
  Models under Domain and Category Shift"</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TD-MPC2: Scalable, Robust World Models for Continuous Control <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.16828v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.16828v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicklas Hansen, Hao Su, Xiaolong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  TD-MPC is a model-based reinforcement learning (RL) algorithm that performs
local trajectory optimization in the latent space of a learned implicit
(decoder-free) world model. In this work, we present TD-MPC2: a series of
improvements upon the TD-MPC algorithm. We demonstrate that TD-MPC2 improves
significantly over baselines across 104 online RL tasks spanning 4 diverse task
domains, achieving consistently strong results with a single set of
hyperparameters. We further show that agent capabilities increase with model
and data size, and successfully train a single 317M parameter agent to perform
80 tasks across multiple task domains, embodiments, and action spaces. We
conclude with an account of lessons, opportunities, and risks associated with
large TD-MPC2 agents. Explore videos, models, data, code, and more at
https://tdmpc2.com
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024. Explore videos, models, data, code, and more at
  https://tdmpc2.com</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Emergent Dominance Hierarchies in Reinforcement Learning Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.12258v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.12258v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ram Rachum, Yonatan Nakar, Bill Tomlinson, Nitay Alon, Reuth Mirsky
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern Reinforcement Learning (RL) algorithms are able to outperform humans
in a wide variety of tasks. Multi-agent reinforcement learning (MARL) settings
present additional challenges, and successful cooperation in mixed-motive
groups of agents depends on a delicate balancing act between individual and
group objectives. Social conventions and norms, often inspired by human
institutions, are used as tools for striking this balance.
  In this paper, we examine a fundamental, well-studied social convention that
underlies cooperation in both animal and human societies: dominance
hierarchies.
  We adapt the ethological theory of dominance hierarchies to artificial
agents, borrowing the established terminology and definitions with as few
amendments as possible. We demonstrate that populations of RL agents, operating
without explicit programming or intrinsic rewards, can invent, learn, enforce,
and transmit a dominance hierarchy to new populations. The dominance
hierarchies that emerge have a similar structure to those studied in chickens,
mice, fish, and other species.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unraveling the Mystery of Scaling Laws: Part I 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06563v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06563v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hui Su, Zhi Tian, Xiaoyu Shen, Xunliang Cai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scaling law principles indicate a power-law correlation between loss and
variables such as model size, dataset size, and computational resources
utilized during training. These principles play a vital role in optimizing
various aspects of model pre-training, ultimately contributing to the success
of large language models such as GPT-4, Llama and Gemini. However, the original
scaling law paper by OpenAI did not disclose the complete details necessary to
derive the precise scaling law formulas, and their conclusions are only based
on models containing up to 1.5 billion parameters. Though some subsequent works
attempt to unveil these details and scale to larger models, they often neglect
the training dependency of important factors such as the learning rate, context
length and batch size, leading to their failure to establish a reliable formula
for predicting the test loss trajectory. In this technical report, we confirm
that the scaling law formulations proposed in the original OpenAI paper remain
valid when scaling the model size up to 33 billion, but the constant
coefficients in these formulas vary significantly with the experiment setup. We
meticulously identify influential factors and provide transparent, step-by-step
instructions to estimate all constant terms in scaling-law formulas by training
on models with only 1M~60M parameters. Using these estimated formulas, we
showcase the capability to accurately predict various attributes for models
with up to 33B parameters before their training, including (1) the minimum
possible test loss; (2) the minimum required training steps and processed
tokens to achieve a specific loss; (3) the critical batch size with an optimal
time/computation trade-off at any loss value; and (4) the complete test loss
trajectory with arbitrary batch size.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Geospatial Approach to Predicting Desert Locust Breeding Grounds in
  Africa 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06860v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06860v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ibrahim Salihu Yusuf, Mukhtar Opeyemi Yusuf, Kobby Panford-Quainoo, Arnu Pretorius
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Desert locust swarms present a major threat to agriculture and food security.
Addressing this challenge, our study develops an operationally-ready model for
predicting locust breeding grounds, which has the potential to enhance early
warning systems and targeted control measures. We curated a dataset from the
United Nations Food and Agriculture Organization's (UN-FAO) locust observation
records and analyzed it using two types of spatio-temporal input features:
remotely-sensed environmental and climate data as well as multi-spectral earth
observation images. Our approach employed custom deep learning models
(three-dimensional and LSTM-based recurrent convolutional networks), along with
the geospatial foundational model Prithvi recently released by Jakubik et al.,
2023. These models notably outperformed existing baselines, with the
Prithvi-based model, fine-tuned on multi-spectral images from NASA's Harmonized
Landsat and Sentinel-2 (HLS) dataset, achieving the highest accuracy, F1 and
ROC-AUC scores (83.03%, 81.53% and 87.69%, respectively). A significant finding
from our research is that multi-spectral earth observation images alone are
sufficient for effective locust breeding ground prediction without the need to
explicitly incorporate climatic or environmental features.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MedMamba: Vision Mamba for Medical Image Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.03849v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.03849v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yubiao Yue, Zhenzhang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical image classification is a very fundamental and crucial task in the
field of computer vision. These years, CNN-based and Transformer-based models
have been widely used to classify various medical images. Unfortunately, The
limitation of CNNs in long-range modeling capabilities prevents them from
effectively extracting features in medical images, while Transformers are
hampered by their quadratic computational complexity. Recent research has shown
that the state space model (SSM) represented by Mamba can efficiently model
long-range interactions while maintaining linear computational complexity.
Inspired by this, we propose Vision Mamba for medical image classification
(MedMamba). More specifically, we introduce a novel Conv-SSM module. Conv-SSM
combines the local feature extraction ability of convolutional layers with the
ability of SSM to capture long-range dependency, thereby modeling medical
images with different modalities. To demonstrate the potential of MedMamba, we
conducted extensive experiments using 14 publicly available medical datasets
with different imaging techniques and two private datasets built by ourselves.
Extensive experimental results demonstrate that the proposed MedMamba performs
well in detecting lesions in various medical images. To the best of our
knowledge, this is the first Vision Mamba tailored for medical image
classification. The purpose of this work is to establish a new baseline for
medical image classification tasks and provide valuable insights for the future
development of more efficient and effective SSM-based artificial intelligence
algorithms and application systems in the medical. Source code has been
available at https://github.com/YubiaoYue/MedMamba.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Let's do the time-warp-attend: Learning topological invariants of
  dynamical systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.09234v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.09234v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Noa Moriel, Matthew Ricci, Mor Nitzan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dynamical systems across the sciences, from electrical circuits to ecological
networks, undergo qualitative and often catastrophic changes in behavior,
called bifurcations, when their underlying parameters cross a threshold.
Existing methods predict oncoming catastrophes in individual systems but are
primarily time-series-based and struggle both to categorize qualitative
dynamical regimes across diverse systems and to generalize to real data. To
address this challenge, we propose a data-driven, physically-informed
deep-learning framework for classifying dynamical regimes and characterizing
bifurcation boundaries based on the extraction of topologically invariant
features. We focus on the paradigmatic case of the supercritical Hopf
bifurcation, which is used to model periodic dynamics across a wide range of
applications. Our convolutional attention method is trained with data
augmentations that encourage the learning of topological invariants which can
be used to detect bifurcation boundaries in unseen systems and to design models
of biological systems like oscillatory gene regulatory networks. We further
demonstrate our method's use in analyzing real data by recovering distinct
proliferation and differentiation dynamics along pancreatic endocrinogenesis
trajectory in gene expression space based on single-cell data. Our method
provides valuable insights into the qualitative, long-term behavior of a wide
range of dynamical systems, and can detect bifurcations or catastrophic
transitions in large-scale physical and biological systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ QuATON: Quantization Aware Training of Optical Neurons 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03049v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03049v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hasindu Kariyawasam, Ramith Hettiarachchi, Quansan Yang, Alex Matlock, Takahiro Nambara, Hiroyuki Kusaka, Yuichiro Kunai, Peter T C So, Edward S Boyden, Dushan Wadduwage
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Optical processors, built with "optical neurons", can efficiently perform
high-dimensional linear operations at the speed of light. Thus they are a
promising avenue to accelerate large-scale linear computations. With the
current advances in micro-fabrication, such optical processors can now be 3D
fabricated, but with a limited precision. This limitation translates to
quantization of learnable parameters in optical neurons, and should be handled
during the design of the optical processor in order to avoid a model mismatch.
Specifically, optical neurons should be trained or designed within the
physical-constraints at a predefined quantized precision level. To address this
critical issues we propose a physics-informed quantization-aware training
framework. Our approach accounts for physical constraints during the training
process, leading to robust designs. We demonstrate that our approach can design
state of the art optical processors using diffractive networks for multiple
physics based tasks despite quantized learnable parameters. We thus lay the
foundation upon which improved optical processors may be 3D fabricated in the
future.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Collaborative Distributed Machine Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.16584v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.16584v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Jin, Niclas Kannengießer, Sascha Rank, Ali Sunyaev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Various collaborative distributed machine learning (CDML) systems, including
federated learning systems and swarm learning systems, with different key
traits were developed to leverage resources for development and use of machine
learning (ML) models in a confidentiality-preserving way. To meet use case
requirements, suitable CDML systems need to be selected. However, comparison
between CDML systems regarding their suitability for use cases is often
difficult. This work presents a CDML system conceptualization and CDML
archetypes to support comparison of CDML systems and introduce scientific and
practical audiences to the principal functioning and key traits of CDML
systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Assessing the Causal Impact of Humanitarian Aid on Food Security 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.11287v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.11287v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jordi Cerdà-Bautista, José María Tárraga, Vasileios Sitokonstantinou, Gustau Camps-Valls
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the face of climate change-induced droughts, vulnerable regions encounter
severe threats to food security, demanding urgent humanitarian assistance. This
paper introduces a causal inference framework for the Horn of Africa, aiming to
assess the impact of cash-based interventions on food crises. Our contributions
include identifying causal relationships within the food security system,
harmonizing a comprehensive database including socio-economic, weather and
remote sensing data, and estimating the causal effect of humanitarian
interventions on malnutrition. On a country level, our results revealed no
significant effects, likely due to limited sample size, suboptimal data
quality, and an imperfect causal graph resulting from our limited understanding
of multidisciplinary systems like food security. Instead, on a district level,
results revealed significant effects, further implying the context-specific
nature of the system. This underscores the need to enhance data collection and
refine causal models with domain experts for more effective future
interventions and policies, improving transparency and accountability in
humanitarian aid.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication and presentation at the International
  Geoscience and Remote Sensing Symposium (IGARSS) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning a Depth Covariance Function <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12157v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12157v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eric Dexheimer, Andrew J. Davison
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose learning a depth covariance function with applications to
geometric vision tasks. Given RGB images as input, the covariance function can
be flexibly used to define priors over depth functions, predictive
distributions given observations, and methods for active point selection. We
leverage these techniques for a selection of downstream tasks: depth
completion, bundle adjustment, and monocular dense visual odometry.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2023. Project page: https://edexheim.github.io/DepthCov/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TMI! Finetuned Models Leak Private Information from their <span class="highlight-title">Pretrain</span>ing
  Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.01181v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.01181v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        John Abascal, Stanley Wu, Alina Oprea, Jonathan Ullman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transfer learning has become an increasingly popular technique in machine
learning as a way to leverage a pretrained model trained for one task to assist
with building a finetuned model for a related task. This paradigm has been
especially popular for $\textit{privacy}$ in machine learning, where the
pretrained model is considered public, and only the data for finetuning is
considered sensitive. However, there are reasons to believe that the data used
for pretraining is still sensitive, making it essential to understand how much
information the finetuned model leaks about the pretraining data. In this work
we propose a new membership-inference threat model where the adversary only has
access to the finetuned model and would like to infer the membership of the
pretraining data. To realize this threat model, we implement a novel
metaclassifier-based attack, $\textbf{TMI}$, that leverages the influence of
memorized pretraining samples on predictions in the downstream task. We
evaluate $\textbf{TMI}$ on both vision and natural language tasks across
multiple transfer learning settings, including finetuning with differential
privacy. Through our evaluation, we find that $\textbf{TMI}$ can successfully
infer membership of pretraining examples using query access to the finetuned
model. An open-source implementation of $\textbf{TMI}$ can be found
$\href{https://github.com/johnmath/tmi-pets24}{\text{on GitHub}}$.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Closing the Gap: Achieving Better Accuracy-Robustness Tradeoffs against
  Query-Based Attacks <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.10132v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.10132v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pascal Zimmer, Sébastien Andreina, Giorgia Azzurra Marson, Ghassan Karame
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although promising, existing defenses against query-based attacks share a
common limitation: they offer increased robustness against attacks at the price
of a considerable accuracy drop on clean samples. In this work, we show how to
efficiently establish, at test-time, a solid tradeoff between robustness and
accuracy when mitigating query-based attacks. Given that these attacks
necessarily explore low-confidence regions, our insight is that activating
dedicated defenses, such as random noise defense and random image
transformations, only for low-confidence inputs is sufficient to prevent them.
Our approach is independent of training and supported by theory. We verify the
effectiveness of our approach for various existing defenses by conducting
extensive experiments on CIFAR-10, CIFAR-100, and ImageNet. Our results confirm
that our proposal can indeed enhance these defenses by providing better
tradeoffs between robustness and accuracy when compared to state-of-the-art
approaches while being completely training-free.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in the Proceedings of the AAAI Conference on Artificial
  Intelligence (AAAI) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EasyInstruct: An Easy-to-use Instruction Processing Framework for Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.03049v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.03049v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixin Ou, Ningyu Zhang, Honghao Gui, Ziwen Xu, Shuofei Qiao, Yida Xue, Runnan Fang, Kangwei Liu, Lei Li, Zhen Bi, Guozhou Zheng, Huajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, instruction tuning has gained increasing attention and
emerged as a crucial technique to enhance the capabilities of Large Language
Models (LLMs). To construct high-quality instruction datasets, many instruction
processing approaches have been proposed, aiming to achieve a delicate balance
between data quantity and data quality. Nevertheless, due to inconsistencies
that persist among various instruction processing methods, there is no standard
open-source instruction processing implementation framework available for the
community, which hinders practitioners from further developing and advancing.
To facilitate instruction processing research and development, we present
EasyInstruct, an easy-to-use instruction processing framework for LLMs, which
modularizes instruction generation, selection, and prompting, while also
considering their combination and interaction. EasyInstruct is publicly
released and actively maintained at https://github.com/zjunlp/EasyInstruct,
along with an online demo app and a demo video for quick-start, calling for
broader research centered on instruction data and synthetic data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project website: https://zjunlp.github.io/project/EasyInstruct Code:
  https://github.com/zjunlp/EasyInstruct Video: https://youtu.be/rfQOWYfziFo
  Demo: https://huggingface.co/spaces/zjunlp/EasyInstruct</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exact and general decoupled solutions of the LMC Multitask Gaussian
  Process model <span class="chip">UAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.12032v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.12032v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Olivier Truffinet, Karim Ammar, Jean-Philippe Argaud, Bertrand Bouriquet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Linear Model of Co-regionalization (LMC) is a very general model of
multitask gaussian process for regression or classification. While its
expressivity and conceptual simplicity are appealing, naive implementations
have cubic complexity in the number of datapoints and number of tasks, making
approximations mandatory for most applications. However, recent work has shown
that under some conditions the latent processes of the model can be decoupled,
leading to a complexity that is only linear in the number of said processes. We
here extend these results, showing from the most general assumptions that the
only condition necessary to an efficient exact computation of the LMC is a mild
hypothesis on the noise model. We introduce a full parametrization of the
resulting \emph{projected LMC} model, and an expression of the marginal
likelihood enabling efficient optimization. We perform a parametric study on
synthetic data to show the excellent performance of our approach, compared to
an unrestricted exact LMC and approximations of the latter. Overall, the
projected LMC appears as a credible and simpler alternative to state-of-the art
models, which greatly facilitates some computations such as leave-one-out
cross-validation and fantasization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 10 figures, submitted to UAI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Privacy of Selection Mechanisms with Gaussian Noise <span class="chip">AISTATS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.06137v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.06137v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonathan Lebensold, Doina Precup, Borja Balle
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Report Noisy Max and Above Threshold are two classical differentially private
(DP) selection mechanisms. Their output is obtained by adding noise to a
sequence of low-sensitivity queries and reporting the identity of the query
whose (noisy) answer satisfies a certain condition. Pure DP guarantees for
these mechanisms are easy to obtain when Laplace noise is added to the queries.
On the other hand, when instantiated using Gaussian noise, standard analyses
only yield approximate DP guarantees despite the fact that the outputs of these
mechanisms lie in a discrete space. In this work, we revisit the analysis of
Report Noisy Max and Above Threshold with Gaussian noise and show that, under
the additional assumption that the underlying queries are bounded, it is
possible to provide pure ex-ante DP bounds for Report Noisy Max and pure
ex-post DP bounds for Above Threshold. The resulting bounds are tight and
depend on closed-form expressions that can be numerically evaluated using
standard methods. Empirically we find these lead to tighter privacy accounting
in the high privacy, low data regime. Further, we propose a simple privacy
filter for composing pure ex-post DP guarantees, and use it to derive a fully
adaptive Gaussian Sparse Vector Technique mechanism. Finally, we provide
experiments on mobility and energy consumption datasets demonstrating that our
Sparse Vector Technique is practically competitive with previous approaches and
requires less hyper-parameter tuning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AISTATS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unsupervised Video Domain Adaptation with Masked <span class="highlight-title">Pre-Train</span>ing and
  Collaborative Self-Training <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.02914v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.02914v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arun Reddy, William Paul, Corban Rivera, Ketul Shah, Celso M. de Melo, Rama Chellappa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we tackle the problem of unsupervised domain adaptation (UDA)
for video action recognition. Our approach, which we call UNITE, uses an image
teacher model to adapt a video student model to the target domain. UNITE first
employs self-supervised pre-training to promote discriminative feature learning
on target domain videos using a teacher-guided masked distillation objective.
We then perform self-training on masked target data, using the video student
model and image teacher model together to generate improved pseudolabels for
unlabeled target videos. Our self-training process successfully leverages the
strengths of both models to achieve strong transfer performance across domains.
We evaluate our approach on multiple video domain adaptation benchmarks and
observe significant improvements upon previously reported results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR 2024. 13 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dodging DeepFake Detection via Implicit Spatial-Domain Notch Filtering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2009.09213v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2009.09213v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yihao Huang, Felix Juefei-Xu, Qing Guo, Yang Liu, Geguang Pu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The current high-fidelity generation and high-precision detection of DeepFake
images are at an arms race. We believe that producing DeepFakes that are highly
realistic and 'detection evasive' can serve the ultimate goal of improving
future generation DeepFake detection capabilities. In this paper, we propose a
simple yet powerful pipeline to reduce the artifact patterns of fake images
without hurting image quality by performing implicit spatial-domain notch
filtering. We first demonstrate that frequency-domain notch filtering, although
famously shown to be effective in removing periodic noise in the spatial
domain, is infeasible for our task at hand due to the manual designs required
for the notch filters. We, therefore, resort to a learning-based approach to
reproduce the notch filtering effects, but solely in the spatial domain. We
adopt a combination of adding overwhelming spatial noise for breaking the
periodic noise pattern and deep image filtering to reconstruct the noise-free
fake images, and we name our method DeepNotch. Deep image filtering provides a
specialized filter for each pixel in the noisy image, producing filtered images
with high fidelity compared to their DeepFake counterparts. Moreover, we also
use the semantic information of the image to generate an adversarial guidance
map to add noise intelligently. Our large-scale evaluation on 3 representative
state-of-the-art DeepFake detection methods (tested on 16 types of DeepFakes)
has demonstrated that our technique significantly reduces the accuracy of these
3 fake image detection methods, 36.79% on average and up to 97.02% in the best
case.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Don't Explain Noise: Robust Counterfactuals for Randomized Ensembles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.14116v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.14116v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexandre Forel, Axel Parmentier, Thibaut Vidal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Counterfactual explanations describe how to modify a feature vector in order
to flip the outcome of a trained classifier. Obtaining robust counterfactual
explanations is essential to provide valid algorithmic recourse and meaningful
explanations. We study the robustness of explanations of randomized ensembles,
which are always subject to algorithmic uncertainty even when the training data
is fixed. We formalize the generation of robust counterfactual explanations as
a probabilistic problem and show the link between the robustness of ensemble
models and the robustness of base learners. We develop a practical method with
good empirical performance and support it with theoretical guarantees for
ensembles of convex base learners. Our results show that existing methods give
surprisingly low robustness: the validity of naive counterfactuals is below
$50\%$ on most data sets and can fall to $20\%$ on problems with many features.
In contrast, our method achieves high robustness with only a small increase in
the distance from counterfactual explanations to their initial observations.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Science and Game Theory
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Language Models Can Reduce Asymmetry in Information Markets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14443v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14443v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nasim Rahaman, Martin Weiss, Manuel Wüthrich, <span class="highlight-author">Yoshua Bengio</span>, Li Erran Li, Chris Pal, Bernhard Schölkopf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work addresses the buyer's inspection paradox for information markets.
The paradox is that buyers need to access information to determine its value,
while sellers need to limit access to prevent theft. To study this, we
introduce an open-source simulated digital marketplace where intelligent
agents, powered by language models, buy and sell information on behalf of
external participants. The central mechanism enabling this marketplace is the
agents' dual capabilities: they not only have the capacity to assess the
quality of privileged information but also come equipped with the ability to
forget. This ability to induce amnesia allows vendors to grant temporary access
to proprietary information, significantly reducing the risk of unauthorized
retention while enabling agents to accurately gauge the information's relevance
to specific queries or tasks. To perform well, agents must make rational
decisions, strategically explore the marketplace through generated sub-queries,
and synthesize answers from purchased information. Concretely, our experiments
(a) uncover biases in language models leading to irrational behavior and
evaluate techniques to mitigate these biases, (b) investigate how price affects
demand in the context of informational goods, and (c) show that inspection and
higher budgets both lead to higher quality outcomes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Emergent Dominance Hierarchies in Reinforcement Learning Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.12258v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.12258v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ram Rachum, Yonatan Nakar, Bill Tomlinson, Nitay Alon, Reuth Mirsky
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern Reinforcement Learning (RL) algorithms are able to outperform humans
in a wide variety of tasks. Multi-agent reinforcement learning (MARL) settings
present additional challenges, and successful cooperation in mixed-motive
groups of agents depends on a delicate balancing act between individual and
group objectives. Social conventions and norms, often inspired by human
institutions, are used as tools for striking this balance.
  In this paper, we examine a fundamental, well-studied social convention that
underlies cooperation in both animal and human societies: dominance
hierarchies.
  We adapt the ethological theory of dominance hierarchies to artificial
agents, borrowing the established terminology and definitions with as few
amendments as possible. We demonstrate that populations of RL agents, operating
without explicit programming or intrinsic rewards, can invent, learn, enforce,
and transmit a dominance hierarchy to new populations. The dominance
hierarchies that emerge have a similar structure to those studied in chickens,
mice, fish, and other species.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Designing Digital Voting Systems for Citizens: Achieving Fairness and
  Legitimacy in Participatory Budgeting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03501v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03501v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joshua C. Yang, Carina I. Hausladen, Dominik Peters, Evangelos Pournaras, Regula Hänggli Fricker, Dirk Helbing
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Participatory Budgeting (PB) has evolved into a key democratic instrument for
resource allocation in cities. Enabled by digital platforms, cities now have
the opportunity to let citizens directly propose and vote on urban projects,
using different voting input and aggregation rules. However, the choices cities
make in terms of the rules of their PB have often not been informed by academic
studies on voter behaviour and preferences. Therefore, this work presents the
results of behavioural experiments where participants were asked to vote in a
fictional PB setting. We identified approaches to designing PB voting that
minimise cognitive load and enhance the perceived fairness and legitimacy of
the digital process from the citizens' perspective. In our study, participants
preferred voting input formats that are more expressive (like rankings and
distributing points) over simpler formats (like approval voting). Participants
also indicated a desire for the budget to be fairly distributed across city
districts and project categories. Participants found the Method of Equal Shares
voting rule to be fairer than the conventional Greedy voting rule. These
findings offer actionable insights for digital governance, contributing to the
development of fairer and more transparent digital systems and collective
decision-making processes for citizens.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review in ACM Digital Government: Research and Practice</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Role of Transparency in Repeated First-Price Auctions with Unknown
  Valuations <span class="chip">STOC 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.09478v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.09478v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicolò Cesa-Bianchi, Tommaso Cesari, Roberto Colomboni, Federico Fusco, Stefano Leonardi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the problem of regret minimization for a single bidder in a sequence
of first-price auctions where the bidder discovers the item's value only if the
auction is won. Our main contribution is a complete characterization, up to
logarithmic factors, of the minimax regret in terms of the auction's
\emph{transparency}, which controls the amount of information on competing bids
disclosed by the auctioneer at the end of each auction. Our results hold under
different assumptions (stochastic, adversarial, and their smoothed variants) on
the environment generating the bidder's valuations and competing bids. These
minimax rates reveal how the interplay between transparency and the nature of
the environment affects how fast one can learn to bid optimally in first-price
auctions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at STOC 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Resolving social dilemmas with minimal reward transfer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.12928v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.12928v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Richard Willis, Yali Du, Joel Z Leibo, Michael Luck
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-agent cooperation is an important topic, and is particularly
challenging in mixed-motive situations where it does not pay to be nice to
others. Consequently, self-interested agents often avoid collective behaviour,
resulting in suboptimal outcomes for the group. In response, in this paper we
introduce a metric to quantify the disparity between what is rational for
individual agents and what is rational for the group, which we call the general
self-interest level. This metric represents the maximum proportion of
individual rewards that all agents can retain while ensuring that achieving
social welfare optimum becomes a dominant strategy. By aligning the individual
and group incentives, rational agents acting to maximise their own reward will
simultaneously maximise the collective reward. As agents transfer their rewards
to motivate others to consider their welfare, we diverge from traditional
concepts of altruism or prosocial behaviours. The general self-interest level
is a property of a game that is useful for assessing the propensity of players
to cooperate and understanding how features of a game impact this. We
illustrate the effectiveness of our method on several novel games
representations of social dilemmas with arbitrary numbers of players.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>34 pages, 13 tables, submitted to the Journal of Autonomous Agents
  and Multi-Agent Systems: Special Issue on Citizen-Centric AI Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-Player Zero-Sum Markov Games with Networked Separable Interactions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.09470v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.09470v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chanwoo Park, Kaiqing Zhang, Asuman Ozdaglar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study a new class of Markov games, \emph(multi-player) zero-sum Markov
Games} with \emph{Networked separable interactions} (zero-sum NMGs), to model
the local interaction structure in non-cooperative multi-agent sequential
decision-making. We define a zero-sum NMG as a model where {the payoffs of the
auxiliary games associated with each state are zero-sum and} have some
separable (i.e., polymatrix) structure across the neighbors over some
interaction network. We first identify the necessary and sufficient conditions
under which an MG can be presented as a zero-sum NMG, and show that the set of
Markov coarse correlated equilibrium (CCE) collapses to the set of Markov Nash
equilibrium (NE) in these games, in that the product of per-state
marginalization of the former for all players yields the latter. Furthermore,
we show that finding approximate Markov \emph{stationary} CCE in
infinite-horizon discounted zero-sum NMGs is \texttt{PPAD}-hard, unless the
underlying network has a ``star topology''. Then, we propose
fictitious-play-type dynamics, the classical learning dynamics in normal-form
games, for zero-sum NMGs, and establish convergence guarantees to Markov
stationary NE under a star-shaped network structure. Finally, in light of the
hardness result, we focus on computing a Markov \emph{non-stationary} NE and
provide finite-iteration guarantees for a series of value-iteration-based
algorithms. We also provide numerical experiments to corroborate our
theoretical results.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Theory
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Maximal $α$-Leakage for Quantum Privacy Mechanisms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14450v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14450v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bo-Yu Yang, Hsuan Yu, Hao-Chung Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, maximal $\alpha$-leakage is introduced to quantify how much a
quantum adversary can learn about any sensitive information of data upon
observing its disturbed version via a quantum privacy mechanism. We first show
that an adversary's maximal expected $\alpha$-gain using optimal measurement is
characterized by measured conditional R\'enyi entropy. This can be viewed as a
parametric generalization of K\"onig et al.'s famous guessing probability
formula [IEEE Trans. Inf. Theory, 55(9), 2009]. Then, we prove that the
$\alpha$-leakage and maximal $\alpha$-leakage for a quantum privacy mechanism
are determined by measured Arimoto information and measured R\'enyi capacity,
respectively. Various properties of maximal $\alpha$-leakage, such as data
processing inequality and composition property are established as well.
Moreover, we show that regularized $\alpha$-leakage and regularized maximal
$\alpha$-leakage for identical and independent quantum privacy mechanisms
coincide with $\alpha$-tilted sandwiched R\'enyi information and sandwiched
R\'enyi capacity, respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quantum Channel Simulation under Purified Distance is no more difficult
  than State Splitting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14416v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14416v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael X. Cao, Rahul Jain, Marco Tomamichel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Characterizing the minimal communication needed for the quantum channel
simulation is a fundamental task in the quantum information theory. In this
paper, we show that, under the purified distance, the quantum channel
simulation can be directly achieved via quantum state splitting without using a
technique known as the de Finetti reduction, and thus provide a pair of tighter
one-shot bounds. Using the bounds, we also recover the quantum reverse Shannon
theorem in a much simpler way.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Achievability Bound for Variable-Length Stop-Feedback Coding over the
  Gaussian Channel 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14360v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14360v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ioannis Papoutsidakis, Robert J. Piechocki, Angela Doufexi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Feedback holds a pivotal role in practical communication schemes, even though
it does not enhance channel capacity. Its main attribute includes adaptability
in transmission that allows for a higher rate of convergence of the error
probability to zero with respect to blocklength. Motivated by this fact, we
present a non-asymptotic achievability bound for variable-length coding with
stop-feedback. Specifically, a general achievability bound is derived, that
employs a random coding ensemble in combination with minimum distance decoding.
The general bound is particularized for the Gaussian channel. Numerical
evaluation of the bound confirms the significant value of feedback compared to
transmission with fixed blocklength coding and without feedback.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to the 2024 International Symposium on Information Theory
  (ISIT)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimal Second-Order Rates for Quantum Information Decoupling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14338v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14338v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu-Chen Shen, Li Gao, Hao-Chung Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we consider the standard quantum information decoupling, in
which Alice aims to decouple her system from the environment by local
operations and discarding some of her systems. To achieve an
$\varepsilon$-decoupling with trace distance as the error criterion, we
establish a near-optimal one-shot characterization for the largest dimension of
the remainder system in terms of the conditional
$(1-\varepsilon)$-hypothesis-testing entropy. When the underlying system is
independent and identically prepared, our result leads to the matched
second-order rate as well as the matched moderate deviation rate. As an
application, we find an achievability bound in entanglement distillation
protocol, where the objective is for Alice and Bob to transform their quantum
state to maximally entangled state with largest possible dimension using only
local operations and one-way classical communications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fundamentals of Delay-Doppler Communications: Practical Implementation
  and Extensions to OTFS 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14192v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14192v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuangyang Li, Peter Jung, Weijie Yuan, Zhiqiang Wei, Jinhong Yuan, Baoming Bai, Giuseppe Caire
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recently proposed orthogonal time frequency space (OTFS) modulation,
which is a typical Delay-Doppler (DD) communication scheme, has attracted
significant attention thanks to its appealing performance over doubly-selective
channels. In this paper, we present the fundamentals of general DD
communications from the viewpoint of the Zak transform. We start our study by
constructing DD domain basis functions aligning with the time-frequency
(TF)-consistency condition, which are globally quasi-periodic and locally
twisted-shifted. We unveil that these features are translated to unique signal
structures in both time and frequency, which are beneficial for communication
purposes. Then, we focus on the practical implementations of DD Nyquist
communications, where we show that rectangular windows achieve perfect DD
orthogonality, while truncated periodic signals can obtain sufficient DD
orthogonality. Particularly, smoothed rectangular window with excess bandwidth
can result in a slightly worse orthogonality but better pulse localization in
the DD domain. Furthermore, we present a practical pulse shaping framework for
general DD communications and derive the corresponding input-output relation
under various shaping pulses. Our numerical results agree with our derivations
and also demonstrate advantages of DD communications over conventional
orthogonal frequency-division multiplexing (OFDM).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On SAT information content, its polynomial-time solvability and fixed
  code algorithms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00947v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00947v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maciej Drozdowski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The amount of information in satisfiability problem (SAT) is considered. SAT
can be polynomial-time solvable when the solving algorithm holds an exponential
amount of information. It is also established that SAT Kolmogorov complexity is
constant. It is argued that the amount of information in SAT grows at least
exponentially with the size of the input instance. The amount of information in
SAT is compared with the amount of information in the fixed code algorithms and
generated over runtime.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 1 table, 0 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Postselected communication over quantum channels 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.02583v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.02583v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaiyuan Ji, Bartosz Regula, Mark M. Wilde
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The single-letter characterisation of the entanglement-assisted capacity of a
quantum channel is one of the seminal results of quantum information theory. In
this paper, we consider a modified communication scenario in which the receiver
is allowed an additional, `inconclusive' measurement outcome, and we employ an
error metric given by the error probability in decoding the transmitted message
conditioned on a conclusive measurement result. We call this setting
postselected communication and the ensuing highest achievable rates the
postselected capacities. Here, we provide a precise single-letter
characterisation of postselected capacities in the setting of entanglement
assistance as well as the more general nonsignalling assistance, establishing
that they are both equal to the channel's projective mutual information -- a
variant of mutual information based on the Hilbert projective metric. We do so
by establishing bounds on the one-shot postselected capacities, with a lower
bound that makes use of a postselected teleportation protocol and an upper
bound in terms of the postselected hypothesis testing relative entropy. As
such, we obtain fundamental limits on a channel's ability to communicate even
when this strong resource of postselection is allowed, implying limitations on
communication even when the receiver has access to postselected closed timelike
curves.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>38 pages, 5 figures, submitted to International Journal of Quantum
  Information (IJQI) as part of a special issue dedicated to Alexander S.
  Holevo on the occasion of his 80th birthday</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Constellation Shaping under Phase Noise Impairment for Sub-THz
  Communications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12433v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12433v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dileepa Marasinghe, Le Hang Nguyen, Jafar Mohammadi, Yejian Chen, Thorsten Wild, Nandana Rajatheva
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The large untapped spectrum in the sub-THz allows for ultra-high throughput
communication to realize many seemingly impossible applications in 6G. One of
the challenges in radio communications in sub-THz is the hardware impairments.
Specifically, phase noise is one key hardware impairment, which is accentuated
as we increase the frequency and bandwidth. Furthermore, the moderate output
power of the sub-THz power amplifier demands limits on peak to average power
ratio (PAPR) signal design. Single carrier frequency domain equalization
(SC-FDE) has been identified as a suitable candidate for sub-THz, although some
challenges such as phase noise and PAPR still remain to be tackled. In this
work, we design a phase noise robust, modest PAPR SC waveform by geometrically
shaping the constellation under practical conditions. We formulate the waveform
optimization problem in its augmented Lagrangian form and use a
back-propagation-inspired technique to obtain a constellation design that is
numerically robust to phase noise, while maintaining a relatively low PAPR
compared to the conventional waveforms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in IEEE ICC 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring Hybrid Active-Passive RIS-Aided MEC Systems: From the
  Mode-Switching Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08298v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08298v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Xie, Dong Li, Bowen Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mobile edge computing (MEC) has been regarded as a promising technique to
support latencysensitivity and computation-intensive serves. However, the low
offloading rate caused by the random channel fading characteristic becomes a
major bottleneck in restricting the performance of the MEC. Fortunately,
reconfigurable intelligent surface (RIS) can alleviate this problem since it
can boost both the spectrum- and energy- efficiency. Different from the
existing works adopting either fully active or fully passive RIS, we propose a
novel hybrid RIS in which reflecting units can flexibly switch between active
and passive modes. To achieve a tradeoff between the latency and energy
consumption, an optimization problem is formulated by minimizing the total
cost. In light of the intractability of the problem, we develop an alternating
optimization-based iterative algorithm by combining the successive convex
approximation method, the variable substitution, and the singular value
decomposition (SVD) to obtain sub-optimal solutions. Furthermore, in order to
gain more insight into the problem, we consider two special cases involving a
latency minimization problem and an energy consumption minimization problem,
and respectively analyze the tradeoff between the number of active and passive
units. Simulation results verify that the proposed algorithm can achieve
flexible mode switching and significantly outperforms existing algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The q-ary Gil<span class="highlight-title">bert</span>-Varshamov bound can be improved for all but finitely
  many positive integers q 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08727v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08727v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xue-Bin Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For any positive integer $q\geq 2$ and any real number $\delta\in(0,1)$, let
$\alpha_q(n,\delta n)$ denote the maximum size of a subset of $\mathbb{Z}_q^n$
with minimum Hamming distance at least $\delta n$, where
$\mathbb{Z}_q=\{0,1,\dotsc,q-1\}$ and $n\in\mathbb{N}$. The asymptotic rate
function is defined by $ R_q(\delta) =
\limsup_{n\rightarrow\infty}\frac{1}{n}\log_q\alpha_q(n,\delta n).$ The famous
$q$-ary asymptotic Gilbert-Varshamov bound, obtained in the 1950s, states that
\[ R_q(\delta) \geq 1 -
\delta\log_q(q-1)-\delta\log_q\frac{1}{\delta}-(1-\delta)\log_q\frac{1}{1-\delta}
\stackrel{\mathrm{def}}{=}R_\mathrm{GV}(\delta,q) \] for all positive integers
$q\geq 2$ and $0<\delta<1-q^{-1}$. In the case that $q$ is an even power of a
prime with $q\geq 49$, the $q$-ary Gilbert-Varshamov bound was firstly improved
by using algebraic geometry codes in the works of Tsfasman, Vladut, and Zink
and of Ihara in the 1980s. These algebraic geometry codes have been modified to
improve the $q$-ary Gilbert-Varshamov bound $R_\mathrm{GV}(\delta,q)$ at a
specific tangent point $\delta=\delta_0\in (0,1)$ of the curve
$R_\mathrm{GV}(\delta,q)$ for each given integer $q\geq 46$. However, the
$q$-ary Gilbert-Varshamov bound $R_\mathrm{GV}(\delta,q)$ at $\delta=1/2$,
i.e., $R_\mathrm{GV}(1/2,q)$, remains the largest known lower bound of
$R_q(1/2)$ for infinitely many positive integers $q$ which is a generic prime
and which is a generic non-prime-power integer. In this paper, by using codes
from geometry of numbers introduced by Lenstra in the 1980s, we prove that the
$q$-ary Gilbert-Varshamov bound $R_\mathrm{GV}(\delta,q)$ with $\delta\in(0,1)$
can be improved for all but finitely many positive integers $q$. It is shown
that the growth defined by $\eta(\delta)=
\liminf_{q\rightarrow\infty}\frac{1}{\log q}\log[1-\delta-R_q(\delta)]^{-1}$
for every $\delta\in(0,1)$ has actually a nontrivial lower bound.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>35 pages; more relevant and connected works are mentioned and
  referenced, with a suitable but slight adjustment for presentation in
  Abstract and Introduction</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Performance
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ iSpLib: A Library for Accelerating Graph Neural Networks using
  Auto-tuned Sparse Operations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14853v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14853v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Saidul Hoque Anik, Pranav Badhe, Rohit Gampa, Ariful Azad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Core computations in Graph Neural Network (GNN) training and inference are
often mapped to sparse matrix operations such as sparse-dense matrix
multiplication (SpMM). These sparse operations are harder to optimize by manual
tuning because their performance depends significantly on the sparsity of input
graphs, GNN models, and computing platforms. To address this challenge, we
present iSpLib, a PyTorch-based C++ library equipped with auto-tuned sparse
operations. iSpLib expedites GNN training with a cache-enabled backpropagation
that stores intermediate matrices in local caches. The library offers a
user-friendly Python plug-in that allows users to take advantage of our
optimized PyTorch operations out-of-the-box for any existing linear
algebra-based PyTorch implementation of popular GNNs (Graph Convolution
Network, GraphSAGE, Graph Inference Network, etc.) with only two lines of
additional code. We demonstrate that iSpLib obtains up to 27x overall training
speedup compared to the equivalent PyTorch 2.1.0 and PyTorch Geometric 2.4.0
implementations on the CPU. Our library is publicly available at
https://github.com/HipGraph/iSpLib (https://doi.org/10.5281/zenodo.10806511).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CASPER: Carbon-Aware Scheduling and Provisioning for Distributed Web
  Services 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14792v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14792v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abel Souza, Shruti Jasoria, Basundhara Chakrabarty, Alexander Bridgwater, Axel Lundberg, Filip Skogh, Ahmed Ali-Eldin, David Irwin, Prashant Shenoy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There has been a significant societal push towards sustainable practices,
including in computing. Modern interactive workloads such as geo-distributed
web-services exhibit various spatiotemporal and performance flexibility,
enabling the possibility to adapt the location, time, and intensity of
processing to align with the availability of renewable and low-carbon energy.
An example is a web application hosted across multiple cloud regions, each with
varying carbon intensity based on their local electricity mix. Distributed
load-balancing enables the exploitation of low-carbon energy through load
migration across regions, reducing web applications carbon footprint. In this
paper, we present CASPER, a carbon-aware scheduling and provisioning system
that primarily minimizes the carbon footprint of distributed web services while
also respecting their Service Level Objectives (SLO). We formulate CASPER as an
multi-objective optimization problem that considers both the variable carbon
intensity and latency constraints of the network. Our evaluation reveals the
significant potential of CASPER in achieving substantial reductions in carbon
emissions. Compared to baseline methods, CASPER demonstrates improvements of up
to 70% with no latency performance degradation.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-20T00:00:00Z">2024-03-20</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Machine Learning
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On <span class="highlight-title">Pretrain</span>ing Data Diversity for <span class="highlight-title">Self-Supervised</span> Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13808v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13808v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hasan Abed Al Kader Hammoud, Tuhin Das, Fabio Pizzati, Philip Torr, Adel Bibi, Bernard Ghanem
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We explore the impact of training with more diverse datasets, characterized
by the number of unique samples, on the performance of self-supervised learning
(SSL) under a fixed computational budget. Our findings consistently demonstrate
that increasing pretraining data diversity enhances SSL performance, albeit
only when the distribution distance to the downstream data is minimal. Notably,
even with an exceptionally large pretraining data diversity achieved through
methods like web crawling or diffusion-generated data, among other ways, the
distribution shift remains a challenge. Our experiments are comprehensive with
seven SSL methods using large-scale datasets such as ImageNet and YFCC100M
amounting to over 200 GPU days. Code and trained models will be available at
https://github.com/hammoudhasan/DiversitySSL .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Editing Massive Concepts in Text-to-Image Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13807v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13807v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianwei Xiong, Yue Wu, Enze Xie, Yue Wu, Zhenguo Li, Xihui Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-image diffusion models suffer from the risk of generating outdated,
copyrighted, incorrect, and biased content. While previous methods have
mitigated the issues on a small scale, it is essential to handle them
simultaneously in larger-scale real-world scenarios. We propose a two-stage
method, Editing Massive Concepts In Diffusion Models (EMCID). The first stage
performs memory optimization for each individual concept with dual
self-distillation from text alignment loss and diffusion noise prediction loss.
The second stage conducts massive concept editing with multi-layer, closed form
model editing. We further propose a comprehensive benchmark, named ImageNet
Concept Editing Benchmark (ICEB), for evaluating massive concept editing for
T2I models with two subtasks, free-form prompts, massive concept categories,
and extensive evaluation metrics. Extensive experiments conducted on our
proposed benchmark and previous benchmarks demonstrate the superior scalability
of EMCID for editing up to 1,000 concepts, providing a practical approach for
fast adjustment and re-deployment of T2I diffusion models in real-world
applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://silentview.github.io/EMCID/ . Code:
  https://github.com/SilentView/EMCID</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RAR: Retrieving And Ranking Augmented MLLMs for Visual Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13805v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13805v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyu Liu, Zeyi Sun, Yuhang Zang, Wei Li, Pan Zhang, Xiaoyi Dong, Yuanjun Xiong, Dahua Lin, Jiaqi Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  CLIP (Contrastive Language-Image Pre-training) uses contrastive learning from
noise image-text pairs to excel at recognizing a wide array of candidates, yet
its focus on broad associations hinders the precision in distinguishing subtle
differences among fine-grained items. Conversely, Multimodal Large Language
Models (MLLMs) excel at classifying fine-grained categories, thanks to their
substantial knowledge from pre-training on web-level corpora. However, the
performance of MLLMs declines with an increase in category numbers, primarily
due to growing complexity and constraints of limited context window size. To
synergize the strengths of both approaches and enhance the few-shot/zero-shot
recognition abilities for datasets characterized by extensive and fine-grained
vocabularies, this paper introduces RAR, a Retrieving And Ranking augmented
method for MLLMs. We initially establish a multi-modal retriever based on CLIP
to create and store explicit memory for different categories beyond the
immediate context window. During inference, RAR retrieves the top-k similar
results from the memory and uses MLLMs to rank and make the final predictions.
Our proposed approach not only addresses the inherent limitations in
fine-grained recognition but also preserves the model's comprehensive knowledge
base, significantly boosting accuracy across a range of vision-language
recognition tasks. Notably, our approach demonstrates a significant improvement
in performance on 5 fine-grained visual recognition benchmarks, 11 few-shot
image recognition datasets, and the 2 object detection datasets under the
zero-shot recognition setting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project: https://github.com/Liuziyu77/RAR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning from Models and Data for Visual Grounding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13804v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13804v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruozhen He, Paola Cascante-Bonilla, Ziyan Yang, Alexander C. Berg, Vicente Ordonez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce SynGround, a novel framework that combines data-driven learning
and knowledge transfer from various large-scale pretrained models to enhance
the visual grounding capabilities of a pretrained vision-and-language model.
The knowledge transfer from the models initiates the generation of image
descriptions through an image description generator. These descriptions serve
dual purposes: they act as prompts for synthesizing images through a
text-to-image generator, and as queries for synthesizing text, from which
phrases are extracted using a large language model. Finally, we leverage an
open-vocabulary object detector to generate synthetic bounding boxes for the
synthetic images and texts. We finetune a pretrained vision-and-language model
on this dataset by optimizing a mask-attention consistency objective that
aligns region annotations with gradient-based model explanations. The resulting
model improves the grounding capabilities of an off-the-shelf
vision-and-language model. Particularly, SynGround improves the pointing game
accuracy of ALBEF on the Flickr30k dataset from 79.38% to 87.26%, and on
RefCOCO+ Test A from 69.35% to 79.06% and on RefCOCO+ Test B from 53.77% to
63.67%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://catherine-r-he.github.io/SynGround/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ZigMa: Zigzag Mamba Diffusion Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13802v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13802v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vincent Tao Hu, Stefan Andreas Baumann, Ming Gui, Olga Grebenkova, Pingchuan Ma, Johannes Fischer, Bjorn Ommer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The diffusion model has long been plagued by scalability and quadratic
complexity issues, especially within transformer-based structures. In this
study, we aim to leverage the long sequence modeling capability of a
State-Space Model called Mamba to extend its applicability to visual data
generation. Firstly, we identify a critical oversight in most current
Mamba-based vision methods, namely the lack of consideration for spatial
continuity in the scan scheme of Mamba. Secondly, building upon this insight,
we introduce a simple, plug-and-play, zero-parameter method named Zigzag Mamba,
which outperforms Mamba-based baselines and demonstrates improved speed and
memory utilization compared to transformer-based baselines. Lastly, we
integrate Zigzag Mamba with the Stochastic Interpolant framework to investigate
the scalability of the model on large-resolution visual datasets, such as
FacesHQ $1024\times 1024$ and UCF101, MultiModal-CelebA-HQ, and MS COCO
$256\times 256$. Code will be released at https://taohu.me/zigma/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://taohu.me/zigma/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hierarchical NeuroSymbolic Approach for Action Quality Assessment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13798v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13798v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lauren Okamoto, Paritosh Parmar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Action quality assessment (AQA) applies computer vision to quantitatively
assess the performance or execution of a human action. Current AQA approaches
are end-to-end neural models, which lack transparency and tend to be biased
because they are trained on subjective human judgements as ground-truth. To
address these issues, we introduce a neuro-symbolic paradigm for AQA, which
uses neural networks to abstract interpretable symbols from video data and
makes quality assessments by applying rules to those symbols. We take diving as
the case study. We found that domain experts prefer our system and find it more
informative than purely neural approaches to AQA in diving. Our system also
achieves state-of-the-art action recognition and temporal segmentation, and
automatically generates a detailed report that breaks the dive down into its
elements and provides objective scoring with visual evidence. As verified by a
group of domain experts, this report may be used to assist judges in scoring,
help train judges, and provide feedback to divers. We will open-source all of
our annotated training data and code for ease of reproducibility.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bridge the Modality and Capacity Gaps in Vision-Language Model Selection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13797v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13797v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chao Yi, De-Chuan Zhan, Han-Jia Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision Language Models (VLMs) excel in zero-shot image classification by
pairing images with textual category names. The expanding variety of
Pre-Trained VLMs enhances the likelihood of identifying a suitable VLM for
specific tasks. Thus, a promising zero-shot image classification strategy is
selecting the most appropriate Pre-Trained VLM from the VLM Zoo, relying solely
on the text data of the target dataset without access to the dataset's images.
In this paper, we analyze two inherent challenges in assessing the ability of a
VLM in this Language-Only VLM selection: the "Modality Gap" -- the disparity in
VLM's embeddings across two different modalities, making text a less reliable
substitute for images; and the "Capability Gap" -- the discrepancy between the
VLM's overall ranking and its ranking for target dataset, hindering direct
prediction of a model's dataset-specific performance from its general
performance. We propose VLM Selection With gAp Bridging (SWAB) to mitigate the
negative impact of these two gaps. SWAB first adopts optimal transport to
capture the relevance between open-source datasets and target dataset with a
transportation matrix. It then uses this matrix to transfer useful statistics
of VLMs from open-source datasets to the target dataset for bridging those two
gaps and enhancing the VLM's capacity estimation for VLM selection. Experiments
across various VLMs and image classification datasets validate SWAB's
effectiveness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating Frontier Models for Dangerous Capabilities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13793v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13793v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mary Phuong, Matthew Aitchison, Elliot Catt, Sarah Cogan, Alexandre Kaskasoli, Victoria Krakovna, David Lindner, Matthew Rahtz, Yannis Assael, Sarah Hodkinson, Heidi Howard, Tom Lieberum, Ramana Kumar, Maria Abi Raad, Albert Webson, Lewis Ho, Sharon Lin, Sebastian Farquhar, Marcus Hutter, Gregoire Deletang, Anian Ruoss, Seliem El-Sayed, Sasha Brown, Anca Dragan, Rohin Shah, Allan Dafoe, Toby Shevlane
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To understand the risks posed by a new AI system, we must understand what it
can and cannot do. Building on prior work, we introduce a programme of new
"dangerous capability" evaluations and pilot them on Gemini 1.0 models. Our
evaluations cover four areas: (1) persuasion and deception; (2) cyber-security;
(3) self-proliferation; and (4) self-reasoning. We do not find evidence of
strong dangerous capabilities in the models we evaluated, but we flag early
warning signs. Our goal is to help advance a rigorous science of dangerous
capability evaluation, in preparation for future models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RewardBench: Evaluating Reward Models for Language Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13787v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13787v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nathan Lambert, Valentina Pyatkin, Jacob Morrison, LJ Miranda, Bill Yuchen Lin, Khyathi Chandu, Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi, Noah A. Smith, Hannaneh Hajishirzi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reward models (RMs) are at the crux of successful RLHF to align pretrained
models to human preferences, yet there has been relatively little study that
focuses on evaluation of those reward models. Evaluating reward models presents
an opportunity to understand the opaque technologies used for alignment of
language models and which values are embedded in them. To date, very few
descriptors of capabilities, training methods, or open-source reward models
exist. In this paper, we present RewardBench, a benchmark dataset and code-base
for evaluation, to enhance scientific understanding of reward models. The
RewardBench dataset is a collection of prompt-win-lose trios spanning chat,
reasoning, and safety, to benchmark how reward models perform on challenging,
structured and out-of-distribution queries. We created specific comparison
datasets for RMs that have subtle, but verifiable reasons (e.g. bugs, incorrect
facts) why one answer should be preferred to another. On the RewardBench
leaderboard, we evaluate reward models trained with a variety of methods, such
as the direct MLE training of classifiers and the implicit reward modeling of
Direct Preference Optimization (DPO), and on a spectrum of datasets. We present
many findings on propensity for refusals, reasoning limitations, and
instruction following shortcomings of various reward models towards a better
understanding of the RLHF process.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>40 pages, 19 figures, 12 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards an extension of Fault Trees in the Predictive Maintenance
  Scenario 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13785v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13785v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Roberta De Fazio, Stefano Marrone, Laura Verde, Vincenzo Reccia, Paolo Valletta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One of the most appreciated features of Fault Trees (FTs) is their
simplicity, making them fit into industrial processes. As such processes evolve
in time, considering new aspects of large modern systems, modelling techniques
based on FTs have adapted to these needs. This paper proposes an extension of
FTs to take into account the problem of Predictive Maintenance, one of the
challenges of the modern dependability field of study. The paper sketches the
Predictive Fault Tree language and proposes some use cases to support their
modelling and analysis in concrete industrial settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>S. Bernardi, T. Zoppi (Editors), Fast Abstracts and Student Forum
  Proceedings - EDCC 2024 - 19th European Dependable Computing Conference,
  Leuven, Belgium, 8-11 April 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Model Openness Framework: Promoting Completeness and Openness for
  Reproducibility, Transparency and Usability in AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13784v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13784v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matt White, Ibrahim Haddad, Cailean Osborne, Xiao-Yang Liu, Ahmed Abdelmonsef, Sachin Varghese
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative AI (GAI) offers unprecedented possibilities but its
commercialization has raised concerns about transparency, reproducibility,
bias, and safety. Many "open-source" GAI models lack the necessary components
for full understanding and reproduction, and some use restrictive licenses, a
practice known as "openwashing." We propose the Model Openness Framework (MOF),
a ranked classification system that rates machine learning models based on
their completeness and openness, following principles of open science, open
source, open data, and open access. The MOF requires specific components of the
model development lifecycle to be included and released under appropriate open
licenses. This framework aims to prevent misrepresentation of models claiming
to be open, guide researchers and developers in providing all model components
under permissive licenses, and help companies, academia, and hobbyists identify
models that can be safely adopted without restrictions. Wide adoption of the
MOF will foster a more open AI ecosystem, accelerating research, innovation,
and adoption.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>45 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sparse Implementation of Versatile Graph-Informed Layers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13781v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13781v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francesco Della Santa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks (GNNs) have emerged as effective tools for learning
tasks on graph-structured data. Recently, Graph-Informed (GI) layers were
introduced to address regression tasks on graph nodes, extending their
applicability beyond classic GNNs. However, existing implementations of GI
layers lack efficiency due to dense memory allocation. This paper presents a
sparse implementation of GI layers, leveraging the sparsity of adjacency
matrices to reduce memory usage significantly. Additionally, a versatile
general form of GI layers is introduced, enabling their application to subsets
of graph nodes. The proposed sparse implementation improves the concrete
computational efficiency and scalability of the GI layers, permitting to build
deeper Graph-Informed Neural Networks (GINNs) and facilitating their
scalability to larger graphs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Describe-and-Dissect: Interpreting Neurons in Vision Networks with
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13771v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13771v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicholas Bai, Rahul A. Iyer, Tuomas Oikarinen, Tsui-Wei Weng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose Describe-and-Dissect (DnD), a novel method to
describe the roles of hidden neurons in vision networks. DnD utilizes recent
advancements in multimodal deep learning to produce complex natural language
descriptions, without the need for labeled training data or a predefined set of
concepts to choose from. Additionally, DnD is training-free, meaning we don't
train any new models and can easily leverage more capable general purpose
models in the future. We have conducted extensive qualitative and quantitative
analysis to show that DnD outperforms prior work by providing higher quality
neuron descriptions. Specifically, our method on average provides the highest
quality labels and is more than 2 times as likely to be selected as the best
explanation for a neuron than the best baseline.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Principled Representation Learning from Videos for Reinforcement
  Learning <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13765v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13765v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dipendra Misra, Akanksha Saran, Tengyang Xie, Alex Lamb, John Langford
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study pre-training representations for decision-making using video data,
which is abundantly available for tasks such as game agents and software
testing. Even though significant empirical advances have been made on this
problem, a theoretical understanding remains absent. We initiate the
theoretical investigation into principled approaches for representation
learning and focus on learning the latent state representations of the
underlying MDP using video data. We study two types of settings: one where
there is iid noise in the observation, and a more challenging setting where
there is also the presence of exogenous noise, which is non-iid noise that is
temporally correlated, such as the motion of people or cars in the background.
We study three commonly used approaches: autoencoding, temporal contrastive
learning, and forward modeling. We prove upper bounds for temporal contrastive
learning and forward modeling in the presence of only iid noise. We show that
these approaches can learn the latent state and use it to do efficient
downstream RL with polynomial sample complexity. When exogenous noise is also
present, we establish a lower bound result showing that the sample complexity
of learning from video data can be exponentially worse than learning from
action-labeled trajectory data. This partially explains why reinforcement
learning with video pre-training is hard. We evaluate these representational
learning methods in two visual domains, yielding results that are consistent
with our theoretical findings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024 Spotlight Conference Paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Weisfeiler and Leman Go Loopy: A New Hierarchy for Graph
  Representational Learning <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13749v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13749v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raffaele Paolino, Sohir Maskey, Pascal Welke, Gitta Kutyniok
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce $r$-loopy Weisfeiler-Leman ($r$-$\ell{}$WL), a novel hierarchy
of graph isomorphism tests and a corresponding GNN framework, $r$-$\ell{}$MPNN,
that can count cycles up to length $r + 2$. Most notably, we show that
$r$-$\ell{}$WL can count homomorphisms of cactus graphs. This strictly extends
classical 1-WL, which can only count homomorphisms of trees and, in fact, is
incomparable to $k$-WL for any fixed $k$. We empirically validate the
expressive and counting power of the proposed $r$-$\ell{}$MPNN on several
synthetic datasets and present state-of-the-art predictive performance on
various real-world datasets. The code is available at
https://github.com/RPaolino/loopy
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICLR 2024 Workshop on Bridging the Gap Between Practice
  and Theory in Deep Learning</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Ordering of Divergences for Variational Inference with Factorized
  Gaussian Approximations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13748v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13748v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Charles C. Margossian, Loucas Pillaud-Vivien, Lawrence K. Saul
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given an intractable distribution $p$, the problem of variational inference
(VI) is to compute the best approximation $q$ from some more tractable family
$\mathcal{Q}$. Most commonly the approximation is found by minimizing a
Kullback-Leibler (KL) divergence. However, there exist other valid choices of
divergences, and when $\mathcal{Q}$ does not contain~$p$, each divergence
champions a different solution. We analyze how the choice of divergence affects
the outcome of VI when a Gaussian with a dense covariance matrix is
approximated by a Gaussian with a diagonal covariance matrix. In this setting
we show that different divergences can be \textit{ordered} by the amount that
their variational approximations misestimate various measures of uncertainty,
such as the variance, precision, and entropy. We also derive an impossibility
theorem showing that no two of these measures can be simultaneously matched by
a factorized approximation; hence, the choice of divergence informs which
measure, if any, is correctly estimated. Our analysis covers the KL divergence,
the R\'enyi divergences, and a score-based divergence that compares $\nabla\log
p$ and $\nabla\log q$. We empirically evaluate whether these orderings hold
when VI is used to approximate non-Gaussian distributions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uncertainty-Aware Explanations Through Probabilistic Self-Explainable
  Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13740v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13740v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jon Vadillo, Roberto Santana, Jose A. Lozano, Marta Kwiatkowska
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The lack of transparency of Deep Neural Networks continues to be a limitation
that severely undermines their reliability and usage in high-stakes
applications. Promising approaches to overcome such limitations are
Prototype-Based Self-Explainable Neural Networks (PSENNs), whose predictions
rely on the similarity between the input at hand and a set of prototypical
representations of the output classes, offering therefore a deep, yet
transparent-by-design, architecture. So far, such models have been designed by
considering pointwise estimates for the prototypes, which remain fixed after
the learning phase of the model. In this paper, we introduce a probabilistic
reformulation of PSENNs, called Prob-PSENN, which replaces point estimates for
the prototypes with probability distributions over their values. This provides
not only a more flexible framework for an end-to-end learning of prototypes,
but can also capture the explanatory uncertainty of the model, which is a
missing feature in previous approaches. In addition, since the prototypes
determine both the explanation and the prediction, Prob-PSENNs allow us to
detect when the model is making uninformed or uncertain predictions, and to
obtain valid explanations for them. Our experiments demonstrate that
Prob-PSENNs provide more meaningful and robust explanations than their
non-probabilistic counterparts, thus enhancing the explainability and
reliability of the models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reinforcement Learning for Online Testing of Autonomous Driving Systems:
  a Replication and Extension Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13729v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13729v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luca Giamattei, Matteo Biagiola, Roberto Pietrantuono, Stefano Russo, Paolo Tonella
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In a recent study, Reinforcement Learning (RL) used in combination with
many-objective search, has been shown to outperform alternative techniques
(random search and many-objective search) for online testing of Deep Neural
Network-enabled systems. The empirical evaluation of these techniques was
conducted on a state-of-the-art Autonomous Driving System (ADS). This work is a
replication and extension of that empirical study. Our replication shows that
RL does not outperform pure random test generation in a comparison conducted
under the same settings of the original study, but with no confounding factor
coming from the way collisions are measured. Our extension aims at eliminating
some of the possible reasons for the poor performance of RL observed in our
replication: (1) the presence of reward components providing contrasting or
useless feedback to the RL agent; (2) the usage of an RL algorithm (Q-learning)
which requires discretization of an intrinsically continuous state space.
Results show that our new RL agent is able to converge to an effective policy
that outperforms random testing. Results also highlight other possible
improvements, which open to further investigations on how to best leverage RL
for online ADS testing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ M-HOF-Opt: Multi-Objective Hierarchical Output Feedback Optimization via
  Multiplier Induced Loss Landscape Scheduling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13728v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13728v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xudong Sun, Nutan Chen, Alexej Gossmann, Yu Xing, Carla Feistner, Emilio Dorigatt, Felix Drost, Daniele Scarcella, Lisa Beer, Carsten Marr
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When a neural network parameterized loss function consists of many terms, the
combinatorial choice of weight multipliers during the optimization process
forms a challenging problem. To address this, we proposed a probabilistic
graphical model (PGM) for the joint model parameter and multiplier evolution
process, with a hypervolume based likelihood that promotes multi-objective
descent of each loss term. The corresponding parameter and multiplier
estimation as a sequential decision process is then cast into an optimal
control problem, where the multi-objective descent goal is dispatched
hierarchically into a series of constraint optimization sub-problems. The
sub-problem constraint automatically adapts itself according to Pareto
dominance and serves as the setpoint for the low level multiplier controller to
schedule loss landscapes via output feedback of each loss term. Our method is
multiplier-free and operates at the timescale of epochs, thus saves tremendous
computational resources compared to full training cycle multiplier tuning. We
applied it to domain invariant variational auto-encoding with 6 loss terms on
the PACS domain generalization task, and observed robust performance across a
range of controller hyperparameters, as well as different multiplier initial
conditions, outperforming other multiplier scheduling methods. We offered
modular implementation of our method, admitting custom definition of many loss
terms for applying our multi-objective hierarchical output feedback training
scheme to other deep learning fields.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Probabilistic Forecasting with Stochastic Interpolants and Föllmer
  Processes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13724v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13724v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Chen, Mark Goldstein, Mengjian Hua, Michael S. Albergo, Nicholas M. Boffi, Eric Vanden-Eijnden
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a framework for probabilistic forecasting of dynamical systems
based on generative modeling. Given observations of the system state over time,
we formulate the forecasting problem as sampling from the conditional
distribution of the future system state given its current state. To this end,
we leverage the framework of stochastic interpolants, which facilitates the
construction of a generative model between an arbitrary base distribution and
the target. We design a fictitious, non-physical stochastic dynamics that takes
as initial condition the current system state and produces as output a sample
from the target conditional distribution in finite time and without bias. This
process therefore maps a point mass centered at the current state onto a
probabilistic ensemble of forecasts. We prove that the drift coefficient
entering the stochastic differential equation (SDE) achieving this task is
non-singular, and that it can be learned efficiently by square loss regression
over the time-series data. We show that the drift and the diffusion
coefficients of this SDE can be adjusted after training, and that a specific
choice that minimizes the impact of the estimation error gives a F\"ollmer
process. We highlight the utility of our approach on several complex,
high-dimensional forecasting problems, including stochastically forced
Navier-Stokes and video prediction on the KTH and CLEVRER datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving the Adaptive Moment Estimation (ADAM) stochastic optimizer
  through an Implicit-Explicit (IMEX) time-stepping approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13704v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13704v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhinab Bhattacharjee, Andrey A. Popov, Arash Sarshar, Adrian Sandu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Adam optimizer, often used in Machine Learning for neural network
training, corresponds to an underlying ordinary differential equation (ODE) in
the limit of very small learning rates. This work shows that the classical Adam
algorithm is a first order implicit-explicit (IMEX) Euler discretization of the
underlying ODE. Employing the time discretization point of view, we propose new
extensions of the Adam scheme obtained by using higher order IMEX methods to
solve the ODE. Based on this approach, we derive a new optimization algorithm
for neural network training that performs better than classical Adam on several
regression and classification problems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ What Matters for Active Texture Recognition With Vision-Based Tactile
  Sensors <span class="chip">ICRA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13701v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13701v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alina Böhm, Tim Schneider, Boris Belousov, Alap Kshirsagar, Lisa Lin, Katja Doerschner, Knut Drewing, Constantin A. Rothkopf, Jan Peters
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper explores active sensing strategies that employ vision-based
tactile sensors for robotic perception and classification of fabric textures.
We formalize the active sampling problem in the context of tactile fabric
recognition and provide an implementation of information-theoretic exploration
strategies based on minimizing predictive entropy and variance of probabilistic
models. Through ablation studies and human experiments, we investigate which
components are crucial for quick and reliable texture recognition. Along with
the active sampling strategies, we evaluate neural network architectures,
representations of uncertainty, influence of data augmentation, and dataset
variability. By evaluating our method on a previously published Active Clothing
Perception Dataset and on a real robotic system, we establish that the choice
of the active exploration strategy has only a minor influence on the
recognition accuracy, whereas data augmentation and dropout rate play a
significantly larger role. In a comparison study, while humans achieve 66.9%
recognition accuracy, our best approach reaches 90.0% in under 5 touches,
highlighting that vision-based tactile sensors are highly effective for fabric
texture recognition.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 9 figures, accepted at 2024 IEEE International Conference on
  Robotics and Automation (ICRA)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Loss Regularizing Robotic Terrain Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13695v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13695v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shakti Deo Kumar, Sudhanshu Tripathi, Krishna Ujjwal, Sarvada Sakshi Jha, Suddhasil De
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Locomotion mechanics of legged robots are suitable when pacing through
difficult terrains. Recognising terrains for such robots are important to fully
yoke the versatility of their movements. Consequently, robotic terrain
classification becomes significant to classify terrains in real time with high
accuracy. The conventional classifiers suffer from overfitting problem, low
accuracy problem, high variance problem, and not suitable for live dataset. On
the other hand, classifying a growing dataset is difficult for convolution
based terrain classification. Supervised recurrent models are also not
practical for this classification. Further, the existing recurrent
architectures are still evolving to improve accuracy of terrain classification
based on live variable-length sensory data collected from legged robots. This
paper proposes a new semi-supervised method for terrain classification of
legged robots, avoiding preprocessing of long variable-length dataset. The
proposed method has a stacked Long Short-Term Memory architecture, including a
new loss regularization. The proposed method solves the existing problems and
improves accuracy. Comparison with the existing architectures show the
improvements.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preliminary draft of the work published in IEEE conference 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PARAMANU-AYN: An Efficient Novel Generative and Instruction-tuned
  Language Model for Indian Legal Case Documents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13681v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13681v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mitodru Niyogi, Arnab Bhattacharya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present PARAMANU-AYN, a language model based exclusively on
case documents of the Supreme Court of India, the Constitution of India, and
the Indian Penal Code. The novel Auto Regressive (AR) decoder based model is
pretrained from scratch at a context size of 8192. We evaluated our pretrained
legal model on perplexity metrics. We also instruction-tuned our pretrained
model on a set of 10,763 instructions covering various legal tasks such as
legal reasoning, judgement explanation, legal clause generation, legal
drafting, legal contract drafting, case summarization, constitutional
question-answering, etc. We also evaluated the responses of prompts for
instruction-tuned models by GPT-3.5-Turbo on clarity, relevance, completeness,
and legal reasoning metrics in a scale of 10. Our model can be run on CPU and
achieved 42.46 tokens/sec CPU inference speed. We found that our models,
despite not being pretrained on legal books, various legal contracts, and legal
documents, were able to learn the domain knowledge required for drafting
various legal contracts and legal clauses, and generalize to draft legal
contracts and legal clauses with limited instruction tuning. Hence, we conclude
that for a strong domain-specialized generative language model (such as legal),
very large amounts of data are not required to develop models from scratch. We
believe that this work is the first attempt to make a dedicated generative
legal language model from scratch for Indian Supreme Court jurisdiction or in
legal NLP overall. We plan to release our Paramanu-Ayn model at
https://www.bharatgpts.com.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Machine Learning Optimized Approach for Parameter Selection in MESHFREE
  Simulations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13672v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13672v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paulami Banerjee, Mohan Padmanabha, Chaitanya Sanghavi, Isabel Michel, Simone Gramsch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Meshfree simulation methods are emerging as compelling alternatives to
conventional mesh-based approaches, particularly in the fields of Computational
Fluid Dynamics (CFD) and continuum mechanics. In this publication, we provide a
comprehensive overview of our research combining Machine Learning (ML) and
Fraunhofer's MESHFREE software (www.meshfree.eu), a powerful tool utilizing a
numerical point cloud in a Generalized Finite Difference Method (GFDM). This
tool enables the effective handling of complex flow domains, moving geometries,
and free surfaces, while allowing users to finely tune local refinement and
quality parameters for an optimal balance between computation time and results
accuracy. However, manually determining the optimal parameter combination poses
challenges, especially for less experienced users. We introduce a novel
ML-optimized approach, using active learning, regression trees, and
visualization on MESHFREE simulation data, demonstrating the impact of input
combinations on results quality and computation time. This research contributes
valuable insights into parameter optimization in meshfree simulations,
enhancing accessibility and usability for a broader user base in scientific and
engineering applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multimodal Variational Autoencoder for Low-cost Cardiac Hemodynamics
  Instability Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13658v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13658v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammod N. I. Suvon, Prasun C. Tripathi, Wenrui Fan, Shuo Zhou, Xianyuan Liu, Samer Alabed, Venet Osmani, Andrew J. Swift, Chen Chen, Haiping Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in non-invasive detection of cardiac hemodynamic
instability (CHDI) primarily focus on applying machine learning techniques to a
single data modality, e.g. cardiac magnetic resonance imaging (MRI). Despite
their potential, these approaches often fall short especially when the size of
labeled patient data is limited, a common challenge in the medical domain.
Furthermore, only a few studies have explored multimodal methods to study CHDI,
which mostly rely on costly modalities such as cardiac MRI and echocardiogram.
In response to these limitations, we propose a novel multimodal variational
autoencoder ($\text{CardioVAE}_\text{X,G}$) to integrate low-cost chest X-ray
(CXR) and electrocardiogram (ECG) modalities with pre-training on a large
unlabeled dataset. Specifically, $\text{CardioVAE}_\text{X,G}$ introduces a
novel tri-stream pre-training strategy to learn both shared and
modality-specific features, thus enabling fine-tuning with both unimodal and
multimodal datasets. We pre-train $\text{CardioVAE}_\text{X,G}$ on a large,
unlabeled dataset of $50,982$ subjects from a subset of MIMIC database and then
fine-tune the pre-trained model on a labeled dataset of $795$ subjects from the
ASPIRE registry. Comprehensive evaluations against existing methods show that
$\text{CardioVAE}_\text{X,G}$ offers promising performance (AUROC $=0.79$ and
Accuracy $=0.77$), representing a significant step forward in non-invasive
prediction of CHDI. Our model also excels in producing fine interpretations of
predictions directly associated with clinical features, thereby supporting
clinical decision-making.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Policy Bifurcation in Safe Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12847v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12847v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenjun Zou, Yao Lv, Jie Li, Yujie Yang, Shengbo Eben Li, Jingliang Duan, Xianyuan Zhan, Jingjing Liu, Yaqin Zhang, Keqiang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Safe reinforcement learning (RL) offers advanced solutions to constrained
optimal control problems. Existing studies in safe RL implicitly assume
continuity in policy functions, where policies map states to actions in a
smooth, uninterrupted manner; however, our research finds that in some
scenarios, the feasible policy should be discontinuous or multi-valued,
interpolating between discontinuous local optima can inevitably lead to
constraint violations. We are the first to identify the generating mechanism of
such a phenomenon, and employ topological analysis to rigorously prove the
existence of policy bifurcation in safe RL, which corresponds to the
contractibility of the reachable tuple. Our theorem reveals that in scenarios
where the obstacle-free state space is non-simply connected, a feasible policy
is required to be bifurcated, meaning its output action needs to change
abruptly in response to the varying state. To train such a bifurcated policy,
we propose a safe RL algorithm called multimodal policy optimization (MUPO),
which utilizes a Gaussian mixture distribution as the policy output. The
bifurcated behavior can be achieved by selecting the Gaussian component with
the highest mixing coefficient. Besides, MUPO also integrates spectral
normalization and forward KL divergence to enhance the policy's capability of
exploring different modes. Experiments with vehicle control tasks show that our
algorithm successfully learns the bifurcated policy and ensures satisfying
safety, while a continuous policy suffers from inevitable constraint
violations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MELTing point: Mobile Evaluation of Language <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12844v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12844v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stefanos Laskaridis, Kleomenis Katevas, Lorenzo Minto, Hamed Haddadi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers have revolutionized the machine learning landscape, gradually
making their way into everyday tasks and equipping our computers with ``sparks
of intelligence''. However, their runtime requirements have prevented them from
being broadly deployed on mobile. As personal devices become increasingly
powerful and prompt privacy becomes an ever more pressing issue, we explore the
current state of mobile execution of Large Language Models (LLMs). To achieve
this, we have created our own automation infrastructure, MELT, which supports
the headless execution and benchmarking of LLMs on device, supporting different
models, devices and frameworks, including Android, iOS and Nvidia Jetson
devices. We evaluate popular instruction fine-tuned LLMs and leverage different
frameworks to measure their end-to-end and granular performance, tracing their
memory and energy requirements along the way.
  Our analysis is the first systematic study of on-device LLM execution,
quantifying performance, energy efficiency and accuracy across various
state-of-the-art models and showcases the state of on-device intelligence in
the era of hyperscale models. Results highlight the performance heterogeneity
across targets and corroborates that LLM inference is largely memory-bound.
Quantization drastically reduces memory requirements and renders execution
viable, but at a non-negligible accuracy cost. Drawing from its energy
footprint and thermal behavior, the continuous execution of LLMs remains
elusive, as both factors negatively affect user experience. Last, our
experience shows that the ecosystem is still in its infancy, and algorithmic as
well as hardware breakthroughs can significantly shift the execution cost. We
expect NPU acceleration, and framework-hardware co-design to be the biggest bet
towards efficient standalone execution, with the alternative of offloading
tailored towards edge deployments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Expressive Power of <span class="highlight-title">Transformer</span>s with Chain of Thought <span class="chip">ICLR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07923v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07923v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        William Merrill, Ashish Sabharwal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent theoretical work has identified surprisingly simple reasoning
problems, such as checking if two nodes in a graph are connected or simulating
finite-state machines, that are provably unsolvable by standard transformers
that answer immediately after reading their input. However, in practice,
transformers' reasoning can be improved by allowing them to use a "chain of
thought" or "scratchpad", i.e., generate and condition on a sequence of
intermediate tokens before answering. Motivated by this, we ask: Does such
intermediate generation fundamentally extend the computational power of a
decoder-only transformer? We show that the answer is yes, but the amount of
increase depends crucially on the amount of intermediate generation. For
instance, we find that transformer decoders with a logarithmic number of
decoding steps (w.r.t. the input length) push the limits of standard
transformers only slightly, while a linear number of decoding steps, assuming a
slight generalization to standard pre-norm, adds a clear new ability (under
standard complexity conjectures): recognizing all regular languages. Our
results also imply that linear steps keep transformer decoders within
context-sensitive languages, and polynomial steps with generalized pre-norm
make them recognize exactly the class of polynomial-time solvable problems --
the first exact characterization of a type of transformers in terms of standard
complexity classes. Together, our results provide a nuanced framework for
understanding how the length of a transformer's chain of thought or scratchpad
impacts its reasoning power.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9-page preprint. Updated March 20 after ICLR acceptance</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Ada-NAV: Adaptive Trajectory Length-Based Sample Efficient Policy
  Learning for Robotic Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.06192v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.06192v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bhrij Patel, Kasun Weerakoon, Wesley A. Suttle, Alec Koppel, Brian M. Sadler, Tianyi Zhou, Amrit Singh Bedi, Dinesh Manocha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Trajectory length stands as a crucial hyperparameter within reinforcement
learning (RL) algorithms, significantly contributing to the sample inefficiency
in robotics applications. Motivated by the pivotal role trajectory length plays
in the training process, we introduce Ada-NAV, a novel adaptive trajectory
length scheme designed to enhance the training sample efficiency of RL
algorithms in robotic navigation tasks. Unlike traditional approaches that
treat trajectory length as a fixed hyperparameter, we propose to dynamically
adjust it based on the entropy of the underlying navigation policy.
Interestingly, Ada-NAV can be applied to both existing on-policy and off-policy
RL methods, which we demonstrate by empirically validating its efficacy on
three popular RL methods: REINFORCE, Proximal Policy Optimization (PPO), and
Soft Actor-Critic (SAC). We demonstrate through simulated and real-world
robotic experiments that Ada-NAV outperforms conventional methods that employ
constant or randomly sampled trajectory lengths. Specifically, for a fixed
sample budget, Ada-NAV achieves an 18\% increase in navigation success rate, a
20-38\% reduction in navigation path length, and a 9.32\% decrease in elevation
costs. Furthermore, we showcase the versatility of Ada-NAV by integrating it
with the Clearpath Husky robot, illustrating its applicability in complex
outdoor environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 9 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Universal consistency of the $k$-NN rule in metric spaces and Nagata
  dimension. II 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.17282v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.17282v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sushma Kumari, Vladimir G. Pestov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We continue to investigate the $k$ nearest neighbour ($k$-NN) learning rule
in complete separable metric spaces. Thanks to the results of C\'erou and
Guyader (2006) and Preiss (1983), this rule is known to be universally
consistent in every such metric space that is sigma-finite dimensional in the
sense of Nagata. Here we show that the rule is strongly universally consistent
in such spaces in the absence of ties. Under the tie-breaking strategy applied
by Devroye, Gy\"{o}rfi, Krzy\.{z}ak, and Lugosi (1994) in the Euclidean
setting, we manage to show the strong universal consistency in non-Archimedian
metric spaces (that is, those of Nagata dimension zero). Combining the theorem
of C\'erou and Guyader with results of Assouad and Quentin de Gromard (2006),
one deduces that the $k$-NN rule is universally consistent in metric spaces
having finite dimension in the sense of de Groot. In particular, the $k$-NN
rule is universally consistent in the Heisenberg group which is not
sigma-finite dimensional in the sense of Nagata as follows from an example
independently constructed by Kor\'anyi and Reimann (1995) and Sawyer and
Wheeden (1992).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Latex 2e, 27 pages, 1 figure. Minor revisions to conform with the
  last set of journal page proofs: two typos corrected, the bibliography
  rearranged in the order of citations (the ESAIM:PS home style), and two
  articles that were no longer cited removed</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Having Beer after Prayer? Measuring Cultural Bias in Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.14456v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.14456v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tarek Naous, Michael J. Ryan, Alan Ritter, Wei Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the reach of large language models (LMs) expands globally, their ability
to cater to diverse cultural contexts becomes crucial. Despite advancements in
multilingual capabilities, models are not designed with appropriate cultural
nuances. In this paper, we show that multilingual and Arabic monolingual LMs
exhibit bias towards entities associated with Western culture. We introduce
CAMeL, a novel resource of 628 naturally-occurring prompts and 20,368 entities
spanning eight types that contrast Arab and Western cultures. CAMeL provides a
foundation for measuring cultural biases in LMs through both extrinsic and
intrinsic evaluations. Using CAMeL, we examine the cross-cultural performance
in Arabic of 16 different LMs on tasks such as story generation, NER, and
sentiment analysis, where we find concerning cases of stereotyping and cultural
unfairness. We further test their text-infilling performance, revealing the
incapability of appropriate adaptation to Arab cultural contexts. Finally, we
analyze 6 Arabic pre-training corpora and find that commonly used sources such
as Wikipedia may not be best suited to build culturally aware LMs, if used as
they are without adjustment. We will make CAMeL publicly available at:
https://github.com/tareknaous/camel
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Unreasonable Effectiveness of Greedy Algorithms in Multi-Armed
  Bandit with Many Arms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2002.10121v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2002.10121v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohsen Bayati, Nima Hamidi, Ramesh Johari, Khashayar Khosravi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate a Bayesian $k$-armed bandit problem in the \emph{many-armed}
regime, where $k \geq \sqrt{T}$ and $T$ represents the time horizon. Initially,
and aligned with recent literature on many-armed bandit problems, we observe
that subsampling plays a key role in designing optimal algorithms; the
conventional UCB algorithm is sub-optimal, whereas a subsampled UCB (SS-UCB),
which selects $\Theta(\sqrt{T})$ arms for execution under the UCB framework,
achieves rate-optimality. However, despite SS-UCB's theoretical promise of
optimal regret, it empirically underperforms compared to a greedy algorithm
that consistently chooses the empirically best arm. This observation extends to
contextual settings through simulations with real-world data. Our findings
suggest a new form of \emph{free exploration} beneficial to greedy algorithms
in the many-armed context, fundamentally linked to a tail event concerning the
prior distribution of arm rewards. This finding diverges from the notion of
free exploration, which relates to covariate variation, as recently discussed
in contextual bandit literature. Expanding upon these insights, we establish
that the subsampled greedy approach not only achieves rate-optimality for
Bernoulli bandits within the many-armed regime but also attains sublinear
regret across broader distributions. Collectively, our research indicates that
in the many-armed regime, practitioners might find greater value in adopting
greedy algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Jaccard Metric Losses: Optimizing the Jaccard Index with Soft Labels <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.05666v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.05666v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zifu Wang, Xuefei Ning, Matthew B. Blaschko
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Intersection over Union (IoU) losses are surrogates that directly optimize
the Jaccard index. Leveraging IoU losses as part of the loss function have
demonstrated superior performance in semantic segmentation tasks compared to
optimizing pixel-wise losses such as the cross-entropy loss alone. However, we
identify a lack of flexibility in these losses to support vital training
techniques like label smoothing, knowledge distillation, and semi-supervised
learning, mainly due to their inability to process soft labels. To address
this, we introduce Jaccard Metric Losses (JMLs), which are identical to the
soft Jaccard loss in standard settings with hard labels but are fully
compatible with soft labels. We apply JMLs to three prominent use cases of soft
labels: label smoothing, knowledge distillation and semi-supervised learning,
and demonstrate their potential to enhance model accuracy and calibration. Our
experiments show consistent improvements over the cross-entropy loss across 4
semantic segmentation datasets (Cityscapes, PASCAL VOC, ADE20K, DeepGlobe Land)
and 13 architectures, including classic CNNs and recent vision transformers.
Remarkably, our straightforward approach significantly outperforms
state-of-the-art knowledge distillation and semi-supervised learning methods.
The code is available at
\href{https://github.com/zifuwanggg/JDTLosses}{https://github.com/zifuwanggg/JDTLosses}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Roto-translated Local Coordinate Frames For Interacting Dynamical
  Systems <span class="chip">NeurIPS 2021</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.14961v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.14961v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Miltiadis Kofinas, Naveen Shankar Nagaraja, Efstratios Gavves
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modelling interactions is critical in learning complex dynamical systems,
namely systems of interacting objects with highly non-linear and time-dependent
behaviour. A large class of such systems can be formalized as
$\textit{geometric graphs}$, $\textit{i.e.}$, graphs with nodes positioned in
the Euclidean space given an $\textit{arbitrarily}$ chosen global coordinate
system, for instance vehicles in a traffic scene. Notwithstanding the arbitrary
global coordinate system, the governing dynamics of the respective dynamical
systems are invariant to rotations and translations, also known as
$\textit{Galilean invariance}$. As ignoring these invariances leads to worse
generalization, in this work we propose local coordinate frames per node-object
to induce roto-translation invariance to the geometric graph of the interacting
dynamical system. Further, the local coordinate frames allow for a natural
definition of anisotropic filtering in graph neural networks. Experiments in
traffic scenes, 3D motion capture, and colliding particles demonstrate that the
proposed approach comfortably outperforms the recent state-of-the-art.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In NeurIPS 2021. Source code: https://github.com/mkofinas/locs</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Reinforcement Learning: A Convex Optimization Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.19212v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.19212v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ather Gattami
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we consider reinforcement learning of nonlinear systems with
continuous state and action spaces. We present an episodic learning algorithm,
where we for each episode use convex optimization to find a two-layer neural
network approximation of the optimal $Q$-function. The convex optimization
approach guarantees that the weights calculated at each episode are optimal,
with respect to the given sampled states and actions of the current episode.
For stable nonlinear systems, we show that the algorithm converges and that the
converging parameters of the trained neural network can be made arbitrarily
close to the optimal neural network parameters. In particular, if the
regularization parameter is $\rho$ and the time horizon is $T$, then the
parameters of the trained neural network converge to $w$, where the distance
between $w$ from the optimal parameters $w^\star$ is bounded by
$\mathcal{O}(\rho T^{-1})$. That is, when the number of episodes goes to
infinity, there exists a constant $C$ such that \[\|w-w^\star\| \le
C\cdot\frac{\rho}{T}.\] In particular, our algorithm converges arbitrarily
close to the optimal neural network parameters as the time horizon increases or
as the regularization parameter decreases.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MCRAGE: Synthetic Healthcare Data for Fairness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.18430v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.18430v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keira Behal, Jiayi Chen, Caleb Fikes, Sophia Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the field of healthcare, electronic health records (EHR) serve as crucial
training data for developing machine learning models for diagnosis, treatment,
and the management of healthcare resources. However, medical datasets are often
imbalanced in terms of sensitive attributes such as race/ethnicity, gender, and
age. Machine learning models trained on class-imbalanced EHR datasets perform
significantly worse in deployment for individuals of the minority classes
compared to those from majority classes, which may lead to inequitable
healthcare outcomes for minority groups. To address this challenge, we propose
Minority Class Rebalancing through Augmentation by Generative modeling
(MCRAGE), a novel approach to augment imbalanced datasets using samples
generated by a deep generative model. The MCRAGE process involves training a
Conditional Denoising Diffusion Probabilistic Model (CDDPM) capable of
generating high-quality synthetic EHR samples from underrepresented classes. We
use this synthetic data to augment the existing imbalanced dataset, resulting
in a more balanced distribution across all classes, which can be used to train
less biased downstream models. We measure the performance of MCRAGE versus
alternative approaches using Accuracy, F1 score and AUROC of these downstream
models. We provide theoretical justification for our method in terms of recent
convergence results for DDPMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Keywords: synthetic electronic health records, conditional denoising
  diffusion probabilistic model, healthcare AI, tabular data, fairness,
  synthetic data. This paper is the result of work completed at the 2023 Emory
  University Department of Mathematics REU/RET program under the direction of
  Project Advisor Dr. Xi Yuanzhe. This work is sponsored by NSF DMS 2051019</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Normalizing flow-based deep variational Bayesian network for seismic
  multi-hazards and impacts estimation from InSAR imagery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.13805v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.13805v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuechun Li, Paula M. Burgi, Wei Ma, Hae Young Noh, David J. Wald, Susu Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Onsite disasters like earthquakes can trigger cascading hazards and impacts,
such as landslides and infrastructure damage, leading to catastrophic losses;
thus, rapid and accurate estimates are crucial for timely and effective
post-disaster responses. Interferometric Synthetic aperture radar (InSAR) data
is important in providing high-resolution onsite information for rapid hazard
estimation. Most recent methods using InSAR imagery signals predict a single
type of hazard and thus often suffer low accuracy due to noisy and complex
signals induced by co-located hazards, impacts, and irrelevant environmental
changes (e.g., vegetation changes, human activities). We introduce a novel
stochastic variational inference with normalizing flows derived to jointly
approximate posteriors of multiple unobserved hazards and impacts from noisy
InSAR imagery.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper needs to be reviewed by the USGS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Graph Neural Networks for Learning Equivariant Representations of Neural
  Networks <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12143v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12143v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Miltiadis Kofinas, Boris Knyazev, Yan Zhang, Yunlu Chen, Gertjan J. Burghouts, Efstratios Gavves, Cees G. M. Snoek, David W. Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural networks that process the parameters of other neural networks find
applications in domains as diverse as classifying implicit neural
representations, generating neural network weights, and predicting
generalization errors. However, existing approaches either overlook the
inherent permutation symmetry in the neural network or rely on intricate
weight-sharing patterns to achieve equivariance, while ignoring the impact of
the network architecture itself. In this work, we propose to represent neural
networks as computational graphs of parameters, which allows us to harness
powerful graph neural networks and transformers that preserve permutation
symmetry. Consequently, our approach enables a single model to encode neural
computational graphs with diverse architectures. We showcase the effectiveness
of our method on a wide range of tasks, including classification and editing of
implicit neural representations, predicting generalization performance, and
learning to optimize, while consistently outperforming state-of-the-art
methods. The source code is open-sourced at
https://github.com/mkofinas/neural-graphs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In ICLR 2024. Source code: https://github.com/mkofinas/neural-graphs</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Latent Field Discovery In Interacting Dynamical Systems With Neural
  Fields <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.20679v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.20679v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Miltiadis Kofinas, Erik J. Bekkers, Naveen Shankar Nagaraja, Efstratios Gavves
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Systems of interacting objects often evolve under the influence of field
effects that govern their dynamics, yet previous works have abstracted away
from such effects, and assume that systems evolve in a vacuum. In this work, we
focus on discovering these fields, and infer them from the observed dynamics
alone, without directly observing them. We theorize the presence of latent
force fields, and propose neural fields to learn them. Since the observed
dynamics constitute the net effect of local object interactions and global
field effects, recently popularized equivariant networks are inapplicable, as
they fail to capture global information. To address this, we propose to
disentangle local object interactions -- which are $\mathrm{SE}(n)$ equivariant
and depend on relative states -- from external global field effects -- which
depend on absolute states. We model interactions with equivariant graph
networks, and combine them with neural fields in a novel graph network that
integrates field forces. Our experiments show that we can accurately discover
the underlying fields in charged particles settings, traffic scenes, and
gravitational n-body problems, and effectively use them to learn the system and
forecast future trajectories.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In NeurIPS 2023. Source code: https://github.com/mkofinas/aether</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dice Semimetric Losses: Optimizing the Dice Score with Soft Labels <span class="chip">MICCAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.16296v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.16296v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zifu Wang, Teodora Popordanoska, Jeroen Bertels, Robin Lemmens, Matthew B. Blaschko
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The soft Dice loss (SDL) has taken a pivotal role in numerous automated
segmentation pipelines in the medical imaging community. Over the last years,
some reasons behind its superior functioning have been uncovered and further
optimizations have been explored. However, there is currently no implementation
that supports its direct utilization in scenarios involving soft labels. Hence,
a synergy between the use of SDL and research leveraging the use of soft
labels, also in the context of model calibration, is still missing. In this
work, we introduce Dice semimetric losses (DMLs), which (i) are by design
identical to SDL in a standard setting with hard labels, but (ii) can be
employed in settings with soft labels. Our experiments on the public QUBIQ,
LiTS and KiTS benchmarks confirm the potential synergy of DMLs with soft labels
(e.g. averaging, label smoothing, and knowledge distillation) over hard labels
(e.g. majority voting and random selection). As a result, we obtain superior
Dice scores and model calibration, which supports the wider adoption of DMLs in
practice. The code is available at https://github.com/zifuwanggg/JDTLosses
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>MICCAI 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Weight-Inherited Distillation for Task-Agnostic <span class="highlight-title">BERT</span> Compression <span class="chip">NAACL2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.09098v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.09098v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taiqiang Wu, Cheng Hou, Shanshan Lao, Jiayi Li, Ngai Wong, Zhe Zhao, Yujiu Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge Distillation (KD) is a predominant approach for BERT compression.
Previous KD-based methods focus on designing extra alignment losses for the
student model to mimic the behavior of the teacher model. These methods
transfer the knowledge in an indirect way. In this paper, we propose a novel
Weight-Inherited Distillation (WID), which directly transfers knowledge from
the teacher. WID does not require any additional alignment loss and trains a
compact student by inheriting the weights, showing a new perspective of
knowledge distillation. Specifically, we design the row compactors and column
compactors as mappings and then compress the weights via structural
re-parameterization. Experimental results on the GLUE and SQuAD benchmarks show
that WID outperforms previous state-of-the-art KD-based baselines. Further
analysis indicates that WID can also learn the attention patterns from the
teacher model without any alignment loss on attention distributions. The code
is available at https://github.com/wutaiqiang/WID-NAACL2024.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 4 figures, NAACL2024 findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ S$Ω$I: Score-based O-INFORMATION Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.05667v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.05667v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mustapha Bounoua, Giulio Franzese, Pietro Michiardi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The analysis of scientific data and complex multivariate systems requires
information quantities that capture relationships among multiple random
variables. Recently, new information-theoretic measures have been developed to
overcome the shortcomings of classical ones, such as mutual information, that
are restricted to considering pairwise interactions. Among them, the concept of
information synergy and redundancy is crucial for understanding the high-order
dependencies between variables. One of the most prominent and versatile
measures based on this concept is O-information, which provides a clear and
scalable way to quantify the synergy-redundancy balance in multivariate
systems. However, its practical application is limited to simplified cases. In
this work, we introduce S$\Omega$I, which allows for the first time to compute
O-information without restrictive assumptions about the system. Our experiments
validate our approach on synthetic data, and demonstrate the effectiveness of
S$\Omega$I in the context of a real-world use case.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Guaranteeing Control Requirements via Reward Shaping in Reinforcement
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10026v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10026v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francesco De Lellis, Marco Coraggio, Giovanni Russo, Mirco Musolesi, Mario di Bernardo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In addressing control problems such as regulation and tracking through
reinforcement learning, it is often required to guarantee that the acquired
policy meets essential performance and stability criteria such as a desired
settling time and steady-state error prior to deployment. Motivated by this
necessity, we present a set of results and a systematic reward shaping
procedure that (i) ensures the optimal policy generates trajectories that align
with specified control requirements and (ii) allows to assess whether any given
policy satisfies them. We validate our approach through comprehensive numerical
experiments conducted in two representative environments from OpenAI Gym: the
Inverted Pendulum swing-up problem and the Lunar Lander. Utilizing both tabular
and deep reinforcement learning methods, our experiments consistently affirm
the efficacy of our proposed framework, highlighting its effectiveness in
ensuring policy adherence to the prescribed control requirements.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Power of Few: Accelerating and Enhancing Data Reweighting with
  Coreset Selection <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12166v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12166v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Jafari, Yimeng Zhang, Yihua Zhang, Sijia Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As machine learning tasks continue to evolve, the trend has been to gather
larger datasets and train increasingly larger models. While this has led to
advancements in accuracy, it has also escalated computational costs to
unsustainable levels. Addressing this, our work aims to strike a delicate
balance between computational efficiency and model accuracy, a persisting
challenge in the field. We introduce a novel method that employs core subset
selection for reweighting, effectively optimizing both computational time and
model performance. By focusing on a strategically selected coreset, our
approach offers a robust representation, as it efficiently minimizes the
influence of outliers. The re-calibrated weights are then mapped back to and
propagated across the entire dataset. Our experimental results substantiate the
effectiveness of this approach, underscoring its potential as a scalable and
precise solution for model training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICASSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Observational and Experimental Insights into Machine Learning-Based
  Defect Classification in Wafers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.10705v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.10705v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kamal Taha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This survey paper offers a comprehensive review of methodologies utilizing
machine learning (ML) classification techniques for identifying wafer defects
in semiconductor manufacturing. Despite the growing body of research
demonstrating the effectiveness of ML in wafer defect identification, there is
a noticeable absence of comprehensive reviews on this subject. This survey
attempts to fill this void by amalgamating available literature and providing
an in-depth analysis of the advantages, limitations, and potential applications
of various ML classification algorithms in the realm of wafer defect detection.
An innovative taxonomy of methodologies that we present provides a detailed
classification of algorithms into more refined categories and techniques. This
taxonomy follows a three-tier structure, starting from broad methodology
categories and ending with specific techniques. It aids researchers in
comprehending the complex relationships between different algorithms and their
techniques. We employ a rigorous Observational and experimental evaluation to
rank these varying techniques. For the Observational evaluation, we assess
techniques based on a set of four criteria. The experimental evaluation ranks
the algorithms employing the same techniques, sub-categories, and categories.
Also the paper illuminates the future prospects of ML classification techniques
for wafer defect identification, underscoring potential advancements and
opportunities for further research in this field
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Interpretable Meta-Learning of Physical Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.00477v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.00477v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthieu Blanke, Marc Lelarge
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning methods can be a valuable aid in the scientific process, but
they need to face challenging settings where data come from inhomogeneous
experimental conditions. Recent meta-learning methods have made significant
progress in multi-task learning, but they rely on black-box neural networks,
resulting in high computational costs and limited interpretability. Leveraging
the structure of the learning problem, we argue that multi-environment
generalization can be achieved using a simpler learning model, with an affine
structure with respect to the learning task. Crucially, we prove that this
architecture can identify the physical parameters of the system, enabling
interpreable learning. We demonstrate the competitive generalization
performance and the low computational cost of our method by comparing it to
state-of-the-art algorithms on physical systems, ranging from toy models to
complex, non-analytical systems. The interpretability of our method is
illustrated with original applications to physical-parameter-induced adaptation
and to adaptive control.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bounce: Reliable High-Dimensional Bayesian Optimization for
  Combinatorial and Mixed Spaces 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.00618v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.00618v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leonard Papenmeier, Luigi Nardi, Matthias Poloczek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Impactful applications such as materials discovery, hardware design, neural
architecture search, or portfolio optimization require optimizing
high-dimensional black-box functions with mixed and combinatorial input spaces.
While Bayesian optimization has recently made significant progress in solving
such problems, an in-depth analysis reveals that the current state-of-the-art
methods are not reliable. Their performances degrade substantially when the
unknown optima of the function do not have a certain structure. To fill the
need for a reliable algorithm for combinatorial and mixed spaces, this paper
proposes Bounce that relies on a novel map of various variable types into
nested embeddings of increasing dimensionality. Comprehensive experiments show
that Bounce reliably achieves and often even improves upon state-of-the-art
performance on a variety of high-dimensional problems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages, 22 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Riemannian Multinomial Logistics Regression for SPD Neural Networks <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.11288v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.11288v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziheng Chen, Yue Song, Gaowen Liu, Ramana Rao Kompella, Xiaojun Wu, Nicu Sebe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks for learning Symmetric Positive Definite (SPD) matrices
are gaining increasing attention in machine learning. Despite the significant
progress, most existing SPD networks use traditional Euclidean classifiers on
an approximated space rather than intrinsic classifiers that accurately capture
the geometry of SPD manifolds. Inspired by Hyperbolic Neural Networks (HNNs),
we propose Riemannian Multinomial Logistics Regression (RMLR) for the
classification layers in SPD networks. We introduce a unified framework for
building Riemannian classifiers under the metrics pulled back from the
Euclidean space, and showcase our framework under the parameterized
Log-Euclidean Metric (LEM) and Log-Cholesky Metric (LCM). Besides, our
framework offers a novel intrinsic explanation for the most popular LogEig
classifier in existing SPD networks. The effectiveness of our method is
demonstrated in three applications: radar recognition, human action
recognition, and electroencephalography (EEG) classification. The code is
available at https://github.com/GitZH-Chen/SPDMLR.git.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Asymptotic generalization error of a single-layer graph convolutional
  network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.03818v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.03818v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        O. Duranthon, L. Zdeborová
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While graph convolutional networks show great practical promises, the
theoretical understanding of their generalization properties as a function of
the number of samples is still in its infancy compared to the more broadly
studied case of supervised fully connected neural networks. In this article, we
predict the performances of a single-layer graph convolutional network (GCN)
trained on data produced by attributed stochastic block models (SBMs) in the
high-dimensional limit. Previously, only ridge regression on contextual-SBM
(CSBM) has been considered in Shi et al. 2022; we generalize the analysis to
arbitrary convex loss and regularization for the CSBM and add the analysis for
another data model, the neural-prior SBM. We also study the high
signal-to-noise ratio limit, detail the convergence rates of the GCN and show
that, while consistent, it does not reach the Bayes-optimal rate for any of the
considered cases.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NetInfoF Framework: Measuring and Exploiting Network Usable Information <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.07999v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.07999v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Meng-Chieh Lee, Haiyang Yu, Jian Zhang, Vassilis N. Ioannidis, Xiang Song, Soji Adeshina, Da Zheng, Christos Faloutsos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given a node-attributed graph, and a graph task (link prediction or node
classification), can we tell if a graph neural network (GNN) will perform well?
More specifically, do the graph structure and the node features carry enough
usable information for the task? Our goals are (1) to develop a fast tool to
measure how much information is in the graph structure and in the node
features, and (2) to exploit the information to solve the task, if there is
enough. We propose NetInfoF, a framework including NetInfoF_Probe and
NetInfoF_Act, for the measurement and the exploitation of network usable
information (NUI), respectively. Given a graph data, NetInfoF_Probe measures
NUI without any model training, and NetInfoF_Act solves link prediction and
node classification, while two modules share the same backbone. In summary,
NetInfoF has following notable advantages: (a) General, handling both link
prediction and node classification; (b) Principled, with theoretical guarantee
and closed-form solution; (c) Effective, thanks to the proposed adjustment to
node similarity; (d) Scalable, scaling linearly with the input size. In our
carefully designed synthetic datasets, NetInfoF correctly identifies the ground
truth of NUI and is the only method being robust to all graph scenarios.
Applied on real-world datasets, NetInfoF wins in 11 out of 12 times on link
prediction compared to general GNN baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICLR 2024 (Spotlight)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ABScribe: Rapid Exploration & Organization of Multiple Writing
  Variations in Human-AI Co-Writing Tasks using Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.00117v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.00117v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohi Reza, Nathan Laundry, Ilya Musabirov, Peter Dushniku, Zhi Yuan "Michael" Yu, Kashish Mittal, Tovi Grossman, Michael Liut, Anastasia Kuzminykh, Joseph Jay Williams
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Exploring alternative ideas by rewriting text is integral to the writing
process. State-of-the-art Large Language Models (LLMs) can simplify writing
variation generation. However, current interfaces pose challenges for
simultaneous consideration of multiple variations: creating new variations
without overwriting text can be difficult, and pasting them sequentially can
clutter documents, increasing workload and disrupting writers' flow. To tackle
this, we present ABScribe, an interface that supports rapid, yet visually
structured, exploration and organization of writing variations in human-AI
co-writing tasks. With ABScribe, users can swiftly modify variations using LLM
prompts, which are auto-converted into reusable buttons. Variations are stored
adjacently within text fields for rapid in-place comparisons using mouse-over
interactions on a popup toolbar. Our user study with 12 writers shows that
ABScribe significantly reduces task workload (d = 1.20, p < 0.001), enhances
user perceptions of the revision process (d = 2.41, p < 0.001) compared to a
popular baseline workflow, and provides insights into how writers explore
variations using LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CHI 2024</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Distributed, Parallel, and Cluster Computing
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Agent-based MST Construction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13716v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13716v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ajay D. Kshemkalyani, Manish Kumar, Anisur Rahaman Molla, Gokarna Sharma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  {\em Minimum-weight spanning tree} (MST) is one of the fundamental and
well-studied problems in distributed computing. In this paper, we initiate the
study of constructing MST using mobile agents (aka robots). Suppose $n$ agents
are positioned initially arbitrarily on the nodes of a connected, undirected,
arbitrary, anonymous, port-labeled, weighted $n$-node, $m$-edge graph $G$ of
diameter $D$ and maximum degree $\Delta$. The agents relocate themselves
autonomously and compute an MST of $G$ such that exactly one agent positions on
a node and tracks in its memory which of its adjacent edges belong to the MST.
The objective is to minimize time and memory requirements. Following the
literature, we consider the synchronous setting in which each agent performs
its operations synchronously with others and hence time can be measured in
rounds. We first establish a generic result: if $n$ and $\Delta$ are known a
priori and memory per agent is as much as node memory in the message-passing
model (of distributed computing), agents can simulate any $O(T)$-round
deterministic algorithm for any problem in the message-passing model to the
agent model in $O(\Delta T \log n+n\log^2n)$ rounds. As a corollary, MST can be
constructed in the agent model in $O(\max\{\Delta \sqrt{n} \log n \log^*n,
\Delta D \log n,n\log^2n\})$ rounds simulating the celebrated $O(\sqrt{n}
\log^*n +D)$-round GKP algorithm for MST in the message-passing model. We then
establish that, without knowing any graph parameter a priori, there exists a
deterministic algorithm to construct MST in the agent model in $O(m+n\log n)$
rounds with $O(n \log n)$ bits memory at each agent. The presented algorithm
needs to overcome highly non-trivial challenges on how to synchronize agents in
computing MST as they may initially be positioned arbitrarily on the graph
nodes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How to Relax Instantly: Elastic Relaxation of Concurrent Data Structures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13644v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13644v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kåre von Geijer, Philippas Tsigas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The sequential semantics of many concurrent data structures, such as stacks
and queues, inevitably lead to memory contention in parallel environments, thus
limiting scalability. Semantic relaxation has the potential to address this
issue, increasing the parallelism at the expense of weakened semantics.
Although prior research has shown that improved performance can be attained by
relaxing concurrent data structure semantics, there is no one-size-fits-all
relaxation that adequately addresses the varying needs of dynamic executions.
  In this paper, we first introduce the concept of elastic relaxation and
consequently present the Lateral structure, which is an algorithmic component
capable of supporting the design of elastically relaxed concurrent data
structures. Using the Lateral , we design novel elastically relaxed, lock-free
queues and stacks capable of reconfiguring relaxation during run time. We
establish linearizability and define upper bounds for relaxation errors in our
designs. Experimental evaluations show that our elastic designs hold up against
state-of-the-art statically relaxed designs, while also swiftly managing
trade-offs between relaxation and operational latency. We also outline how to
use the Lateral to design elastically relaxed lock-free counters and deques.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CheckMate: Evaluating Checkpointing Protocols for Streaming Dataflows 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13629v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13629v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        George Siachamis, Kyriakos Psarakis, Marios Fragkoulis, Arie van Deursen, Paris Carbone, Asterios Katsifodimos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stream processing in the last decade has seen broad adoption in both
commercial and research settings. One key element for this success is the
ability of modern stream processors to handle failures while ensuring
exactly-once processing guarantees. At the moment of writing, virtually all
stream processors that guarantee exactly-once processing implement a variant of
Apache Flink's coordinated checkpoints - an extension of the original
Chandy-Lamport checkpoints from 1985. However, the reasons behind this
prevalence of the coordinated approach remain anecdotal, as reported by
practitioners of the stream processing community. At the same time, common
checkpointing approaches, such as the uncoordinated and the
communication-induced ones, remain largely unexplored.
  This paper is the first to address this gap by i) shedding light on why
practitioners have favored the coordinated approach and ii) by investigating
whether there are viable alternatives. To this end, we implement three
checkpointing approaches that we surveyed and adapted for the distinct needs of
streaming dataflows. Our analysis shows that the coordinated approach
outperforms the uncoordinated and communication-induced protocols under
uniformly distributed workloads. To our surprise, however, the uncoordinated
approach is not only competitive to the coordinated one in uniformly
distributed workloads, but it also outperforms the coordinated approach in
skewed workloads. We conclude that rather than blindly employing coordinated
checkpointing, research should focus on optimizing the very promising
uncoordinated approach, as it can address issues with skew and support
prevalent cyclic queries. We believe that our findings can trigger further
research into checkpointing mechanisms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dynamic Resource Allocation for Virtual Machine Migration Optimization
  using Machine Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13619v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13619v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yulu Gong, Jiaxin Huang, Bo Liu, Jingyu Xu, Binbin Wu, Yifan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The paragraph is grammatically correct and logically coherent. It discusses
the importance of mobile terminal cloud computing migration technology in
meeting the demands of evolving computer and cloud computing technologies. It
emphasizes the need for efficient data access and storage, as well as the
utilization of cloud computing migration technology to prevent additional time
delays. The paragraph also highlights the contributions of cloud computing
migration technology to expanding cloud computing services. Additionally, it
acknowledges the role of virtualization as a fundamental capability of cloud
computing while emphasizing that cloud computing and virtualization are not
inherently interconnected. Finally, it introduces machine learning-based
virtual machine migration optimization and dynamic resource allocation as a
critical research direction in cloud computing, citing the limitations of
static rules or manual settings in traditional cloud computing environments.
Overall, the paragraph effectively communicates the importance of machine
learning technology in addressing resource allocation and virtual machine
migration challenges in cloud computing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive time step selection for Spectral Deferred Corrections 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13454v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13454v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Baumann, Sebastian Götschel, Thibaut Lunet, Daniel Ruprecht, Robert Speck
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spectral Deferred Corrections (SDC) is an iterative method for the numerical
solution of ordinary differential equations. It works by refining the numerical
solution for an initial value problem by approximately solving differential
equations for the error, and can be interpreted as a preconditioned fixed-point
iteration for solving the fully implicit collocation problem. We adopt
techniques from embedded Runge-Kutta Methods (RKM) to SDC in order to provide a
mechanism for adaptive time step size selection and thus increase computational
efficiency of SDC. We propose two SDC-specific estimates of the local error
that are generic and require only minimal problem specific tuning. We
demonstrate a gain in efficiency over standard SDC with fixed step size,
compare efficiency favorably against state-of-the-art adaptive RKM and show
that due to its iterative nature, adaptive SDC can cope efficiently with silent
data corruption.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>34 pages including references, 12 figures. Submitted to Springer
  Numerical Algorithms</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimal Fixed Priority Scheduling in Multi-Stage Multi-Resource
  Distributed Real-Time Systems <span class="chip">DATE</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13411v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13411v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Niraj Kumar, Chuanchao Gao, Arvind Easwaran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work studies fixed priority (FP) scheduling of real-time jobs with
end-to-end deadlines in a distributed system. Specifically, given a multi-stage
pipeline with multiple heterogeneous resources of the same type at each stage,
the problem is to assign priorities to a set of real-time jobs with different
release times to access a resource at each stage of the pipeline subject to the
end-to-end deadline constraints. Note, in such a system, jobs may compete with
different sets of jobs at different stages of the pipeline depending on the
job-to-resource mapping. To this end, following are the two major contributions
of this work. We show that an OPA-compatible schedulability test based on the
delay composition algebra can be constructed, which we then use with an optimal
priority assignment algorithm to compute a priority ordering. Further, we
establish the versatility of pairwise priority assignment in such a multi-stage
multi-resource system, compared to a total priority ordering. In particular, we
show that a pairwise priority assignment may be feasible even if a priority
ordering does not exist. We propose an integer linear programming formulation
and a scalable heuristic to compute a pairwise priority assignment. We also
show through simulation experiments that the proposed approaches can be used
for the holistic scheduling of real-time jobs in edge computing systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in DATE (Design, Automation and Test in Europe Conference)
  2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Causal Graph Dynamics and Kan Extensions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13393v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13393v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luidnel Maignan, Antoine Spicher
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  On the one side, the formalism of Global Transformations comes with the claim
of capturing any transformation of space that is local, synchronous and
deterministic.The claim has been proven for different classes of models such as
mesh refinements from computer graphics, Lindenmayer systems from morphogenesis
modeling and cellular automata from biological, physical and parallel
computation modeling.The Global Transformation formalism achieves this by using
category theory for its genericity, and more precisely the notion of Kan
extension to determine the global behaviors based on the local ones.On the
other side, Causal Graph Dynamics describe the transformation of port graphs in
a synchronous and deterministic way and has not yet being tackled.In this
paper, we show the precise sense in which the claim of Global Transformations
holds for them as well.This is done by showing different ways in which they can
be expressed as Kan extensions, each of them highlighting different features of
Causal Graph Dynamics.Along the way, this work uncovers the interesting class
of Monotonic Causal Graph Dynamics and their universality among General Causal
Graph Dynamics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Regent based parallel meshfree LSKUM solver for heterogenous HPC
  platforms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13287v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13287v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sanath Salil, Nischay Ram Mamidi, Anil Nemili, Elliott Slaughter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Regent is an implicitly parallel programming language that allows the
development of a single codebase for heterogeneous platforms targeting CPUs and
GPUs. This paper presents the development of a parallel meshfree solver in
Regent for two-dimensional inviscid compressible flows. The meshfree solver is
based on the least squares kinetic upwind method. Example codes are presented
to show the difference between the Regent and CUDA-C implementations of the
meshfree solver on a GPU node. For CPU parallel computations, details are
presented on how the data communication and synchronisation are handled by
Regent and Fortran+MPI codes. The Regent solver is verified by applying it to
the standard test cases for inviscid flows. Benchmark simulations are performed
on coarse to very fine point distributions to assess the solver's performance.
The computational efficiency of the Regent solver on an A100 GPU is compared
with an equivalent meshfree solver written in CUDA-C. The codes are then
profiled to investigate the differences in their performance. The performance
of the Regent solver on CPU cores is compared with an equivalent explicitly
parallel Fortran meshfree solver based on MPI. Scalability results are shown to
offer insights into performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Decentralized Federated Learning: Model Update Tracking Under Imperfect
  Information Sharing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13247v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13247v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vishnu Pandi Chellapandi, Antesh Upadhyay, Abolfazl Hashemi, Stanislaw H. Żak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A novel Decentralized Noisy Model Update Tracking Federated Learning
algorithm (FedNMUT) is proposed, which is tailored to function efficiently in
the presence of noisy communication channels that reflect imperfect information
exchange. This algorithm uses gradient tracking to minimize the impact of data
heterogeneity while minimizing communication overhead. The proposed algorithm
incorporates noise into its parameters to mimic the conditions of noisy
communication channels, thereby enabling consensus among clients through a
communication graph topology in such challenging environments. FedNMUT
prioritizes parameter sharing and noise incorporation to increase the
resilience of decentralized learning systems against noisy communications.
Through theoretical and empirical validation, it is demonstrated that the
performance of FedNMUT is superior compared to the existing state-of-the-art
methods and conventional parameter-mixing approaches in dealing with imperfect
information sharing. This proves the capability of the proposed algorithm to
counteract the negative effects of communication noise in a decentralized
learning framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2303.10695</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Federated reinforcement learning for robot motion planning with
  zero-shot generalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13245v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13245v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenyuan Yuan, Siyuan Xu, Minghui Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper considers the problem of learning a control policy for robot
motion planning with zero-shot generalization, i.e., no data collection and
policy adaptation is needed when the learned policy is deployed in new
environments. We develop a federated reinforcement learning framework that
enables collaborative learning of multiple learners and a central server, i.e.,
the Cloud, without sharing their raw data. In each iteration, each learner
uploads its local control policy and the corresponding estimated normalized
arrival time to the Cloud, which then computes the global optimum among the
learners and broadcasts the optimal policy to the learners. Each learner then
selects between its local control policy and that from the Cloud for next
iteration. The proposed framework leverages on the derived zero-shot
generalization guarantees on arrival time and safety. Theoretical guarantees on
almost-sure convergence, almost consensus, Pareto improvement and optimality
gap are also provided. Monte Carlo simulation is conducted to evaluate the
proposed framework.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automated Calibration of Parallel and Distributed Computing Simulators:
  A Case Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13918v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13918v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jesse McDonald, Maximilian Horzela, Frédéric Suter, Henri Casanova
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many parallel and distributed computing research results are obtained in
simulation, using simulators that mimic real-world executions on some target
system. Each such simulator is configured by picking values for parameters that
define the behavior of the underlying simulation models it implements. The main
concern for a simulator is accuracy: simulated behaviors should be as close as
possible to those observed in the real-world target system. This requires that
values for each of the simulator's parameters be carefully picked, or
"calibrated," based on ground-truth real-world executions. Examining the
current state of the art shows that simulator calibration, at least in the
field of parallel and distributed computing, is often undocumented (and thus
perhaps often not performed) and, when documented, is described as a
labor-intensive, manual process. In this work we evaluate the benefit of
automating simulation calibration using simple algorithms. Specifically, we use
a real-world case study from the field of High Energy Physics and compare
automated calibration to calibration performed by a domain scientist. Our main
finding is that automated calibration is on par with or significantly
outperforms the calibration performed by the domain scientist. Furthermore,
automated calibration makes it straightforward to operate desirable trade-offs
between simulation accuracy and simulation speed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in Proc. of the 25th IEEE International Workshop on
  Parallel and Distributed Scientific and Engineering Computing (PDSEC 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Accelerating Biclique Counting on GPU <span class="chip">ICDE24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07858v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07858v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linshan Qiu, Zhonggen Li, Xiangyu Ke, Lu Chen, Yunjun Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Counting (p,q)-bicliques in bipartite graphs poses a foundational challenge
with broad applications, from densest subgraph discovery in algorithmic
research to personalized content recommendation in practical scenarios. Despite
its significance, current leading (p,q)-biclique counting algorithms fall
short, particularly when faced with larger graph sizes and clique scales.
Fortunately, the problem's inherent structure, allowing for the independent
counting of each biclique starting from every vertex, combined with a
substantial set intersections, makes it highly amenable to parallelization.
Recent successes in GPU-accelerated algorithms across various domains motivate
our exploration into harnessing the parallelism power of GPUs to efficiently
address the (p,q)-biclique counting challenge. We introduce GBC (GPU-based
Biclique Counting), a novel approach designed to enable efficient and scalable
(p,q)-biclique counting on GPUs. To address major bottleneck arising from
redundant comparisons in set intersections (occupying an average of 90% of the
runtime), we introduce a novel data structure that hashes adjacency lists into
truncated bitmaps to enable efficient set intersection on GPUs via bit-wise AND
operations. Our innovative hybrid DFS-BFS exploration strategy further enhances
thread utilization and effectively manages memory constraints. A composite load
balancing strategy, integrating pre-runtime and runtime workload allocation,
ensures equitable distribution among threads. Additionally, we employ vertex
reordering and graph partitioning strategies for improved compactness and
scalability. Experimental evaluations on eight real-life and two synthetic
datasets demonstrate that GBC outperforms state-of-the-art algorithms by a
substantial margin. In particular, GBC achieves an average speedup of 497.8x,
with the largest instance achieving a remarkable 1217.7x speedup when p = q =
8.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted by ICDE24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Topology of Local Computing in Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2003.03255v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2003.03255v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pierre Fraigniaud, Ami Paz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modeling distributed computing in a way enabling the use of formal methods is
a challenge that has been approached from different angles, among which two
techniques emerged at the turn of the century: protocol complexes, and directed
algebraic topology. In both cases, the considered computational model generally
assumes communication via shared objects, typically a shared memory consisting
of a collection of read-write registers. Our paper is concerned with network
computing, where the processes are located at the nodes of a network, and
communicate by exchanging messages along the edges of that network. Applying
the topological approach for verification in network computing is a
considerable challenge, mainly because the presence of identifiers assigned to
the nodes yields protocol complexes whose size grows exponentially with the
size of the underlying network. However, many of the problems studied in this
context are of local nature, and their definitions do not depend on the
identifiers or on the size of the network. We leverage this independence in
order to meet the above challenge, and present $\textit{local}$ protocol
complexes, whose sizes do not depend on the size of the network. As an
application of the design of "compact" protocol complexes, we reformulate the
celebrated lower bound of $\Omega(\log^*n)$ rounds for 3-coloring the $n$-node
ring, in the algebraic topology framework.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Arrow Matrix Decomposition: A Novel Approach for Communication-Efficient
  Sparse Matrix Multiplication 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.19364v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.19364v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lukas Gianinazzi, Alexandros Nikolaos Ziogas, Langwen Huang, Piotr Luczynski, Saleh Ashkboos, Florian Scheidl, Armon Carigiet, Chio Ge, Nabil Abubaker, Maciej Besta, Tal Ben-Nun, Torsten Hoefler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel approach to iterated sparse matrix dense matrix
multiplication, a fundamental computational kernel in scientific computing and
graph neural network training. In cases where matrix sizes exceed the memory of
a single compute node, data transfer becomes a bottleneck. An approach based on
dense matrix multiplication algorithms leads to suboptimal scalability and
fails to exploit the sparsity in the problem. To address these challenges, we
propose decomposing the sparse matrix into a small number of highly structured
matrices called arrow matrices, which are connected by permutations. Our
approach enables communication-avoiding multiplications, achieving a polynomial
reduction in communication volume per iteration for matrices corresponding to
planar graphs and other minor-excluded families of graphs. Our evaluation
demonstrates that our approach outperforms a state-of-the-art method for sparse
matrix multiplication on matrices with hundreds of millions of rows, offering
near-linear strong and weak scaling.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Comprehensive Evaluation of GNN Training Systems: A Data Management
  Perspective <span class="chip">VLDB 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13279v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13279v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Yuan, Yajiong Liu, Yanfeng Zhang, Xin Ai, Qiange Wang, Chaoyi Chen, Yu Gu, Ge Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many Graph Neural Network (GNN) training systems have emerged recently to
support efficient GNN training. Since GNNs embody complex data dependencies
between training samples, the training of GNNs should address distinct
challenges different from DNN training in data management, such as data
partitioning, batch preparation for mini-batch training, and data transferring
between CPUs and GPUs. These factors, which take up a large proportion of
training time, make data management in GNN training more significant. This
paper reviews GNN training from a data management perspective and provides a
comprehensive analysis and evaluation of the representative approaches. We
conduct extensive experiments on various benchmark datasets and show many
interesting and valuable results. We also provide some practical tips learned
from these experiments, which are helpful for designing GNN training systems in
the future.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 18 figures. (Accepted by VLDB 2024)</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Performance
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Network Calculus Bounds for Time-Sensitive Networks: A Revisit 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13656v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13656v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuming Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Network calculus (NC), particularly its min-plus branch, has been extensively
utilized to construct service models and compute delay bounds for
time-sensitive networks (TSNs). This paper provides a revisit to the
fundamental results. In particular, counterexamples to the most basic min-plus
service models, which have been proposed for TSNs and used for computing delay
bounds, indicate that the packetization effect has often been overlooked. To
address, the max-plus branch of NC is also considered in this paper, whose
models handle packetized traffic more explicitly. It is found that mapping the
min-plus models to the max-plus models may bring in an immediate improvement
over delay bounds derived from the min-plus analysis. In addition, an
integrated analytical approach that combines models from both the min-plus and
the max-plus NC branches is introduced. In this approach, the max-plus
$g$-server model is extended and the extended model, called $g^{x}$-server, is
used together with the min-plus arrival curve traffic model. By applying the
integrated NC approach, service and delay bounds are derived for several
settings that are fundamental in TSNs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Starlink on the Road: A First Look at Mobile Starlink Performance in
  Central Europe 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13497v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13497v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dominic Laniewski, Eric Lanfer, Simon Beginn, Jan Dunker, Michael Dückers, Nils Aschenbruck
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low Earth Orbit Satellite Networks such as Starlink promise to provide
world-wide Internet access. While traditionally designed for stationary use, a
new dish, released in April 2023 in Europe, provides mobile Internet access
including in-motion usage, e.g., while mounted on a car. In this paper, we
design and build a mobile measurement setup. Our goal is to fully autonomously
conduct continuous Starlink measurements while the car is in motion. We share
our practical experiences, including challenges regarding the permanent power
supply. We measure the Starlink performance over the span of two months from
mid-January to mid-March 2024 when the car is in motion. The measurements
consist of all relevant network parameters, such as the download and upload
throughput, the RTT, and packet loss, as well as detailed power consumption
data. We analyze our dataset to assess Starlink's mobile performance in Central
Europe, Germany, and compare it to stationary measurements in proximity. We
find that the mobile performance is significantly worse than stationary
performance. The power consumption of the new dish is higher, but seems to be
more correlated to the heating function of the dish than to the speed of the
vehicle.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the 2024 Network Traffic Measurement
  and Analysis Conference (TMA) for possible publication. Copyright may be
  transferred without notice, after which this version may no longer be
  accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automated Calibration of Parallel and Distributed Computing Simulators:
  A Case Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13918v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13918v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jesse McDonald, Maximilian Horzela, Frédéric Suter, Henri Casanova
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many parallel and distributed computing research results are obtained in
simulation, using simulators that mimic real-world executions on some target
system. Each such simulator is configured by picking values for parameters that
define the behavior of the underlying simulation models it implements. The main
concern for a simulator is accuracy: simulated behaviors should be as close as
possible to those observed in the real-world target system. This requires that
values for each of the simulator's parameters be carefully picked, or
"calibrated," based on ground-truth real-world executions. Examining the
current state of the art shows that simulator calibration, at least in the
field of parallel and distributed computing, is often undocumented (and thus
perhaps often not performed) and, when documented, is described as a
labor-intensive, manual process. In this work we evaluate the benefit of
automating simulation calibration using simple algorithms. Specifically, we use
a real-world case study from the field of High Energy Physics and compare
automated calibration to calibration performed by a domain scientist. Our main
finding is that automated calibration is on par with or significantly
outperforms the calibration performed by the domain scientist. Furthermore,
automated calibration makes it straightforward to operate desirable trade-offs
between simulation accuracy and simulation speed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in Proc. of the 25th IEEE International Workshop on
  Parallel and Distributed Scientific and Engineering Computing (PDSEC 2024)</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Artificial Intelligence
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On <span class="highlight-title">Pretrain</span>ing Data Diversity for <span class="highlight-title">Self-Supervised</span> Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13808v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13808v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hasan Abed Al Kader Hammoud, Tuhin Das, Fabio Pizzati, Philip Torr, Adel Bibi, Bernard Ghanem
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We explore the impact of training with more diverse datasets, characterized
by the number of unique samples, on the performance of self-supervised learning
(SSL) under a fixed computational budget. Our findings consistently demonstrate
that increasing pretraining data diversity enhances SSL performance, albeit
only when the distribution distance to the downstream data is minimal. Notably,
even with an exceptionally large pretraining data diversity achieved through
methods like web crawling or diffusion-generated data, among other ways, the
distribution shift remains a challenge. Our experiments are comprehensive with
seven SSL methods using large-scale datasets such as ImageNet and YFCC100M
amounting to over 200 GPU days. Code and trained models will be available at
https://github.com/hammoudhasan/DiversitySSL .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RAR: Retrieving And Ranking Augmented MLLMs for Visual Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13805v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13805v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyu Liu, Zeyi Sun, Yuhang Zang, Wei Li, Pan Zhang, Xiaoyi Dong, Yuanjun Xiong, Dahua Lin, Jiaqi Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  CLIP (Contrastive Language-Image Pre-training) uses contrastive learning from
noise image-text pairs to excel at recognizing a wide array of candidates, yet
its focus on broad associations hinders the precision in distinguishing subtle
differences among fine-grained items. Conversely, Multimodal Large Language
Models (MLLMs) excel at classifying fine-grained categories, thanks to their
substantial knowledge from pre-training on web-level corpora. However, the
performance of MLLMs declines with an increase in category numbers, primarily
due to growing complexity and constraints of limited context window size. To
synergize the strengths of both approaches and enhance the few-shot/zero-shot
recognition abilities for datasets characterized by extensive and fine-grained
vocabularies, this paper introduces RAR, a Retrieving And Ranking augmented
method for MLLMs. We initially establish a multi-modal retriever based on CLIP
to create and store explicit memory for different categories beyond the
immediate context window. During inference, RAR retrieves the top-k similar
results from the memory and uses MLLMs to rank and make the final predictions.
Our proposed approach not only addresses the inherent limitations in
fine-grained recognition but also preserves the model's comprehensive knowledge
base, significantly boosting accuracy across a range of vision-language
recognition tasks. Notably, our approach demonstrates a significant improvement
in performance on 5 fine-grained visual recognition benchmarks, 11 few-shot
image recognition datasets, and the 2 object detection datasets under the
zero-shot recognition setting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project: https://github.com/Liuziyu77/RAR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ZigMa: Zigzag Mamba Diffusion Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13802v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13802v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vincent Tao Hu, Stefan Andreas Baumann, Ming Gui, Olga Grebenkova, Pingchuan Ma, Johannes Fischer, Bjorn Ommer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The diffusion model has long been plagued by scalability and quadratic
complexity issues, especially within transformer-based structures. In this
study, we aim to leverage the long sequence modeling capability of a
State-Space Model called Mamba to extend its applicability to visual data
generation. Firstly, we identify a critical oversight in most current
Mamba-based vision methods, namely the lack of consideration for spatial
continuity in the scan scheme of Mamba. Secondly, building upon this insight,
we introduce a simple, plug-and-play, zero-parameter method named Zigzag Mamba,
which outperforms Mamba-based baselines and demonstrates improved speed and
memory utilization compared to transformer-based baselines. Lastly, we
integrate Zigzag Mamba with the Stochastic Interpolant framework to investigate
the scalability of the model on large-resolution visual datasets, such as
FacesHQ $1024\times 1024$ and UCF101, MultiModal-CelebA-HQ, and MS COCO
$256\times 256$. Code will be released at https://taohu.me/zigma/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://taohu.me/zigma/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Natural Language as Polices: Reasoning for Coordinate-Level Embodied
  Control with LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13801v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13801v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yusuke Mikami, Andrew Melnik, Jun Miura, Ville Hautamäki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We demonstrate experimental results with LLMs that address robotics action
planning problems. Recently, LLMs have been applied in robotics action
planning, particularly using a code generation approach that converts complex
high-level instructions into mid-level policy codes. In contrast, our approach
acquires text descriptions of the task and scene objects, then formulates
action planning through natural language reasoning, and outputs coordinate
level control commands, thus reducing the necessity for intermediate
representation code as policies. Our approach is evaluated on a multi-modal
prompt simulation benchmark, demonstrating that our prompt engineering
experiments with natural language reasoning significantly enhance success rates
compared to its absence. Furthermore, our approach illustrates the potential
for natural language descriptions to transfer robotics skills from known tasks
to previously unseen tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reverse Training to Nurse the Reversal Curse 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13799v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13799v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Olga Golovneva, Zeyuan Allen-Zhu, Jason Weston, Sainbayar Sukhbaatar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have a surprising failure: when trained on "A
has a feature B", they do not generalize to "B is a feature of A", which is
termed the Reversal Curse. Even when training with trillions of tokens this
issue still appears due to Zipf's law - hence even if we train on the entire
internet. This work proposes an alternative training scheme, called reverse
training, whereby all words are used twice, doubling the amount of available
tokens. The LLM is trained in both forward and reverse directions by reversing
the training strings while preserving (i.e., not reversing) chosen substrings,
such as entities. We show that data-matched reverse-trained models provide
superior performance to standard models on standard tasks, and compute-matched
reverse-trained models provide far superior performance on reversal tasks,
helping resolve the reversal curse issue.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hierarchical NeuroSymbolic Approach for Action Quality Assessment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13798v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13798v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lauren Okamoto, Paritosh Parmar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Action quality assessment (AQA) applies computer vision to quantitatively
assess the performance or execution of a human action. Current AQA approaches
are end-to-end neural models, which lack transparency and tend to be biased
because they are trained on subjective human judgements as ground-truth. To
address these issues, we introduce a neuro-symbolic paradigm for AQA, which
uses neural networks to abstract interpretable symbols from video data and
makes quality assessments by applying rules to those symbols. We take diving as
the case study. We found that domain experts prefer our system and find it more
informative than purely neural approaches to AQA in diving. Our system also
achieves state-of-the-art action recognition and temporal segmentation, and
automatically generates a detailed report that breaks the dive down into its
elements and provides objective scoring with visual evidence. As verified by a
group of domain experts, this report may be used to assist judges in scoring,
help train judges, and provide feedback to divers. We will open-source all of
our annotated training data and code for ease of reproducibility.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Model Openness Framework: Promoting Completeness and Openness for
  Reproducibility, Transparency and Usability in AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13784v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13784v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matt White, Ibrahim Haddad, Cailean Osborne, Xiao-Yang Liu, Ahmed Abdelmonsef, Sachin Varghese
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative AI (GAI) offers unprecedented possibilities but its
commercialization has raised concerns about transparency, reproducibility,
bias, and safety. Many "open-source" GAI models lack the necessary components
for full understanding and reproduction, and some use restrictive licenses, a
practice known as "openwashing." We propose the Model Openness Framework (MOF),
a ranked classification system that rates machine learning models based on
their completeness and openness, following principles of open science, open
source, open data, and open access. The MOF requires specific components of the
model development lifecycle to be included and released under appropriate open
licenses. This framework aims to prevent misrepresentation of models claiming
to be open, guide researchers and developers in providing all model components
under permissive licenses, and help companies, academia, and hobbyists identify
models that can be safely adopted without restrictions. Wide adoption of the
MOF will foster a more open AI ecosystem, accelerating research, innovation,
and adoption.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>45 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Information-Theoretic Distillation for Reference-less Summarization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13780v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13780v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaehun Jung, Ximing Lu, Liwei Jiang, Faeze Brahman, Peter West, Pang Wei Koh, Yejin Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The current winning recipe for automatic summarization is using proprietary
large-scale language models (LLMs) such as ChatGPT as is, or imitation learning
from them as teacher models. While increasingly ubiquitous dependence on such
large-scale language models is convenient, there remains an important question
of whether small-scale models could have achieved competitive results, if we
were to seek an alternative learning method -- that allows for a more
cost-efficient, controllable, yet powerful summarizer. We present InfoSumm, a
novel framework to distill a powerful summarizer based on the
information-theoretic objective for summarization, without relying on either
the LLM's capability or human-written references. To achieve this, we first
propose a novel formulation of the desiderata of summarization (saliency,
faithfulness and brevity) through the lens of mutual information between the
original document and the summary. Based on this formulation, we start off from
Pythia-2.8B as the teacher model, which is not yet capable of summarization,
then self-train the model to optimize for the information-centric measures of
ideal summaries. Distilling from the improved teacher, we arrive at a compact
but powerful summarizer with only 568M parameters that performs competitively
against ChatGPT, without ever relying on ChatGPT's capabilities. Extensive
analysis demonstrates that our approach outperforms in-domain supervised models
in human evaluation, let alone state-of-the-art unsupervised methods, and wins
over ChatGPT in controllable summarization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Principled Representation Learning from Videos for Reinforcement
  Learning <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13765v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13765v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dipendra Misra, Akanksha Saran, Tengyang Xie, Alex Lamb, John Langford
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study pre-training representations for decision-making using video data,
which is abundantly available for tasks such as game agents and software
testing. Even though significant empirical advances have been made on this
problem, a theoretical understanding remains absent. We initiate the
theoretical investigation into principled approaches for representation
learning and focus on learning the latent state representations of the
underlying MDP using video data. We study two types of settings: one where
there is iid noise in the observation, and a more challenging setting where
there is also the presence of exogenous noise, which is non-iid noise that is
temporally correlated, such as the motion of people or cars in the background.
We study three commonly used approaches: autoencoding, temporal contrastive
learning, and forward modeling. We prove upper bounds for temporal contrastive
learning and forward modeling in the presence of only iid noise. We show that
these approaches can learn the latent state and use it to do efficient
downstream RL with polynomial sample complexity. When exogenous noise is also
present, we establish a lower bound result showing that the sample complexity
of learning from video data can be exponentially worse than learning from
action-labeled trajectory data. This partially explains why reinforcement
learning with video pre-training is hard. We evaluate these representational
learning methods in two visual domains, yielding results that are consistent
with our theoretical findings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024 Spotlight Conference Paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hyper Strategy Logic <span class="chip">AAMAS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13741v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13741v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raven Beutner, Bernd Finkbeiner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Strategy logic (SL) is a powerful temporal logic that enables strategic
reasoning in multi-agent systems. SL supports explicit (first-order)
quantification over strategies and provides a logical framework to express many
important properties such as Nash equilibria, dominant strategies, etc. While
in SL the same strategy can be used in multiple strategy profiles, each such
profile is evaluated w.r.t. a path-property, i.e., a property that considers
the single path resulting from a particular strategic interaction. In this
paper, we present Hyper Strategy Logic (HyperSL), a strategy logic where the
outcome of multiple strategy profiles can be compared w.r.t. a hyperproperty,
i.e., a property that relates multiple paths. We show that HyperSL can capture
important properties that cannot be expressed in SL, including
non-interference, quantitative Nash equilibria, optimal adversarial planning,
and reasoning under imperfect information. On the algorithmic side, we identify
an expressive fragment of HyperSL with decidable model checking and present a
model-checking algorithm. We contribute a prototype implementation of our
algorithm and report on encouraging experimental results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAMAS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reinforcement Learning for Online Testing of Autonomous Driving Systems:
  a Replication and Extension Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13729v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13729v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luca Giamattei, Matteo Biagiola, Roberto Pietrantuono, Stefano Russo, Paolo Tonella
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In a recent study, Reinforcement Learning (RL) used in combination with
many-objective search, has been shown to outperform alternative techniques
(random search and many-objective search) for online testing of Deep Neural
Network-enabled systems. The empirical evaluation of these techniques was
conducted on a state-of-the-art Autonomous Driving System (ADS). This work is a
replication and extension of that empirical study. Our replication shows that
RL does not outperform pure random test generation in a comparison conducted
under the same settings of the original study, but with no confounding factor
coming from the way collisions are measured. Our extension aims at eliminating
some of the possible reasons for the poor performance of RL observed in our
replication: (1) the presence of reward components providing contrasting or
useless feedback to the RL agent; (2) the usage of an RL algorithm (Q-learning)
which requires discretization of an intrinsically continuous state space.
Results show that our new RL agent is able to converge to an effective policy
that outperforms random testing. Results also highlight other possible
improvements, which open to further investigations on how to best leverage RL
for online ADS testing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ M-HOF-Opt: Multi-Objective Hierarchical Output Feedback Optimization via
  Multiplier Induced Loss Landscape Scheduling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13728v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13728v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xudong Sun, Nutan Chen, Alexej Gossmann, Yu Xing, Carla Feistner, Emilio Dorigatt, Felix Drost, Daniele Scarcella, Lisa Beer, Carsten Marr
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When a neural network parameterized loss function consists of many terms, the
combinatorial choice of weight multipliers during the optimization process
forms a challenging problem. To address this, we proposed a probabilistic
graphical model (PGM) for the joint model parameter and multiplier evolution
process, with a hypervolume based likelihood that promotes multi-objective
descent of each loss term. The corresponding parameter and multiplier
estimation as a sequential decision process is then cast into an optimal
control problem, where the multi-objective descent goal is dispatched
hierarchically into a series of constraint optimization sub-problems. The
sub-problem constraint automatically adapts itself according to Pareto
dominance and serves as the setpoint for the low level multiplier controller to
schedule loss landscapes via output feedback of each loss term. Our method is
multiplier-free and operates at the timescale of epochs, thus saves tremendous
computational resources compared to full training cycle multiplier tuning. We
applied it to domain invariant variational auto-encoding with 6 loss terms on
the PACS domain generalization task, and observed robust performance across a
range of controller hyperparameters, as well as different multiplier initial
conditions, outperforming other multiplier scheduling methods. We offered
modular implementation of our method, admitting custom definition of many loss
terms for applying our multi-objective hierarchical output feedback training
scheme to other deep learning fields.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Language Models meet Network Slicing Management and Orchestration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13721v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13721v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdulhalim Dandoush, Viswanath Kumarskandpriya, Mueen Uddin, Usman Khalil
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Network slicing, a cornerstone technology for future networks, enables the
creation of customized virtual networks on a shared physical infrastructure.
This fosters innovation and agility by providing dedicated resources tailored
to specific applications. However, current orchestration and management
approaches face limitations in handling the complexity of new service demands
within multi-administrative domain environments. This paper proposes a future
vision for network slicing powered by Large Language Models (LLMs) and
multi-agent systems, offering a framework that can be integrated with existing
Management and Orchestration (MANO) frameworks. This framework leverages LLMs
to translate user intent into technical requirements, map network functions to
infrastructure, and manage the entire slice lifecycle, while multi-agent
systems facilitate collaboration across different administrative domains. We
also discuss the challenges associated with implementing this framework and
potential solutions to mitigate them.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Research Re: search & Re-search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13705v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13705v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aske Plaat
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Search algorithms are often categorized by their node expansion strategy. One
option is the depth-first strategy, a simple backtracking strategy that
traverses the search space in the order in which successor nodes are generated.
An alternative is the best-first strategy, which was designed to make it
possible to use domain-specific heuristic information. By exploring promising
parts of the search space first, best-first algorithms are usually more
efficient than depth-first algorithms.
  In programs that play minimax games such as chess and checkers, the
efficiency of the search is of crucial importance. Given the success of
best-first algorithms in other domains, one would expect them to be used for
minimax games too. However, all high-performance game-playing programs are
based on a depth-first algorithm.
  This study takes a closer look at a depth-first algorithm, AB, and a
best-first algorithm, SSS. The prevailing opinion on these algorithms is that
SSS offers the potential for a more efficient search, but that its complicated
formulation and exponential memory requirements render it impractical. The
theoretical part of this work shows that there is a surprisingly
straightforward link between the two algorithms -- for all practical purposes,
SSS is a special case of AB. Subsequent empirical evidence proves the
prevailing opinion on SSS to be wrong: it is not a complicated algorithm, it
does not need too much memory, and it is also not more efficient than
depth-first search.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>PhD thesis Aske Plaat 20 June 1996. AlphaBeta, SSS*, MTD(f)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fostc3net:A Lightweight YOLOv5 Based On the Network Structure
  Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13703v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13703v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Danqing Ma, Shaojie Li, Bo Dang, Hengyi Zang, Xinqi Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transmission line detection technology is crucial for automatic monitoring
and ensuring the safety of electrical facilities. The YOLOv5 series is
currently one of the most advanced and widely used methods for object
detection. However, it faces inherent challenges, such as high computational
load on devices and insufficient detection accuracy. To address these concerns,
this paper presents an enhanced lightweight YOLOv5 technique customized for
mobile devices, specifically intended for identifying objects associated with
transmission lines. The C3Ghost module is integrated into the convolutional
network of YOLOv5 to reduce floating point operations per second (FLOPs) in the
feature channel fusion process and improve feature expression performance. In
addition, a FasterNet module is introduced to replace the c3 module in the
YOLOv5 Backbone. The FasterNet module uses Partial Convolutions to process only
a portion of the input channels, improving feature extraction efficiency and
reducing computational overhead. To address the imbalance between simple and
challenging samples in the dataset and the diversity of aspect ratios of
bounding boxes, the wIoU v3 LOSS is adopted as the loss function. To validate
the performance of the proposed approach, Experiments are conducted on a custom
dataset of transmission line poles. The results show that the proposed model
achieves a 1% increase in detection accuracy, a 13% reduction in FLOPs, and a
26% decrease in model parameters compared to the existing YOLOv5.In the
ablation experiment, it was also discovered that while the Fastnet module and
the CSghost module improved the precision of the original YOLOv5 baseline
model, they caused a decrease in the mAP@.5-.95 metric. However, the
improvement of the wIoUv3 loss function significantly mitigated the decline of
the mAP@.5-.95 metric.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SPTNet: An Efficient Alternative Framework for Generalized Category
  Discovery with Spatial <span class="highlight-title">Prompt</span> Tuning <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13684v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13684v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongjun Wang, Sagar Vaze, Kai Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generalized Category Discovery (GCD) aims to classify unlabelled images from
both `seen' and `unseen' classes by transferring knowledge from a set of
labelled `seen' class images. A key theme in existing GCD approaches is
adapting large-scale pre-trained models for the GCD task. An alternate
perspective, however, is to adapt the data representation itself for better
alignment with the pre-trained model. As such, in this paper, we introduce a
two-stage adaptation approach termed SPTNet, which iteratively optimizes model
parameters (i.e., model-finetuning) and data parameters (i.e., prompt
learning). Furthermore, we propose a novel spatial prompt tuning method (SPT)
which considers the spatial property of image data, enabling the method to
better focus on object parts, which can transfer between seen and unseen
classes. We thoroughly evaluate our SPTNet on standard benchmarks and
demonstrate that our method outperforms existing GCD methods. Notably, we find
our method achieves an average accuracy of 61.4% on the SSB, surpassing prior
state-of-the-art methods by approximately 10%. The improvement is particularly
remarkable as our method yields extra parameters amounting to only 0.117% of
those in the backbone architecture. Project page:
https://visual-ai.github.io/sptnet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as a conference paper at ICLR 2024; Project page:
  https://visual-ai.github.io/sptnet</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Threats, Attacks, and Defenses in Machine Unlearning: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13682v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13682v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyao Liu, Huanyi Ye, Chen Chen, Kwok-Yan Lam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, Machine Unlearning (MU) has gained considerable attention for its
potential to improve AI safety by removing the influence of specific data from
trained Machine Learning (ML) models. This process, known as knowledge removal,
addresses concerns about data such as sensitivity, copyright restrictions,
obsolescence, or low quality. This capability is also crucial for ensuring
compliance with privacy regulations such as the Right To Be Forgotten (RTBF).
Therefore, strategic knowledge removal mitigates the risk of harmful outcomes,
safeguarding against biases, misinformation, and unauthorized data
exploitation, thereby enhancing the ethical use and reliability of AI systems.
Efforts have been made to design efficient unlearning approaches, with MU
services being examined for integration with existing machine learning as a
service (MLaaS), allowing users to submit requests to erase data. However,
recent research highlights vulnerabilities in machine unlearning systems, such
as information leakage and malicious unlearning requests, that can lead to
significant security and privacy concerns. Moreover, extensive research
indicates that unlearning methods and prevalent attacks fulfill diverse roles
within MU systems. For instance, unlearning can act as a mechanism to recover
models from backdoor attacks, while backdoor attacks themselves can serve as an
evaluation metric for unlearning effectiveness. This underscores the intricate
relationship and complex interplay between these elements in maintaining system
functionality and safety. Therefore, this survey seeks to bridge the gap
between the extensive number of studies on threats, attacks, and defenses in
machine unlearning and the absence of a comprehensive review that categorizes
their taxonomy, methods, and solutions, thus offering valuable insights for
future research directions and practical implementations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PARAMANU-AYN: An Efficient Novel Generative and Instruction-tuned
  Language Model for Indian Legal Case Documents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13681v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13681v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mitodru Niyogi, Arnab Bhattacharya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present PARAMANU-AYN, a language model based exclusively on
case documents of the Supreme Court of India, the Constitution of India, and
the Indian Penal Code. The novel Auto Regressive (AR) decoder based model is
pretrained from scratch at a context size of 8192. We evaluated our pretrained
legal model on perplexity metrics. We also instruction-tuned our pretrained
model on a set of 10,763 instructions covering various legal tasks such as
legal reasoning, judgement explanation, legal clause generation, legal
drafting, legal contract drafting, case summarization, constitutional
question-answering, etc. We also evaluated the responses of prompts for
instruction-tuned models by GPT-3.5-Turbo on clarity, relevance, completeness,
and legal reasoning metrics in a scale of 10. Our model can be run on CPU and
achieved 42.46 tokens/sec CPU inference speed. We found that our models,
despite not being pretrained on legal books, various legal contracts, and legal
documents, were able to learn the domain knowledge required for drafting
various legal contracts and legal clauses, and generalize to draft legal
contracts and legal clauses with limited instruction tuning. Hence, we conclude
that for a strong domain-specialized generative language model (such as legal),
very large amounts of data are not required to develop models from scratch. We
believe that this work is the first attempt to make a dedicated generative
legal language model from scratch for Indian Supreme Court jurisdiction or in
legal NLP overall. We plan to release our Paramanu-Ayn model at
https://www.bharatgpts.com.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning User Embeddings from Human Gaze for Personalised Saliency
  Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13653v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13653v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florian Strohm, Mihai Bâce, Andreas Bulling
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reusable embeddings of user behaviour have shown significant performance
improvements for the personalised saliency prediction task. However, prior
works require explicit user characteristics and preferences as input, which are
often difficult to obtain. We present a novel method to extract user embeddings
from pairs of natural images and corresponding saliency maps generated from a
small amount of user-specific eye tracking data. At the core of our method is a
Siamese convolutional neural encoder that learns the user embeddings by
contrasting the image and personal saliency map pairs of different users.
Evaluations on two public saliency datasets show that the generated embeddings
have high discriminative power, are effective at refining universal saliency
maps to the individual users, and generalise well across users and images.
Finally, based on our model's ability to encode individual user
characteristics, our work points towards other applications that can benefit
from reusable embeddings of gaze behaviour.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dynamic Resource Allocation for Virtual Machine Migration Optimization
  using Machine Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13619v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13619v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yulu Gong, Jiaxin Huang, Bo Liu, Jingyu Xu, Binbin Wu, Yifan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The paragraph is grammatically correct and logically coherent. It discusses
the importance of mobile terminal cloud computing migration technology in
meeting the demands of evolving computer and cloud computing technologies. It
emphasizes the need for efficient data access and storage, as well as the
utilization of cloud computing migration technology to prevent additional time
delays. The paragraph also highlights the contributions of cloud computing
migration technology to expanding cloud computing services. Additionally, it
acknowledges the role of virtualization as a fundamental capability of cloud
computing while emphasizing that cloud computing and virtualization are not
inherently interconnected. Finally, it introduces machine learning-based
virtual machine migration optimization and dynamic resource allocation as a
critical research direction in cloud computing, citing the limitations of
static rules or manual settings in traditional cloud computing environments.
Overall, the paragraph effectively communicates the importance of machine
learning technology in addressing resource allocation and virtual machine
migration challenges in cloud computing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ No more optimization rules: LLM-enabled policy-based multi-modal query
  optimizer (version 1) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13597v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13597v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Wang, Haodi Ma, Daisy Zhe Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language model (LLM) has marked a pivotal moment in the field of
machine learning and deep learning. Recently its capability for query planning
has been investigated, including both single-modal and multi-modal queries.
However, there is no work on the query optimization capability of LLM. As a
critical (or could even be the most important) step that significantly impacts
the execution performance of the query plan, such analysis and attempts should
not be missed. From another aspect, existing query optimizers are usually
rule-based or rule-based + cost-based, i.e., they are dependent on manually
created rules to complete the query plan rewrite/transformation. Given the fact
that modern optimizers include hundreds to thousands of rules, designing a
multi-modal query optimizer following a similar way is significantly
time-consuming since we will have to enumerate as many multi-modal optimization
rules as possible, which has not been well addressed today. In this paper, we
investigate the query optimization ability of LLM and use LLM to design LaPuda,
a novel LLM and Policy based multi-modal query optimizer. Instead of
enumerating specific and detailed rules, LaPuda only needs a few abstract
policies to guide LLM in the optimization, by which much time and human effort
are saved. Furthermore, to prevent LLM from making mistakes or negative
optimization, we borrow the idea of gradient descent and propose a guided cost
descent (GCD) algorithm to perform the optimization, such that the optimization
can be kept in the correct direction. In our evaluation, our methods
consistently outperform the baselines in most cases. For example, the optimized
plans generated by our methods result in 1~3x higher execution speed than those
by the baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Yifan and Haodi contribute equally to the work</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Large Language Model Enhanced Sequential Recommender for Joint Video
  and Comment Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13574v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13574v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bowen Zheng, Zihan Lin, Enze Liu, Chen Yang, Enyang Bai, Cheng Ling, Wayne Xin Zhao, Ji-Rong Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In online video platforms, reading or writing comments on interesting videos
has become an essential part of the video watching experience. However,
existing video recommender systems mainly model users' interaction behaviors
with videos, lacking consideration of comments in user behavior modeling. In
this paper, we propose a novel recommendation approach called LSVCR by
leveraging user interaction histories with both videos and comments, so as to
jointly conduct personalized video and comment recommendation. Specifically,
our approach consists of two key components, namely sequential recommendation
(SR) model and supplemental large language model (LLM) recommender. The SR
model serves as the primary recommendation backbone (retained in deployment) of
our approach, allowing for efficient user preference modeling. Meanwhile, we
leverage the LLM recommender as a supplemental component (discarded in
deployment) to better capture underlying user preferences from heterogeneous
interaction behaviors. In order to integrate the merits of the SR model and the
supplemental LLM recommender, we design a twostage training paradigm. The first
stage is personalized preference alignment, which aims to align the preference
representations from both components, thereby enhancing the semantics of the SR
model. The second stage is recommendation-oriented fine-tuning, in which the
alignment-enhanced SR model is fine-tuned according to specific objectives.
Extensive experiments in both video and comment recommendation tasks
demonstrate the effectiveness of LSVCR. Additionally, online A/B testing on the
KuaiShou platform verifies the actual benefits brought by our approach. In
particular, we achieve a significant overall gain of 4.13% in comment watch
time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Find n' Propagate: Open-Vocabulary 3D Object Detection in Urban
  Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13556v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13556v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Djamahl Etchegaray, Zi Huang, Tatsuya Harada, Yadan Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we tackle the limitations of current LiDAR-based 3D object
detection systems, which are hindered by a restricted class vocabulary and the
high costs associated with annotating new object classes. Our exploration of
open-vocabulary (OV) learning in urban environments aims to capture novel
instances using pre-trained vision-language models (VLMs) with multi-sensor
data. We design and benchmark a set of four potential solutions as baselines,
categorizing them into either top-down or bottom-up approaches based on their
input data strategies. While effective, these methods exhibit certain
limitations, such as missing novel objects in 3D box estimation or applying
rigorous priors, leading to biases towards objects near the camera or of
rectangular geometries. To overcome these limitations, we introduce a universal
\textsc{Find n' Propagate} approach for 3D OV tasks, aimed at maximizing the
recall of novel objects and propagating this detection capability to more
distant areas thereby progressively capturing more. In particular, we utilize a
greedy box seeker to search against 3D novel boxes of varying orientations and
depth in each generated frustum and ensure the reliability of newly identified
boxes by cross alignment and density ranker. Additionally, the inherent bias
towards camera-proximal objects is alleviated by the proposed remote simulator,
which randomly diversifies pseudo-labeled novel instances in the self-training
process, combined with the fusion of base samples in the memory bank. Extensive
experiments demonstrate a 53% improvement in novel recall across diverse OV
settings, VLMs, and 3D detectors. Notably, we achieve up to a 3.97-fold
increase in Average Precision (AP) for novel object classes. The source code is
made available in the supplementary material.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VCounselor: A Psychological Intervention Chat Agent Based on a
  Knowledge-Enhanced Large Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13553v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13553v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        H. Zhang, Z. Qiao, H. Wang, B. Duan, J. Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conversational artificial intelligence can already independently engage in
brief conversations with clients with psychological problems and provide
evidence-based psychological interventions. The main objective of this study is
to improve the effectiveness and credibility of the large language model in
psychological intervention by creating a specialized agent, the VCounselor, to
address the limitations observed in popular large language models such as
ChatGPT in domain applications. We achieved this goal by proposing a new
affective interaction structure and knowledge-enhancement structure. In order
to evaluate VCounselor, this study compared the general large language model,
the fine-tuned large language model, and VCounselor's knowledge-enhanced large
language model. At the same time, the general large language model and the
fine-tuned large language model will also be provided with an avatar to compare
them as an agent with VCounselor. The comparison results indicated that the
affective interaction structure and knowledge-enhancement structure of
VCounselor significantly improved the effectiveness and credibility of the
psychological intervention, and VCounselor significantly provided positive
tendencies for clients' emotions. The conclusion of this study strongly
supports that VConselor has a significant advantage in providing psychological
support to clients by being able to analyze the patient's problems with
relative accuracy and provide professional-level advice that enhances support
for clients.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ What explains the success of cross-modal fine-tuning with ORCA? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13537v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13537v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paloma García-de-Herreros, Vagrant Gautam, Philipp Slusallek, Dietrich Klakow, Marius Mosbach
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  ORCA (Shen et al., 2023) is a recent technique for cross-modal fine-tuning,
i.e., applying pre-trained transformer models to modalities beyond their
training data. The technique consists primarily of training an embedder and
fine-tuning the embedder and model. Despite its high performance on a variety
of downstream tasks, we do not understand precisely how each of these
components contribute to ORCA's success. Therefore, we run a series of
ablations and find that embedder training does not help 2D tasks at all,
contrary to what the original paper posits. In 1D tasks, some amount of
embedder training is necessary but more is not better. In 4 out of 6 datasets
we experiment with, it is model fine-tuning that makes the biggest difference.
Through our ablations and baselines, we contribute a better understanding of
the individual components of ORCA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Compress3D: a Compressed Latent Space for 3D Generation from a Single
  Image 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13524v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13524v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bowen Zhang, Tianyu Yang, Yu Li, Lei Zhang, Xi Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D generation has witnessed significant advancements, yet efficiently
producing high-quality 3D assets from a single image remains challenging. In
this paper, we present a triplane autoencoder, which encodes 3D models into a
compact triplane latent space to effectively compress both the 3D geometry and
texture information. Within the autoencoder framework, we introduce a 3D-aware
cross-attention mechanism, which utilizes low-resolution latent representations
to query features from a high-resolution 3D feature volume, thereby enhancing
the representation capacity of the latent space. Subsequently, we train a
diffusion model on this refined latent space. In contrast to solely relying on
image embedding for 3D generation, our proposed method advocates for the
simultaneous utilization of both image embedding and shape embedding as
conditions. Specifically, the shape embedding is estimated via a diffusion
prior model conditioned on the image embedding. Through comprehensive
experiments, we demonstrate that our method outperforms state-of-the-art
algorithms, achieving superior performance while requiring less training data
and time. Our approach enables the generation of high-quality 3D assets in
merely 7 seconds on a single A100 GPU.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Have You Poisoned My Data? Defending Neural Networks against Data
  Poisoning <span class="chip">ESORICS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13523v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13523v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fabio De Gaspari, Dorjan Hitaj, Luigi V. Mancini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The unprecedented availability of training data fueled the rapid development
of powerful neural networks in recent years. However, the need for such large
amounts of data leads to potential threats such as poisoning attacks:
adversarial manipulations of the training data aimed at compromising the
learned model to achieve a given adversarial goal.
  This paper investigates defenses against clean-label poisoning attacks and
proposes a novel approach to detect and filter poisoned datapoints in the
transfer learning setting. We define a new characteristic vector representation
of datapoints and show that it effectively captures the intrinsic properties of
the data distribution. Through experimental analysis, we demonstrate that
effective poisons can be successfully differentiated from clean points in the
characteristic vector space. We thoroughly evaluate our proposed approach and
compare it to existing state-of-the-art defenses using multiple architectures,
datasets, and poison budgets. Our evaluation shows that our proposal
outperforms existing approaches in defense rate and final trained model
performance across all experimental settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper accepted for publication at European Symposium on Research in
  Computer Security (ESORICS) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Motion Generation from Fine-grained Textual Descriptions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13518v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13518v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kunhang Li, Yansong Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The task of text2motion is to generate motion sequences from given textual
descriptions, where a model should explore the interactions between natural
language instructions and human body movements. While most existing works are
confined to coarse-grained motion descriptions (e.g., "A man squats."),
fine-grained ones specifying movements of relevant body parts are barely
explored. Models trained with coarse texts may not be able to learn mappings
from fine-grained motion-related words to motion primitives, resulting in the
failure in generating motions from unseen descriptions. In this paper, we build
a large-scale language-motion dataset with fine-grained textual descriptions,
FineHumanML3D, by feeding GPT-3.5-turbo with delicate prompts. Accordingly, we
design a new text2motion model, FineMotionDiffuse, which makes full use of
fine-grained textual information. Our experiments show that FineMotionDiffuse
trained on FineHumanML3D acquires good results in quantitative evaluation. We
also find this model can better generate spatially/chronologically composite
motions by learning the implicit mappings from simple descriptions to the
corresponding basic motions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ What if...?: Counterfactual Inception to Mitigate Hallucination Effects
  in Large Multimodal Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13513v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13513v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junho Kim, Yeon Ju Kim, Yong Man Ro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a way of enhancing the reliability of Large Multimodal
Models (LMMs) in addressing hallucination effects, where models generate
incorrect or unrelated responses. Without additional instruction tuning
paradigm, we introduce Counterfactual Inception, a novel method that implants
counterfactual thoughts into LMMs using carefully chosen, misaligned
counterfactual keywords. This method is grounded in the concept of
counterfactual thinking, a cognitive process where humans consider alternative
realities and outcomes. By applying this human-like reasoning mechanism to
LMMs, we aim to reduce hallucination effects and improve the models'
trustworthiness. We also propose Dual-modality Verification Process (DVP), a
rigorous framework for selecting optimal counterfactual keywords to trigger
counterfactual thinking into LMMs, concurrently considering visual and
linguistic context. Our extensive experiments across various LMMs, including
both open-source and proprietary models, corroborate that our method
significantly mitigates hallucination phenomena across different datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>under review, code available:
  https://github.com/IVY-LVLM/Counterfactual-Inception</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AnyHome: Open-Vocabulary Generation of Structured and Textured 3D Homes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.06644v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.06644v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rao Fu, Zehao Wen, Zichen Liu, Srinath Sridhar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inspired by cognitive theories, we introduce AnyHome, a framework that
translates any text into well-structured and textured indoor scenes at a
house-scale. By prompting Large Language Models (LLMs) with designed templates,
our approach converts provided textual narratives into amodal structured
representations. These representations guarantee consistent and realistic
spatial layouts by directing the synthesis of a geometry mesh within defined
constraints. A Score Distillation Sampling process is then employed to refine
the geometry, followed by an egocentric inpainting process that adds lifelike
textures to it. AnyHome stands out with its editability, customizability,
diversity, and realism. The structured representations for scenes allow for
extensive editing at varying levels of granularity. Capable of interpreting
texts ranging from simple labels to detailed narratives, AnyHome generates
detailed geometries and textures that outperform existing methods in both
quantitative and qualitative measures.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Magic-Me: Identity-Specific Video Customized Diffusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.09368v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.09368v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ze Ma, Daquan Zhou, Chun-Hsiao Yeh, Xue-She Wang, Xiuyu Li, Huanrui Yang, Zhen Dong, Kurt Keutzer, Jiashi Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Creating content with specified identities (ID) has attracted significant
interest in the field of generative models. In the field of text-to-image
generation (T2I), subject-driven creation has achieved great progress with the
identity controlled via reference images. However, its extension to video
generation is not well explored. In this work, we propose a simple yet
effective subject identity controllable video generation framework, termed
Video Custom Diffusion (VCD). With a specified identity defined by a few
images, VCD reinforces the identity characteristics and injects frame-wise
correlation at the initialization stage for stable video outputs. To achieve
this, we propose three novel components that are essential for high-quality
identity preservation and stable video generation: 1) a noise initialization
method with 3D Gaussian Noise Prior for better inter-frame stability; 2) an ID
module based on extended Textual Inversion trained with the cropped identity to
disentangle the ID information from the background 3) Face VCD and Tiled VCD
modules to reinforce faces and upscale the video to higher resolution while
preserving the identity's features. We conducted extensive experiments to
verify that VCD is able to generate stable videos with better ID over the
baselines. Besides, with the transferability of the encoded identity in the ID
module, VCD is also working well with personalized text-to-image models
available publicly. The codes are available at
https://github.com/Zhen-Dong/Magic-Me.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page at https://magic-me-webpage.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Ada-NAV: Adaptive Trajectory Length-Based Sample Efficient Policy
  Learning for Robotic Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.06192v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.06192v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bhrij Patel, Kasun Weerakoon, Wesley A. Suttle, Alec Koppel, Brian M. Sadler, Tianyi Zhou, Amrit Singh Bedi, Dinesh Manocha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Trajectory length stands as a crucial hyperparameter within reinforcement
learning (RL) algorithms, significantly contributing to the sample inefficiency
in robotics applications. Motivated by the pivotal role trajectory length plays
in the training process, we introduce Ada-NAV, a novel adaptive trajectory
length scheme designed to enhance the training sample efficiency of RL
algorithms in robotic navigation tasks. Unlike traditional approaches that
treat trajectory length as a fixed hyperparameter, we propose to dynamically
adjust it based on the entropy of the underlying navigation policy.
Interestingly, Ada-NAV can be applied to both existing on-policy and off-policy
RL methods, which we demonstrate by empirically validating its efficacy on
three popular RL methods: REINFORCE, Proximal Policy Optimization (PPO), and
Soft Actor-Critic (SAC). We demonstrate through simulated and real-world
robotic experiments that Ada-NAV outperforms conventional methods that employ
constant or randomly sampled trajectory lengths. Specifically, for a fixed
sample budget, Ada-NAV achieves an 18\% increase in navigation success rate, a
20-38\% reduction in navigation path length, and a 9.32\% decrease in elevation
costs. Furthermore, we showcase the versatility of Ada-NAV by integrating it
with the Clearpath Husky robot, illustrating its applicability in complex
outdoor environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 9 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TrackDiffusion: Tracklet-Conditioned Video Generation via Diffusion
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.00651v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.00651v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengxiang Li, Kai Chen, Zhili Liu, Ruiyuan Gao, Lanqing Hong, Guo Zhou, Hua Yao, Dit-Yan Yeung, Huchuan Lu, Xu Jia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite remarkable achievements in video synthesis, achieving granular
control over complex dynamics, such as nuanced movement among multiple
interacting objects, still presents a significant hurdle for dynamic world
modeling, compounded by the necessity to manage appearance and disappearance,
drastic scale changes, and ensure consistency for instances across frames.
These challenges hinder the development of video generation that can faithfully
mimic real-world complexity, limiting utility for applications requiring
high-level realism and controllability, including advanced scene simulation and
training of perception systems. To address that, we propose TrackDiffusion, a
novel video generation framework affording fine-grained trajectory-conditioned
motion control via diffusion models, which facilitates the precise manipulation
of the object trajectories and interactions, overcoming the prevalent
limitation of scale and continuity disruptions. A pivotal component of
TrackDiffusion is the instance enhancer, which explicitly ensures inter-frame
consistency of multiple objects, a critical factor overlooked in the current
literature. Moreover, we demonstrate that generated video sequences by our
TrackDiffusion can be used as training data for visual perception models. To
the best of our knowledge, this is the first work to apply video diffusion
models with tracklet conditions and demonstrate that generated frames can be
beneficial for improving the performance of object trackers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Having Beer after Prayer? Measuring Cultural Bias in Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.14456v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.14456v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tarek Naous, Michael J. Ryan, Alan Ritter, Wei Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the reach of large language models (LMs) expands globally, their ability
to cater to diverse cultural contexts becomes crucial. Despite advancements in
multilingual capabilities, models are not designed with appropriate cultural
nuances. In this paper, we show that multilingual and Arabic monolingual LMs
exhibit bias towards entities associated with Western culture. We introduce
CAMeL, a novel resource of 628 naturally-occurring prompts and 20,368 entities
spanning eight types that contrast Arab and Western cultures. CAMeL provides a
foundation for measuring cultural biases in LMs through both extrinsic and
intrinsic evaluations. Using CAMeL, we examine the cross-cultural performance
in Arabic of 16 different LMs on tasks such as story generation, NER, and
sentiment analysis, where we find concerning cases of stereotyping and cultural
unfairness. We further test their text-infilling performance, revealing the
incapability of appropriate adaptation to Arab cultural contexts. Finally, we
analyze 6 Arabic pre-training corpora and find that commonly used sources such
as Wikipedia may not be best suited to build culturally aware LMs, if used as
they are without adjustment. We will make CAMeL publicly available at:
https://github.com/tareknaous/camel
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Jaccard Metric Losses: Optimizing the Jaccard Index with Soft Labels <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.05666v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.05666v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zifu Wang, Xuefei Ning, Matthew B. Blaschko
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Intersection over Union (IoU) losses are surrogates that directly optimize
the Jaccard index. Leveraging IoU losses as part of the loss function have
demonstrated superior performance in semantic segmentation tasks compared to
optimizing pixel-wise losses such as the cross-entropy loss alone. However, we
identify a lack of flexibility in these losses to support vital training
techniques like label smoothing, knowledge distillation, and semi-supervised
learning, mainly due to their inability to process soft labels. To address
this, we introduce Jaccard Metric Losses (JMLs), which are identical to the
soft Jaccard loss in standard settings with hard labels but are fully
compatible with soft labels. We apply JMLs to three prominent use cases of soft
labels: label smoothing, knowledge distillation and semi-supervised learning,
and demonstrate their potential to enhance model accuracy and calibration. Our
experiments show consistent improvements over the cross-entropy loss across 4
semantic segmentation datasets (Cityscapes, PASCAL VOC, ADE20K, DeepGlobe Land)
and 13 architectures, including classic CNNs and recent vision transformers.
Remarkably, our straightforward approach significantly outperforms
state-of-the-art knowledge distillation and semi-supervised learning methods.
The code is available at
\href{https://github.com/zifuwanggg/JDTLosses}{https://github.com/zifuwanggg/JDTLosses}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AutoMix: Automatically Mixing Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.12963v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.12963v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aman Madaan, Pranjal Aggarwal, Ankit Anand, Srividya Pranavi Potharaju, Swaroop Mishra, Pei Zhou, Aditya Gupta, Dheeraj Rajagopal, Karthik Kappaganthu, Yiming Yang, Shyam Upadhyay,  Mausam, Manaal Faruqui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are now available from cloud API providers in
various sizes and configurations. While this diversity offers a broad spectrum
of choices, effectively leveraging the options to optimize computational cost
and performance remains challenging. In this work, we present AutoMix, an
approach that strategically routes queries to larger LMs, based on the
approximate correctness of outputs from a smaller LM. Central to AutoMix is a
few-shot self-verification mechanism, which estimates the reliability of its
own outputs without requiring training. Given that verifications can be noisy,
we employ a meta-verifier in AutoMix to refine the accuracy of these
assessments. Our experiments using LLAMA2-13B and GPT-4, on five
context-grounded reasoning datasets demonstrate that AutoMix surpasses
established baselines, improving the incremental benefit per cost by up to 86%.
Our code and data are available at https://github.com/automix-llm/automix.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The first two authors contributed equally. Work started and partly
  done during Aman's internship at Google. This version adds results on
  additional models and datasets</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Algorithms for Verification of Markov Decision Processes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09184v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09184v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tomáš Brázdil, Krishnendu Chatterjee, Martin Chmelik, Vojtěch Forejt, Jan Křetínský, Marta Kwiatkowska, Tobias Meggendorfer, David Parker, Mateusz Ujma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a general framework for applying learning algorithms and
heuristical guidance to the verification of Markov decision processes (MDPs).
The primary goal of our techniques is to improve performance by avoiding an
exhaustive exploration of the state space, instead focussing on particularly
relevant areas of the system, guided by heuristics. Our work builds on the
previous results of Br{\'{a}}zdil et al., significantly extending it as well as
refining several details and fixing errors.
  The presented framework focuses on probabilistic reachability, which is a
core problem in verification, and is instantiated in two distinct scenarios.
The first assumes that full knowledge of the MDP is available, in particular
precise transition probabilities. It performs a heuristic-driven partial
exploration of the model, yielding precise lower and upper bounds on the
required probability. The second tackles the case where we may only sample the
MDP without knowing the exact transition dynamics. Here, we obtain
probabilistic guarantees, again in terms of both the lower and upper bounds,
which provides efficient stopping criteria for the approximation. In
particular, the latter is an extension of statistical model-checking (SMC) for
unbounded properties in MDPs. In contrast to other related approaches, we do
not restrict our attention to time-bounded (finite-horizon) or discounted
properties, nor assume any particular structural properties of the MDP.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MMICL: Empowering Vision-language Model with Multi-Modal In-Context
  Learning <span class="chip">ICLR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.07915v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.07915v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haozhe Zhao, Zefan Cai, Shuzheng Si, Xiaojian Ma, Kaikai An, Liang Chen, Zixuan Liu, Sheng Wang, Wenjuan Han, Baobao Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Since the resurgence of deep learning, vision-language models (VLMs) enhanced
by large language models (LLMs) have grown exponentially in popularity.
However, while LLMs can utilize extensive background knowledge and task
information with in-context learning, most VLMs still struggle with
understanding complex multi-modal prompts with multiple images, making VLMs
less effective in downstream vision-language tasks. In this paper, we address
the limitation above by 1) introducing vision-language Model with Multi-Modal
In-Context Learning(MMICL), a new approach to allow the VLM to deal with
multi-modal inputs efficiently; 2) proposing a novel context scheme to augment
the in-context learning ability of the VLM; 3) constructing the Multi-modal
In-Context Learning (MIC) dataset, designed to enhance the VLM's ability to
understand complex multi-modal prompts. Our experiments confirm that MMICL
achieves new state-of-the-art zero-shot performance on a wide range of general
vision-language tasks, especially for complex benchmarks, including MME and
MMBench. Our analysis demonstrates that MMICL effectively tackles the challenge
of complex multi-modal prompt understanding and emerges the impressive ICL
ability. Furthermore, we observe that MMICL successfully alleviates language
bias in VLMs, a common issue for VLMs that often leads to hallucination when
faced with extensive textual context. Our code, dataset, dataset tool, and
model are available at https://github.com/PKUnlp-icler/MIC
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Graph Neural Networks for Learning Equivariant Representations of Neural
  Networks <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12143v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12143v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Miltiadis Kofinas, Boris Knyazev, Yan Zhang, Yunlu Chen, Gertjan J. Burghouts, Efstratios Gavves, Cees G. M. Snoek, David W. Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural networks that process the parameters of other neural networks find
applications in domains as diverse as classifying implicit neural
representations, generating neural network weights, and predicting
generalization errors. However, existing approaches either overlook the
inherent permutation symmetry in the neural network or rely on intricate
weight-sharing patterns to achieve equivariance, while ignoring the impact of
the network architecture itself. In this work, we propose to represent neural
networks as computational graphs of parameters, which allows us to harness
powerful graph neural networks and transformers that preserve permutation
symmetry. Consequently, our approach enables a single model to encode neural
computational graphs with diverse architectures. We showcase the effectiveness
of our method on a wide range of tasks, including classification and editing of
implicit neural representations, predicting generalization performance, and
learning to optimize, while consistently outperforming state-of-the-art
methods. The source code is open-sourced at
https://github.com/mkofinas/neural-graphs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In ICLR 2024. Source code: https://github.com/mkofinas/neural-graphs</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dice Semimetric Losses: Optimizing the Dice Score with Soft Labels <span class="chip">MICCAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.16296v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.16296v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zifu Wang, Teodora Popordanoska, Jeroen Bertels, Robin Lemmens, Matthew B. Blaschko
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The soft Dice loss (SDL) has taken a pivotal role in numerous automated
segmentation pipelines in the medical imaging community. Over the last years,
some reasons behind its superior functioning have been uncovered and further
optimizations have been explored. However, there is currently no implementation
that supports its direct utilization in scenarios involving soft labels. Hence,
a synergy between the use of SDL and research leveraging the use of soft
labels, also in the context of model calibration, is still missing. In this
work, we introduce Dice semimetric losses (DMLs), which (i) are by design
identical to SDL in a standard setting with hard labels, but (ii) can be
employed in settings with soft labels. Our experiments on the public QUBIQ,
LiTS and KiTS benchmarks confirm the potential synergy of DMLs with soft labels
(e.g. averaging, label smoothing, and knowledge distillation) over hard labels
(e.g. majority voting and random selection). As a result, we obtain superior
Dice scores and model calibration, which supports the wider adoption of DMLs in
practice. The code is available at https://github.com/zifuwanggg/JDTLosses
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>MICCAI 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Observational and Experimental Insights into Machine Learning-Based
  Defect Classification in Wafers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.10705v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.10705v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kamal Taha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This survey paper offers a comprehensive review of methodologies utilizing
machine learning (ML) classification techniques for identifying wafer defects
in semiconductor manufacturing. Despite the growing body of research
demonstrating the effectiveness of ML in wafer defect identification, there is
a noticeable absence of comprehensive reviews on this subject. This survey
attempts to fill this void by amalgamating available literature and providing
an in-depth analysis of the advantages, limitations, and potential applications
of various ML classification algorithms in the realm of wafer defect detection.
An innovative taxonomy of methodologies that we present provides a detailed
classification of algorithms into more refined categories and techniques. This
taxonomy follows a three-tier structure, starting from broad methodology
categories and ending with specific techniques. It aids researchers in
comprehending the complex relationships between different algorithms and their
techniques. We employ a rigorous Observational and experimental evaluation to
rank these varying techniques. For the Observational evaluation, we assess
techniques based on a set of four criteria. The experimental evaluation ranks
the algorithms employing the same techniques, sub-categories, and categories.
Also the paper illuminates the future prospects of ML classification techniques
for wafer defect identification, underscoring potential advancements and
opportunities for further research in this field
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ABScribe: Rapid Exploration & Organization of Multiple Writing
  Variations in Human-AI Co-Writing Tasks using Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.00117v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.00117v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohi Reza, Nathan Laundry, Ilya Musabirov, Peter Dushniku, Zhi Yuan "Michael" Yu, Kashish Mittal, Tovi Grossman, Michael Liut, Anastasia Kuzminykh, Joseph Jay Williams
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Exploring alternative ideas by rewriting text is integral to the writing
process. State-of-the-art Large Language Models (LLMs) can simplify writing
variation generation. However, current interfaces pose challenges for
simultaneous consideration of multiple variations: creating new variations
without overwriting text can be difficult, and pasting them sequentially can
clutter documents, increasing workload and disrupting writers' flow. To tackle
this, we present ABScribe, an interface that supports rapid, yet visually
structured, exploration and organization of writing variations in human-AI
co-writing tasks. With ABScribe, users can swiftly modify variations using LLM
prompts, which are auto-converted into reusable buttons. Variations are stored
adjacently within text fields for rapid in-place comparisons using mouse-over
interactions on a popup toolbar. Our user study with 12 writers shows that
ABScribe significantly reduces task workload (d = 1.20, p < 0.001), enhances
user perceptions of the revision process (d = 2.41, p < 0.001) compared to a
popular baseline workflow, and provides insights into how writers explore
variations using LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CHI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Surfer: Progressive Reasoning with World Models for Robotic Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.11335v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.11335v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengzhen Ren, Kaidong Zhang, Hetao Zheng, Zixuan Li, Yuhang Wen, Fengda Zhu, Mas Ma, Xiaodan Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Considering how to make the model accurately understand and follow natural
language instructions and perform actions consistent with world knowledge is a
key challenge in robot manipulation. This mainly includes human fuzzy
instruction reasoning and the following of physical knowledge. Therefore, the
embodied intelligence agent must have the ability to model world knowledge from
training data. However, most existing vision and language robot manipulation
methods mainly operate in less realistic simulator and language settings and
lack explicit modeling of world knowledge. To bridge this gap, we introduce a
novel and simple robot manipulation framework, called Surfer. It is based on
the world model, treats robot manipulation as a state transfer of the visual
scene, and decouples it into two parts: action and scene. Then, the
generalization ability of the model on new instructions and new scenes is
enhanced by explicit modeling of the action and scene prediction in multi-modal
information. In addition to the framework, we also built a robot manipulation
simulator that supports full physics execution based on the MuJoCo physics
engine. It can automatically generate demonstration training data and test
data, effectively reducing labor costs. To conduct a comprehensive and
systematic evaluation of the robot manipulation model in terms of language
understanding and physical execution, we also created a robotic manipulation
benchmark with progressive reasoning tasks, called SeaWave. It contains 4
levels of progressive reasoning tasks and can provide a standardized testing
platform for embedded AI agents in multi-modal environments. On average, Surfer
achieved a success rate of 54.74% on the defined four levels of manipulation
tasks, exceeding the best baseline performance of 47.64%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLM3:Large Language Model-based Task and Motion Planning with Motion
  Failure Reasoning <span class="chip">IROS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11552v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11552v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shu Wang, Muzhi Han, Ziyuan Jiao, Zeyu Zhang, Ying Nian Wu, Song-Chun Zhu, Hangxin Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conventional Task and Motion Planning (TAMP) approaches rely on manually
crafted interfaces connecting symbolic task planning with continuous motion
generation. These domain-specific and labor-intensive modules are limited in
addressing emerging tasks in real-world settings. Here, we present LLM^3, a
novel Large Language Model (LLM)-based TAMP framework featuring a
domain-independent interface. Specifically, we leverage the powerful reasoning
and planning capabilities of pre-trained LLMs to propose symbolic action
sequences and select continuous action parameters for motion planning.
Crucially, LLM^3 incorporates motion planning feedback through prompting,
allowing the LLM to iteratively refine its proposals by reasoning about motion
failure. Consequently, LLM^3 interfaces between task planning and motion
planning, alleviating the intricate design process of handling domain-specific
messages between them. Through a series of simulations in a box-packing domain,
we quantitatively demonstrate the effectiveness of LLM^3 in solving TAMP
problems and the efficiency in selecting action parameters. Ablation studies
underscore the significant contribution of motion failure reasoning to the
success of LLM^3. Furthermore, we conduct qualitative experiments on a physical
manipulator, demonstrating the practical applicability of our approach in
real-world settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IROS 2024. Codes available:
  https://github.com/AssassinWS/LLM-TAMP</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Do Language Models Know When They're Hallucinating References? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.18248v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.18248v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ayush Agrawal, Mirac Suzgun, Lester Mackey, Adam Tauman Kalai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-art language models (LMs) are notoriously susceptible to
generating hallucinated information. Such inaccurate outputs not only undermine
the reliability of these models but also limit their use and raise serious
concerns about misinformation and propaganda. In this work, we focus on
hallucinated book and article references and present them as the "model
organism" of language model hallucination research, due to their frequent and
easy-to-discern nature. We posit that if a language model cites a particular
reference in its output, then it should ideally possess sufficient information
about its authors and content, among other relevant details. Using this basic
insight, we illustrate that one can identify hallucinated references without
ever consulting any external resources, by asking a set of direct or indirect
queries to the language model about the references. These queries can be
considered as "consistency checks." Our findings highlight that while LMs,
including GPT-4, often produce inconsistent author lists for hallucinated
references, they also often accurately recall the authors of real references.
In this sense, the LM can be said to "know" when it is hallucinating
references. Furthermore, these findings show how hallucinated references can be
dissected to shed light on their nature. Replication code and results can be
found at https://github.com/microsoft/hallucinated-references.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Vulnerability analysis of captcha using Deep learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.09389v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.09389v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaskaran Singh Walia, Aryan Odugoudar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Several websites improve their security and avoid dangerous Internet attacks
by implementing CAPTCHAs (Completely Automated Public Turing test to tell
Computers and Humans Apart), a type of verification to identify whether the
end-user is human or a robot. The most prevalent type of CAPTCHA is text-based,
designed to be easily recognized by humans while being unsolvable towards
machines or robots. However, as deep learning technology progresses,
development of convolutional neural network (CNN) models that predict
text-based CAPTCHAs becomes easier. The purpose of this research is to
investigate the flaws and vulnerabilities in the CAPTCHA generating systems in
order to design more resilient CAPTCHAs. To achieve this, we created CapNet, a
Convolutional Neural Network. The proposed platform can evaluate both numerical
and alphanumerical CAPTCHAs
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Analyzing and Improving the Training Dynamics of Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.02696v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.02696v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tero Karras, Miika Aittala, Jaakko Lehtinen, Janne Hellsten, Timo Aila, Samuli Laine
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models currently dominate the field of data-driven image synthesis
with their unparalleled scaling to large datasets. In this paper, we identify
and rectify several causes for uneven and ineffective training in the popular
ADM diffusion model architecture, without altering its high-level structure.
Observing uncontrolled magnitude changes and imbalances in both the network
activations and weights over the course of training, we redesign the network
layers to preserve activation, weight, and update magnitudes on expectation. We
find that systematic application of this philosophy eliminates the observed
drifts and imbalances, resulting in considerably better networks at equal
computational complexity. Our modifications improve the previous record FID of
2.41 in ImageNet-512 synthesis to 1.81, achieved using fast deterministic
sampling.
  As an independent contribution, we present a method for setting the
exponential moving average (EMA) parameters post-hoc, i.e., after completing
the training run. This allows precise tuning of EMA length without the cost of
performing several training runs, and reveals its surprising interactions with
network architecture, training time, and guidance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Real-Fake: Effective Training Data Synthesis Through Distribution
  Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.10402v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.10402v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianhao Yuan, Jie Zhang, Shuyang Sun, Philip Torr, Bo Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Synthetic training data has gained prominence in numerous learning tasks and
scenarios, offering advantages such as dataset augmentation, generalization
evaluation, and privacy preservation. Despite these benefits, the efficiency of
synthetic data generated by current methodologies remains inferior when
training advanced deep models exclusively, limiting its practical utility. To
address this challenge, we analyze the principles underlying training data
synthesis for supervised learning and elucidate a principled theoretical
framework from the distribution-matching perspective that explicates the
mechanisms governing synthesis efficacy. Through extensive experiments, we
demonstrate the effectiveness of our synthetic data across diverse image
classification tasks, both as a replacement for and augmentation to real
datasets, while also benefits such as out-of-distribution generalization,
privacy preservation, and scalability. Specifically, we achieve 70.9% top1
classification accuracy on ImageNet1K when training solely with synthetic data
equivalent to 1 X the original real data size, which increases to 76.0% when
scaling up to 10 X synthetic data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code released at
  (https://github.com/BAAI-DCAI/Training-Data-Synthesis)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LogPrécis: Unleashing Language Models for Automated Shell Log Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.08309v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.08309v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matteo Boffa, Rodolfo Vieira Valentim, Luca Vassio, Danilo Giordano, Idilio Drago, Marco Mellia, Zied Ben Houidi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The collection of security-related logs holds the key to understanding attack
behaviors and diagnosing vulnerabilities. Still, their analysis remains a
daunting challenge. Recently, Language Models (LMs) have demonstrated unmatched
potential in understanding natural and programming languages. The question
arises whether and how LMs could be also useful for security experts since
their logs contain intrinsically confused and obfuscated information. In this
paper, we systematically study how to benefit from the state-of-the-art in LM
to automatically analyze text-like Unix shell attack logs. We present a
thorough design methodology that leads to LogPr\'ecis. It receives as input raw
shell sessions and automatically identifies and assigns the attacker tactic to
each portion of the session, i.e., unveiling the sequence of the attacker's
goals. We demonstrate LogPr\'ecis capability to support the analysis of two
large datasets containing about 400,000 unique Unix shell attacks. LogPr\'ecis
reduces them into about 3,000 fingerprints, each grouping sessions with the
same sequence of tactics. The abstraction it provides lets the analyst better
understand attacks, identify fingerprints, detect novelty, link similar
attacks, and track families and mutations. Overall, LogPr\'ecis, released as
open source, paves the way for better and more responsive defense against
cyberattacks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, Computer&Security
  (https://www.sciencedirect.com/science/article/pii/S0167404824001068), code
  available at https://github.com/SmartData-Polito/logprecis, models available
  at https://huggingface.co/SmartDataPolito</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ IVAC-P2L: Leveraging Irregular Repetition Priors for Improving Video
  Action Counting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11959v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11959v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hang Wang, Zhi-Qi Cheng, Youtian Du, Lei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video Action Counting (VAC) is crucial in analyzing sports, fitness, and
everyday activities by quantifying repetitive actions in videos. However,
traditional VAC methods have overlooked the complexity of action repetitions,
such as interruptions and the variability in cycle duration. Our research
addresses the shortfall by introducing a novel approach to VAC, called
Irregular Video Action Counting (IVAC). IVAC prioritizes modeling irregular
repetition patterns in videos, which we define through two primary aspects:
Inter-cycle Consistency and Cycle-interval Inconsistency. Inter-cycle
Consistency ensures homogeneity in the spatial-temporal representations of
cycle segments, signifying action uniformity within cycles. Cycle-interval
inconsistency highlights the importance of distinguishing between cycle
segments and intervals based on their inherent content differences. To
encapsulate these principles, we propose a new methodology that includes
consistency and inconsistency modules, supported by a unique pull-push loss
(P2L) mechanism. The IVAC-P2L model applies a pull loss to promote coherence
among cycle segment features and a push loss to clearly distinguish features of
cycle segments from interval segments. Empirical evaluations conducted on the
RepCount dataset demonstrate that the IVAC-P2L model sets a new benchmark in
VAC task performance. Furthermore, the model demonstrates exceptional
adaptability and generalization across various video contents, outperforming
existing models on two additional datasets, UCFRep and Countix, without the
need for dataset-specific optimization. These results confirm the efficacy of
our approach in addressing irregular repetitions in videos and pave the way for
further advancements in video analysis and understanding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Source code: https://github.com/hwang-cs-ime/IVAC-P2L</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Databases
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Overview</span> of Publicly Available Degradation Data Sets for Tasks within
  Prognostics and Health Management 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13694v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13694v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fabian Mauthe, Christopher Braun, Julian Raible, Peter Zeiler, Marco F. Huber
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Central to the efficacy of prognostics and health management methods is the
acquisition and analysis of degradation data, which encapsulates the evolving
health condition of engineering systems over time. Degradation data serves as a
rich source of information, offering invaluable insights into the underlying
degradation processes, failure modes, and performance trends of engineering
systems. This paper provides an overview of publicly available degradation data
sets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CheckMate: Evaluating Checkpointing Protocols for Streaming Dataflows 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13629v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13629v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        George Siachamis, Kyriakos Psarakis, Marios Fragkoulis, Arie van Deursen, Paris Carbone, Asterios Katsifodimos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stream processing in the last decade has seen broad adoption in both
commercial and research settings. One key element for this success is the
ability of modern stream processors to handle failures while ensuring
exactly-once processing guarantees. At the moment of writing, virtually all
stream processors that guarantee exactly-once processing implement a variant of
Apache Flink's coordinated checkpoints - an extension of the original
Chandy-Lamport checkpoints from 1985. However, the reasons behind this
prevalence of the coordinated approach remain anecdotal, as reported by
practitioners of the stream processing community. At the same time, common
checkpointing approaches, such as the uncoordinated and the
communication-induced ones, remain largely unexplored.
  This paper is the first to address this gap by i) shedding light on why
practitioners have favored the coordinated approach and ii) by investigating
whether there are viable alternatives. To this end, we implement three
checkpointing approaches that we surveyed and adapted for the distinct needs of
streaming dataflows. Our analysis shows that the coordinated approach
outperforms the uncoordinated and communication-induced protocols under
uniformly distributed workloads. To our surprise, however, the uncoordinated
approach is not only competitive to the coordinated one in uniformly
distributed workloads, but it also outperforms the coordinated approach in
skewed workloads. We conclude that rather than blindly employing coordinated
checkpointing, research should focus on optimizing the very promising
uncoordinated approach, as it can address issues with skew and support
prevalent cyclic queries. We believe that our findings can trigger further
research into checkpointing mechanisms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ No more optimization rules: LLM-enabled policy-based multi-modal query
  optimizer (version 1) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13597v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13597v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Wang, Haodi Ma, Daisy Zhe Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language model (LLM) has marked a pivotal moment in the field of
machine learning and deep learning. Recently its capability for query planning
has been investigated, including both single-modal and multi-modal queries.
However, there is no work on the query optimization capability of LLM. As a
critical (or could even be the most important) step that significantly impacts
the execution performance of the query plan, such analysis and attempts should
not be missed. From another aspect, existing query optimizers are usually
rule-based or rule-based + cost-based, i.e., they are dependent on manually
created rules to complete the query plan rewrite/transformation. Given the fact
that modern optimizers include hundreds to thousands of rules, designing a
multi-modal query optimizer following a similar way is significantly
time-consuming since we will have to enumerate as many multi-modal optimization
rules as possible, which has not been well addressed today. In this paper, we
investigate the query optimization ability of LLM and use LLM to design LaPuda,
a novel LLM and Policy based multi-modal query optimizer. Instead of
enumerating specific and detailed rules, LaPuda only needs a few abstract
policies to guide LLM in the optimization, by which much time and human effort
are saved. Furthermore, to prevent LLM from making mistakes or negative
optimization, we borrow the idea of gradient descent and propose a guided cost
descent (GCD) algorithm to perform the optimization, such that the optimization
can be kept in the correct direction. In our evaluation, our methods
consistently outperform the baselines in most cases. For example, the optimized
plans generated by our methods result in 1~3x higher execution speed than those
by the baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Yifan and Haodi contribute equally to the work</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Secure Query Processing with Linear Complexity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13492v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13492v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiyao Luo, Yilei Wang, Wei Dong, Ke Yi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present LINQ, the first join protocol with linear complexity (in both
running time and communication) under the secure multi-party computation model
(MPC). It can also be extended to support all free-connex queries, a large
class of select-join-aggregate queries, still with linear complexity. This
matches the plaintext result for the query processing problem, as free-connex
queries are the largest class of queries known to be solvable in linear time in
plaintext. We have then built a query processing system based on LINQ, and the
experimental results show that LINQ significantly outperforms the state of the
art. For example, it can finish a query on three relations with an output size
of 1 million tuples in around 100s in the LAN setting, while existing protocols
that support the query cannot finish in an hour. Thus LINQ brings MPC query
processing closer to practicality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Distance Comparison Operators for Approximate Nearest Neighbor Search:
  Exploration and Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13491v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13491v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeyu Wang, Haoran Xiong, Zhenying He, Peng Wang, Wei wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Approximate nearest neighbor search (ANNS) on high-dimensional vectors has
become a fundamental and essential component in various machine learning tasks.
Prior research has shown that the distance comparison operation is the
bottleneck of ANNS, which determines the query and indexing performance. To
overcome this challenge, some novel methods have been proposed recently. The
basic idea is to estimate the actual distance with fewer calculations, at the
cost of accuracy loss. Inspired by this, we also propose that some classical
techniques and deep learning models can also be adapted to this purpose. In
this paper, we systematically categorize the techniques that have been or can
be used to accelerate distance approximation. And to help the users understand
the pros and cons of different techniques, we design a fair and comprehensive
benchmark, Fudist implements these techniques with the same base index and
evaluates them on 16 real datasets with several evaluation metrics. Designed as
an independent and portable library, Fudist is orthogonal to the specific index
structure and thus can be easily utilized in the current ANNS library to
achieve significant improvements.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Sampling-based Framework for Hypothesis Testing on Large Attributed
  Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13286v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13286v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yun Wang, Chrysanthi Kosyfaki, Sihem Amer-Yahia, Reynold Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hypothesis testing is a statistical method used to draw conclusions about
populations from sample data, typically represented in tables. With the
prevalence of graph representations in real-life applications, hypothesis
testing in graphs is gaining importance. In this work, we formalize node, edge,
and path hypotheses in attributed graphs. We develop a sampling-based
hypothesis testing framework, which can accommodate existing
hypothesis-agnostic graph sampling methods. To achieve accurate and efficient
sampling, we then propose a Path-Hypothesis-Aware SamplEr, PHASE, an m-
dimensional random walk that accounts for the paths specified in a hypothesis.
We further optimize its time efficiency and propose PHASEopt. Experiments on
real datasets demonstrate the ability of our framework to leverage common graph
sampling methods for hypothesis testing, and the superiority of
hypothesis-aware sampling in terms of accuracy and time efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unifews: Unified Entry-Wise Sparsification for Efficient Graph Neural
  Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13268v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13268v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ningyi Liao, Zihao Yu, Siqiang Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks (GNNs) have shown promising performance in various
graph learning tasks, but at the cost of resource-intensive computations. The
primary overhead of GNN update stems from graph propagation and weight
transformation, both involving operations on graph-scale matrices. Previous
studies attempt to reduce the computational budget by leveraging graph-level or
network-level sparsification techniques, resulting in downsized graph or
weights. In this work, we propose Unifews, which unifies the two operations in
an entry-wise manner considering individual matrix elements, and conducts joint
edge-weight sparsification to enhance learning efficiency. The entry-wise
design of Unifews enables adaptive compression across GNN layers with
progressively increased sparsity, and is applicable to a variety of
architectural designs with on-the-fly operation simplification. Theoretically,
we establish a novel framework to characterize sparsified GNN learning in view
of a graph optimization process, and prove that Unifews effectively
approximates the learning objective with bounded error and reduced
computational load. We conduct extensive experiments to evaluate the
performance of our method in diverse settings. Unifews is advantageous in
jointly removing more than 90% of edges and weight entries with comparable or
better accuracy than baseline models. The sparsification offers remarkable
efficiency improvements including 10-20x matrix operation reduction and up to
100x acceleration in graph propagation time for the largest graph at the
billion-edge scale.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Database Dependencies and Formal Concept Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13914v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13914v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaume Baixeries
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This is an account of the characterization of database dependencies with
Formal Concept Analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DiffImpute: Tabular Data Imputation With Denoising Diffusion
  Probabilistic Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13863v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13863v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yizhu Wen, Kai Yi, Jing Ke, Yiqing Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tabular data plays a crucial role in various domains but often suffers from
missing values, thereby curtailing its potential utility. Traditional
imputation techniques frequently yield suboptimal results and impose
substantial computational burdens, leading to inaccuracies in subsequent
modeling tasks. To address these challenges, we propose DiffImpute, a novel
Denoising Diffusion Probabilistic Model (DDPM). Specifically, DiffImpute is
trained on complete tabular datasets, ensuring that it can produce credible
imputations for missing entries without undermining the authenticity of the
existing data. Innovatively, it can be applied to various settings of Missing
Completely At Random (MCAR) and Missing At Random (MAR). To effectively handle
the tabular features in DDPM, we tailor four tabular denoising networks,
spanning MLP, ResNet, Transformer, and U-Net. We also propose Harmonization to
enhance coherence between observed and imputed data by infusing the data back
and denoising them multiple times during the sampling stage. To enable
efficient inference while maintaining imputation performance, we propose a
refined non-Markovian sampling process that works along with Harmonization.
Empirical evaluations on seven diverse datasets underscore the prowess of
DiffImpute. Specifically, when paired with the Transformer as the denoising
network, it consistently outperforms its competitors, boasting an average
ranking of 1.7 and the most minimal standard deviation. In contrast, the next
best method lags with a ranking of 2.8 and a standard deviation of 0.9. The
code is available at https://github.com/Dendiiiii/DiffImpute.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Enforcing Existence and Non-Existence Constraints in MatBase 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14726v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14726v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christian Mancas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existence constraints were defined in the Relational Data Model, but,
unfortunately, are not provided by any Relational Database Management System,
except for their NOT NULL particular case. Our (Elementary) Mathematical Data
Model extended them to function products and introduced their dual
non-existence constraints. MatBase, an intelligent data and knowledge base
management system prototype based on both these data models, not only provides
existence and non-existence constraints, but also automatically generates code
for their enforcement. This paper presents and discusses the algorithms used by
MatBase to enforce these types of constraints.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to the BOHR International Journal of Computer Science
  (BIJCS), ISSN: 2583-455X, on March 20, 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On enforcing dyadic-type homogeneous binary function product constraints
  in MatBase 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.06502v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.06502v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christian Mancas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Homogeneous binary function products are often encountered in the
sub-universes modeled by databases, from genealogical trees to sports, from
education to healthcare, etc. Their properties must be discovered and enforced
by the software applications managing such data to guarantee plausibility. The
(Elementary) Mathematical Data Model provides 18 dyadic-type homogeneous binary
function product constraint types. MatBase, an intelligent data and knowledge
base management system prototype, allows database designers to simply declare
them by only clicking corresponding checkboxes and automatically generates code
for enforcing them. This paper describes the algorithms that MatBase uses for
enforcing all these 18 homogeneous binary function product constraint types,
which may also be used by developers not having access to MatBase.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted on Dec. 7, 2023, to the Journal of Data Science and
  Intelligent Systems (JDSIS), on Dec. 20, 2023, to the Journal of
  Computational and Cognitive Engineering, both of Bon View Publishing,
  Singapore, on Dec. 30 to the Journal of Current Research and Studies, and on
  Jan. 25, 2024, to the Journal of Computer Science Research, Bilingual
  Publishing Group, Singapore</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ WaZI: A Learned and Workload-aware Z-Index <span class="chip">EDBT 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04268v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04268v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sachith Pai, Michael Mathioudakis, Yanhao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learned indexes fit machine learning (ML) models to the data and use them to
make query operations more time and space-efficient. Recent works propose using
learned spatial indexes to improve spatial query performance by optimizing the
storage layout or internal search structures according to the data
distribution. However, only a few learned indexes exploit the query workload
distribution to enhance their performance. In addition, building and updating
learned spatial indexes are often costly on large datasets due to the
inefficiency of (re)training ML models. In this paper, we present WaZI, a
learned and workload-aware variant of the Z-index, which jointly optimizes the
storage layout and search structures, as a viable solution for the above
challenges of spatial indexing. Specifically, we first formulate a cost
function to measure the performance of a Z-index on a dataset for a range-query
workload. Then, we optimize the Z-index structure by minimizing the cost
function through adaptive partitioning and ordering for index construction.
Moreover, we design a novel page-skipping mechanism to improve the query
performance of WaZI by reducing access to irrelevant data pages. Our extensive
experiments show that the WaZI index improves range query time by 40% on
average over the baselines while always performing better or comparably to
state-of-the-art spatial indexes. Additionally, it also maintains good point
query performance. Generally, WaZI provides favorable tradeoffs among query
latency, construction time, and index size.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Camera-ready version accepted to EDBT 2024</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Science and Game Theory
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Analysing Guarantees in Australian Senate Outcomes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13275v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13275v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michelle Blom
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Single Transferable Vote (STV) is used to elect candidates to the 76 seat
Australian Senate across six states and two territories. These eight STV
contests are counted using a combination of ballot scanners, manual data entry
and tabulation software. On election night, some properties of the set of cast
ballots are determined by hand. This includes the first preference tallies of
each party. This technical report considers whether there are some properties,
such as individual candidates' first preference tallies, that, if assumed to be
accurate, imply a portion of the election outcome. The paper also presents an
interesting example showing that the rules of STV tabulation used for the
Australian Senate can allow bizarre behaviour, such as votes increasing in
value over time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Unified Toll Lane Framework for Autonomous and High-Occupancy Vehicles
  in Interactive Mixed Autonomy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14011v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14011v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruolin Li, Philip N. Brown, Roberto Horowitz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we introduce a toll lane framework that optimizes the mixed
flow of autonomous and high-occupancy vehicles on freeways, where human-driven
and autonomous vehicles of varying commuter occupancy share a segment.
Autonomous vehicles, with their ability to maintain shorter headways, boost
traffic throughput. Our framework designates a toll lane for autonomous
vehicles with high occupancy to use free of charge, while others pay a toll. We
explore the lane choice equilibria when all vehicles minimize travel costs, and
characterize the equilibria by ranking vehicles by their mobility enhancement
potential, a concept we term the mobility degree. Through numerical examples,
we demonstrate the framework's utility in addressing design challenges such as
setting optimal tolls, determining occupancy thresholds, and designing lane
policies, showing how it facilitates the integration of high-occupancy and
autonomous vehicles. We also propose an algorithm for assigning rational tolls
to decrease total commuter delay and examine the effects of toll
non-compliance. Our findings suggest that self-interest-driven behavior
mitigates moderate non-compliance impacts, highlighting the framework's
resilience. This work presents a pioneering comprehensive analysis of a toll
lane framework that emphasizes the coexistence of autonomous and high-occupancy
vehicles, offering insights for traffic management improvements and the
integration of autonomous vehicles into existing transportation
infrastructures.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cardinal-Utility Matching Markets: The Quest for Envy-Freeness,
  Pareto-Optimality, and Efficient Computability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.08851v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.08851v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thorben Tröbst, Vijay V. Vazirani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unlike ordinal-utility matching markets, which are well-developed from the
viewpoint of both theory and practice, recent insights from a computer science
perspective have left cardinal-utility matching markets in a quandary. The
celebrated pricing-based mechanism for one-sided cardinal-utility matching
markets due to Hylland and Zeckhauser, which had long eluded efficient
algorithms, was finally shown to be PPAD-complete.
  This led us to ask the question: is there an alternative, polynomial time,
mechanism for one-sided cardinal-utility matching markets which achieves the
desirable properties of HZ, i.e.\ (ex-ante) envy-freeness (EF) and
Pareto-optimality (PO)? In this paper we show:
  1. The problem of finding an EF+PO lottery in a one-sided cardinal-utility
matching market is PPAD-complete.
  2. A $(2 + \epsilon)$-approximately envy-free and (exactly) Pareto-optimal
lottery can be found in polynomial time using Nash bargaining. Moreover, the
resulting mechanism is $(2 + \epsilon)$-approximately incentive compatible.
  We also present several results on two-sided cardinal-utility matching
markets, including non-existence of EF+PO lotteries as well as existence of
justified-envy-free and weak Pareto-optimal lotteries.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Entropy-Based Strategies for Multi-Bracket Pools 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.14339v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.14339v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryan S. Brill, Abraham J. Wyner, Ian J. Barnett
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Much work in the parimutuel betting literature has discussed estimating event
outcome probabilities or developing optimal wagering strategies, particularly
for horse race betting. Some betting pools, however, involve betting not just
on a single event, but on a tuple of events. For example, pick six betting in
horse racing, March Madness bracket challenges, and predicting a randomly drawn
bitstring each involve making a series of individual forecasts. Although
traditional optimal wagering strategies work well when the size of the tuple is
very small (e.g., betting on the winner of a horse race), they are intractable
for more general betting pools in higher dimensions (e.g., March Madness
bracket challenges). Hence we pose the multi-brackets problem: supposing we
wish to predict a tuple of events and that we know the true probabilities of
each potential outcome of each event, what is the best way to tractably
generate a set of $n$ predicted tuples? The most general version of this
problem is extremely difficult, so we begin with a simpler setting. In
particular, we generate $n$ independent predicted tuples according to a
distribution having optimal entropy. This entropy-based approach is tractable,
scalable, and performs well.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Theory
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Extremality of stabilizer states 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13632v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13632v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaifeng Bu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate the extremality of stabilizer states to reveal their
exceptional role in the space of all $n$-qubit/qudit states. We establish
uncertainty principles for the characteristic function and the Wigner function
of states, respectively. We find that only stabilizer states achieve saturation
in these principles. Furthermore, we prove a general theorem that stabilizer
states are extremal for convex information measures invariant under local
unitaries. We explore this extremality in the context of various quantum
information and correlation measures, including entanglement entropy,
conditional entropy and other entanglement measures. Additionally, leveraging
the recent discovery that stabilizer states are the limit states under quantum
convolution, we establish the monotonicity of the entanglement entropy and
conditional entropy under quantum convolution. These results highlight the
remarkable information-theoretic properties of stabilizer states. Their
extremality provides valuable insights into their ability to capture
information content and correlations, paving the way for further exploration of
their potential in quantum information processing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6+3 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MIMO Channel as a Neural Function: Implicit Neural Representations for
  Extreme CSI Compression in Massive MIMO Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13615v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13615v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haotian Wu, Maojun Zhang, Yulin Shao, Krystian Mikolajczyk, Deniz Gündüz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Acquiring and utilizing accurate channel state information (CSI) can
significantly improve transmission performance, thereby holding a crucial role
in realizing the potential advantages of massive multiple-input multiple-output
(MIMO) technology. Current prevailing CSI feedback approaches improve precision
by employing advanced deep-learning methods to learn representative CSI
features for a subsequent compression process. Diverging from previous works,
we treat the CSI compression problem in the context of implicit neural
representations. Specifically, each CSI matrix is viewed as a neural function
that maps the CSI coordinates (antenna number and subchannel) to the
corresponding channel gains. Instead of transmitting the parameters of the
implicit neural functions directly, we transmit modulations based on the CSI
matrix derived through a meta-learning algorithm. Modulations are then applied
to a shared base network to generate the elements of the CSI matrix.
Modulations corresponding to the CSI matrix are quantized and entropy-coded to
further reduce the communication bandwidth, thus achieving extreme CSI
compression ratios. Numerical results show that our proposed approach achieves
state-of-the-art performance and showcases flexibility in feedback strategies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Movable Antenna Enabled Interference Network: Joint Antenna Position and
  Beamforming Design 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13573v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13573v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Honghao Wang, Qingqing Wu, Wen Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates the utility of movable antenna (MA) assistance for
the multiple-input single-output (MISO) interference channel. We exploit an
additional design degree of freedom provided by MA to enhance the desired
signal and suppress interference so as to reduce the total transmit power of
interference network. To this end, we jointly optimize the MA positions and
transmit beamforming, subject to the signal-to-interference-plus-noise ratio
constraints of users. To address the non-convex optimization problem, we
propose an efficient iterative algorithm to alternately optimize the MA
positions via successive convex approximation method and the transmit
beamforming via second-order cone program approach. Numerical results
demonstrate that the proposed MA-enabled MISO interference network outperforms
its conventional counterpart without MA, which significantly enhances the
capability of inter-cell frequency reuse and reduces the complexity of
transmitter design.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Massive MIMO CSI Feedback using Channel Prediction: How to Avoid Machine
  Learning at UE? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13363v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13363v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Karam Shehzad, Luca Rose, Mohamad Assaad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the literature, machine learning (ML) has been implemented at the base
station (BS) and user equipment (UE) to improve the precision of downlink
channel state information (CSI). However, ML implementation at the UE can be
infeasible for various reasons, such as UE power consumption. Motivated by this
issue, we propose a CSI learning mechanism at BS, called CSILaBS, to avoid ML
at UE. To this end, by exploiting channel predictor (CP) at BS, a light-weight
predictor function (PF) is considered for feedback evaluation at the UE.
CSILaBS reduces over-the-air feedback overhead, improves CSI quality, and
lowers the computation cost of UE. Besides, in a multiuser environment, we
propose various mechanisms to select the feedback by exploiting PF while aiming
to improve CSI accuracy. We also address various ML-based CPs, such as
NeuralProphet (NP), an ML-inspired statistical algorithm. Furthermore, inspired
to use a statistical model and ML together, we propose a novel hybrid framework
composed of a recurrent neural network and NP, which yields better prediction
accuracy than individual models. The performance of CSILaBS is evaluated
through an empirical dataset recorded at Nokia Bell-Labs. The outcomes show
that ML elimination at UE can retain performance gains, for example, precoding
quality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Construction of Minimal Binary Linear Codes of dimension $n+3$ 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13350v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13350v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wajid M. Shaikh, Rupali S. Jain, B. Surendranath Reddy, Bhagyashri S. Patil
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we will give the generic construction of a binary linear code
of dimension $n+3$ and derive the necessary and sufficient conditions for the
constructed code to be minimal. Using generic construction, a new family of
minimal binary linear code will be constructed from a special class of Boolean
functions violating the Ashikhmin-Barg condition. We also obtain the weight
distribution of the constructed minimal binary linear code.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Local Approximation of Secrecy Capacity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13345v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13345v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emmanouil M. Athanasakos, Nicholas Kalouptsidis, Hariprasad Manjunath
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper uses Euclidean Information Theory (EIT) to analyze the wiretap
channel. We investigate a scenario of efficiently transmitting a small amount
of information subject to compression rate and secrecy constraints. We
transform the information-theoretic problem into a linear algebra problem and
obtain the perturbed probability distributions such that secrecy is achievable.
Local approximations are being used in order to obtain an estimate of the
secrecy capacity by solving a generalized eigenvalue problem.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to EUSIPCO 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ S$Ω$I: Score-based O-INFORMATION Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.05667v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.05667v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mustapha Bounoua, Giulio Franzese, Pietro Michiardi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The analysis of scientific data and complex multivariate systems requires
information quantities that capture relationships among multiple random
variables. Recently, new information-theoretic measures have been developed to
overcome the shortcomings of classical ones, such as mutual information, that
are restricted to considering pairwise interactions. Among them, the concept of
information synergy and redundancy is crucial for understanding the high-order
dependencies between variables. One of the most prominent and versatile
measures based on this concept is O-information, which provides a clear and
scalable way to quantify the synergy-redundancy balance in multivariate
systems. However, its practical application is limited to simplified cases. In
this work, we introduce S$\Omega$I, which allows for the first time to compute
O-information without restrictive assumptions about the system. Our experiments
validate our approach on synthetic data, and demonstrate the effectiveness of
S$\Omega$I in the context of a real-world use case.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Quantum-inspired identification of complex cellular automata 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2103.14053v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2103.14053v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthew Ho, Andri Pradana, Thomas J. Elliott, Lock Yue Chew, Mile Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Elementary cellular automata (ECA) present iconic examples of complex
systems. Though described only by one-dimensional strings of binary cells
evolving according to nearest-neighbour update rules, certain ECA rules
manifest complex dynamics capable of universal computation. Yet, the
classification of precisely which rules exhibit complex behaviour remains a
significant challenge. Here we approach this question using tools from quantum
stochastic modelling, where quantum statistical memory -- the memory required
to model a stochastic process using a class of quantum machines -- can be used
to quantify the structure of a stochastic process. By viewing ECA rules as
transformations of stochastic patterns, we ask: Does an ECA generate structure
as quantified by the quantum statistical memory, and if so, how quickly? We
illustrate how the growth of this measure over time correctly distinguishes
simple ECA from complex counterparts. Moreover, it provides a more refined
means for quantitatively identifying complex ECAs -- providing a spectrum on
which we can rank the complexity of ECA by the rate in which they generate
structure.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Near-Field Beam Training: Joint Angle and Range Estimation with DFT
  Codebook 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.11872v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.11872v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xun Wu, Changsheng You, Jiapeng Li, Yunpu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prior works on near-field beam training have mostly assumed dedicated
polar-domain codebook and on-grid range estimation, which, however, may suffer
long training overhead and degraded estimation accuracy. To address these
issues, we propose in this paper new and efficient beam training schemes with
off-grid range estimation by using conventional discrete Fourier transform
(DFT) codebook. Specifically, we first analyze the received beam pattern at the
user when far-field beamforming vectors are used for beam scanning, and show an
interesting result that this beam pattern contains useful user angle and range
information. Then, we propose two efficient schemes to jointly estimate the
user angle and range with the DFT codebook. The first scheme estimates the user
angle based on a defined angular support and resolves the user range by
leveraging an approximated angular support width, while the second scheme
estimates the user range by minimizing a power ratio mean square error (MSE) to
improve the range estimation accuracy. Finally, numerical simulations show that
our proposed schemes greatly reduce the near-field beam training overhead and
improve the range estimation accuracy as compared to various benchmark schemes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work is submitted to IEEE for possible publication. Our other
  works on near-field beam training include: 1) two-phase near-field beam
  training (arXiv:2209.14798), 2) hierarchical near-field beam training
  (arXiv:2302.12511), and 3) near-field beam management (arXiv:2306.16206)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PAGE: Prototype-Based Model-Level Explanations for Graph Neural Networks <span class="chip">AAAI-22</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.17159v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.17159v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yong-Min Shin, Sun-Woo Kim, Won-Yong Shin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aside from graph neural networks (GNNs) attracting significant attention as a
powerful framework revolutionizing graph representation learning, there has
been an increasing demand for explaining GNN models. Although various
explanation methods for GNNs have been developed, most studies have focused on
instance-level explanations, which produce explanations tailored to a given
graph instance. In our study, we propose Prototype-bAsed GNN-Explainer (PAGE),
a novel model-level GNN explanation method that explains what the underlying
GNN model has learned for graph classification by discovering
human-interpretable prototype graphs. Our method produces explanations for a
given class, thus being capable of offering more concise and comprehensive
explanations than those of instance-level explanations. First, PAGE selects
embeddings of class-discriminative input graphs on the graph-level embedding
space after clustering them. Then, PAGE discovers a common subgraph pattern by
iteratively searching for high matching node tuples using node-level embeddings
via a prototype scoring function, thereby yielding a prototype graph as our
explanation. Using six graph classification datasets, we demonstrate that PAGE
qualitatively and quantitatively outperforms the state-of-the-art model-level
explanation method. We also carry out systematic experimental studies by
demonstrating the relationship between PAGE and instance-level explanation
methods, the robustness of PAGE to input data scarce environments, and the
computational efficiency of the proposed prototype scoring function in PAGE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 12 figures, 5 tables; to appear in the IEEE Transactions on
  Pattern Analysis and Machine Intelligence (Please cite our journal version
  that will appear in an upcoming issue. Its two-page extended summary was
  presented in the AAAI-22 Student Abstract and Poster Program.)</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-19T00:00:00Z">2024-03-19</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Distributed, Parallel, and Cluster Computing
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Toward Sustainable GenAI using Generation Directives for Carbon-Friendly
  Large Language Model Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12900v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12900v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Baolin Li, Yankai Jiang, Vijay Gadepally, Devesh Tiwari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancement of Generative Artificial Intelligence (GenAI) across
diverse sectors raises significant environmental concerns, notably the carbon
emissions from their cloud and high performance computing (HPC) infrastructure.
This paper presents Sprout, an innovative framework designed to address these
concerns by reducing the carbon footprint of generative Large Language Model
(LLM) inference services. Sprout leverages the innovative concept of
"generation directives" to guide the autoregressive generation process, thereby
enhancing carbon efficiency. Our proposed method meticulously balances the need
for ecological sustainability with the demand for high-quality generation
outcomes. Employing a directive optimizer for the strategic assignment of
generation directives to user prompts and an original offline quality
evaluator, Sprout demonstrates a significant reduction in carbon emissions by
over 40% in real-world evaluations using the Llama2 LLM and global electricity
grid data. This research marks a critical step toward aligning AI technology
with sustainable practices, highlighting the potential for mitigating
environmental impacts in the rapidly expanding domain of generative artificial
intelligence.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimizing Service Placement in Edge-to-Cloud AR/VR Systems using a
  Multi-Objective Genetic Algorithm 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12849v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12849v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammadsadeq Garshasbi Herabad, Javid Taheri, Bestoun S. Ahmed, Calin Curescu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Augmented Reality (AR) and Virtual Reality (VR) systems involve
computationally intensive image processing algorithms that can burden
end-devices with limited resources, leading to poor performance in providing
low latency services. Edge-to-cloud computing overcomes the limitations of
end-devices by offloading their computations to nearby edge devices or remote
cloud servers. Although this proves to be sufficient for many applications,
optimal placement of latency sensitive AR/VR services in edge-to-cloud
infrastructures (to provide desirable service response times and reliability)
remain a formidable challenging. To address this challenge, this paper develops
a Multi-Objective Genetic Algorithm (MOGA) to optimize the placement of
AR/VR-based services in multi-tier edge-to-cloud environments. The primary
objective of the proposed MOGA is to minimize the response time of all running
services, while maximizing the reliability of the underlying system from both
software and hardware perspectives. To evaluate its performance, we
mathematically modeled all components and developed a tailor-made simulator to
assess its effectiveness on various scales. MOGA was compared with several
heuristics to prove that intuitive solutions, which are usually assumed
sufficient, are not efficient enough for the stated problem. The experimental
results indicated that MOGA can significantly reduce the response time of
deployed services by an average of 67\% on different scales, compared to other
heuristic methods. MOGA also ensures reliability of the 97\% infrastructure
(hardware) and 95\% services (software).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Parallel Gaussian process with kernel approximation in CUDA 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12797v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12797v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Davide Carminati
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a parallel implementation in CUDA/C++ of the Gaussian
process with a decomposed kernel. This recent formulation, introduced by Joukov
and Kuli\'c (2022), is characterized by an approximated -- but much smaller --
matrix to be inverted compared to plain Gaussian process. However, it exhibits
a limitation when dealing with higher-dimensional samples which degrades
execution times. The solution presented in this paper relies on parallelizing
the computation of the predictive posterior statistics on a GPU using CUDA and
its libraries. The CPU code and GPU code are then benchmarked on different
CPU-GPU configurations to show the benefits of the parallel implementation on
GPU over the CPU.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Performance Portable Monte Carlo Particle Transport on Intel, NVIDIA,
  and AMD GPUs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12345v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12345v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        John Tramm, Paul Romano, Patrick Shriwise, Amanda Lund, Johannes Doerfert, Patrick Steinbrecher, Andrew Siegel, Gavin Ridley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  OpenMC is an open source Monte Carlo neutral particle transport application
that has recently been ported to GPU using the OpenMP target offloading model.
We examine the performance of OpenMC at scale on the Frontier, Polaris, and
Aurora supercomputers, demonstrating that performance portability has been
achieved by OpenMC across all three major GPU vendors (AMD, NVIDIA, and Intel).
OpenMC's GPU performance is compared to both the traditional CPU-based version
of OpenMC as well as several other state-of-the-art CPU-based Monte Carlo
particle transport applications. We also provide historical context by
analyzing OpenMC's performance on several legacy GPU and CPU architectures.
This work includes some of the first published results for a scientific
simulation application at scale on a supercomputer featuring Intel's Max series
"Ponte Vecchio" GPUs. It is also one of the first demonstrations of a large
scientific production application using the OpenMP target offloading model to
achieve high performance on all three major GPU platforms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FedFisher: Leveraging Fisher Information for One-Shot Federated Learning <span class="chip">AISTATS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12329v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12329v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Divyansh Jhunjhunwala, Shiqiang Wang, Gauri Joshi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Standard federated learning (FL) algorithms typically require multiple rounds
of communication between the server and the clients, which has several
drawbacks, including requiring constant network connectivity, repeated
investment of computational resources, and susceptibility to privacy attacks.
One-Shot FL is a new paradigm that aims to address this challenge by enabling
the server to train a global model in a single round of communication. In this
work, we present FedFisher, a novel algorithm for one-shot FL that makes use of
Fisher information matrices computed on local client models, motivated by a
Bayesian perspective of FL. First, we theoretically analyze FedFisher for
two-layer over-parameterized ReLU neural networks and show that the error of
our one-shot FedFisher global model becomes vanishingly small as the width of
the neural networks and amount of local training at clients increases. Next, we
propose practical variants of FedFisher using the diagonal Fisher and K-FAC
approximation for the full Fisher and highlight their communication and compute
efficiency for FL. Finally, we conduct extensive experiments on various
datasets, which show that these variants of FedFisher consistently improve over
competing baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to AISTATS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DecentNeRFs: Decentralized Neural Radiance Fields from Crowdsourced
  Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13199v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13199v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zaid Tasneem, Akshat Dave, Abhishek Singh, Kushagra Tiwary, Praneeth Vepakomma, Ashok Veeraraghavan, Ramesh Raskar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural radiance fields (NeRFs) show potential for transforming images
captured worldwide into immersive 3D visual experiences. However, most of this
captured visual data remains siloed in our camera rolls as these images contain
personal details. Even if made public, the problem of learning 3D
representations of billions of scenes captured daily in a centralized manner is
computationally intractable. Our approach, DecentNeRF, is the first attempt at
decentralized, crowd-sourced NeRFs that require $\sim 10^4\times$ less server
computing for a scene than a centralized approach. Instead of sending the raw
data, our approach requires users to send a 3D representation, distributing the
high computation cost of training centralized NeRFs between the users. It
learns photorealistic scene representations by decomposing users' 3D views into
personal and global NeRFs and a novel optimally weighted aggregation of only
the latter. We validate the advantage of our approach to learn NeRFs with
photorealism and minimal server computation cost on structured synthetic and
real-world photo tourism datasets. We further analyze how secure aggregation of
global NeRFs in DecentNeRF minimizes the undesired reconstruction of personal
content by the server.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Parallel Workflow for Polar Sea-Ice Classification using Auto-labeling
  of Sentinel-2 Imagery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13135v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13135v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jurdana Masuma Iqrah, Wei Wang, Hongjie Xie, Sushil Prasad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The observation of the advancing and retreating pattern of polar sea ice
cover stands as a vital indicator of global warming. This research aims to
develop a robust, effective, and scalable system for classifying polar sea ice
as thick/snow-covered, young/thin, or open water using Sentinel-2 (S2) images.
Since the S2 satellite is actively capturing high-resolution imagery over the
earth's surface, there are lots of images that need to be classified. One major
obstacle is the absence of labeled S2 training data (images) to act as the
ground truth. We demonstrate a scalable and accurate method for segmenting and
automatically labeling S2 images using carefully determined color thresholds.
We employ a parallel workflow using PySpark to scale and achieve 9-fold data
loading and 16-fold map-reduce speedup on auto-labeling S2 images based on thin
cloud and shadow-filtered color-based segmentation to generate label data. The
auto-labeled data generated from this process are then employed to train a
U-Net machine learning model, resulting in good classification accuracy. As
training the U-Net classification model is computationally heavy and
time-consuming, we distribute the U-Net model training to scale it over 8 GPUs
using the Horovod framework over a DGX cluster with a 7.21x speedup without
affecting the accuracy of the model. Using the Antarctic's Ross Sea region as
an example, the U-Net model trained on auto-labeled data achieves a
classification accuracy of 98.97% for auto-labeled training datasets when the
thin clouds and shadows from the S2 images are filtered out.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in the 25th IEEE International Workshop on Parallel and
  Distributed Scientific and Engineering Computing (PDSEC 2024), May 2024.
  arXiv admin note: substantial text overlap with arXiv:2303.12719</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Analyzing the Impact of Partial Sharing on the Resilience of Online
  Federated Learning Against Model Poisoning Attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13108v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13108v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ehsan Lari, Vinay Chakravarthi Gogineni, Reza Arablouei, Stefan Werner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We scrutinize the resilience of the partial-sharing online federated learning
(PSO-Fed) algorithm against model-poisoning attacks. PSO-Fed reduces the
communication load by enabling clients to exchange only a fraction of their
model estimates with the server at each update round. Partial sharing of model
estimates also enhances the robustness of the algorithm against model-poisoning
attacks. To gain better insights into this phenomenon, we analyze the
performance of the PSO-Fed algorithm in the presence of Byzantine clients,
malicious actors who may subtly tamper with their local models by adding noise
before sharing them with the server. Through our analysis, we demonstrate that
PSO-Fed maintains convergence in both mean and mean-square senses, even under
the strain of model-poisoning attacks. We further derive the theoretical mean
square error (MSE) of PSO-Fed, linking it to various parameters such as
stepsize, attack probability, number of Byzantine clients, client participation
rate, partial-sharing ratio, and noise variance. We also show that there is a
non-trivial optimal stepsize for PSO-Fed when faced with model-poisoning
attacks. The results of our extensive numerical experiments affirm our
theoretical assertions and highlight the superior ability of PSO-Fed to
counteract Byzantine attacks, outperforming other related leading algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AdaptSFL: Adaptive Split Federated Learning in Resource-constrained Edge
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13101v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13101v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheng Lin, Guanqiao Qu, Wei Wei, Xianhao Chen, Kin K. Leung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing complexity of deep neural networks poses significant barriers
to democratizing them to resource-limited edge devices. To address this
challenge, split federated learning (SFL) has emerged as a promising solution
by of floading the primary training workload to a server via model partitioning
while enabling parallel training among edge devices. However, although system
optimization substantially influences the performance of SFL under
resource-constrained systems, the problem remains largely uncharted. In this
paper, we provide a convergence analysis of SFL which quantifies the impact of
model splitting (MS) and client-side model aggregation (MA) on the learning
performance, serving as a theoretical foundation. Then, we propose AdaptSFL, a
novel resource-adaptive SFL framework, to expedite SFL under
resource-constrained edge computing systems. Specifically, AdaptSFL adaptively
controls client-side MA and MS to balance communication-computing latency and
training convergence. Extensive simulations across various datasets validate
that our proposed AdaptSFL framework takes considerably less time to achieve a
target accuracy than benchmarks, demonstrating the effectiveness of the
proposed strategies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FedSR: A Semi-Decentralized Federated Learning Algorithm for Non-IIDness
  in IoT System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14718v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14718v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianjun Huang, Lixin Ye, Li Kang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the Industrial Internet of Things (IoT), a large amount of data will be
generated every day. Due to privacy and security issues, it is difficult to
collect all these data together to train deep learning models, thus the
federated learning, a distributed machine learning paradigm that protects data
privacy, has been widely used in IoT. However, in practical federated learning,
the data distributions usually have large differences across devices, and the
heterogeneity of data will deteriorate the performance of the model. Moreover,
federated learning in IoT usually has a large number of devices involved in
training, and the limited communication resource of cloud servers become a
bottleneck for training. To address the above issues, in this paper, we combine
centralized federated learning with decentralized federated learning to design
a semi-decentralized cloud-edge-device hierarchical federated learning
framework, which can mitigate the impact of data heterogeneity, and can be
deployed at lage scale in IoT. To address the effect of data heterogeneity, we
use an incremental subgradient optimization algorithm in each ring cluster to
improve the generalization ability of the ring cluster models. Our extensive
experiments show that our approach can effectively mitigate the impact of data
heterogeneity and alleviate the communication bottleneck in cloud servers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Distributed Learning based on 1-Bit Gradient Coding in the Presence of
  Stragglers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14716v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14716v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengxi Li, Mikael Skoglund
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper considers the problem of distributed learning (DL) in the presence
of stragglers. For this problem, DL methods based on gradient coding have been
widely investigated, which redundantly distribute the training data to the
workers to guarantee convergence when some workers are stragglers. However,
these methods require the workers to transmit real-valued vectors during the
process of learning, which induces very high communication burden. To overcome
this drawback, we propose a novel DL method based on 1-bit gradient coding
(1-bit GCDL), where 1-bit data encoded from the locally computed gradients are
transmitted by the workers to reduce the communication overhead. We
theoretically provide the convergence guarantees of the proposed method for
both the convex loss functions and nonconvex loss functions. It is shown
empirically that 1-bit GC-DL outperforms the baseline methods, which attains
better learning performance under the same communication overhead.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE TCOM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating Emerging AI/ML Accelerators: IPU, RDU, and NVIDIA/AMD GPUs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.04417v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.04417v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongwu Peng, Caiwen Ding, Tong Geng, Sutanay Choudhury, Kevin Barker, Ang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The relentless advancement of artificial intelligence (AI) and machine
learning (ML) applications necessitates the development of specialized hardware
accelerators capable of handling the increasing complexity and computational
demands. Traditional computing architectures, based on the von Neumann model,
are being outstripped by the requirements of contemporary AI/ML algorithms,
leading to a surge in the creation of accelerators like the Graphcore
Intelligence Processing Unit (IPU), Sambanova Reconfigurable Dataflow Unit
(RDU), and enhanced GPU platforms. These hardware accelerators are
characterized by their innovative data-flow architectures and other design
optimizations that promise to deliver superior performance and energy
efficiency for AI/ML tasks.
  This research provides a preliminary evaluation and comparison of these
commercial AI/ML accelerators, delving into their hardware and software design
features to discern their strengths and unique capabilities. By conducting a
series of benchmark evaluations on common DNN operators and other AI/ML
workloads, we aim to illuminate the advantages of data-flow architectures over
conventional processor designs and offer insights into the performance
trade-offs of each platform. The findings from our study will serve as a
valuable reference for the design and performance expectations of research
prototypes, thereby facilitating the development of next-generation hardware
accelerators tailored for the ever-evolving landscape of AI/ML applications.
Through this analysis, we aspire to contribute to the broader understanding of
current accelerator technologies and to provide guidance for future innovations
in the field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICPE 2024 accepted publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Vertical Federated Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.07931v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.07931v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paul K. Mandal, Cole Leo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the popularization of AI solutions for image based problems, there has
been a growing concern for both data privacy and acquisition. In a large number
of cases, information is located on separate data silos and it can be difficult
for a developer to consolidate all of it in a fashion that is appropriate for
machine learning model development. Alongside this, a portion of these
localized data regions may not have access to a labelled ground truth. This
indicates that they have the capacity to reach conclusions numerically, but are
not able to assign classifications amid a lack of pertinent information. Such a
determination is often negligible, especially when attempting to develop image
based solutions that often necessitate this capability. With this being the
case, we propose an innovative vertical federated learning (VFL) model
architecture that can operate under this common set of conditions. This is the
first (and currently the only) implementation of a system that can work under
the constraints of a VFL environment and perform image segmentation while
maintaining nominal accuracies. We achieved this by utilizing an FCN that
boasts the ability to operate on federates that lack labelled data and
privately share the respective weights with a central server, that of which
hosts the necessary features for classification. Tests were conducted on the
CamVid dataset in order to determine the impact of heavy feature compression
required for the transfer of information between federates, as well as to reach
nominal conclusions about the overall performance metrics when working under
such constraints.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ JORA: JAX Tensor-Parallel LoRA Library for Retrieval Augmented
  Fine-Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11366v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11366v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anique Tahir, Lu Cheng, Huan Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The scaling of Large Language Models (LLMs) for retrieval-based tasks,
particularly in Retrieval Augmented Generation (RAG), faces significant memory
constraints, especially when fine-tuning extensive prompt sequences. Current
open-source libraries support full-model inference and fine-tuning across
multiple GPUs but fall short of accommodating the efficient parameter
distribution required for retrieved context. Addressing this gap, we introduce
a novel framework for PEFT-compatible fine-tuning of Llama-2 models, leveraging
distributed training. Our framework uniquely utilizes JAX's just-in-time (JIT)
compilation and tensor-sharding for efficient resource management, thereby
enabling accelerated fine-tuning with reduced memory requirements. This
advancement significantly improves the scalability and feasibility of
fine-tuning LLMs for complex RAG applications, even on systems with limited GPU
resources. Our experiments show more than 12x improvement in runtime compared
to Hugging Face/DeepSpeed implementation with four GPUs while consuming less
than half the VRAM per GPU.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Predicting Dynamic Memory Requirements for Scientific Workflow Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.08185v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.08185v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonathan Bader, Nils Diedrich, Lauritz Thamsen, Odej Kao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the increasing amount of data available to scientists in disciplines as
diverse as bioinformatics, physics, and remote sensing, scientific workflow
systems are becoming increasingly important for composing and executing
scalable data analysis pipelines. When writing such workflows, users need to
specify the resources to be reserved for tasks so that sufficient resources are
allocated on the target cluster infrastructure. Crucially, underestimating a
task's memory requirements can result in task failures. Therefore, users often
resort to overprovisioning, resulting in significant resource wastage and
decreased throughput. In this paper, we propose a novel online method that uses
monitoring time series data to predict task memory usage in order to reduce the
memory wastage of scientific workflow tasks. Our method predicts a task's
runtime, divides it into k equally-sized segments, and learns the peak memory
value for each segment depending on the total file input size. We evaluate the
prototype implementation of our method using workflows from the publicly
available nf-core repository, showing an average memory wastage reduction of
29.48% compared to the best state-of-the-art approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper accepted in 2023 IEEE International Conference on Big Data</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Xorbits: Automating Operator Tiling for Distributed Data Science <span class="chip">ICDE 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00865v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00865v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weizheng Lu, Kaisheng He, Xuye Qin, Chengjie Li, Zhong Wang, Tao Yuan, Xia Liao, Feng Zhang, Yueguo Chen, Xiaoyong Du
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data science pipelines commonly utilize dataframe and array operations for
tasks such as data preprocessing, analysis, and machine learning. The most
popular tools for these tasks are pandas and NumPy. However, these tools are
limited to executing on a single node, making them unsuitable for processing
large-scale data. Several systems have attempted to distribute data science
applications to clusters while maintaining interfaces similar to single-node
libraries, enabling data scientists to scale their workloads without
significant effort. However, existing systems often struggle with processing
large datasets due to Out-of-Memory (OOM) problems caused by poor data
partitioning. To overcome these challenges, we develop Xorbits, a
high-performance, scalable data science framework specifically designed to
distribute data science workloads across clusters while retaining familiar
APIs. The key differentiator of Xorbits is its ability to dynamically switch
between graph construction and graph execution. Xorbits has been successfully
deployed in production environments with up to 5k CPU cores. Its applications
span various domains, including user behavior analysis and recommendation
systems in the e-commerce sector, as well as credit assessment and risk
management in the finance industry. Users can easily scale their data science
workloads by simply changing the import line of their pandas and NumPy code.
Our experiments demonstrate that Xorbits can effectively process very large
datasets without encountering OOM or data-skewing problems. Over the fastest
state-of-the-art solutions, Xorbits achieves an impressive 2.66* speedup on
average. In terms of API coverage, Xorbits attains a compatibility rate of
96.7%, surpassing the fastest framework by an impressive margin of 60
percentage points. Xorbits is available at
https://github.com/xorbitsai/xorbits.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICDE 2024 Industrial and Application Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DistServe: Disaggregating Prefill and Decoding for Goodput-optimized
  Large Language Model Serving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.09670v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.09670v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinmin Zhong, Shengyu Liu, Junda Chen, Jianbo Hu, Yibo Zhu, Xuanzhe Liu, Xin Jin, Hao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  DistServe improves the performance of large language models (LLMs) serving by
disaggregating the prefill and decoding computation. Existing LLM serving
systems colocate the two phases and batch the computation of prefill and
decoding across all users and requests. We find that this strategy not only
leads to strong prefill-decoding interferences but also couples the resource
allocation and parallelism plans for both phases. LLM applications often
emphasize individual latency for each phase: time to first token (TTFT) for the
prefill phase and time per output token (TPOT) of each request for the decoding
phase. In the presence of stringent latency requirements, existing systems have
to prioritize one latency over the other, or over-provision compute resources
to meet both.
  DistServe assigns prefill and decoding computation to different GPUs, hence
eliminating prefill-decoding interferences. Given the application's TTFT and
TPOT requirements, DistServe co-optimizes the resource allocation and
parallelism strategy tailored for each phase. DistServe also places the two
phases according to the serving cluster's bandwidth to minimize the
communication caused by disaggregation. As a result, DistServe significantly
improves LLM serving performance in terms of the maximum rate that can be
served within both TTFT and TPOT constraints on each GPU. Our evaluations show
that on various popular LLMs, applications, and latency requirements, DistServe
can serve 4.48x more requests or 10.2x tighter SLO, compared to
state-of-the-art systems, while staying within latency constraints for > 90% of
requests.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MaxK-GNN: Extremely Fast GPU Kernel Design for Accelerating Graph Neural
  Networks Training <span class="chip">ASPLOS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.08656v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.08656v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongwu Peng, Xi Xie, Kaustubh Shivdikar, MD Amit Hasan, Jiahui Zhao, Shaoyi Huang, Omer Khan, David Kaeli, Caiwen Ding
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the acceleration of deep neural network training, the GPU has become the
mainstream platform. GPUs face substantial challenges on GNNs, such as workload
imbalance and memory access irregularities, leading to underutilized hardware.
Existing solutions such as PyG, DGL with cuSPARSE, and GNNAdvisor frameworks
partially address these challenges but memory traffic is still significant.
  We argue that drastic performance improvements can only be achieved by the
vertical optimization of algorithm and system innovations, rather than treating
the speedup optimization as an "after-thought" (i.e., (i) given a GNN
algorithm, designing an accelerator, or (ii) given hardware, mainly optimizing
the GNN algorithm). In this paper, we present MaxK-GNN, an advanced
high-performance GPU training system integrating algorithm and system
innovation. (i) We introduce the MaxK nonlinearity and provide a theoretical
analysis of MaxK nonlinearity as a universal approximator, and present the
Compressed Balanced Sparse Row (CBSR) format, designed to store the data and
index of the feature matrix after nonlinearity; (ii) We design a coalescing
enhanced forward computation with row-wise product-based SpGEMM Kernel using
CBSR for input feature matrix fetching and strategic placement of a sparse
output accumulation buffer in shared memory; (iii) We develop an optimized
backward computation with outer product-based and SSpMM Kernel.
  We conduct extensive evaluations of MaxK-GNN and report the end-to-end system
run-time. Experiments show that MaxK-GNN system could approach the theoretical
speedup limit according to Amdahl's law. We achieve comparable accuracy to SOTA
GNNs, but at a significantly increased speed: 3.22/4.24 times speedup (vs.
theoretical limits, 5.52/7.27 times) on Reddit compared to DGL and GNNAdvisor
implementations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ASPLOS 2024 accepted publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Extending JSON CRDTs with Move Operations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.14007v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.14007v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liangrun Da, Martin Kleppmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conflict-Free Replicated Data Types (CRDTs) for JSON allow users to
concurrently update a JSON document and automatically merge the updates into a
consistent state. Moving a subtree in a map or reordering elements in a list
within a JSON CRDT is challenging: naive merge algorithms may introduce
unexpected results such as duplicates or cycles. In this paper, we introduce an
algorithm for move operations in a JSON CRDT that handles the interaction with
concurrent non-move operations, and uses novel optimisations to improve
performance. We plan to integrate this algorithm into the Automerge CRDT
library.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Performance
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating Emerging AI/ML Accelerators: IPU, RDU, and NVIDIA/AMD GPUs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.04417v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.04417v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongwu Peng, Caiwen Ding, Tong Geng, Sutanay Choudhury, Kevin Barker, Ang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The relentless advancement of artificial intelligence (AI) and machine
learning (ML) applications necessitates the development of specialized hardware
accelerators capable of handling the increasing complexity and computational
demands. Traditional computing architectures, based on the von Neumann model,
are being outstripped by the requirements of contemporary AI/ML algorithms,
leading to a surge in the creation of accelerators like the Graphcore
Intelligence Processing Unit (IPU), Sambanova Reconfigurable Dataflow Unit
(RDU), and enhanced GPU platforms. These hardware accelerators are
characterized by their innovative data-flow architectures and other design
optimizations that promise to deliver superior performance and energy
efficiency for AI/ML tasks.
  This research provides a preliminary evaluation and comparison of these
commercial AI/ML accelerators, delving into their hardware and software design
features to discern their strengths and unique capabilities. By conducting a
series of benchmark evaluations on common DNN operators and other AI/ML
workloads, we aim to illuminate the advantages of data-flow architectures over
conventional processor designs and offer insights into the performance
trade-offs of each platform. The findings from our study will serve as a
valuable reference for the design and performance expectations of research
prototypes, thereby facilitating the development of next-generation hardware
accelerators tailored for the ever-evolving landscape of AI/ML applications.
Through this analysis, we aspire to contribute to the broader understanding of
current accelerator technologies and to provide guidance for future innovations
in the field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICPE 2024 accepted publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhanced Scalability in Assessing Quantum Integer Factorization
  Performance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.05249v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.05249v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junseo Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the advancement of quantum technologies, there is a potential threat to
traditional encryption systems based on integer factorization. Therefore,
developing techniques for accurately measuring the performance of associated
quantum algorithms is crucial, as it can provide insights into the practical
feasibility from the current perspective. In this chapter, we aim to analyze
the time required for integer factorization tasks using Shor's algorithm within
a gate-based quantum circuit simulator of the matrix product state type.
Additionally, we observe the impact of parameter pre-selection in Shor's
algorithm. Specifically, this pre-selection is expected to increase the success
rate of integer factorization by reducing the number of iterations and
facilitating performance measurement under fixed conditions, thus enabling
scalable performance evaluation even on real quantum hardware.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 3 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Artificial Intelligence
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TexTile: A Differentiable Metric for Texture Tileability <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12961v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12961v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Carlos Rodriguez-Pardo, Dan Casas, Elena Garces, Jorge Lopez-Moreno
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce TexTile, a novel differentiable metric to quantify the degree
upon which a texture image can be concatenated with itself without introducing
repeating artifacts (i.e., the tileability). Existing methods for tileable
texture synthesis focus on general texture quality, but lack explicit analysis
of the intrinsic repeatability properties of a texture. In contrast, our
TexTile metric effectively evaluates the tileable properties of a texture,
opening the door to more informed synthesis and analysis of tileable textures.
Under the hood, TexTile is formulated as a binary classifier carefully built
from a large dataset of textures of different styles, semantics, regularities,
and human annotations.Key to our method is a set of architectural modifications
to baseline pre-train image classifiers to overcome their shortcomings at
measuring tileability, along with a custom data augmentation and training
regime aimed at increasing robustness and accuracy. We demonstrate that TexTile
can be plugged into different state-of-the-art texture synthesis methods,
including diffusion-based strategies, and generate tileable textures while
keeping or even improving the overall texture quality. Furthermore, we show
that TexTile can objectively evaluate any tileable texture synthesis method,
whereas the current mix of existing metrics produces uncorrelated scores which
heavily hinders progress in the field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024. Project page: https://mslab.es/projects/TexTile/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ WHAC: World-grounded Humans and Cameras 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12959v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12959v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wanqi Yin, Zhongang Cai, Ruisi Wang, Fanzhou Wang, Chen Wei, Haiyi Mei, Weiye Xiao, Zhitao Yang, Qingping Sun, Atsushi Yamashita, Ziwei Liu, Lei Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Estimating human and camera trajectories with accurate scale in the world
coordinate system from a monocular video is a highly desirable yet challenging
and ill-posed problem. In this study, we aim to recover expressive parametric
human models (i.e., SMPL-X) and corresponding camera poses jointly, by
leveraging the synergy between three critical players: the world, the human,
and the camera. Our approach is founded on two key observations. Firstly,
camera-frame SMPL-X estimation methods readily recover absolute human depth.
Secondly, human motions inherently provide absolute spatial cues. By
integrating these insights, we introduce a novel framework, referred to as
WHAC, to facilitate world-grounded expressive human pose and shape estimation
(EHPS) alongside camera pose estimation, without relying on traditional
optimization techniques. Additionally, we present a new synthetic dataset,
WHAC-A-Mole, which includes accurately annotated humans and cameras, and
features diverse interactive human motions as well as realistic camera
trajectories. Extensive experiments on both standard and newly established
benchmarks highlight the superiority and efficacy of our framework. We will
make the code and dataset publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Homepage: https://wqyin.github.io/projects/WHAC/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Just Shift It: Test-Time Prototype Shifting for Zero-Shot Generalization
  with Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12952v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12952v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elaine Sui, Xiaohan Wang, Serena Yeung-Levy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advancements in vision-language models (VLMs) have propelled the field of
computer vision, particularly in the zero-shot learning setting. Despite their
promise, the effectiveness of these models often diminishes due to domain
shifts in test environments. To address this, we introduce the Test-Time
Prototype Shifting (TPS) framework, a pioneering approach designed to adapt
VLMs to test datasets using unlabeled test inputs. Our method is based on the
notion of modulating per-class prototypes in the shared embedding space. By
pre-computing and caching prototypes generated with the pre-trained text
encoder, TPS not only facilitates optimization-free prototype reuse for
subsequent predictions but also enables seamless integration with current
advancements in prompt engineering. At test-time, TPS dynamically learns shift
vectors for each prototype based solely on the given test sample, effectively
bridging the domain gap and enhancing classification accuracy. A notable aspect
of our framework is its significantly reduced memory and computational demands
when compared to conventional text-prompt tuning methods. Extensive evaluations
across 15 datasets involving natural distribution shifts and cross-dataset
generalization demonstrate TPS's superior performance, achieving
state-of-the-art results while reducing resource requirements.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Vid2Robot: End-to-end Video-conditioned Policy Learning with
  Cross-Attention <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12943v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12943v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vidhi Jain, Maria Attarian, Nikhil J Joshi, Ayzaan Wahid, Danny Driess, Quan Vuong, Pannag R Sanketi, Pierre Sermanet, Stefan Welker, Christine Chan, Igor Gilitschenski, Yonatan Bisk, Debidatta Dwibedi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While large-scale robotic systems typically rely on textual instructions for
tasks, this work explores a different approach: can robots infer the task
directly from observing humans? This shift necessitates the robot's ability to
decode human intent and translate it into executable actions within its
physical constraints and environment. We introduce Vid2Robot, a novel
end-to-end video-based learning framework for robots. Given a video
demonstration of a manipulation task and current visual observations, Vid2Robot
directly produces robot actions. This is achieved through a unified
representation model trained on a large dataset of human video and robot
trajectory. The model leverages cross-attention mechanisms to fuse prompt video
features to the robot's current state and generate appropriate actions that
mimic the observed task. To further improve policy performance, we propose
auxiliary contrastive losses that enhance the alignment between human and robot
video representations. We evaluate Vid2Robot on real-world robots,
demonstrating a 20% improvement in performance compared to other
video-conditioned policies when using human demonstration videos. Additionally,
our model exhibits emergent capabilities, such as successfully transferring
observed motions from one object to another, and long-horizon composition, thus
showcasing its potential for real-world applications. Project website:
vid2robot.github.io
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Robot learning: Imitation Learning, Robot Perception, Sensing &
  Vision, Grasping & Manipulation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automatic Information Extraction From Employment Tribunal Judgements
  Using Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12936v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12936v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joana Ribeiro de Faria, Huiyuan Xie, Felix Steffek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Court transcripts and judgments are rich repositories of legal knowledge,
detailing the intricacies of cases and the rationale behind judicial decisions.
The extraction of key information from these documents provides a concise
overview of a case, crucial for both legal experts and the public. With the
advent of large language models (LLMs), automatic information extraction has
become increasingly feasible and efficient. This paper presents a comprehensive
study on the application of GPT-4, a large language model, for automatic
information extraction from UK Employment Tribunal (UKET) cases. We
meticulously evaluated GPT-4's performance in extracting critical information
with a manual verification process to ensure the accuracy and relevance of the
extracted data. Our research is structured around two primary extraction tasks:
the first involves a general extraction of eight key aspects that hold
significance for both legal specialists and the general public, including the
facts of the case, the claims made, references to legal statutes, references to
precedents, general case outcomes and corresponding labels, detailed order and
remedies and reasons for the decision. The second task is more focused, aimed
at analysing three of those extracted features, namely facts, claims and
outcomes, in order to facilitate the development of a tool capable of
predicting the outcome of employment law disputes. Through our analysis, we
demonstrate that LLMs like GPT-4 can obtain high accuracy in legal information
extraction, highlighting the potential of LLMs in revolutionising the way legal
information is processed and utilised, offering significant implications for
legal research and practice.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generalizable and Stable Finetuning of <span class="highlight-title">Pretrain</span>ed Language Models on
  Low-Resource Texts <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12918v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12918v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sai Ashish Somayajula, Youwei Liang, Abhishek Singh, Li Zhang, Pengtao Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pretrained Language Models (PLMs) have advanced Natural Language Processing
(NLP) tasks significantly, but finetuning PLMs on low-resource datasets poses
significant challenges such as instability and overfitting. Previous methods
tackle these issues by finetuning a strategically chosen subnetwork on a
downstream task, while keeping the remaining weights fixed to the pretrained
weights. However, they rely on a suboptimal criteria for sub-network selection,
leading to suboptimal solutions. To address these limitations, we propose a
regularization method based on attention-guided weight mixup for finetuning
PLMs. Our approach represents each network weight as a mixup of task-specific
weight and pretrained weight, controlled by a learnable attention parameter,
providing finer control over sub-network selection. Furthermore, we employ a
bi-level optimization (BLO) based framework on two separate splits of the
training dataset, improving generalization and combating overfitting. We
validate the efficacy of our proposed method through extensive experiments,
demonstrating its superiority over previous methods, particularly in the
context of finetuning PLMs on low-resource datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as a long paper to NAACL 2024 Main Conference; 18 pages, 11
  tables, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Yell At Your Robot: Improving On-the-Fly from Language Corrections 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12910v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12910v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lucy Xiaoyang Shi, Zheyuan Hu, Tony Z. Zhao, Archit Sharma, Karl Pertsch, Jianlan Luo, Sergey Levine, Chelsea Finn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hierarchical policies that combine language and low-level control have been
shown to perform impressively long-horizon robotic tasks, by leveraging either
zero-shot high-level planners like pretrained language and vision-language
models (LLMs/VLMs) or models trained on annotated robotic demonstrations.
However, for complex and dexterous skills, attaining high success rates on
long-horizon tasks still represents a major challenge -- the longer the task
is, the more likely it is that some stage will fail. Can humans help the robot
to continuously improve its long-horizon task performance through intuitive and
natural feedback? In this paper, we make the following observation: high-level
policies that index into sufficiently rich and expressive low-level
language-conditioned skills can be readily supervised with human feedback in
the form of language corrections. We show that even fine-grained corrections,
such as small movements ("move a bit to the left"), can be effectively
incorporated into high-level policies, and that such corrections can be readily
obtained from humans observing the robot and making occasional suggestions.
This framework enables robots not only to rapidly adapt to real-time language
feedback, but also incorporate this feedback into an iterative training scheme
that improves the high-level policy's ability to correct errors in both
low-level execution and high-level decision-making purely from verbal feedback.
Our evaluation on real hardware shows that this leads to significant
performance improvement in long-horizon, dexterous manipulation tasks without
the need for any additional teleoperation. Videos and code are available at
https://yay-robot.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project website: https://yay-robot.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Toward Sustainable GenAI using Generation Directives for Carbon-Friendly
  Large Language Model Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12900v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12900v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Baolin Li, Yankai Jiang, Vijay Gadepally, Devesh Tiwari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancement of Generative Artificial Intelligence (GenAI) across
diverse sectors raises significant environmental concerns, notably the carbon
emissions from their cloud and high performance computing (HPC) infrastructure.
This paper presents Sprout, an innovative framework designed to address these
concerns by reducing the carbon footprint of generative Large Language Model
(LLM) inference services. Sprout leverages the innovative concept of
"generation directives" to guide the autoregressive generation process, thereby
enhancing carbon efficiency. Our proposed method meticulously balances the need
for ecological sustainability with the demand for high-quality generation
outcomes. Employing a directive optimizer for the strategic assignment of
generation directives to user prompts and an original offline quality
evaluator, Sprout demonstrates a significant reduction in carbon emissions by
over 40% in real-world evaluations using the Llama2 LLM and global electricity
grid data. This research marks a critical step toward aligning AI technology
with sustainable practices, highlighting the potential for mitigating
environmental impacts in the rapidly expanding domain of generative artificial
intelligence.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive Visual Imitation Learning for Robotic Assisted Feeding Across
  Varied Bowl Configurations and Food Types 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12891v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12891v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Liu, Amisha Bhaskar, Pratap Tokekar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we introduce a novel visual imitation network with a spatial
attention module for robotic assisted feeding (RAF). The goal is to acquire
(i.e., scoop) food items from a bowl. However, achieving robust and adaptive
food manipulation is particularly challenging. To deal with this, we propose a
framework that integrates visual perception with imitation learning to enable
the robot to handle diverse scenarios during scooping. Our approach, named AVIL
(adaptive visual imitation learning), exhibits adaptability and robustness
across different bowl configurations in terms of material, size, and position,
as well as diverse food types including granular, semi-solid, and liquid, even
in the presence of distractors. We validate the effectiveness of our approach
by conducting experiments on a real robot. We also compare its performance with
a baseline. The results demonstrate improvement over the baseline across all
scenarios, with an enhancement of up to 2.5 times in terms of a success metric.
Notably, our model, trained solely on data from a transparent glass bowl
containing granular cereals, showcases generalization ability when tested
zero-shot on other bowl configurations with different types of food.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Regularization in Spider-Style Strategy Discovery and Schedule
  Construction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12869v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12869v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Filip Bártek, Karel Chvalovský, Martin Suda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To achieve the best performance, automatic theorem provers often rely on
schedules of diverse proving strategies to be tried out (either sequentially or
in parallel) on a given problem. In this paper, we report on a large-scale
experiment with discovering strategies for the Vampire prover, targeting the
FOF fragment of the TPTP library and constructing a schedule for it, based on
the ideas of Andrei Voronkov's system Spider. We examine the process from
various angles, discuss the difficulty (or ease) of obtaining a strong Vampire
schedule for the CASC competition, and establish how well a schedule can be
expected to generalize to unseen problems and what factors influence this
property.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 8 figures, submitted to IJCAR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RASP: A Drone-based Reconfigurable Actuation and Sensing Platform
  Towards Ambient Intelligent Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12853v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12853v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minghui Zhao, Junxi Xia, Kaiyuan Hou, Yanchen Liu, Stephen Xia, Xiaofan Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Realizing consumer-grade drones that are as useful as robot vacuums
throughout our homes or personal smartphones in our daily lives requires drones
to sense, actuate, and respond to general scenarios that may arise. Towards
this vision, we propose RASP, a modular and reconfigurable sensing and
actuation platform that allows drones to autonomously swap onboard sensors and
actuators in only 25 seconds, allowing a single drone to quickly adapt to a
diverse range of tasks. RASP consists of a mechanical layer to physically swap
sensor modules, an electrical layer to maintain power and communication lines
to the sensor/actuator, and a software layer to maintain a common interface
between the drone and any sensor module in our platform. Leveraging recent
advances in large language and visual language models, we further introduce the
architecture, implementation, and real-world deployments of a personal
assistant system utilizing RASP. We demonstrate that RASP can enable a diverse
range of useful tasks in home, office, lab, and other indoor settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Answer Set Programming for Flexible Payroll Management 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12823v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12823v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benjamin Callewaert, Joost Vennekens
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Payroll management is a critical business task that is subject to a large
number of rules, which vary widely between companies, sectors, and countries.
Moreover, the rules are often complex and change regularly. Therefore, payroll
management systems must be flexible in design. In this paper, we suggest an
approach based on a flexible Answer Set Programming (ASP) model and an
easy-to-read tabular representation based on the Decision Model and Notation
(DMN) standard. It allows HR consultants to represent complex rules without the
need for a software engineer, and to ultimately design payroll systems for a
variety of different scenarios. We show how the multi-shot solving capabilities
of the clingo ASP system can be used to reach the performance that is necessary
to handle real-world instances.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under consideration in Theory and Practice of Logic Programming
  (TPLP)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FlowerFormer: Empowering Neural Architecture Encoding using a Flow-aware
  Graph <span class="highlight-title">Transformer</span> <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12821v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12821v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongyeong Hwang, Hyunju Kim, Sunwoo Kim, Kijung Shin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The success of a specific neural network architecture is closely tied to the
dataset and task it tackles; there is no one-size-fits-all solution. Thus,
considerable efforts have been made to quickly and accurately estimate the
performances of neural architectures, without full training or evaluation, for
given tasks and datasets. Neural architecture encoding has played a crucial
role in the estimation, and graphbased methods, which treat an architecture as
a graph, have shown prominent performance. For enhanced representation learning
of neural architectures, we introduce FlowerFormer, a powerful graph
transformer that incorporates the information flows within a neural
architecture. FlowerFormer consists of two key components: (a) bidirectional
asynchronous message passing, inspired by the flows; (b) global attention built
on flow-based masking. Our extensive experiments demonstrate the superiority of
FlowerFormer over existing neural encoding methods, and its effectiveness
extends beyond computer vision models to include graph neural networks and auto
speech recognition models. Our code is available at
http://github.com/y0ngjaenius/CVPR2024_FLOWERFormer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024 Camera-Ready</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Re-identification from histopathology images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12816v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12816v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonathan Ganz, Jonas Ammeling, Samir Jabari, Katharina Breininger, Marc Aubreville
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In numerous studies, deep learning algorithms have proven their potential for
the analysis of histopathology images, for example, for revealing the subtypes
of tumors or the primary origin of metastases. These models require large
datasets for training, which must be anonymized to prevent possible patient
identity leaks. This study demonstrates that even relatively simple deep
learning algorithms can re-identify patients in large histopathology datasets
with substantial accuracy. We evaluated our algorithms on two TCIA datasets
including lung squamous cell carcinoma (LSCC) and lung adenocarcinoma (LUAD).
We also demonstrate the algorithm's performance on an in-house dataset of
meningioma tissue. We predicted the source patient of a slide with F1 scores of
50.16 % and 52.30 % on the LSCC and LUAD datasets, respectively, and with 62.31
% on our meningioma dataset. Based on our findings, we formulated a risk
assessment scheme to estimate the risk to the patient's privacy prior to
publication.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 7 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Comparing Explanation Faithfulness between Multilingual and Monolingual
  Fine-tuned Language Models <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12809v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12809v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhixue Zhao, Nikolaos Aletras
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In many real natural language processing application scenarios, practitioners
not only aim to maximize predictive performance but also seek faithful
explanations for the model predictions. Rationales and importance distribution
given by feature attribution methods (FAs) provide insights into how different
parts of the input contribute to a prediction. Previous studies have explored
how different factors affect faithfulness, mainly in the context of monolingual
English models. On the other hand, the differences in FA faithfulness between
multilingual and monolingual models have yet to be explored. Our extensive
experiments, covering five languages and five popular FAs, show that FA
faithfulness varies between multilingual and monolingual models. We find that
the larger the multilingual model, the less faithful the FAs are compared to
its counterpart monolingual models.Our further analysis shows that the
faithfulness disparity is potentially driven by the differences between model
tokenizers. Our code is available:
https://github.com/casszhao/multilingual-faith.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NAACL 2024 Main Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Contextual Moral Value Alignment Through Context-Based Aggregation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12805v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12805v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pierre Dognin, Jesus Rios, Ronny Luss, Inkit Padhi, Matthew D Riemer, Miao Liu, Prasanna Sattigeri, Manish Nagireddy, Kush R. Varshney, Djallel Bouneffouf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Developing value-aligned AI agents is a complex undertaking and an ongoing
challenge in the field of AI. Specifically within the domain of Large Language
Models (LLMs), the capability to consolidate multiple independently trained
dialogue agents, each aligned with a distinct moral value, into a unified
system that can adapt to and be aligned with multiple moral values is of
paramount importance. In this paper, we propose a system that does contextual
moral value alignment based on contextual aggregation. Here, aggregation is
defined as the process of integrating a subset of LLM responses that are best
suited to respond to a user input, taking into account features extracted from
the user's input. The proposed system shows better results in term of alignment
to human value compared to the state of the art.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Investigating Text Shortening Strategy in <span class="highlight-title">BERT</span>: Truncation vs
  Summarization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12799v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12799v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mirza Alim Mutasodirin, Radityo Eko Prasojo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The parallelism of Transformer-based models comes at the cost of their input
max-length. Some studies proposed methods to overcome this limitation, but none
of them reported the effectiveness of summarization as an alternative. In this
study, we investigate the performance of document truncation and summarization
in text classification tasks. Each of the two was investigated with several
variations. This study also investigated how close their performances are to
the performance of full-text. We used a dataset of summarization tasks based on
Indonesian news articles (IndoSum) to do classification tests. This study shows
how the summaries outperform the majority of truncation method variations and
lose to only one. The best strategy obtained in this study is taking the head
of the document. The second is extractive summarization. This study explains
what happened to the result, leading to further research in order to exploit
the potential of document summarization as a shortening alternative. The code
and data used in this work are publicly available in
https://github.com/mirzaalimm/TruncationVsSummarization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The 13th International Conference on Advanced Computer Science and
  Information Systems (ICACSIS 2021)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Discover and Mitigate Multiple Biased Subgroups in Image Classifiers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12777v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12777v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeliang Zhang, Mingqian Feng, Zhiheng Li, Chenliang Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning models can perform well on in-distribution data but often
fail on biased subgroups that are underrepresented in the training data,
hindering the robustness of models for reliable applications. Such subgroups
are typically unknown due to the absence of subgroup labels. Discovering biased
subgroups is the key to understanding models' failure modes and further
improving models' robustness. Most previous works of subgroup discovery make an
implicit assumption that models only underperform on a single biased subgroup,
which does not hold on in-the-wild data where multiple biased subgroups exist.
  In this work, we propose Decomposition, Interpretation, and Mitigation (DIM),
a novel method to address a more challenging but also more practical problem of
discovering multiple biased subgroups in image classifiers. Our approach
decomposes the image features into multiple components that represent multiple
subgroups. This decomposition is achieved via a bilinear dimension reduction
method, Partial Least Square (PLS), guided by useful supervision from the image
classifier. We further interpret the semantic meaning of each subgroup
component by generating natural language descriptions using vision-language
foundation models. Finally, DIM mitigates multiple biased subgroups
simultaneously via two strategies, including the data- and model-centric
strategies. Extensive experiments on CIFAR-100 and Breeds datasets demonstrate
the effectiveness of DIM in discovering and mitigating multiple biased
subgroups. Furthermore, DIM uncovers the failure modes of the classifier on
Hard ImageNet, showcasing its broader applicability to understanding model bias
in image classifiers. The code is available at
https://github.com/ZhangAIPI/DIM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Building Brain Tumor Segmentation Networks with User-Assisted Filter
  Estimation and Selection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12748v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12748v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matheus A. Cerqueira, Flávia Sprenger, Bernardo C. A. Teixeira, Alexandre X. Falcão
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Brain tumor image segmentation is a challenging research topic in which
deep-learning models have presented the best results. However, the traditional
way of training those models from many pre-annotated images leaves several
unanswered questions. Hence methodologies, such as Feature Learning from Image
Markers (FLIM), have involved an expert in the learning loop to reduce human
effort in data annotation and build models sufficiently deep for a given
problem. FLIM has been successfully used to create encoders, estimating the
filters of all convolutional layers from patches centered at marker voxels. In
this work, we present Multi-Step (MS) FLIM - a user-assisted approach to
estimating and selecting the most relevant filters from multiple FLIM
executions. MS-FLIM is used only for the first convolutional layer, and the
results already indicate improvement over FLIM. For evaluation, we build a
simple U-shaped encoder-decoder network, named sU-Net, for glioblastoma
segmentation using T1Gd and FLAIR MRI scans, varying the encoder's training
method, using FLIM, MS-FLIM, and backpropagation algorithm. Also, we compared
these sU-Nets with two State-Of-The-Art (SOTA) deep-learning models using two
datasets. The results show that the sU-Net based on MS-FLIM outperforms the
other training methods and achieves effectiveness within the standard
deviations of the SOTA models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 5 figures, 2 tables, 24 references, manuscript of
  conference paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ What Does Evaluation of Explainable Artificial Intelligence Actually
  Tell Us? A Case for Compositional and Contextual Validation of XAI Building
  Blocks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12730v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12730v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kacper Sokol, Julia E. Vogt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite significant progress, evaluation of explainable artificial
intelligence remains elusive and challenging. In this paper we propose a
fine-grained validation framework that is not overly reliant on any one facet
of these sociotechnical systems, and that recognises their inherent modular
structure: technical building blocks, user-facing explanatory artefacts and
social communication protocols. While we concur that user studies are
invaluable in assessing the quality and effectiveness of explanation
presentation and delivery strategies from the explainees' perspective in a
particular deployment context, the underlying explanation generation mechanisms
require a separate, predominantly algorithmic validation strategy that accounts
for the technical and human-centred desiderata of their (numerical) outputs.
Such a comprehensive sociotechnical utility-based evaluation framework could
allow to systematically reason about the properties and downstream influence of
different building blocks from which explainable artificial intelligence
systems are composed -- accounting for a diverse range of their engineering and
social aspects -- in view of the anticipated use case.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in Extended Abstracts of the 2024 CHI Conference on Human
  Factors in Computing Systems (CHI EA '24)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Python Fuzzing for Trustworthy Machine Learning Frameworks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12723v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12723v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ilya Yegorov, Eli Kobrin, Darya Parygina, Alexey Vishnyakov, Andrey Fedotov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ensuring the security and reliability of machine learning frameworks is
crucial for building trustworthy AI-based systems. Fuzzing, a popular technique
in secure software development lifecycle (SSDLC), can be used to develop secure
and robust software. Popular machine learning frameworks such as PyTorch and
TensorFlow are complex and written in multiple programming languages including
C/C++ and Python. We propose a dynamic analysis pipeline for Python projects
using the Sydr-Fuzz toolset. Our pipeline includes fuzzing, corpus
minimization, crash triaging, and coverage collection. Crash triaging and
severity estimation are important steps to ensure that the most critical
vulnerabilities are addressed promptly. Furthermore, the proposed pipeline is
integrated in GitLab CI. To identify the most vulnerable parts of the machine
learning frameworks, we analyze their potential attack surfaces and develop
fuzz targets for PyTorch, TensorFlow, and related projects such as h5py.
Applying our dynamic analysis pipeline to these targets, we were able to
discover 3 new bugs and propose fixes for them.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RLingua: Improving Reinforcement Learning Sample Efficiency in Robotic
  Manipulations With Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06420v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06420v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liangliang Chen, Yutian Lei, Shiyu Jin, Ying Zhang, Liangjun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning (RL) has demonstrated its capability in solving
various tasks but is notorious for its low sample efficiency. In this paper, we
propose RLingua, a framework that can leverage the internal knowledge of large
language models (LLMs) to reduce the sample complexity of RL in robotic
manipulations. To this end, we first present a method for extracting the prior
knowledge of LLMs by prompt engineering so that a preliminary rule-based robot
controller for a specific task can be generated in a user-friendly manner.
Despite being imperfect, the LLM-generated robot controller is utilized to
produce action samples during rollouts with a decaying probability, thereby
improving RL's sample efficiency. We employ TD3, the widely-used RL baseline
method, and modify the actor loss to regularize the policy learning towards the
LLM-generated controller. RLingua also provides a novel method of improving the
imperfect LLM-generated robot controllers by RL. We demonstrate that RLingua
can significantly reduce the sample complexity of TD3 in four robot tasks of
panda_gym and achieve high success rates in 12 sampled sparsely rewarded robot
tasks in RLBench, where the standard TD3 fails. Additionally, We validated
RLingua's effectiveness in real-world robot experiments through Sim2Real,
demonstrating that the learned policies are effectively transferable to real
robot tasks. Further details about our work are available at our project
website https://rlingua.github.io.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring Facial Expression Recognition through Semi-Supervised
  <span class="highlight-title">Pretrain</span>ing and Temporal Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11942v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11942v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun Yu, Zhihong Wei, Zhongpeng Cai, Gongpeng Zhao, Zerui Zhang, Yongqi Wang, Guochen Xie, Jichao Zhu, Wangyuan Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Facial Expression Recognition (FER) plays a crucial role in computer vision
and finds extensive applications across various fields. This paper aims to
present our approach for the upcoming 6th Affective Behavior Analysis
in-the-Wild (ABAW) competition, scheduled to be held at CVPR2024. In the facial
expression recognition task, The limited size of the FER dataset poses a
challenge to the expression recognition model's generalization ability,
resulting in subpar recognition performance. To address this problem, we employ
a semi-supervised learning technique to generate expression category
pseudo-labels for unlabeled face data. At the same time, we uniformly sampled
the labeled facial expression samples and implemented a debiased feedback
learning strategy to address the problem of category imbalance in the dataset
and the possible data bias in semi-supervised learning. Moreover, to further
compensate for the limitation and bias of features obtained only from static
images, we introduced a Temporal Encoder to learn and capture temporal
relationships between neighbouring expression image features. In the 6th ABAW
competition, our method achieved outstanding results on the official validation
set, a result that fully confirms the effectiveness and competitiveness of our
proposed method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Align before Adapt: Leveraging Entity-to-Region Alignments for
  Generalizable Video Action Recognition <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.15619v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.15619v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifei Chen, Dapeng Chen, Ruijin Liu, Sai Zhou, Wenyuan Xue, Wei Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale visual-language pre-trained models have achieved significant
success in various video tasks. However, most existing methods follow an "adapt
then align" paradigm, which adapts pre-trained image encoders to model
video-level representations and utilizes one-hot or text embedding of the
action labels for supervision. This paradigm overlooks the challenge of mapping
from static images to complicated activity concepts. In this paper, we propose
a novel "Align before Adapt" (ALT) paradigm. Prior to adapting to video
representation learning, we exploit the entity-to-region alignments for each
frame. The alignments are fulfilled by matching the region-aware image
embeddings to an offline-constructed text corpus. With the aligned entities, we
feed their text embeddings to a transformer-based video adapter as the queries,
which can help extract the semantics of the most important entities from a
video to a vector. This paradigm reuses the visual-language alignment of VLP
during adaptation and tries to explain an action by the underlying entities.
This helps understand actions by bridging the gap with complex activity
semantics, particularly when facing unfamiliar or unseen categories. ALT
demonstrates competitive performance while maintaining remarkably low
computational costs. In fully supervised experiments, it achieves 88.1% top-1
accuracy on Kinetics-400 with only 4947 GFLOPs. Moreover, ALT outperforms the
previous state-of-the-art methods in both zero-shot and few-shot experiments,
emphasizing its superior generalizability across various learning scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Alignment Problem from a Deep Learning Perspective <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.00626v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.00626v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Richard Ngo, Lawrence Chan, Sören Mindermann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In coming years or decades, artificial general intelligence (AGI) may surpass
human capabilities at many critical tasks. We argue that, without substantial
effort to prevent it, AGIs could learn to pursue goals that are in conflict
(i.e. misaligned) with human interests. If trained like today's most capable
models, AGIs could learn to act deceptively to receive higher reward, learn
misaligned internally-represented goals which generalize beyond their
fine-tuning distributions, and pursue those goals using power-seeking
strategies. We review emerging evidence for these properties. AGIs with these
properties would be difficult to align and may appear aligned even when they
are not. Finally, we briefly outline how the deployment of misaligned AGIs
might irreversibly undermine human control over the world, and we review
research directions aimed at preventing this outcome.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Vertical Federated Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.07931v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.07931v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paul K. Mandal, Cole Leo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the popularization of AI solutions for image based problems, there has
been a growing concern for both data privacy and acquisition. In a large number
of cases, information is located on separate data silos and it can be difficult
for a developer to consolidate all of it in a fashion that is appropriate for
machine learning model development. Alongside this, a portion of these
localized data regions may not have access to a labelled ground truth. This
indicates that they have the capacity to reach conclusions numerically, but are
not able to assign classifications amid a lack of pertinent information. Such a
determination is often negligible, especially when attempting to develop image
based solutions that often necessitate this capability. With this being the
case, we propose an innovative vertical federated learning (VFL) model
architecture that can operate under this common set of conditions. This is the
first (and currently the only) implementation of a system that can work under
the constraints of a VFL environment and perform image segmentation while
maintaining nominal accuracies. We achieved this by utilizing an FCN that
boasts the ability to operate on federates that lack labelled data and
privately share the respective weights with a central server, that of which
hosts the necessary features for classification. Tests were conducted on the
CamVid dataset in order to determine the impact of heavy feature compression
required for the transfer of information between federates, as well as to reach
nominal conclusions about the overall performance metrics when working under
such constraints.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SmartRefine: A Scenario-Adaptive Refinement Framework for Efficient
  Motion Prediction <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11492v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11492v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Zhou, Hao Shao, Letian Wang, Steven L. Waslander, Hongsheng Li, Yu Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Predicting the future motion of surrounding agents is essential for
autonomous vehicles (AVs) to operate safely in dynamic, human-robot-mixed
environments. Context information, such as road maps and surrounding agents'
states, provides crucial geometric and semantic information for motion behavior
prediction. To this end, recent works explore two-stage prediction frameworks
where coarse trajectories are first proposed, and then used to select critical
context information for trajectory refinement. However, they either incur a
large amount of computation or bring limited improvement, if not both. In this
paper, we introduce a novel scenario-adaptive refinement strategy, named
SmartRefine, to refine prediction with minimal additional computation.
Specifically, SmartRefine can comprehensively adapt refinement configurations
based on each scenario's properties, and smartly chooses the number of
refinement iterations by introducing a quality score to measure the prediction
quality and remaining refinement potential of each scenario. SmartRefine is
designed as a generic and flexible approach that can be seamlessly integrated
into most state-of-the-art motion prediction models. Experiments on Argoverse
(1 & 2) show that our method consistently improves the prediction accuracy of
multiple state-of-the-art prediction models. Specifically, by adding
SmartRefine to QCNet, we outperform all published ensemble-free works on the
Argoverse 2 leaderboard (single agent track) at submission. Comprehensive
studies are also conducted to ablate design choices and explore the mechanism
behind multi-iteration refinement. Codes are available at
https://github.com/opendilab/SmartRefine/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Camera-ready version for CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Radiology-<span class="highlight-title">GPT</span>: A Large Language Model for Radiology 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.08666v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.08666v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengliang Liu, Aoxiao Zhong, Yiwei Li, Longtao Yang, Chao Ju, Zihao Wu, Chong Ma, Peng Shu, Cheng Chen, Sekeun Kim, Haixing Dai, Lin Zhao, Lichao Sun, Dajiang Zhu, Jun Liu, Wei Liu, Dinggang Shen, Xiang Li, Quanzheng Li, Tianming Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Radiology-GPT, a large language model for radiology. Using an
instruction tuning approach on an extensive dataset of radiology domain
knowledge, Radiology-GPT demonstrates superior performance compared to general
language models such as StableLM, Dolly and LLaMA. It exhibits significant
versatility in radiological diagnosis, research, and communication. This work
serves as a catalyst for future developments in clinical NLP. The successful
implementation of Radiology-GPT is indicative of the potential of localizing
generative large language models, specifically tailored for distinctive medical
specialties, while ensuring adherence to privacy standards such as HIPAA. The
prospect of developing individualized, large-scale language models that cater
to specific needs of various hospitals presents a promising direction. The
fusion of conversational competence and domain-specific knowledge in these
models is set to foster future development in healthcare AI. A demo of
Radiology-GPT is available at
https://huggingface.co/spaces/allen-eric/radiology-gpt.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SynCDR : Training Cross Domain Retrieval Models with Synthetic Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00420v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00420v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samarth Mishra, Carlos D. Castillo, Hongcheng Wang, Kate Saenko, Venkatesh Saligrama
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In cross-domain retrieval, a model is required to identify images from the
same semantic category across two visual domains. For instance, given a sketch
of an object, a model needs to retrieve a real image of it from an online
store's catalog. A standard approach for such a problem is learning a feature
space of images where Euclidean distances reflect similarity. Even without
human annotations, which may be expensive to acquire, prior methods function
reasonably well using unlabeled images for training. Our problem constraint
takes this further to scenarios where the two domains do not necessarily share
any common categories in training data. This can occur when the two domains in
question come from different versions of some biometric sensor recording
identities of different people. We posit a simple solution, which is to
generate synthetic data to fill in these missing category examples across
domains. This, we do via category preserving translation of images from one
visual domain to another. We compare approaches specifically trained for this
translation for a pair of domains, as well as those that can use large-scale
pre-trained text-to-image diffusion models via prompts, and find that the
latter can generate better replacement synthetic data, leading to more accurate
cross-domain retrieval models. Our best SynCDR model can outperform prior art
by up to 15\%. Code for our work is available at
https://github.com/samarth4149/SynCDR .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Pre-print</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Clinical Reasoning over Tabular Data and Text with Bayesian Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09481v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09481v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paloma Rabaey, Johannes Deleu, Stefan Heytens, Thomas Demeester
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bayesian networks are well-suited for clinical reasoning on tabular data, but
are less compatible with natural language data, for which neural networks
provide a successful framework. This paper compares and discusses strategies to
augment Bayesian networks with neural text representations, both in a
generative and discriminative manner. This is illustrated with simulation
results for a primary care use case (diagnosis of pneumonia) and discussed in a
broader clinical context.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Reducing Diagnostic Errors with Interpretable Risk Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.10109v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.10109v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Denis Jered McInerney, William Dickinson, Lucy C. Flynn, Andrea C. Young, Geoffrey S. Young, Jan-Willem van de Meent, Byron C. Wallace
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many diagnostic errors occur because clinicians cannot easily access relevant
information in patient Electronic Health Records (EHRs). In this work we
propose a method to use LLMs to identify pieces of evidence in patient EHR data
that indicate increased or decreased risk of specific diagnoses; our ultimate
aim is to increase access to evidence and reduce diagnostic errors. In
particular, we propose a Neural Additive Model to make predictions backed by
evidence with individualized risk estimates at time-points where clinicians are
still uncertain, aiming to specifically mitigate delays in diagnosis and errors
stemming from an incomplete differential. To train such a model, it is
necessary to infer temporally fine-grained retrospective labels of eventual
"true" diagnoses. We do so with LLMs, to ensure that the input text is from
before a confident diagnosis can be made. We use an LLM to retrieve an initial
pool of evidence, but then refine this set of evidence according to
correlations learned by the model. We conduct an in-depth evaluation of the
usefulness of our approach by simulating how it might be used by a clinician to
decide between a pre-defined list of differential diagnoses.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BugNIST - a Large Volumetric <span class="highlight-title">Dataset</span> for Object Detection under Domain
  Shift 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.01838v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.01838v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Patrick Møller Jensen, Vedrana Andersen Dahl, Carsten Gundlach, Rebecca Engberg, Hans Martin Kjer, Anders Bjorholm Dahl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Domain shift significantly influences the performance of deep learning
algorithms, particularly for object detection within volumetric 3D images.
Annotated training data is essential for deep learning-based object detection.
However, annotating densely packed objects is time-consuming and costly.
Instead, we suggest training models on individually scanned objects, causing a
domain shift between training and detection data. To address this challenge, we
introduce the BugNIST dataset, comprising 9154 micro-CT volumes of 12 bug types
and 388 volumes of tightly packed bug mixtures. This dataset is characterized
by having objects with the same appearance in the source and target domain,
which is uncommon for other benchmark datasets for domain shift. During
training, individual bug volumes labeled by class are utilized, while testing
employs mixtures with center point annotations and bug type labels. Together
with the dataset, we provide a baseline detection analysis, aiming at advancing
the field of 3D object detection methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 6 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Do Physicians Know How to <span class="highlight-title">Prompt</span>? The Need for Automatic <span class="highlight-title">Prompt</span>
  Optimization Help in Clinical Note Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.09684v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.09684v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zonghai Yao, Ahmed Jaafar, Beining Wang, Zhichao Yang, Hong Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study examines the effect of prompt engineering on the performance of
Large Language Models (LLMs) in clinical note generation. We introduce an
Automatic Prompt Optimization (APO) framework to refine initial prompts and
compare the outputs of medical experts, non-medical experts, and APO-enhanced
GPT3.5 and GPT4. Results highlight GPT4 APO's superior performance in
standardizing prompt quality across clinical note sections. A human-in-the-loop
approach shows that experts maintain content quality post-APO, with a
preference for their own modifications, suggesting the value of expert
customization. We recommend a two-phase optimization process, leveraging
APO-GPT4 for consistency and expert input for personalization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Attacks, Defenses and Evaluations for LLM Conversation Safety: A <span class="highlight-title">Survey</span> <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.09283v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.09283v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhichen Dong, Zhanhui Zhou, Chao Yang, Jing Shao, Yu Qiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are now commonplace in conversation
applications. However, their risks of misuse for generating harmful responses
have raised serious societal concerns and spurred recent research on LLM
conversation safety. Therefore, in this survey, we provide a comprehensive
overview of recent studies, covering three critical aspects of LLM conversation
safety: attacks, defenses, and evaluations. Our goal is to provide a structured
summary that enhances understanding of LLM conversation safety and encourages
further investigation into this important subject. For easy reference, we have
categorized all the studies mentioned in this survey according to our taxonomy,
available at: https://github.com/niconi19/LLM-conversation-safety.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Guiding Masked Representation Learning to Capture Spatio-Temporal
  Relationship of Electrocardiogram <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.09450v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.09450v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yeongyeon Na, Minje Park, Yunwon Tae, Sunghoon Joo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Electrocardiograms (ECG) are widely employed as a diagnostic tool for
monitoring electrical signals originating from a heart. Recent machine learning
research efforts have focused on the application of screening various diseases
using ECG signals. However, adapting to the application of screening disease is
challenging in that labeled ECG data are limited. Achieving general
representation through self-supervised learning (SSL) is a well-known approach
to overcome the scarcity of labeled data; however, a naive application of SSL
to ECG data, without considering the spatial-temporal relationships inherent in
ECG signals, may yield suboptimal results. In this paper, we introduce ST-MEM
(Spatio-Temporal Masked Electrocardiogram Modeling), designed to learn
spatio-temporal features by reconstructing masked 12-lead ECG data. ST-MEM
outperforms other SSL baseline methods in various experimental settings for
arrhythmia classification tasks. Moreover, we demonstrate that ST-MEM is
adaptable to various lead combinations. Through quantitative and qualitative
analysis, we show a spatio-temporal relationship within ECG data. Our code is
available at https://github.com/bakqui/ST-MEM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024. The first three authors contribute equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Ada-NAV: Adaptive Trajectory Length-Based Sample Efficient Policy
  Learning for Robotic Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.06192v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.06192v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bhrij Patel, Kasun Weerakoon, Wesley A. Suttle, Alec Koppel, Brian M. Sadler, Tianyi Zhou, Amrit Singh Bedi, Dinesh Manocha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Trajectory length stands as a crucial hyperparameter within reinforcement
learning (RL) algorithms, significantly contributing to the sample inefficiency
in robotics applications. Motivated by the pivotal role trajectory length plays
in the training process, we introduce Ada-NAV, a novel adaptive trajectory
length scheme designed to enhance the training sample efficiency of RL
algorithms in robotic navigation tasks. Unlike traditional approaches that
treat trajectory length as a fixed hyperparameter, we propose to dynamically
adjust it based on the entropy of the underlying navigation policy.
Interestingly, Ada-NAV can be applied to both existing on-policy and off-policy
RL methods, which we demonstrate by empirically validating its efficacy on
three popular RL methods: REINFORCE, Proximal Policy Optimization (PPO), and
Soft Actor-Critic (SAC). We demonstrate through simulated and real-world
robotic experiments that Ada-NAV outperforms conventional methods that employ
constant or randomly sampled trajectory lengths. Specifically, for a fixed
sample budget, Ada-NAV achieves an 18\% increase in navigation success rate, a
20-38\% reduction in navigation path length, and a 9.32\% decrease in elevation
costs. Furthermore, we showcase the versatility of Ada-NAV by integrating it
with the Clearpath Husky robot, illustrating its applicability in complex
outdoor environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 9 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Impossible Distillation: from Low-Quality Model to High-Quality <span class="highlight-title">Dataset</span>
  & Model for Summarization and Paraphrasing <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.16635v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.16635v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaehun Jung, Peter West, Liwei Jiang, Faeze Brahman, Ximing Lu, Jillian Fisher, Taylor Sorensen, Yejin Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Impossible Distillation, a novel framework for paraphrasing and
sentence summarization, that distills a high-quality dataset and model from a
low-quality teacher that itself cannot perform these tasks. Unlike prior works
that rely on an extreme-scale teacher model (e.g., GPT3) or task-specific
architecture, we hypothesize and verify the paraphrastic proximity intrinsic to
pre-trained LMs (e.g., GPT2), where paraphrases occupy a proximal subspace in
the LM distribution. By identifying and distilling generations from these
subspaces, Impossible Distillation produces a high-quality dataset and model
even from GPT2-scale LMs. We evaluate our method on multiple benchmarks
spanning unconstrained / syntax-controlled paraphrase generation and sentence
summarization. Our model with 770M parameters consistently outperforms strong
baselines, including models distilled from ChatGPT, and sometimes, even ChatGPT
itself. Also, we find that our distilled dataset from 1.5B LMs exhibits higher
diversity and fidelity than up to 13 times larger datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Low-power, Continuous Remote Behavioral Localization with Event Cameras 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.03799v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.03799v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Friedhelm Hamann, Suman Ghosh, Ignacio Juarez Martinez, Tom Hart, Alex Kacelnik, Guillermo Gallego
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Researchers in natural science need reliable methods for quantifying animal
behavior. Recently, numerous computer vision methods emerged to automate the
process. However, observing wild species at remote locations remains a
challenging task due to difficult lighting conditions and constraints on power
supply and data storage. Event cameras offer unique advantages for
battery-dependent remote monitoring due to their low power consumption and high
dynamic range capabilities. We use this novel sensor to quantify a behavior in
Chinstrap penguins called ecstatic display. We formulate the problem as a
temporal action detection task, determining the start and end times of the
behavior. For this purpose, we recorded a colony of breeding penguins in
Antarctica for several weeks and labeled event data on 16 nests. The developed
method consists of a generator of candidate time intervals (proposals) and a
classifier of the actions within them. The experiments show that the event
cameras' natural response to motion is effective for continuous behavior
monitoring and detection, reaching a mean average precision (mAP) of 58% (which
increases to 63% in good weather conditions). The results also demonstrate the
robustness against various lighting conditions contained in the challenging
dataset. The low-power capabilities of the event camera allow it to record
significantly longer than with a conventional camera. This work pioneers the
use of event cameras for remote wildlife observation, opening new
interdisciplinary opportunities. https://tub-rip.github.io/eventpenguins/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 8 figures, 12 tables, Project page:
  https://tub-rip.github.io/eventpenguins/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Recurrent Learning Through Long Short Term Memory and TOPSIS 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.00693v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.00693v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rossi Kamal, Zuzana Kubincova, Mosaddek Hossain Kamal, Upama Kabir
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Enterprise resource planning (ERP) software brings resources, data together
to keep software-flow within business processes in a company. However, cloud
computing's cheap, easy and quick management promise pushes business-owners for
a transition from monolithic to a data-center/cloud based ERP. Since cloud-ERP
development involves a cyclic process, namely planning, implementing, testing
and upgrading, its adoption is realized as a deep recurrent neural network
problem. Eventually, a classification algorithm based on long short term memory
(LSTM) and TOPSIS is proposed to identify and rank, respectively, adoption
features. Our theoretical model is validated over a reference model by
articulating key players, services, architecture, functionalities. Qualitative
survey is conducted among users by considering technology, innovation and
resistance issues, to formulate hypotheses on key adoption factors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This submission has been withdrawn by arXiv administrators as the
  second author was added without their knowledge or consent</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Gaussian Process Neural Additive Models <span class="chip">AAAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.12518v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.12518v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Zhang, Brian Barr, John Paisley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks have revolutionized many fields, but their black-box
nature also occasionally prevents their wider adoption in fields such as
healthcare and finance, where interpretable and explainable models are
required. The recent development of Neural Additive Models (NAMs) is a
significant step in the direction of interpretable deep learning for tabular
datasets. In this paper, we propose a new subclass of NAMs that use a
single-layer neural network construction of the Gaussian process via random
Fourier features, which we call Gaussian Process Neural Additive Models
(GP-NAM). GP-NAMs have the advantage of a convex objective function and number
of trainable parameters that grows linearly with feature dimensionality. It
suffers no loss in performance compared to deeper NAM approaches because GPs
are well-suited for learning complex non-parametric univariate functions. We
demonstrate the performance of GP-NAM on several tabular datasets, showing that
it achieves comparable or better performance in both classification and
regression tasks with a large reduction in the number of parameters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Appears at AAAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RA-Rec: An Efficient ID Representation Alignment Framework for LLM-based
  Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.04527v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.04527v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaohan Yu, Li Zhang, Xin Zhao, Yue Wang, Zhongrui Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLM) have recently emerged as a powerful tool for a
variety of natural language processing tasks, bringing a new surge of combining
LLM with recommendation systems, termed as LLM-based RS. Current approaches
generally fall into two main paradigms, the ID direct usage paradigm and the ID
translation paradigm, noting their core weakness stems from lacking
recommendation knowledge and uniqueness. To address this limitation, we propose
a new paradigm, ID representation, which incorporates pre-trained ID embeddings
into LLMs in a complementary manner. In this work, we present RA-Rec, an
efficient ID representation alignment framework for LLM-based recommendation,
which is compatible with multiple ID-based methods and LLM architectures.
Specifically, we treat ID embeddings as soft prompts and design an innovative
alignment module and an efficient tuning method with tailored data construction
for alignment. Extensive experiments demonstrate RA-Rec substantially
outperforms current state-of-the-art methods, achieving up to 3.0% absolute
HitRate@100 improvements while utilizing less than 10x training data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLMs as Hackers: Autonomous Linux Privilege Escalation Attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.11409v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.11409v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andreas Happe, Aaron Kaplan, Jürgen Cito
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Penetration testing, an essential component of software security testing,
allows organizations to proactively identify and remediate vulnerabilities in
their systems, thus bolstering their defense mechanisms against potential
cyberattacks. One recent advancement in the realm of penetration testing is the
utilization of Language Models (LLMs).
  We explore the intersection of LLMs and penetration testing to gain insight
into their capabilities and challenges in the context of privilege escalation.
We create an automated Linux privilege-escalation benchmark utilizing local
virtual machines. We introduce an LLM-guided privilege-escalation tool designed
for evaluating different LLMs and prompt strategies against our benchmark.
  Our results show that GPT-4 is well suited for detecting file-based exploits
as it can typically solve 75-100\% of test-cases of that vulnerability class.
GPT-3.5-turbo was only able to solve 25-50% of those, while local models, such
as Llama2 were not able to detect any exploits. We analyze the impact of
different prompt designs, the benefits of in-context learning, and the
advantages of offering high-level guidance to LLMs. We discuss challenging
areas for LLMs, including maintaining focus during testing, coping with errors,
and finally comparing them with both stochastic parrots as well as with human
hackers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Using construction waste hauling trucks' GPS data to classify
  earthwork-related locations: A Chengdu case study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.14698v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.14698v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Yu, Ke Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Earthwork-related locations (ERLs), such as construction sites, earth dumping
ground, and concrete mixing stations, are major sources of urban dust pollution
(particulate matters). The effective management of ERLs is crucial and requires
timely and efficient tracking of these locations throughout the city. This work
aims to identify and classify urban ERLs using GPS trajectory data of over
16,000 construction waste hauling trucks (CWHTs), as well as 58 urban features
encompassing geographic, land cover, POI and transport dimensions. We compare
several machine learning models and examine the impact of various
spatial-temporal features on classification performance using real-world data
in Chengdu, China. The results demonstrate that 77.8% classification accuracy
can be achieved with a limited number of features. This classification
framework was implemented in the Alpha MAPS system in Chengdu, which has
successfully identified 724 construction cites/earth dumping ground, 48
concrete mixing stations, and 80 truck parking locations in the city during
December 2023, which has enabled local authority to effectively manage urban
dust pollution at low personnel costs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Fast and Optimal Learning-based Path Planning Method for Planetary
  Rovers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.04792v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.04792v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiming Ji, Yang Liu, Guanghu Xie, Boyu Ma, Zongwu Xie, Baoshi Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Intelligent autonomous path planning is crucial to improve the exploration
efficiency of planetary rovers. In this paper, we propose a learning-based
method to quickly search for optimal paths in an elevation map, which is called
NNPP. The NNPP model learns semantic information about start and goal
locations, as well as map representations, from numerous pre-annotated optimal
path demonstrations, and produces a probabilistic distribution over each pixel
representing the likelihood of it belonging to an optimal path on the map. More
specifically, the paper computes the traversal cost for each grid cell from the
slope, roughness and elevation difference obtained from the DEM. Subsequently,
the start and goal locations are encoded using a Gaussian distribution and
different location encoding parameters are analyzed for their effect on model
performance. After training, the NNPP model is able to perform path planning on
novel maps. Experiments show that the guidance field generated by the NNPP
model can significantly reduce the search time for optimal paths under the same
hardware conditions, and the advantage of NNPP increases with the scale of the
map.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AI-enhanced Collective Intelligence: The State of the Art and Prospects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10433v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10433v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Cui, Taha Yasseri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The current societal challenges exceed the capacity of human individual or
collective effort alone. As AI evolves, its role within human collectives is
poised to vary from an assistive tool to a participatory member. Humans and AI
possess complementary capabilities that, when synergized, can achieve a level
of collective intelligence that surpasses the collective capabilities of either
humans or AI in isolation. However, the interactions in human-AI systems are
inherently complex, involving intricate processes and interdependencies. This
review incorporates perspectives from network science to conceptualize a
multilayer representation of human-AI collective intelligence, comprising a
cognition layer, a physical layer, and an information layer. Within this
multilayer network, humans and AI agents exhibit varying characteristics;
humans differ in diversity from surface-level to deep-level attributes, while
AI agents range in degrees of functionality and anthropomorphism. The interplay
among these agents shapes the overall structure and dynamics of the system. We
explore how agents' diversity and interactions influence the system's
collective intelligence. Furthermore, we present an analysis of real-world
instances of AI-enhanced collective intelligence. We conclude by addressing the
potential challenges in AI-enhanced collective intelligence and offer
perspectives on future developments in this field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Reinforcement Learning Approach to Dairy Farm Battery Management using
  Q Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09499v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09499v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nawazish Ali, Abdul Wahid, Rachael Shaw, Karl Mason
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dairy farming consumes a significant amount of energy, making it an
energy-intensive sector within agriculture. Integrating renewable energy
generation into dairy farming could help address this challenge. Effective
battery management is important for integrating renewable energy generation.
Managing battery charging and discharging poses significant challenges because
of fluctuations in electrical consumption, the intermittent nature of renewable
energy generation, and fluctuations in energy prices. Artificial Intelligence
(AI) has the potential to significantly improve the use of renewable energy in
dairy farming, however, there is limited research conducted in this particular
domain. This research considers Ireland as a case study as it works towards
attaining its 2030 energy strategy centered on the utilization of renewable
sources. This study proposes a Q-learning-based algorithm for scheduling
battery charging and discharging in a dairy farm setting. This research also
explores the effect of the proposed algorithm by adding wind generation data
and considering additional case studies. The proposed algorithm reduces the
cost of imported electricity from the grid by 13.41\%, peak demand by 2\%, and
24.49\% when utilizing wind generation. These results underline how
reinforcement learning is highly effective in managing batteries in the dairy
farming sector.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cross or Wait? Predicting Pedestrian Interaction Outcomes at
  Unsignalized Crossings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.08260v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.08260v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chi Zhang, Amir Hossein Kalantari, Yue Yang, Zhongjun Ni, Gustav Markkula, Natasha Merat, Christian Berger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Predicting pedestrian behavior when interacting with vehicles is one of the
most critical challenges in the field of automated driving. Pedestrian crossing
behavior is influenced by various interaction factors, including time to
arrival, pedestrian waiting time, the presence of zebra crossing, and the
properties and personality traits of both pedestrians and drivers. However,
these factors have not been fully explored for use in predicting interaction
outcomes. In this paper, we use machine learning to predict pedestrian crossing
behavior including pedestrian crossing decision, crossing initiation time
(CIT), and crossing duration (CD) when interacting with vehicles at
unsignalized crossings. Distributed simulator data are utilized for predicting
and analyzing the interaction factors. Compared with the logistic regression
baseline model, our proposed neural network model improves the prediction
accuracy and F1 score by 4.46% and 3.23%, respectively. Our model also reduces
the root mean squared error (RMSE) for CIT and CD by 21.56% and 30.14% compared
with the linear regression model. Additionally, we have analyzed the importance
of interaction factors, and present the results of models using fewer factors.
This provides information for model selection in different scenarios with
limited input features.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 7 figures, 9 tables. Accepted in 2023 IEEE Intelligent
  Vehicles Symposium (IV). DOI: 10.1109/IV55152.2023.10186616</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Meta-<span class="highlight-title">Prompt</span>ing for Automating Zero-shot Visual Recognition with LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11755v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11755v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        M. Jehanzeb Mirza, Leonid Karlinsky, Wei Lin, Sivan Doveh, Jakub Micorek, Mateusz Kozinski, Hilde Kuhene, Horst Possegger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prompt ensembling of Large Language Model (LLM) generated category-specific
prompts has emerged as an effective method to enhance zero-shot recognition
ability of Vision-Language Models (VLMs). To obtain these category-specific
prompts, the present methods rely on hand-crafting the prompts to the LLMs for
generating VLM prompts for the downstream tasks. However, this requires
manually composing these task-specific prompts and still, they might not cover
the diverse set of visual concepts and task-specific styles associated with the
categories of interest. To effectively take humans out of the loop and
completely automate the prompt generation process for zero-shot recognition, we
propose Meta-Prompting for Visual Recognition (MPVR). Taking as input only
minimal information about the target task, in the form of its short natural
language description, and a list of associated class labels, MPVR automatically
produces a diverse set of category-specific prompts resulting in a strong
zero-shot classifier. MPVR generalizes effectively across various popular
zero-shot image recognition benchmarks belonging to widely different domains
when tested with multiple LLMs and VLMs. For example, MPVR obtains a zero-shot
recognition improvement over CLIP by up to 19.8% and 18.2% (5.0% and 4.5% on
average over 20 datasets) leveraging GPT and Mixtral LLMs, respectively
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page (Code and Data):
  https://jmiemirza.github.io/Meta-Prompting/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Phenomenal Yet Puzzling: Testing Inductive Reasoning Capabilities of
  Language Models with Hypothesis Refinement <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08559v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08559v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linlu Qiu, Liwei Jiang, Ximing Lu, Melanie Sclar, Valentina Pyatkin, Chandra Bhagavatula, Bailin Wang, Yoon Kim, Yejin Choi, Nouha Dziri, Xiang Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability to derive underlying principles from a handful of observations
and then generalize to novel situations -- known as inductive reasoning -- is
central to human intelligence. Prior work suggests that language models (LMs)
often fall short on inductive reasoning, despite achieving impressive success
on research benchmarks. In this work, we conduct a systematic study of the
inductive reasoning capabilities of LMs through iterative hypothesis
refinement, a technique that more closely mirrors the human inductive process
than standard input-output prompting. Iterative hypothesis refinement employs a
three-step process: proposing, selecting, and refining hypotheses in the form
of textual rules. By examining the intermediate rules, we observe that LMs are
phenomenal hypothesis proposers (i.e., generating candidate rules), and when
coupled with a (task-specific) symbolic interpreter that is able to
systematically filter the proposed set of rules, this hybrid approach achieves
strong results across inductive reasoning benchmarks that require inducing
causal relations, language-like instructions, and symbolic concepts. However,
they also behave as puzzling inductive reasoners, showing notable performance
gaps between rule induction (i.e., identifying plausible rules) and rule
application (i.e., applying proposed rules to instances), suggesting that LMs
are proposing hypotheses without being able to actually apply the rules.
Through empirical and human analyses, we further reveal several discrepancies
between the inductive reasoning processes of LMs and humans, shedding light on
both the potentials and limitations of using LMs in inductive reasoning tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TabRepo: A Large Scale Repository of Tabular Model Evaluations and its
  AutoML Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.02971v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.02971v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Salinas, Nick Erickson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce TabRepo, a new dataset of tabular model evaluations and
predictions. TabRepo contains the predictions and metrics of 1310 models
evaluated on 200 classification and regression datasets. We illustrate the
benefit of our dataset in multiple ways. First, we show that it allows to
perform analysis such as comparing Hyperparameter Optimization against current
AutoML systems while also considering ensembling at marginal cost by using
precomputed model predictions. Second, we show that our dataset can be readily
leveraged to perform transfer-learning. In particular, we show that applying
standard transfer-learning techniques allows to outperform current
state-of-the-art tabular systems in accuracy, runtime and latency.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Databases
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Benchmark for Data Management Challenges in Microservices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12605v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12605v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rodrigo Laigner, Zhexiang Zhang, Yijian Liu, Leonardo Freitas Gomes, Yongluan Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Microservice architectures emerged as a popular architecture for designing
scalable distributed applications. Although microservices have been extensively
employed in industry settings for over a decade, there is little understanding
of the data management challenges that arise in these applications. As a
result, it is difficult to advance data system technologies for supporting
microservice applications. To fill this gap, we present Online Marketplace, a
microservice benchmark that incorporates core data management challenges that
existing benchmarks have not sufficiently addressed. These challenges include
transaction processing, query processing, event processing, constraint
enforcement, and data replication. We have defined criteria for various data
management issues to enable proper comparison across data systems and
platforms.
  After specifying the benchmark, we present the challenges we faced in
creating workloads that accurately reflect the dynamic state of the
microservices. We also discuss implementation issues that we encountered when
developing Online Marketplace in state-of-the-art data platforms, which
prevented us from meeting the specified data management requirements and
criteria. Our evaluation demonstrates that the benchmark is a valuable tool for
testing important properties sought by microservice practitioners. As a result,
our proposed benchmark will facilitate the design of future data systems to
meet the expectations of microservice practitioners.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quantixar: High-performance Vector Data Management System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12583v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12583v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gulshan Yadav, RahulKumar Yadav, Mansi Viramgama, Mayank Viramgama, Apeksha Mohite
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional database management systems need help efficiently represent and
querying the complex, high-dimensional data prevalent in modern applications.
Vector databases offer a solution by storing data as numerical vectors within a
multi-dimensional space. This enables similarity-based search and analysis,
such as image retrieval, recommendation engine generation, and natural language
processing. This paper introduces Quantixar, a vector database project designed
for efficiency in high-dimensional settings. Quantixar tackles the challenge of
managing high-dimensional data by strategically combining advanced indexing and
quantization techniques. It employs HNSW indexing for accelerated ANN search.
Additionally, Quantixar incorporates binary and product quantization to
compress high-dimensional vectors, reducing storage requirements and
computational costs during search. The paper delves into Quantixar's
architecture, specific implementation, and experimental methodology.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating Datalog over Semirings: A Grounding-based Approach <span class="chip">PODS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12436v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12436v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hangdong Zhao, Shaleen Deep, Paraschos Koutris, Sudeepa Roy, Val Tannen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Datalog is a powerful yet elegant language that allows expressing recursive
computation. Although Datalog evaluation has been extensively studied in the
literature, so far, only loose upper bounds are known on how fast a Datalog
program can be evaluated. In this work, we ask the following question: given a
Datalog program over a naturally-ordered semiring $\sigma$, what is the
tightest possible runtime? To this end, our main contribution is a general
two-phase framework for analyzing the data complexity of Datalog over $\sigma$:
first ground the program into an equivalent system of polynomial equations
(i.e. grounding) and then find the least fixpoint of the grounding over
$\sigma$. We present algorithms that use structure-aware query evaluation
techniques to obtain the smallest possible groundings. Next, efficient
algorithms for fixpoint evaluation are introduced over two classes of
semirings: (1) finite-rank semirings and (2) absorptive semirings of total
order. Combining both phases, we obtain state-of-the-art and new algorithmic
results. Finally, we complement our results with a matching fine-grained lower
bound.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear at PODS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Algorithmic Complexity Attacks on Dynamic Learned Indexes <span class="chip">VLDB 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12433v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12433v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Yang, Evgenios M. Kornaropoulos, Yue Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learned Index Structures (LIS) view a sorted index as a model that learns the
data distribution, takes a data element key as input, and outputs the predicted
position of the key. The original LIS can only handle lookup operations with no
support for updates, rendering it impractical to use for typical workloads. To
address this limitation, recent studies have focused on designing efficient
dynamic learned indexes. ALEX, as the pioneering dynamic learned index
structures, enables dynamism by incorporating a series of design choices,
including adaptive key space partitioning, dynamic model retraining, and
sophisticated engineering and policies that prioritize read/write performance.
While these design choices offer improved average-case performance, the
emphasis on flexibility and performance increases the attack surface by
allowing adversarial behaviors that maximize ALEX's memory space and time
complexity in worst-case scenarios. In this work, we present the first
systematic investigation of algorithmic complexity attacks (ACAs) targeting the
worst-case scenarios of ALEX. We introduce new ACAs that fall into two
categories, space ACAs and time ACAs, which target the memory space and time
complexity, respectively. First, our space ACA on data nodes exploits ALEX's
gapped array layout and uses Multiple-Choice Knapsack (MCK) to generate an
optimal adversarial insertion plan for maximizing the memory consumption at the
data node level. Second, our space ACA on internal nodes exploits ALEX's
catastrophic cost mitigation mechanism, causing an out-of-memory error with
only a few hundred adversarial insertions. Third, our time ACA generates
pathological insertions to increase the disparity between the actual key
distribution and the linear models of data nodes, deteriorating the runtime
performance by up to 1,641X compared to ALEX operating under legitimate
workloads.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>VLDB 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient k-step Weighted Reachability Query Processing Algorithms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13181v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13181v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lian Chen, Junfeng Zhou, Ming Du, Sheng Yu, Xian Tang, Ziyang Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given a data graph G, a source vertex u and a target vertex v of a
reachability query, the reachability query is used to answer whether there
exists a path from u to v in G. Reachability query processing is one of the
fundamental operations in graph data management, which is widely used in
biological networks, communication networks, and social networks to assist data
analysis. The data graphs in practical applications usually contain information
such as quantization weights associated with the structural relationships, in
addition to the structural relationships between vertices. Thus, in addition to
the traditional reachability relationships, users may want to further
understand whether such reachability relationships satisfy specific
constraints. In this paper, we study the problem of efficiently processing k
-step reachability queries with weighted constraints in weighted graphs. The k
-step weighted reachability query questions are used to answer the question of
whether there exists a path from a source vertex u to a goal vertex v in a
given weighted graph. If it exists, the path needs to satisfy 1) all edges in
the path satisfy the given weight constraints, and 2) the length of the path
does not exceed the given distance threshold k. To address the problem,
firstly, WKRI index supporting k -step weighted reachability query processing
and index construction methods based on efficient pruning strategies are
proposed. Secondly, the idea of constructing index based on part of the vertexs
is proposed to reduce the size of the index. We design and implement two
optimized indexes GWKRI and LWKRI based on the vertex coverage set. Finally,
experiments are conducted on several real datasets. The experimental results
verify the efficiency of the method proposed in this paper in answering k -step
weighted reachability queries.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Grafite: Taming Adversarial Queries with Optimal Range Filters <span class="chip">SIGMOD 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.15380v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.15380v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marco Costa, Paolo Ferragina, Giorgio Vinciguerra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Range filters allow checking whether a query range intersects a given set of
keys with a chance of returning a false positive answer, thus generalising the
functionality of Bloom filters from point to range queries. Existing practical
range filters have addressed this problem heuristically, resulting in high
false positive rates and query times when dealing with adversarial inputs, such
as in the common scenario where queries are correlated with the keys.
  We introduce Grafite, a novel range filter that solves these issues with a
simple design and clear theoretical guarantees that hold regardless of the
input data and query distribution: given a fixed space budget of $B$ bits per
key, the query time is $O(1)$, and the false positive probability is upper
bounded by $\ell/2^{B-2}$, where $\ell$ is the query range size. Our
experimental evaluation shows that Grafite is the only range filter to date to
achieve robust and predictable false positive rates across all combinations of
datasets, query workloads, and range sizes, while providing faster queries and
construction times, and dominating all competitors in the case of correlated
queries.
  As a further contribution, we introduce a very simple heuristic range filter
whose performance on uncorrelated queries is very close to or better than the
one achieved by the best heuristic range filters proposed in the literature so
far.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in Proceedings of the ACM on Management of
  Data (SIGMOD 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Counting Answers to Unions of Conjunctive Queries: Natural Tractability
  Criteria and Meta-Complexity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10634v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10634v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jacob Focke, Leslie Ann Goldberg, Marc Roth, Stanislav Živný
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the problem of counting answers to unions of conjunctive queries
(UCQs) under structural restrictions on the input query. Concretely, given a
class C of UCQs, the problem #UCQ(C) provides as input a UCQ Q in C and a
database D and the problem is to compute the number of answers of Q in D.
  Chen and Mengel [PODS'16] have shown that for any recursively enumerable
class C, the problem #UCQ(C) is either fixed-parameter tractable or hard for
one of the parameterised complexity classes W[1] or #W[1]. However, their
tractability criterion is unwieldy in the sense that, given any concrete class
C of UCQs, it is not easy to determine how hard it is to count answers to
queries in C. Moreover, given a single specific UCQ Q, it is not easy to
determine how hard it is to count answers to Q.
  In this work, we address the question of finding a natural tractability
criterion: The combined conjunctive query of a UCQ $\varphi_1 \vee \dots \vee
\varphi_\ell$ is the conjunctive query $\varphi_1 \wedge \dots \wedge
\varphi_\ell$. We show that under natural closure properties of C, the problem
#UCQ(C) is fixed-parameter tractable if and only if the combined conjunctive
queries of UCQs in C, and their contracts, have bounded treewidth. A contract
of a conjunctive query is an augmented structure, taking into account how the
quantified variables are connected to the free variables. If all variables are
free, then a conjunctive query is equal to its contract; in this special case
the criterion for fixed-parameter tractability of #UCQ(C) thus simplifies to
the combined queries having bounded treewidth.
  Finally, we give evidence that a closure property on C is necessary for
obtaining a natural tractability criterion: We show that even for a single UCQ
Q, the meta problem of deciding whether #UCQ({Q}) can be solved in time
$O(|D|^d)$ is NP-hard for any fixed $d\geq 1$.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>41 pages, 2 figures, abstract shortened due to ArXiv requirements</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Automated data processing and feature engineering for deep learning and
  big data applications: a <span class="highlight-title">survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11395v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11395v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alhassan Mumuni, Fuseini Mumuni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern approach to artificial intelligence (AI) aims to design algorithms
that learn directly from data. This approach has achieved impressive results
and has contributed significantly to the progress of AI, particularly in the
sphere of supervised deep learning. It has also simplified the design of
machine learning systems as the learning process is highly automated. However,
not all data processing tasks in conventional deep learning pipelines have been
automated. In most cases data has to be manually collected, preprocessed and
further extended through data augmentation before they can be effective for
training. Recently, special techniques for automating these tasks have emerged.
The automation of data processing tasks is driven by the need to utilize large
volumes of complex, heterogeneous data for machine learning and big data
applications. Today, end-to-end automated data processing systems based on
automated machine learning (AutoML) techniques are capable of taking raw data
and transforming them into useful features for Big Data tasks by automating all
intermediate processing stages. In this work, we present a thorough review of
approaches for automating data processing tasks in deep learning pipelines,
including automated data preprocessing--e.g., data cleaning, labeling, missing
data imputation, and categorical data encoding--as well as data augmentation
(including synthetic data generation using generative AI methods) and feature
engineering--specifically, automated feature extraction, feature construction
and feature selection. In addition to automating specific data processing
tasks, we discuss the use of AutoML methods and tools to simultaneously
optimize all stages of the machine learning pipeline.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Journal of Information and Intelligence (2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bangladesh Agricultural Knowledge Graph: Enabling Semantic Integration
  and Data-driven Analysis--Full Version 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11920v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11920v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rudra Pratap Deb Nath, Tithi Rani Das, Tonmoy Chandro Das, S. M. Shafkat Raihan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In Bangladesh, agriculture is a crucial driver for addressing Sustainable
Development Goal 1 (No Poverty) and 2 (Zero Hunger), playing a fundamental role
in the economy and people's livelihoods. To enhance the sustainability and
resilience of the agriculture industry through data-driven insights, the
Bangladesh Bureau of Statistics and other organizations consistently collect
and publish agricultural data on the Web. Nevertheless, the current datasets
encounter various challenges: 1) they are presented in an unsustainable,
static, read-only, and aggregated format, 2) they do not conform to the
Findability, Accessibility, Interoperability, and Reusability (FAIR)
principles, and 3) they do not facilitate interactive analysis and integration
with other data sources. In this paper, we present a thorough solution,
delineating a systematic procedure for developing BDAKG: a knowledge graph that
semantically and analytically integrates agriculture data in Bangladesh. BDAKG
incorporates multidimensional semantics, is linked with external knowledge
graphs, is compatible with OLAP, and adheres to the FAIR principles. Our
experimental evaluation centers on evaluating the integration process and
assessing the quality of the resultant knowledge graph in terms of
completeness, timeliness, FAIRness, OLAP compatibility and data-driven
analysis. Our federated data analysis recommend a strategic approach focused on
decreasing CO$_2$ emissions, fostering economic growth, and promoting
sustainable forestry.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>40 pages, 15 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLMLingua-2: Data Distillation for Efficient and Faithful Task-Agnostic
  <span class="highlight-title">Prompt</span> Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12968v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12968v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuoshi Pan, Qianhui Wu, Huiqiang Jiang, Menglin Xia, Xufang Luo, Jue Zhang, Qingwei Lin, Victor Rühle, Yuqing Yang, Chin-Yew Lin, H. Vicky Zhao, Lili Qiu, Dongmei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper focuses on task-agnostic prompt compression for better
generalizability and efficiency. Considering the redundancy in natural
language, existing approaches compress prompts by removing tokens or lexical
units according to their information entropy obtained from a causal language
model such as LLaMa-7B. The challenge is that information entropy may be a
suboptimal compression metric: (i) it only leverages unidirectional context and
may fail to capture all essential information needed for prompt compression;
(ii) it is not aligned with the prompt compression objective.
  To address these issues, we propose a data distillation procedure to derive
knowledge from an LLM to compress prompts without losing crucial information,
and meantime, introduce an extractive text compression dataset. We formulate
prompt compression as a token classification problem to guarantee the
faithfulness of the compressed prompt to the original one, and use a
Transformer encoder as the base architecture to capture all essential
information for prompt compression from the full bidirectional context. Our
approach leads to lower latency by explicitly learning the compression
objective with smaller models such as XLM-RoBERTa-large and mBERT.
  We evaluate our method on both in-domain and out-of-domain datasets,
including MeetingBank, LongBench, ZeroScrolls, GSM8K, and BBH. Despite its
small size, our model shows significant performance gains over strong baselines
and demonstrates robust generalization ability across different LLMs.
Additionally, our model is 3x-6x faster than existing prompt compression
methods, while accelerating the end-to-end latency by 1.6x-2.9x with
compression ratios of 2x-5x.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TexTile: A Differentiable Metric for Texture Tileability <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12961v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12961v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Carlos Rodriguez-Pardo, Dan Casas, Elena Garces, Jorge Lopez-Moreno
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce TexTile, a novel differentiable metric to quantify the degree
upon which a texture image can be concatenated with itself without introducing
repeating artifacts (i.e., the tileability). Existing methods for tileable
texture synthesis focus on general texture quality, but lack explicit analysis
of the intrinsic repeatability properties of a texture. In contrast, our
TexTile metric effectively evaluates the tileable properties of a texture,
opening the door to more informed synthesis and analysis of tileable textures.
Under the hood, TexTile is formulated as a binary classifier carefully built
from a large dataset of textures of different styles, semantics, regularities,
and human annotations.Key to our method is a set of architectural modifications
to baseline pre-train image classifiers to overcome their shortcomings at
measuring tileability, along with a custom data augmentation and training
regime aimed at increasing robustness and accuracy. We demonstrate that TexTile
can be plugged into different state-of-the-art texture synthesis methods,
including diffusion-based strategies, and generate tileable textures while
keeping or even improving the overall texture quality. Furthermore, we show
that TexTile can objectively evaluate any tileable texture synthesis method,
whereas the current mix of existing metrics produces uncorrelated scores which
heavily hinders progress in the field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024. Project page: https://mslab.es/projects/TexTile/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ WHAC: World-grounded Humans and Cameras 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12959v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12959v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wanqi Yin, Zhongang Cai, Ruisi Wang, Fanzhou Wang, Chen Wei, Haiyi Mei, Weiye Xiao, Zhitao Yang, Qingping Sun, Atsushi Yamashita, Ziwei Liu, Lei Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Estimating human and camera trajectories with accurate scale in the world
coordinate system from a monocular video is a highly desirable yet challenging
and ill-posed problem. In this study, we aim to recover expressive parametric
human models (i.e., SMPL-X) and corresponding camera poses jointly, by
leveraging the synergy between three critical players: the world, the human,
and the camera. Our approach is founded on two key observations. Firstly,
camera-frame SMPL-X estimation methods readily recover absolute human depth.
Secondly, human motions inherently provide absolute spatial cues. By
integrating these insights, we introduce a novel framework, referred to as
WHAC, to facilitate world-grounded expressive human pose and shape estimation
(EHPS) alongside camera pose estimation, without relying on traditional
optimization techniques. Additionally, we present a new synthetic dataset,
WHAC-A-Mole, which includes accurately annotated humans and cameras, and
features diverse interactive human motions as well as realistic camera
trajectories. Extensive experiments on both standard and newly established
benchmarks highlight the superiority and efficacy of our framework. We will
make the code and dataset publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Homepage: https://wqyin.github.io/projects/WHAC/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Just Shift It: Test-Time Prototype Shifting for Zero-Shot Generalization
  with Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12952v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12952v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elaine Sui, Xiaohan Wang, Serena Yeung-Levy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advancements in vision-language models (VLMs) have propelled the field of
computer vision, particularly in the zero-shot learning setting. Despite their
promise, the effectiveness of these models often diminishes due to domain
shifts in test environments. To address this, we introduce the Test-Time
Prototype Shifting (TPS) framework, a pioneering approach designed to adapt
VLMs to test datasets using unlabeled test inputs. Our method is based on the
notion of modulating per-class prototypes in the shared embedding space. By
pre-computing and caching prototypes generated with the pre-trained text
encoder, TPS not only facilitates optimization-free prototype reuse for
subsequent predictions but also enables seamless integration with current
advancements in prompt engineering. At test-time, TPS dynamically learns shift
vectors for each prototype based solely on the given test sample, effectively
bridging the domain gap and enhancing classification accuracy. A notable aspect
of our framework is its significantly reduced memory and computational demands
when compared to conventional text-prompt tuning methods. Extensive evaluations
across 15 datasets involving natural distribution shifts and cross-dataset
generalization demonstrate TPS's superior performance, achieving
state-of-the-art results while reducing resource requirements.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimal and Adaptive Non-Stationary Dueling Bandits Under a Generalized
  Borda Criterion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12950v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12950v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joe Suk, Arpit Agarwal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In dueling bandits, the learner receives preference feedback between arms,
and the regret of an arm is defined in terms of its suboptimality to a winner
arm. The more challenging and practically motivated non-stationary variant of
dueling bandits, where preferences change over time, has been the focus of
several recent works (Saha and Gupta, 2022; Buening and Saha, 2023; Suk and
Agarwal, 2023). The goal is to design algorithms without foreknowledge of the
amount of change.
  The bulk of known results here studies the Condorcet winner setting, where an
arm preferred over any other exists at all times. Yet, such a winner may not
exist and, to contrast, the Borda version of this problem (which is always
well-defined) has received little attention. In this work, we establish the
first optimal and adaptive Borda dynamic regret upper bound, which highlights
fundamental differences in the learnability of severe non-stationarity between
Condorcet vs. Borda regret objectives in dueling bandits.
  Surprisingly, our techniques for non-stationary Borda dueling bandits also
yield improved rates within the Condorcet winner setting, and reveal new
preference models where tighter notions of non-stationarity are adaptively
learnable. This is accomplished through a novel generalized Borda score
framework which unites the Borda and Condorcet problems, thus allowing
reduction of Condorcet regret to a Borda-like task. Such a generalization was
not previously known and is likely to be of independent interest.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Safety in Safe Bayesian Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12948v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12948v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christian Fiedler, Johanna Menn, Lukas Kreisköther, Sebastian Trimpe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Optimizing an unknown function under safety constraints is a central task in
robotics, biomedical engineering, and many other disciplines, and increasingly
safe Bayesian Optimization (BO) is used for this. Due to the safety critical
nature of these applications, it is of utmost importance that theoretical
safety guarantees for these algorithms translate into the real world. In this
work, we investigate three safety-related issues of the popular class of
SafeOpt-type algorithms. First, these algorithms critically rely on frequentist
uncertainty bounds for Gaussian Process (GP) regression, but concrete
implementations typically utilize heuristics that invalidate all safety
guarantees. We provide a detailed analysis of this problem and introduce
Real-\b{eta}-SafeOpt, a variant of the SafeOpt algorithm that leverages recent
GP bounds and thus retains all theoretical guarantees. Second, we identify
assuming an upper bound on the reproducing kernel Hilbert space (RKHS) norm of
the target function, a key technical assumption in SafeOpt-like algorithms, as
a central obstacle to real-world usage. To overcome this challenge, we
introduce the Lipschitz-only Safe Bayesian Optimization (LoSBO) algorithm,
which guarantees safety without an assumption on the RKHS bound, and
empirically show that this algorithm is not only safe, but also exhibits
superior performance compared to the state-of-the-art on several function
classes. Third, SafeOpt and derived algorithms rely on a discrete search space,
making them difficult to apply to higher-dimensional problems. To widen the
applicability of these algorithms, we introduce Lipschitz-only GP-UCB
(LoS-GP-UCB), a variant of LoSBO applicable to moderately high-dimensional
problems, while retaining safety.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sample Complexity of Offline Distributionally Robust Linear Markov
  Decision Processes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12946v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12946v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        He Wang, Laixi Shi, Yuejie Chi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In offline reinforcement learning (RL), the absence of active exploration
calls for attention on the model robustness to tackle the sim-to-real gap,
where the discrepancy between the simulated and deployed environments can
significantly undermine the performance of the learned policy. To endow the
learned policy with robustness in a sample-efficient manner in the presence of
high-dimensional state-action space, this paper considers the sample complexity
of distributionally robust linear Markov decision processes (MDPs) with an
uncertainty set characterized by the total variation distance using offline
data. We develop a pessimistic model-based algorithm and establish its sample
complexity bound under minimal data coverage assumptions, which outperforms
prior art by at least $\tilde{O}(d)$, where $d$ is the feature dimension. We
further improve the performance guarantee of the proposed algorithm by
incorporating a carefully-designed variance estimator.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural Differential Algebraic Equations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12938v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12938v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        James Koch, Madelyn Shapiro, Himanshu Sharma, Draguna Vrabie, Jan Drgona
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Differential-Algebraic Equations (DAEs) describe the temporal evolution of
systems that obey both differential and algebraic constraints. Of particular
interest are systems that contain implicit relationships between their
components, such as conservation relationships. Here, we present Neural
Differential-Algebraic Equations (NDAEs) suitable for data-driven modeling of
DAEs. This methodology is built upon the concept of the Universal Differential
Equation; that is, a model constructed as a system of Neural Ordinary
Differential Equations informed by theory from particular science domains. In
this work, we show that the proposed NDAEs abstraction is suitable for relevant
system-theoretic data-driven modeling tasks. Presented examples include (i) the
inverse problem of tank-manifold dynamics and (ii) discrepancy modeling of a
network of pumps, tanks, and pipes. Our experiments demonstrate the proposed
method's robustness to noise and extrapolation ability to (i) learn the
behaviors of the system components and their interaction physics and (ii)
disambiguate between data trends and mechanistic relationships contained in the
system.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generalizable and Stable Finetuning of <span class="highlight-title">Pretrain</span>ed Language Models on
  Low-Resource Texts <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12918v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12918v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sai Ashish Somayajula, Youwei Liang, Abhishek Singh, Li Zhang, Pengtao Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pretrained Language Models (PLMs) have advanced Natural Language Processing
(NLP) tasks significantly, but finetuning PLMs on low-resource datasets poses
significant challenges such as instability and overfitting. Previous methods
tackle these issues by finetuning a strategically chosen subnetwork on a
downstream task, while keeping the remaining weights fixed to the pretrained
weights. However, they rely on a suboptimal criteria for sub-network selection,
leading to suboptimal solutions. To address these limitations, we propose a
regularization method based on attention-guided weight mixup for finetuning
PLMs. Our approach represents each network weight as a mixup of task-specific
weight and pretrained weight, controlled by a learnable attention parameter,
providing finer control over sub-network selection. Furthermore, we employ a
bi-level optimization (BLO) based framework on two separate splits of the
training dataset, improving generalization and combating overfitting. We
validate the efficacy of our proposed method through extensive experiments,
demonstrating its superiority over previous methods, particularly in the
context of finetuning PLMs on low-resource datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as a long paper to NAACL 2024 Main Conference; 18 pages, 11
  tables, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Yell At Your Robot: Improving On-the-Fly from Language Corrections 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12910v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12910v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lucy Xiaoyang Shi, Zheyuan Hu, Tony Z. Zhao, Archit Sharma, Karl Pertsch, Jianlan Luo, Sergey Levine, Chelsea Finn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hierarchical policies that combine language and low-level control have been
shown to perform impressively long-horizon robotic tasks, by leveraging either
zero-shot high-level planners like pretrained language and vision-language
models (LLMs/VLMs) or models trained on annotated robotic demonstrations.
However, for complex and dexterous skills, attaining high success rates on
long-horizon tasks still represents a major challenge -- the longer the task
is, the more likely it is that some stage will fail. Can humans help the robot
to continuously improve its long-horizon task performance through intuitive and
natural feedback? In this paper, we make the following observation: high-level
policies that index into sufficiently rich and expressive low-level
language-conditioned skills can be readily supervised with human feedback in
the form of language corrections. We show that even fine-grained corrections,
such as small movements ("move a bit to the left"), can be effectively
incorporated into high-level policies, and that such corrections can be readily
obtained from humans observing the robot and making occasional suggestions.
This framework enables robots not only to rapidly adapt to real-time language
feedback, but also incorporate this feedback into an iterative training scheme
that improves the high-level policy's ability to correct errors in both
low-level execution and high-level decision-making purely from verbal feedback.
Our evaluation on real hardware shows that this leads to significant
performance improvement in long-horizon, dexterous manipulation tasks without
the need for any additional teleoperation. Videos and code are available at
https://yay-robot.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project website: https://yay-robot.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Toward Sustainable GenAI using Generation Directives for Carbon-Friendly
  Large Language Model Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12900v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12900v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Baolin Li, Yankai Jiang, Vijay Gadepally, Devesh Tiwari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancement of Generative Artificial Intelligence (GenAI) across
diverse sectors raises significant environmental concerns, notably the carbon
emissions from their cloud and high performance computing (HPC) infrastructure.
This paper presents Sprout, an innovative framework designed to address these
concerns by reducing the carbon footprint of generative Large Language Model
(LLM) inference services. Sprout leverages the innovative concept of
"generation directives" to guide the autoregressive generation process, thereby
enhancing carbon efficiency. Our proposed method meticulously balances the need
for ecological sustainability with the demand for high-quality generation
outcomes. Employing a directive optimizer for the strategic assignment of
generation directives to user prompts and an original offline quality
evaluator, Sprout demonstrates a significant reduction in carbon emissions by
over 40% in real-world evaluations using the Llama2 LLM and global electricity
grid data. This research marks a critical step toward aligning AI technology
with sustainable practices, highlighting the potential for mitigating
environmental impacts in the rapidly expanding domain of generative artificial
intelligence.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding the training of infinitely deep and wide ResNets with
  Conditional Optimal Transport 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12887v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12887v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raphaël Barboni, Gabriel Peyré, François-Xavier Vialard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the convergence of gradient flow for the training of deep neural
networks. If Residual Neural Networks are a popular example of very deep
architectures, their training constitutes a challenging optimization problem
due notably to the non-convexity and the non-coercivity of the objective. Yet,
in applications, those tasks are successfully solved by simple optimization
algorithms such as gradient descent. To better understand this phenomenon, we
focus here on a ``mean-field'' model of infinitely deep and arbitrarily wide
ResNet, parameterized by probability measures over the product set of layers
and parameters and with constant marginal on the set of layers. Indeed, in the
case of shallow neural networks, mean field models have proven to benefit from
simplified loss-landscapes and good theoretical guarantees when trained with
gradient flow for the Wasserstein metric on the set of probability measures.
Motivated by this approach, we propose to train our model with gradient flow
w.r.t. the conditional Optimal Transport distance: a restriction of the
classical Wasserstein distance which enforces our marginal condition. Relying
on the theory of gradient flows in metric spaces we first show the
well-posedness of the gradient flow equation and its consistency with the
training of ResNets at finite width. Performing a local Polyak-\L{}ojasiewicz
analysis, we then show convergence of the gradient flow for well-chosen
initializations: if the number of features is finite but sufficiently large and
the risk is sufficiently small at initialization, the gradient flow converges
towards a global minimizer. This is the first result of this type for
infinitely deep and arbitrarily wide ResNets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Short-Term Solar Irradiance Forecasting Under Data Transmission
  Constraints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12873v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12873v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joshua Edward Hammond, Ricardo A. Lara Orozco, Michael Baldea, Brian A. Korgel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We report a data-parsimonious machine learning model for short-term
forecasting of solar irradiance. The model inputs include sky camera images
that are reduced to scalar features to meet data transmission constraints. The
output irradiance values are transformed to focus on unknown short-term
dynamics. Inspired by control theory, a noise input is used to reflect
unmeasured variables and is shown to improve model predictions, often
considerably. Five years of data from the NREL Solar Radiation Research
Laboratory were used to create three rolling train-validate sets and determine
the best representations for time, the optimal span of input measurements, and
the most impactful model input data (features). For the chosen test data, the
model achieves a mean absolute error of 74.34 $W/m^2$ compared to a baseline
134.35 $W/m^2$ using the persistence of cloudiness model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Wildfire danger prediction optimization with transfer learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12871v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12871v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Spiros Maggioros, Nikos Tsalkitzis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Convolutional Neural Networks (CNNs) have proven instrumental across various
computer science domains, enabling advancements in object detection,
classification, and anomaly detection. This paper explores the application of
CNNs to analyze geospatial data specifically for identifying wildfire-affected
areas. Leveraging transfer learning techniques, we fine-tuned CNN
hyperparameters and integrated the Canadian Fire Weather Index (FWI) to assess
moisture conditions. The study establishes a methodology for computing wildfire
risk levels on a scale of 0 to 5, dynamically linked to weather patterns.
Notably, through the integration of transfer learning, the CNN model achieved
an impressive accuracy of 95\% in identifying burnt areas. This research sheds
light on the inner workings of CNNs and their practical, real-time utility in
predicting and mitigating wildfires. By combining transfer learning and CNNs,
this study contributes a robust approach to assess burnt areas, facilitating
timely interventions and preventative measures against conflagrations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Comparison of Deep Learning Architectures for Spacecraft Anomaly
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12864v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12864v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Lakey, Tim Schlippe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spacecraft operations are highly critical, demanding impeccable reliability
and safety. Ensuring the optimal performance of a spacecraft requires the early
detection and mitigation of anomalies, which could otherwise result in unit or
mission failures. With the advent of deep learning, a surge of interest has
been seen in leveraging these sophisticated algorithms for anomaly detection in
space operations. This study aims to compare the efficacy of various deep
learning architectures in detecting anomalies in spacecraft data. The deep
learning models under investigation include Convolutional Neural Networks
(CNNs), Recurrent Neural Networks (RNNs), Long Short-Term Memory (LSTM)
networks, and Transformer-based architectures. Each of these models was trained
and validated using a comprehensive dataset sourced from multiple spacecraft
missions, encompassing diverse operational scenarios and anomaly types. Initial
results indicate that while CNNs excel in identifying spatial patterns and may
be effective for some classes of spacecraft data, LSTMs and RNNs show a marked
proficiency in capturing temporal anomalies seen in time-series spacecraft
telemetry. The Transformer-based architectures, given their ability to focus on
both local and global contexts, have showcased promising results, especially in
scenarios where anomalies are subtle and span over longer durations.
Additionally, considerations such as computational efficiency, ease of
deployment, and real-time processing capabilities were evaluated. While CNNs
and LSTMs demonstrated a balance between accuracy and computational demands,
Transformer architectures, though highly accurate, require significant
computational resources. In conclusion, the choice of deep learning
architecture for spacecraft anomaly detection is highly contingent on the
nature of the data, the type of anomalies, and operational constraints.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted for IEEE Aeroconf 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ D-Cubed: Latent Diffusion Trajectory Optimisation for Dexterous
  Deformable Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12861v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12861v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun Yamada, Shaohong Zhong, Jack Collins, Ingmar Posner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mastering dexterous robotic manipulation of deformable objects is vital for
overcoming the limitations of parallel grippers in real-world applications.
Current trajectory optimisation approaches often struggle to solve such tasks
due to the large search space and the limited task information available from a
cost function. In this work, we propose D-Cubed, a novel trajectory
optimisation method using a latent diffusion model (LDM) trained from a
task-agnostic play dataset to solve dexterous deformable object manipulation
tasks. D-Cubed learns a skill-latent space that encodes short-horizon actions
in the play dataset using a VAE and trains a LDM to compose the skill latents
into a skill trajectory, representing a long-horizon action trajectory in the
dataset. To optimise a trajectory for a target task, we introduce a novel
gradient-free guided sampling method that employs the Cross-Entropy method
within the reverse diffusion process. In particular, D-Cubed samples a small
number of noisy skill trajectories using the LDM for exploration and evaluates
the trajectories in simulation. Then, D-Cubed selects the trajectory with the
lowest cost for the subsequent reverse process. This effectively explores
promising solution areas and optimises the sampled trajectories towards a
target task throughout the reverse diffusion process. Through empirical
evaluation on a public benchmark of dexterous deformable object manipulation
tasks, we demonstrate that D-Cubed outperforms traditional trajectory
optimisation and competitive baseline approaches by a significant margin. We
further demonstrate that trajectories found by D-Cubed readily transfer to a
real-world LEAP hand on a folding task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://applied-ai-lab.github.io/D-cubed/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Primal Methods for Variational Inequality Problems with Functional
  Constraints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12859v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12859v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liang Zhang, Niao He, Michael Muehlebach
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Constrained variational inequality problems are recognized for their broad
applications across various fields including machine learning and operations
research. First-order methods have emerged as the standard approach for solving
these problems due to their simplicity and scalability. However, they typically
rely on projection or linear minimization oracles to navigate the feasible set,
which becomes computationally expensive in practical scenarios featuring
multiple functional constraints. Existing efforts to tackle such functional
constrained variational inequality problems have centered on primal-dual
algorithms grounded in the Lagrangian function. These algorithms along with
their theoretical analysis often require the existence and prior knowledge of
the optimal Lagrange multipliers. In this work, we propose a simple primal
method, termed Constrained Gradient Method (CGM), for addressing functional
constrained variational inequality problems, without necessitating any
information on the optimal Lagrange multipliers. We establish a non-asymptotic
convergence analysis of the algorithm for variational inequality problems with
monotone operators under smooth constraints. Remarkably, our algorithms match
the complexity of projection-based methods in terms of operator queries for
both monotone and strongly monotone settings, while utilizing significantly
cheaper oracles based on quadratic programming. Furthermore, we provide several
numerical examples to evaluate the efficacy of our algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Equivariant Ensembles and Regularization for Reinforcement Learning in
  Map-based Path Planning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12856v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12856v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mirco Theile, Hongpeng Cao, Marco Caccamo, Alberto L. Sangiovanni-Vincentelli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In reinforcement learning (RL), exploiting environmental symmetries can
significantly enhance efficiency, robustness, and performance. However,
ensuring that the deep RL policy and value networks are respectively
equivariant and invariant to exploit these symmetries is a substantial
challenge. Related works try to design networks that are equivariant and
invariant by construction, limiting them to a very restricted library of
components, which in turn hampers the expressiveness of the networks. This
paper proposes a method to construct equivariant policies and invariant value
functions without specialized neural network components, which we term
equivariant ensembles. We further add a regularization term for adding
inductive bias during training. In a map-based path planning case study, we
show how equivariant ensembles and regularization benefit sample efficiency and
performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted for possible publication. A video can be found here:
  https://youtu.be/L6NOdvU7n7s</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Has Approximate Machine Unlearning been evaluated properly? From
  Auditing to Side Effects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12830v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12830v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cheng-Long Wang, Qi Li, Zihang Xiang, Di Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The growing concerns surrounding data privacy and security have underscored
the critical necessity for machine unlearning--aimed at fully removing data
lineage from machine learning models. MLaaS providers expect this to be their
ultimate safeguard for regulatory compliance. Despite its critical importance,
the pace at which privacy communities have been developing and implementing
strong methods to verify the effectiveness of machine unlearning has been
disappointingly slow, with this vital area often receiving insufficient focus.
This paper seeks to address this shortfall by introducing well-defined and
effective metrics for black-box unlearning auditing tasks. We transform the
auditing challenge into a question of non-membership inference and develop
efficient metrics for auditing. By relying exclusively on the original and
unlearned models--eliminating the need to train additional shadow models--our
approach simplifies the evaluation of unlearning at the individual data point
level. Utilizing these metrics, we conduct an in-depth analysis of current
approximate machine unlearning algorithms, identifying three key directions
where these approaches fall short: utility, resilience, and equity. Our aim is
that this work will greatly improve our understanding of approximate machine
unlearning methods, taking a significant stride towards converting the
theoretical right to data erasure into a auditable reality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RLingua: Improving Reinforcement Learning Sample Efficiency in Robotic
  Manipulations With Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06420v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06420v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liangliang Chen, Yutian Lei, Shiyu Jin, Ying Zhang, Liangjun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning (RL) has demonstrated its capability in solving
various tasks but is notorious for its low sample efficiency. In this paper, we
propose RLingua, a framework that can leverage the internal knowledge of large
language models (LLMs) to reduce the sample complexity of RL in robotic
manipulations. To this end, we first present a method for extracting the prior
knowledge of LLMs by prompt engineering so that a preliminary rule-based robot
controller for a specific task can be generated in a user-friendly manner.
Despite being imperfect, the LLM-generated robot controller is utilized to
produce action samples during rollouts with a decaying probability, thereby
improving RL's sample efficiency. We employ TD3, the widely-used RL baseline
method, and modify the actor loss to regularize the policy learning towards the
LLM-generated controller. RLingua also provides a novel method of improving the
imperfect LLM-generated robot controllers by RL. We demonstrate that RLingua
can significantly reduce the sample complexity of TD3 in four robot tasks of
panda_gym and achieve high success rates in 12 sampled sparsely rewarded robot
tasks in RLBench, where the standard TD3 fails. Additionally, We validated
RLingua's effectiveness in real-world robot experiments through Sim2Real,
demonstrating that the learned policies are effectively transferable to real
robot tasks. Further details about our work are available at our project
website https://rlingua.github.io.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DRESS: Instructing Large Vision-Language Models to Align and Interact
  with Humans via Natural Language Feedback <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10081v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10081v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yangyi Chen, Karan Sikka, Michael Cogswell, Heng Ji, Ajay Divakaran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present DRESS, a large vision language model (LVLM) that innovatively
exploits Natural Language feedback (NLF) from Large Language Models to enhance
its alignment and interactions by addressing two key limitations in the
state-of-the-art LVLMs. First, prior LVLMs generally rely only on the
instruction finetuning stage to enhance alignment with human preferences.
Without incorporating extra feedback, they are still prone to generate
unhelpful, hallucinated, or harmful responses. Second, while the visual
instruction tuning data is generally structured in a multi-turn dialogue
format, the connections and dependencies among consecutive conversational turns
are weak. This reduces the capacity for effective multi-turn interactions. To
tackle these, we propose a novel categorization of the NLF into two key types:
critique and refinement. The critique NLF identifies the strengths and
weaknesses of the responses and is used to align the LVLMs with human
preferences. The refinement NLF offers concrete suggestions for improvement and
is adopted to improve the interaction ability of the LVLMs-- which focuses on
LVLMs' ability to refine responses by incorporating feedback in multi-turn
interactions. To address the non-differentiable nature of NLF, we generalize
conditional reinforcement learning for training. Our experimental results
demonstrate that DRESS can generate more helpful (9.76%), honest (11.52%), and
harmless (21.03%) responses, and more effectively learn from feedback during
multi-turn interactions compared to SOTA LVMLs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024. The feedback datasets are released at:
  https://huggingface.co/datasets/YangyiYY/LVLM_NLF</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Training Process of Many Deep Networks Explores the Same
  Low-Dimensional Manifold 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.01604v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.01604v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jialin Mao, Itay Griniasty, Han Kheng Teoh, Rahul Ramesh, Rubing Yang, Mark K. Transtrum, James P. Sethna, Pratik Chaudhari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We develop information-geometric techniques to analyze the trajectories of
the predictions of deep networks during training. By examining the underlying
high-dimensional probabilistic models, we reveal that the training process
explores an effectively low-dimensional manifold. Networks with a wide range of
architectures, sizes, trained using different optimization methods,
regularization techniques, data augmentation techniques, and weight
initializations lie on the same manifold in the prediction space. We study the
details of this manifold to find that networks with different architectures
follow distinguishable trajectories but other factors have a minimal influence;
larger networks train along a similar manifold as that of smaller networks,
just faster; and networks initialized at very different parts of the prediction
space converge to the solution along a similar manifold.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluation of GlassNet for physics-informed machine learning of glass
  stability and glass-forming ability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10682v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10682v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sarah I. Allec, Xiaonan Lu, Daniel R. Cassar, Xuan T. Nguyen, Vinay I. Hegde, Thiruvillamalai Mahadevan, Miroslava Peterson, Jincheng Du, Brian J. Riley, John D. Vienna, James E. Saal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Glasses form the basis of many modern applications and also hold great
potential for future medical and environmental applications. However, their
structural complexity and large composition space make design and optimization
challenging for certain applications. Of particular importance for glass
processing is an estimate of a given composition's glass-forming ability (GFA).
However, there remain many open questions regarding the physical mechanisms of
glass formation, especially in oxide glasses. It is apparent that a proxy for
GFA would be highly useful in glass processing and design, but identifying such
a surrogate property has proven itself to be difficult. Here, we explore the
application of an open-source pre-trained NN model, GlassNet, that can predict
the characteristic temperatures necessary to compute glass stability (GS) and
assess the feasibility of using these physics-informed ML (PIML)-predicted GS
parameters to estimate GFA. In doing so, we track the uncertainties at each
step of the computation - from the original ML prediction errors, to the
compounding of errors during GS estimation, and finally to the final estimation
of GFA. While GlassNet exhibits reasonable accuracy on all individual
properties, we observe a large compounding of error in the combination of these
individual predictions for the prediction of GS, finding that random forest
models offer similar accuracy to GlassNet. We also breakdown the ML performance
on different glass families and find that the error in GS prediction is
correlated with the error in crystallization peak temperature prediction.
Lastly, we utilize this finding to assess the relationship between
top-performing GS parameters and GFA for two ternary glass systems: sodium
borosilicate and sodium iron phosphate glasses. We conclude that to obtain true
ML predictive capability of GFA, significantly more data needs to be collected.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Best of Both Worlds: Hybrid SNN-ANN Architecture for Event-based Optical
  Flow Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02960v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02960v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shubham Negi, Deepika Sharma, Adarsh Kumar Kosta, Kaushik Roy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the field of robotics, event-based cameras are emerging as a promising
low-power alternative to traditional frame-based cameras for capturing
high-speed motion and high dynamic range scenes. This is due to their sparse
and asynchronous event outputs. Spiking Neural Networks (SNNs) with their
asynchronous event-driven compute, show great potential for extracting the
spatio-temporal features from these event streams. In contrast, the standard
Analog Neural Networks (ANNs) fail to process event data effectively. However,
training SNNs is difficult due to additional trainable parameters (thresholds
and leaks), vanishing spikes at deeper layers, and a non-differentiable binary
activation function. Furthermore, an additional data structure, membrane
potential, responsible for keeping track of temporal information, must be
fetched and updated at every timestep in SNNs. To overcome these challenges, we
propose a novel SNN-ANN hybrid architecture that combines the strengths of
both. Specifically, we leverage the asynchronous compute capabilities of SNN
layers to effectively extract the input temporal information. Concurrently, the
ANN layers facilitate training and efficient hardware deployment on traditional
machine learning hardware such as GPUs. We provide extensive experimental
analysis for assigning each layer to be spiking or analog, leading to a network
configuration optimized for performance and ease of training. We evaluate our
hybrid architecture for optical flow estimation on DSEC-flow and Multi-Vehicle
Stereo Event-Camera (MVSEC) datasets. On the DSEC-flow dataset, the hybrid
SNN-ANN architecture achieves a 40% reduction in average endpoint error (AEE)
with 22% lower energy consumption compared to Full-SNN, and 48% lower AEE
compared to Full-ANN, while maintaining comparable energy usage.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Lossless ANN-SNN Conversion under Ultra-Low Latency with
  Dual-Phase Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.07473v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.07473v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziming Wang, Shuang Lian, Yuhao Zhang, Xiaoxin Cui, Rui Yan, Huajin Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spiking neural networks (SNNs) operating with asynchronous discrete events
show higher energy efficiency with sparse computation. A popular approach for
implementing deep SNNs is ANN-SNN conversion combining both efficient training
of ANNs and efficient inference of SNNs. However, the accuracy loss is usually
non-negligible, especially under a few time steps, which restricts the
applications of SNN on latency-sensitive edge devices greatly. In this paper,
we first identify that such performance degradation stems from the
misrepresentation of the negative or overflow residual membrane potential in
SNNs. Inspired by this, we decompose the conversion error into three parts:
quantization error, clipping error, and residual membrane potential
representation error. With such insights, we propose a two-stage conversion
algorithm to minimize those errors respectively. Besides, We show each stage
achieves significant performance gains in a complementary manner. By evaluating
on challenging datasets including CIFAR-10, CIFAR- 100 and ImageNet, the
proposed method demonstrates the state-of-the-art performance in terms of
accuracy, latency and energy preservation. Furthermore, our method is evaluated
using a more challenging object detection task, revealing notable gains in
regression performance under ultra-low latency when compared to existing
spike-based detection algorithms. Codes are available at
https://github.com/Windere/snn-cvt-dual-phase.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE Transactions on Neural Networks and Learning Systems
  (TNNLS)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fixed points of nonnegative neural networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2106.16239v8">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2106.16239v8.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tomasz J. Piotrowski, Renato L. G. Cavalcante, Mateusz Gabor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We use fixed point theory to analyze nonnegative neural networks, which we
define as neural networks that map nonnegative vectors to nonnegative vectors.
We first show that nonnegative neural networks with nonnegative weights and
biases can be recognized as monotonic and (weakly) scalable mappings within the
framework of nonlinear Perron-Frobenius theory. This fact enables us to provide
conditions for the existence of fixed points of nonnegative neural networks
having inputs and outputs of the same dimension, and these conditions are
weaker than those recently obtained using arguments in convex analysis.
Furthermore, we prove that the shape of the fixed point set of nonnegative
neural networks with nonnegative weights and biases is an interval, which under
mild conditions degenerates to a point. These results are then used to obtain
the existence of fixed points of more general nonnegative neural networks. From
a practical perspective, our results contribute to the understanding of the
behavior of autoencoders, and we also offer valuable mathematical machinery for
future developments in deep equilibrium models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>39 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Alignment Problem from a Deep Learning Perspective <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.00626v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.00626v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Richard Ngo, Lawrence Chan, Sören Mindermann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In coming years or decades, artificial general intelligence (AGI) may surpass
human capabilities at many critical tasks. We argue that, without substantial
effort to prevent it, AGIs could learn to pursue goals that are in conflict
(i.e. misaligned) with human interests. If trained like today's most capable
models, AGIs could learn to act deceptively to receive higher reward, learn
misaligned internally-represented goals which generalize beyond their
fine-tuning distributions, and pursue those goals using power-seeking
strategies. We review emerging evidence for these properties. AGIs with these
properties would be difficult to align and may appear aligned even when they
are not. Finally, we briefly outline how the deployment of misaligned AGIs
might irreversibly undermine human control over the world, and we review
research directions aimed at preventing this outcome.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Vertical Federated Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.07931v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.07931v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paul K. Mandal, Cole Leo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the popularization of AI solutions for image based problems, there has
been a growing concern for both data privacy and acquisition. In a large number
of cases, information is located on separate data silos and it can be difficult
for a developer to consolidate all of it in a fashion that is appropriate for
machine learning model development. Alongside this, a portion of these
localized data regions may not have access to a labelled ground truth. This
indicates that they have the capacity to reach conclusions numerically, but are
not able to assign classifications amid a lack of pertinent information. Such a
determination is often negligible, especially when attempting to develop image
based solutions that often necessitate this capability. With this being the
case, we propose an innovative vertical federated learning (VFL) model
architecture that can operate under this common set of conditions. This is the
first (and currently the only) implementation of a system that can work under
the constraints of a VFL environment and perform image segmentation while
maintaining nominal accuracies. We achieved this by utilizing an FCN that
boasts the ability to operate on federates that lack labelled data and
privately share the respective weights with a central server, that of which
hosts the necessary features for classification. Tests were conducted on the
CamVid dataset in order to determine the impact of heavy feature compression
required for the transfer of information between federates, as well as to reach
nominal conclusions about the overall performance metrics when working under
such constraints.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Defining Effective Engagement For Enhancing Cancer Patients' Well-being
  with Mobile Digital Behavior Change Interventions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12007v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12007v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aneta Lisowska, Szymon Wilk, Laura Locati, Mimma Rizzo, Lucia Sacchi, Silvana Quaglini, Matteo Terzaghi, Valentina Tibollo, Mor Peleg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Digital Behavior Change Interventions (DBCIs) are supporting development of
new health behaviors. Evaluating their effectiveness is crucial for their
improvement and understanding of success factors. However, comprehensive
guidance for developers, particularly in small-scale studies with ethical
constraints, is limited. Building on the CAPABLE project, this study aims to
define effective engagement with DBCIs for supporting cancer patients in
enhancing their quality of life. We identify metrics for measuring engagement,
explore the interest of both patients and clinicians in DBCIs, and propose
hypotheses for assessing the impact of DBCIs in such contexts. Our findings
suggest that clinician prescriptions significantly increase sustained
engagement with mobile DBCIs. In addition, while one weekly engagement with a
DBCI is sufficient to maintain well-being, transitioning from extrinsic to
intrinsic motivation may require a higher level of engagement.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rethinking of Encoder-based Warm-start Methods in Hyperparameter
  Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.04720v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.04720v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dawid Płudowski, Antoni Zajko, Anna Kozak, Katarzyna Woźnica
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Effectively representing heterogeneous tabular datasets for meta-learning
remains an open problem. Previous approaches rely on predefined meta-features,
for example, statistical measures or landmarkers. Encoder-based models, such as
Dataset2Vec, allow us to extract significant meta-features automatically
without human intervention. This research introduces a novel encoder-based
representation of tabular datasets implemented within the liltab package
available on GitHub https://github.com/azoz01/liltab. Our package is based on
an established model for heterogeneous tabular data proposed in [Tomoharu Iwata
and Atsutoshi Kumagai. Meta-learning from Tasks with Heterogeneous Attribute
Spaces. In Advances in Neural Information Processing Systems, 2020]. The
proposed approach employs a different model for encoding feature relationships,
generating alternative representations compared to existing methods like
Dataset2Vec. Both of them leverage the fundamental assumption of dataset
similarity learning. In this work, we evaluate Dataset2Vec and liltab on two
common meta-tasks - representing entire datasets and hyperparameter
optimization warm-start. However, validation on an independent metaMIMIC
dataset highlights the nuanced challenges in representation learning. We show
that general representations may not suffice for some meta-tasks where
requirements are not explicitly considered during extraction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bayesian Nonparametrics Meets Data-Driven Robust Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.15771v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.15771v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicola Bariletto, Nhat Ho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training machine learning and statistical models often involves optimizing a
data-driven risk criterion. The risk is usually computed with respect to the
empirical data distribution, but this may result in poor and unstable
out-of-sample performance due to distributional uncertainty. In the spirit of
distributionally robust optimization, we propose a novel robust criterion by
combining insights from Bayesian nonparametric (i.e., Dirichlet Process) theory
and recent decision-theoretic models of smooth ambiguity-averse preferences.
First, we highlight novel connections with standard regularized empirical risk
minimization techniques, among which Ridge and LASSO regressions. Then, we
theoretically demonstrate the existence of favorable finite-sample and
asymptotic statistical guarantees on the performance of the robust optimization
procedure. For practical implementation, we propose and study tractable
approximations of the criterion based on well-known Dirichlet Process
representations. We also show that the smoothness of the criterion naturally
leads to standard gradient-based numerical optimization. Finally, we provide
insights into the workings of our method by applying it to high-dimensional
sparse linear regression, binary classification, and robust location parameter
estimation tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Reducing Diagnostic Errors with Interpretable Risk Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.10109v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.10109v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Denis Jered McInerney, William Dickinson, Lucy C. Flynn, Andrea C. Young, Geoffrey S. Young, Jan-Willem van de Meent, Byron C. Wallace
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many diagnostic errors occur because clinicians cannot easily access relevant
information in patient Electronic Health Records (EHRs). In this work we
propose a method to use LLMs to identify pieces of evidence in patient EHR data
that indicate increased or decreased risk of specific diagnoses; our ultimate
aim is to increase access to evidence and reduce diagnostic errors. In
particular, we propose a Neural Additive Model to make predictions backed by
evidence with individualized risk estimates at time-points where clinicians are
still uncertain, aiming to specifically mitigate delays in diagnosis and errors
stemming from an incomplete differential. To train such a model, it is
necessary to infer temporally fine-grained retrospective labels of eventual
"true" diagnoses. We do so with LLMs, to ensure that the input text is from
before a confident diagnosis can be made. We use an LLM to retrieve an initial
pool of evidence, but then refine this set of evidence according to
correlations learned by the model. We conduct an in-depth evaluation of the
usefulness of our approach by simulating how it might be used by a clinician to
decide between a pre-defined list of differential diagnoses.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Conformal Monte Carlo Meta-learners for Predictive Inference of
  Individual Treatment Effects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.04906v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.04906v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jef Jonkers, Jarne Verhaeghe, Glenn Van Wallendael, Luc Duchateau, Sofie Van Hoecke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge of the effect of interventions, called the treatment effect, is
paramount for decision-making. Approaches to estimating this treatment effect,
e.g. by using Conditional Average Treatment Effect (CATE) estimators, often
only provide a point estimate of this treatment effect, while additional
uncertainty quantification is frequently desired instead. Therefore, we present
a novel method, the Conformal Monte Carlo (CMC) meta-learners, leveraging
conformal predictive systems, Monte Carlo sampling, and CATE meta-learners, to
instead produce a predictive distribution usable in individualized
decision-making. Furthermore, we show how specific assumptions on the noise
distribution of the outcome heavily affect these uncertainty predictions.
Nonetheless, the CMC framework shows strong experimental coverage while
retaining small interval widths to provide estimates of the true individual
treatment effect.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Yet Another ICU Benchmark: A Flexible Multi-Center Framework for
  Clinical ML <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05109v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05109v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Robin van de Water, Hendrik Schmidt, Paul Elbers, Patrick Thoral, Bert Arnrich, Patrick Rockenschaub
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical applications of machine learning (ML) have experienced a surge in
popularity in recent years. The intensive care unit (ICU) is a natural habitat
for ML given the abundance of available data from electronic health records.
Models have been proposed to address numerous ICU prediction tasks like the
early detection of complications. While authors frequently report
state-of-the-art performance, it is challenging to verify claims of
superiority. Datasets and code are not always published, and cohort
definitions, preprocessing pipelines, and training setups are difficult to
reproduce. This work introduces Yet Another ICU Benchmark (YAIB), a modular
framework that allows researchers to define reproducible and comparable
clinical ML experiments; we offer an end-to-end solution from cohort definition
to model evaluation. The framework natively supports most open-access ICU
datasets (MIMIC III/IV, eICU, HiRID, AUMCdb) and is easily adaptable to future
ICU datasets. Combined with a transparent preprocessing pipeline and extensible
training code for multiple ML and deep learning models, YAIB enables unified
model development. Our benchmark comes with five predefined established
prediction tasks (mortality, acute kidney injury, sepsis, kidney function, and
length of stay) developed in collaboration with clinicians. Adding further
tasks is straightforward by design. Using YAIB, we demonstrate that the choice
of dataset, cohort definition, and preprocessing have a major impact on the
prediction performance - often more so than model class - indicating an urgent
need for YAIB as a holistic benchmarking tool. We provide our work to the
clinical ML community to accelerate method development and enable real-world
clinical implementations. Software Repository:
https://github.com/rvandewater/YAIB.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Main benchmark: https://github.com/rvandewater/YAIB, Cohort
  generation: https://github.com/rvandewater/YAIB-cohorts, Models:
  https://github.com/rvandewater/YAIB-models. To be published in ICLR 2024
  proceedings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MM1: Methods, Analysis & Insights from Multimodal LLM <span class="highlight-title">Pre-train</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09611v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09611v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brandon McKinzie, Zhe Gan, Jean-Philippe Fauconnier, Sam Dodge, Bowen Zhang, Philipp Dufter, Dhruti Shah, Xianzhi Du, Futang Peng, Floris Weers, Anton Belyi, Haotian Zhang, Karanjeet Singh, Doug Kang, Ankur Jain, Hongyu Hè, Max Schwarzer, Tom Gunter, Xiang Kong, Aonan Zhang, Jianyu Wang, Chong Wang, Nan Du, Tao Lei, Sam Wiseman, Mark Lee, Zirui Wang, Ruoming Pang, Peter Grasch, Alexander Toshev, Yinfei Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we discuss building performant Multimodal Large Language Models
(MLLMs). In particular, we study the importance of various architecture
components and data choices. Through careful and comprehensive ablations of the
image encoder, the vision language connector, and various pre-training data
choices, we identified several crucial design lessons. For example, we
demonstrate that for large-scale multimodal pre-training using a careful mix of
image-caption, interleaved image-text, and text-only data is crucial for
achieving state-of-the-art (SOTA) few-shot results across multiple benchmarks,
compared to other published pre-training results. Further, we show that the
image encoder together with image resolution and the image token count has
substantial impact, while the vision-language connector design is of
comparatively negligible importance. By scaling up the presented recipe, we
build MM1, a family of multimodal models up to 30B parameters, including both
dense models and mixture-of-experts (MoE) variants, that are SOTA in
pre-training metrics and achieve competitive performance after supervised
fine-tuning on a range of established multimodal benchmarks. Thanks to
large-scale pre-training, MM1 enjoys appealing properties such as enhanced
in-context learning, and multi-image reasoning, enabling few-shot
chain-of-thought prompting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cross-Domain Few-Shot Object Detection via Enhanced Open-Set Object
  Detector 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.03094v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.03094v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuqian Fu, Yu Wang, Yixuan Pan, Lian Huai, Xingyu Qiu, Zeyu Shangguan, Tong Liu, Yanwei Fu, Luc Van Gool, Xingqun Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper studies the challenging cross-domain few-shot object detection
(CD-FSOD), aiming to develop an accurate object detector for novel domains with
minimal labeled examples. While transformer-based open-set detectors, such as
DE-ViT, show promise in traditional few-shot object detection, their
generalization to CD-FSOD remains unclear: 1) can such open-set detection
methods easily generalize to CD-FSOD? 2) If not, how can models be enhanced
when facing huge domain gaps? To answer the first question, we employ measures
including style, inter-class variance (ICV), and indefinable boundaries (IB) to
understand the domain gap. Based on these measures, we establish a new
benchmark named CD-FSOD to evaluate object detection methods, revealing that
most of the current approaches fail to generalize across domains. Technically,
we observe that the performance decline is associated with our proposed
measures: style, ICV, and IB. Consequently, we propose several novel modules to
address these issues. First, the learnable instance features align initial
fixed instances with target categories, enhancing feature distinctiveness.
Second, the instance reweighting module assigns higher importance to
high-quality instances with slight IB. Third, the domain prompter encourages
features resilient to different styles by synthesizing imaginary domains
without altering semantic contents. These techniques collectively contribute to
the development of the Cross-Domain Vision Transformer for CD-FSOD (CD-ViTO),
significantly improving upon the base DE-ViT. Experimental results validate the
efficacy of our model. All datasets, codes, and models will be released to the
community.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Attacks, Defenses and Evaluations for LLM Conversation Safety: A <span class="highlight-title">Survey</span> <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.09283v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.09283v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhichen Dong, Zhanhui Zhou, Chao Yang, Jing Shao, Yu Qiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are now commonplace in conversation
applications. However, their risks of misuse for generating harmful responses
have raised serious societal concerns and spurred recent research on LLM
conversation safety. Therefore, in this survey, we provide a comprehensive
overview of recent studies, covering three critical aspects of LLM conversation
safety: attacks, defenses, and evaluations. Our goal is to provide a structured
summary that enhances understanding of LLM conversation safety and encourages
further investigation into this important subject. For easy reference, we have
categorized all the studies mentioned in this survey according to our taxonomy,
available at: https://github.com/niconi19/LLM-conversation-safety.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ JORA: JAX Tensor-Parallel LoRA Library for Retrieval Augmented
  Fine-Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11366v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11366v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anique Tahir, Lu Cheng, Huan Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The scaling of Large Language Models (LLMs) for retrieval-based tasks,
particularly in Retrieval Augmented Generation (RAG), faces significant memory
constraints, especially when fine-tuning extensive prompt sequences. Current
open-source libraries support full-model inference and fine-tuning across
multiple GPUs but fall short of accommodating the efficient parameter
distribution required for retrieved context. Addressing this gap, we introduce
a novel framework for PEFT-compatible fine-tuning of Llama-2 models, leveraging
distributed training. Our framework uniquely utilizes JAX's just-in-time (JIT)
compilation and tensor-sharding for efficient resource management, thereby
enabling accelerated fine-tuning with reduced memory requirements. This
advancement significantly improves the scalability and feasibility of
fine-tuning LLMs for complex RAG applications, even on systems with limited GPU
resources. Our experiments show more than 12x improvement in runtime compared
to Hugging Face/DeepSpeed implementation with four GPUs while consuming less
than half the VRAM per GPU.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Guiding Masked Representation Learning to Capture Spatio-Temporal
  Relationship of Electrocardiogram <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.09450v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.09450v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yeongyeon Na, Minje Park, Yunwon Tae, Sunghoon Joo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Electrocardiograms (ECG) are widely employed as a diagnostic tool for
monitoring electrical signals originating from a heart. Recent machine learning
research efforts have focused on the application of screening various diseases
using ECG signals. However, adapting to the application of screening disease is
challenging in that labeled ECG data are limited. Achieving general
representation through self-supervised learning (SSL) is a well-known approach
to overcome the scarcity of labeled data; however, a naive application of SSL
to ECG data, without considering the spatial-temporal relationships inherent in
ECG signals, may yield suboptimal results. In this paper, we introduce ST-MEM
(Spatio-Temporal Masked Electrocardiogram Modeling), designed to learn
spatio-temporal features by reconstructing masked 12-lead ECG data. ST-MEM
outperforms other SSL baseline methods in various experimental settings for
arrhythmia classification tasks. Moreover, we demonstrate that ST-MEM is
adaptable to various lead combinations. Through quantitative and qualitative
analysis, we show a spatio-temporal relationship within ECG data. Our code is
available at https://github.com/bakqui/ST-MEM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024. The first three authors contribute equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Ada-NAV: Adaptive Trajectory Length-Based Sample Efficient Policy
  Learning for Robotic Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.06192v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.06192v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bhrij Patel, Kasun Weerakoon, Wesley A. Suttle, Alec Koppel, Brian M. Sadler, Tianyi Zhou, Amrit Singh Bedi, Dinesh Manocha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Trajectory length stands as a crucial hyperparameter within reinforcement
learning (RL) algorithms, significantly contributing to the sample inefficiency
in robotics applications. Motivated by the pivotal role trajectory length plays
in the training process, we introduce Ada-NAV, a novel adaptive trajectory
length scheme designed to enhance the training sample efficiency of RL
algorithms in robotic navigation tasks. Unlike traditional approaches that
treat trajectory length as a fixed hyperparameter, we propose to dynamically
adjust it based on the entropy of the underlying navigation policy.
Interestingly, Ada-NAV can be applied to both existing on-policy and off-policy
RL methods, which we demonstrate by empirically validating its efficacy on
three popular RL methods: REINFORCE, Proximal Policy Optimization (PPO), and
Soft Actor-Critic (SAC). We demonstrate through simulated and real-world
robotic experiments that Ada-NAV outperforms conventional methods that employ
constant or randomly sampled trajectory lengths. Specifically, for a fixed
sample budget, Ada-NAV achieves an 18\% increase in navigation success rate, a
20-38\% reduction in navigation path length, and a 9.32\% decrease in elevation
costs. Furthermore, we showcase the versatility of Ada-NAV by integrating it
with the Clearpath Husky robot, illustrating its applicability in complex
outdoor environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 9 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Impossible Distillation: from Low-Quality Model to High-Quality <span class="highlight-title">Dataset</span>
  & Model for Summarization and Paraphrasing <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.16635v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.16635v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaehun Jung, Peter West, Liwei Jiang, Faeze Brahman, Ximing Lu, Jillian Fisher, Taylor Sorensen, Yejin Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Impossible Distillation, a novel framework for paraphrasing and
sentence summarization, that distills a high-quality dataset and model from a
low-quality teacher that itself cannot perform these tasks. Unlike prior works
that rely on an extreme-scale teacher model (e.g., GPT3) or task-specific
architecture, we hypothesize and verify the paraphrastic proximity intrinsic to
pre-trained LMs (e.g., GPT2), where paraphrases occupy a proximal subspace in
the LM distribution. By identifying and distilling generations from these
subspaces, Impossible Distillation produces a high-quality dataset and model
even from GPT2-scale LMs. We evaluate our method on multiple benchmarks
spanning unconstrained / syntax-controlled paraphrase generation and sentence
summarization. Our model with 770M parameters consistently outperforms strong
baselines, including models distilled from ChatGPT, and sometimes, even ChatGPT
itself. Also, we find that our distilled dataset from 1.5B LMs exhibits higher
diversity and fidelity than up to 13 times larger datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hypergraph-MLP: Learning on Hypergraphs without Message Passing <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.09778v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.09778v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bohan Tang, Siheng Chen, Xiaowen Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hypergraphs are vital in modelling data with higher-order relations
containing more than two entities, gaining prominence in machine learning and
signal processing. Many hypergraph neural networks leverage message passing
over hypergraph structures to enhance node representation learning, yielding
impressive performances in tasks like hypergraph node classification. However,
these message-passing-based models face several challenges, including
oversmoothing as well as high latency and sensitivity to structural
perturbations at inference time. To tackle those challenges, we propose an
alternative approach where we integrate the information about hypergraph
structures into training supervision without explicit message passing, thus
also removing the reliance on it at inference. Specifically, we introduce
Hypergraph-MLP, a novel learning framework for hypergraph-structured data,
where the learning model is a straightforward multilayer perceptron (MLP)
supervised by a loss function based on a notion of signal smoothness on
hypergraphs. Experiments on hypergraph node classification tasks demonstrate
that Hypergraph-MLP achieves competitive performance compared to existing
baselines, and is considerably faster and more robust against structural
perturbations at inference.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Behave-XAI: Deep Explainable Learning of Behavioral Representational
  Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.00016v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.00016v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rossi Kamal, Zuzana Kubincova
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  According to the latest trend of artificial intelligence, AI-systems needs to
clarify regarding general,specific decisions,services provided by it. Only
consumer is satisfied, with explanation , for example, why any classification
result is the outcome of any given time. This actually motivates us using
explainable or human understandable AI for a behavioral mining scenario, where
users engagement on digital platform is determined from context, such as
emotion, activity, weather, etc. However, the output of AI-system is not always
systematically correct, and often systematically correct, but apparently
not-perfect and thereby creating confusions, such as, why the decision is
given? What is the reason underneath? In this context, we first formulate the
behavioral mining problem in deep convolutional neural network architecture.
Eventually, we apply a recursive neural network due to the presence of
time-series data from users physiological and environmental sensor-readings.
Once the model is developed, explanations are presented with the advent of XAI
models in front of users. This critical step involves extensive trial with
users preference on explanations over conventional AI, judgement of credibility
of explanation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This submission has been withdrawn by arXiv administrators as the
  second author was added without their knowledge or consent</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Recurrent Learning Through Long Short Term Memory and TOPSIS 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.00693v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.00693v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rossi Kamal, Zuzana Kubincova, Mosaddek Hossain Kamal, Upama Kabir
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Enterprise resource planning (ERP) software brings resources, data together
to keep software-flow within business processes in a company. However, cloud
computing's cheap, easy and quick management promise pushes business-owners for
a transition from monolithic to a data-center/cloud based ERP. Since cloud-ERP
development involves a cyclic process, namely planning, implementing, testing
and upgrading, its adoption is realized as a deep recurrent neural network
problem. Eventually, a classification algorithm based on long short term memory
(LSTM) and TOPSIS is proposed to identify and rank, respectively, adoption
features. Our theoretical model is validated over a reference model by
articulating key players, services, architecture, functionalities. Qualitative
survey is conducted among users by considering technology, innovation and
resistance issues, to formulate hypotheses on key adoption factors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This submission has been withdrawn by arXiv administrators as the
  second author was added without their knowledge or consent</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mitigating Gradient Bias in Multi-objective Learning: A Provably
  Convergent Stochastic Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.12624v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.12624v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Heshan Fernando, Han Shen, Miao Liu, Subhajit Chaudhury, Keerthiram Murugesan, Tianyi Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning problems with multiple objective functions appear either in
learning with multiple criteria where learning has to make a trade-off between
multiple performance metrics such as fairness, safety and accuracy; or, in
multi-task learning where multiple tasks are optimized jointly, sharing
inductive bias between them. This problems are often tackled by the
multi-objective optimization framework. However, existing stochastic
multi-objective gradient methods and its variants (e.g., MGDA, PCGrad, CAGrad,
etc.) all adopt a biased noisy gradient direction, which leads to degraded
empirical performance. To this end, we develop a stochastic Multi-objective
gradient Correction (MoCo) method for multi-objective optimization. The unique
feature of our method is that it can guarantee convergence without increasing
the batch size even in the non-convex setting. Simulations on multi-task
supervised and reinforcement learning demonstrate the effectiveness of our
method relative to state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Changed hyper-parameter choice which affects some of the convergence
  rate results in the paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GeoShapley: A Game Theory Approach to Measuring Spatial Effects in
  Machine Learning Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.03675v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.03675v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziqi Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces GeoShapley, a game theory approach to measuring spatial
effects in machine learning models. GeoShapley extends the Nobel Prize-winning
Shapley value framework in game theory by conceptualizing location as a player
in a model prediction game, which enables the quantification of the importance
of location and the synergies between location and other features in a model.
GeoShapley is a model-agnostic approach and can be applied to statistical or
black-box machine learning models in various structures. The interpretation of
GeoShapley is directly linked with spatially varying coefficient models for
explaining spatial effects and additive models for explaining non-spatial
effects. Using simulated data, GeoShapley values are validated against known
data-generating processes and are used for cross-comparison of seven
statistical and machine learning models. An empirical example of house price
modeling is used to illustrate GeoShapley's utility and interpretation with
real world data. The method is available as an open-source Python package named
geoshapley.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages, 10 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Gaussian Process Neural Additive Models <span class="chip">AAAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.12518v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.12518v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Zhang, Brian Barr, John Paisley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks have revolutionized many fields, but their black-box
nature also occasionally prevents their wider adoption in fields such as
healthcare and finance, where interpretable and explainable models are
required. The recent development of Neural Additive Models (NAMs) is a
significant step in the direction of interpretable deep learning for tabular
datasets. In this paper, we propose a new subclass of NAMs that use a
single-layer neural network construction of the Gaussian process via random
Fourier features, which we call Gaussian Process Neural Additive Models
(GP-NAM). GP-NAMs have the advantage of a convex objective function and number
of trainable parameters that grows linearly with feature dimensionality. It
suffers no loss in performance compared to deeper NAM approaches because GPs
are well-suited for learning complex non-parametric univariate functions. We
demonstrate the performance of GP-NAM on several tabular datasets, showing that
it achieves comparable or better performance in both classification and
regression tasks with a large reduction in the number of parameters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Appears at AAAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Oracle-Efficient Smoothed Online Learning for Piecewise Continuous
  Decision Making 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.05430v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.05430v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adam Block, Alexander Rakhlin, Max Simchowitz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Smoothed online learning has emerged as a popular framework to mitigate the
substantial loss in statistical and computational complexity that arises when
one moves from classical to adversarial learning. Unfortunately, for some
spaces, it has been shown that efficient algorithms suffer an exponentially
worse regret than that which is minimax optimal, even when the learner has
access to an optimization oracle over the space. To mitigate that exponential
dependence, this work introduces a new notion of complexity, the generalized
bracketing numbers, which marries constraints on the adversary to the size of
the space, and shows that an instantiation of Follow-the-Perturbed-Leader can
attain low regret with the number of calls to the optimization oracle scaling
optimally with respect to average regret. We then instantiate our bounds in
several problems of interest, including online prediction and planning of
piecewise continuous functions, which has many applications in fields as
diverse as econometrics and robotics.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Science and Game Theory
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Freshness-aware Block Propagation Optimization in 6G-based Web 3.0: An
  Evolutionary Game Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12807v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12807v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinbo Wen, Jiawen Kang, Zehui Xiong, Hongyang Du, Zhaohui Yang, Dusit Niyato, Meng Shen, Yutao Jiao, Yang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Driven by the aspiration to establish a decentralized digital economy, Web
3.0 is emerging as the fundamental technology for digital transformation.
Incorporating the promising sixth-generation (6G) technology with large
bandwidth and space-air-ground integrated coverage, 6G-based Web 3.0 holds
great potential in empowering users with enhanced data control and facilitating
secure peer-to-peer transactions, especially in consumer electronics, through
the utilization of blockchain technologies. However, 6G-based Web 3.0 is still
in its infancy, such as ensuring block freshness and optimizing block
propagation to improve blockchain performance. In this paper, we develop a
freshness-aware block propagation optimization framework for 6G-based Web 3.0.
We first propose a novel metric called Age of Block Information (AoBI) based on
the concept of age of information to quantify block freshness. To make block
propagation optimization tractable, we classify miners into five different
states and propose a block propagation model for public blockchains inspired by
epidemic models. Moreover, considering that the miners are bounded rational, we
propose an incentive mechanism based on the evolutionary game for block
propagation to improve block propagation efficiency. Numerical results
demonstrate that compared with other block propagation mechanisms, the proposed
scheme has a higher block forwarding probability, which improves block
propagation efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uber Stable: Formulating the Rideshare System as a Stable Matching
  Problem 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13083v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13083v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rhea Acharya, Jessica Chen, Helen Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Peer-to-peer ride-sharing platforms like Uber, Lyft, and DiDi have
revolutionized the transportation industry and labor market. At its essence,
these systems tackle the bipartite matching problem between two populations:
riders and drivers. This research paper comprises two main components: an
initial literature review of existing ride-sharing platforms and efforts to
enhance driver satisfaction, and the development of a novel algorithm
implemented through simulation testing to allow us to make our own
observations. The core algorithm utilized is the Gale-Shapley deferred
acceptance algorithm, applied to a static matching problem over multiple time
periods. In this simulation, we construct a preference-aware task assignment
model, considering both overall revenue maximization and individual preference
satisfaction. Specifically, the algorithm design incorporates factors such as
passenger willingness-to-pay, driver preferences, and location attractiveness,
with an overarching goal of achieving equitable income distribution for drivers
while maintaining overall system efficiency.
  Through simulation, the paper compares the performance of the proposed
algorithm with random matching and closest neighbor algorithms, looking at
metrics such as total revenue, revenue per ride, and standard deviation to
identify trends and impacts of shifting priorities. Additionally, the DA
algorithm is compared to the Boston algorithm, and the paper explores the
effect of prioritizing proximity to passengers versus distance from city
center. Ultimately, the research underscores the importance of continued
exploration in areas such as dynamic pricing models and additional modeling for
unconventional driving times to further enhance the findings on the
effectiveness and fairness of ride-sharing platforms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ One-Shot Strategic Classification Under Unknown Costs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.02761v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.02761v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elan Rosenfeld, Nir Rosenfeld
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The goal of strategic classification is to learn decision rules which are
robust to strategic input manipulation. Earlier works assume that these
responses are known; while some recent works handle unknown responses, they
exclusively study online settings with repeated model deployments. But there
are many domains$\unicode{x2014}$particularly in public policy, a common
motivating use case$\unicode{x2014}$where multiple deployments are infeasible,
or where even one bad round is unacceptable. To address this gap, we initiate
the formal study of one-shot strategic classification under unknown responses,
which requires committing to a single classifier once. Focusing on uncertainty
in the users' cost function, we begin by proving that for a broad class of
costs, even a small mis-estimation of the true cost can entail trivial accuracy
in the worst case. In light of this, we frame the task as a minimax problem,
with the goal of identifying the classifier with the smallest worst-case risk
over an uncertainty set of possible costs. We design efficient algorithms for
both the full-batch and stochastic settings, which we prove converge (offline)
to the minimax solution at the dimension-independent rate of
$\tilde{\mathcal{O}}(T^{-\frac{1}{2}})$. Our theoretical analysis reveals
important structure stemming from strategic responses, particularly the value
of dual norm regularization with respect to the cost function.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Fixed a bug in Algorithm 1, significantly strengthened Theorem 4.2,
  and added Figure 1 to help visualize the lower bound in Theorem 3.2</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fundamental Limits of Throughput and Availability: Applications to
  prophet inequalities & transaction fee mechanism design 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.19292v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.19292v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aadityan Ganesh, Jason Hartline, Atanu R Sinha, Matthew vonAllmen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper studies the fundamental limits of availability and throughput for
independent and heterogeneous demands of a limited resource. Availability is
the probability that the demands are below the capacity of the resource.
Throughput is the expected fraction of the resource that is utilized by the
demands. We offer a concentration inequality generator that gives lower bounds
on feasible availability and throughput pairs with a given capacity and
independent but not necessarily identical distributions of up-to-unit demands.
We show that availability and throughput cannot both be poor. These bounds are
analogous to tail inequalities on sums of independent random variables, but
hold throughout the support of the demand distribution. This analysis gives
analytically tractable bounds supporting the unit-demand characterization of
Chawla, Devanur, and Lykouris (2023) and generalizes to up-to-unit demands. Our
bounds also provide an approach towards improved multi-unit prophet
inequalities (Hajiaghayi, Kleinberg, and Sandholm, 2007). They have
applications to transaction fee mechanism design (for blockchains) where high
availability limits the probability of profitable user-miner coalitions (Chung
and Shi, 2023).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>34 pages, 7 figures; updated author information to include
  institutions and email addresses</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Game Theory with Simulation of Other Players 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.11261v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.11261v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vojtech Kovarik, Caspar Oesterheld, Vincent Conitzer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Game-theoretic interactions with AI agents could differ from traditional
human-human interactions in various ways. One such difference is that it may be
possible to simulate an AI agent (for example because its source code is
known), which allows others to accurately predict the agent's actions. This
could lower the bar for trust and cooperation. In this paper, we formalize
games in which one player can simulate another at a cost. We first derive some
basic properties of such games and then prove a number of results for them,
including: (1) introducing simulation into generic-payoff normal-form games
makes them easier to solve; (2) if the only obstacle to cooperation is a lack
of trust in the possibly-simulated agent, simulation enables equilibria that
improve the outcome for both agents; and however (3) there are settings where
introducing simulation results in strictly worse outcomes for both players.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The latest version fixes some typos in the proof of Theorem 5</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Theory
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Wideband Modeling and Beamforming for Beyond Diagonal Reconfigurable
  Intelligent Surfaces 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12893v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12893v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongyu Li, Matteo Nerini, Shanpu Shen, Bruno Clerckx
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work studies the wideband modeling and beamforming design of beyond
diagonal reconfigurable intelligent surface (BD-RIS), which generalizes and
goes beyond conventional RIS with diagonal phase shift matrices to achieve
enhanced channel gain. Specifically, we investigate the response of BD-RIS in
wideband systems by going back to its hardware circuit realizations. We propose
a novel wideband model which has simple expressions while capturing the
response variations of BD-RIS for signals with different frequencies. With this
wideband model, we propose a BD-RIS design algorithm for an orthogonal
frequency division multiplexing system to maximize the average rate over all
subcarriers. Finally, we provide simulation results to evaluate the performance
of the proposed design and show the importance of wideband modeling for BD-RIS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE for publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Knowledge and Data Dual-Driven Channel Estimation and Feedback for
  Ultra-Massive MIMO Systems under Hybrid Field Beam Squint Effect 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12813v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12813v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kuiyu Wang, Zhen Gao, Sheng Chen, Boyu Ning, Gaojie Chen, Yu Su, Zhaocheng Wang, H. Vincent Poor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Acquiring accurate channel state information (CSI) at an access point (AP) is
challenging for wideband millimeter wave (mmWave) ultra-massive multiple-input
and multiple-output (UMMIMO) systems, due to the high-dimensional channel
matrices, hybrid near- and far- field channel feature, beam squint effects, and
imperfect hardware constraints, such as low-resolution analog-to-digital
converters, and in-phase and quadrature imbalance. To overcome these
challenges, this paper proposes an efficient downlink channel estimation (CE)
and CSI feedback approach based on knowledge and data dual-driven deep learning
(DL) networks. Specifically, we first propose a data-driven residual neural
network de-quantizer (ResNet-DQ) to pre-process the received pilot signals at
user equipment (UEs), where the noise and distortion brought by imperfect
hardware can be mitigated. A knowledge-driven generalized multiple measurement
vector learned approximate message passing (GMMV-LAMP) network is then
developed to jointly estimate the channels by exploiting the approximately same
physical angle shared by different subcarriers. In particular, two wideband
redundant dictionaries (WRDs) are proposed such that the measurement matrices
of the GMMV-LAMP network can accommodate the far-field and near-field beam
squint effect, respectively. Finally, we propose an encoder at the UEs and a
decoder at the AP by a data-driven CSI residual network (CSI-ResNet) to
compress the CSI matrix into a low-dimensional quantized bit vector for
feedback, thereby reducing the feedback overhead substantially. Simulation
results show that the proposed knowledge and data dual-driven approach
outperforms conventional downlink CE and CSI feedback methods, especially in
the case of low signal-to-noise ratios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 22 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Machine Learning of the Prime Distribution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12588v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12588v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Kolpakov, Aidan Rocke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the present work we use maximum entropy methods to derive several theorems
in probabilistic number theory, including a version of the Hardy-Ramanujan
Theorem. We also provide a theoretical argument explaining the experimental
observations of Y.-H. He about the learnability of primes, and posit that the
Erd\H{o}s-Kac law would very unlikely be discovered by current machine learning
techniques. Numerical experiments that we perform corroborate our theoretical
findings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages; parts of arXiv:2308.10817 reworked and amended; author's
  draft; accepted in PLOS ONE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self-Orthogonal Codes from Vectorial Dual-Bent Functions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12578v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12578v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaxin Wang, Yadi Wei, Fang-Wei Fu, Juan Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-orthogonal codes are a significant class of linear codes in coding
theory and have attracted a lot of attention. In \cite{HLL2023Te,LH2023Se},
$p$-ary self-orthogonal codes were constructed by using $p$-ary weakly regular
bent functions, where $p$ is an odd prime. In \cite{WH2023Se}, two classes of
non-degenerate quadratic forms were used to construct $q$-ary self-orthogonal
codes, where $q$ is a power of a prime. In this paper, we construct new
families of $q$-ary self-orthogonal codes using vectorial dual-bent functions.
Some classes of at least almost optimal linear codes are obtained from the dual
codes of the constructed self-orthogonal codes. In some cases, we completely
determine the weight distributions of the constructed self-orthogonal codes.
From the view of vectorial dual-bent functions, we illustrate that the works on
constructing self-orthogonal codes from $p$-ary weakly regular bent functions
\cite{HLL2023Te,LH2023Se} and non-degenerate quadratic forms with $q$ being odd
\cite{WH2023Se} can be obtained by our results. We partially answer an open
problem on determining the weight distribution of a class of self-orthogonal
codes given in \cite{LH2023Se}. As applications, we construct new infinite
families of at least almost optimal $q$-ary linear complementary dual codes
(for short, LCD codes) and quantum codes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimizing Reconfigurable Antenna MIMO Systems with Coherent Ising
  Machines 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12571v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12571v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ioannis Krikidis, Abhishek Kumar Singh, Kyle Jamieson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reconfigurable antenna multiple-input multiple-output (MIMO) is a promising
technology for upcoming 6G communication systems. In this paper, we deal with
the problem of configuration selection for reconfigurable antenna MIMO by
leveraging Coherent Ising Machines (CIMs). By adopting the CIM as a heuristic
solver for the Ising problem, the optimal antenna configuration that maximizes
the received signal-to-noise ratio is investigated. A mathematical framework
that converts the selection problem into a CIM-compatible unconstrained
quadratic formulation is presented. Numerical studies show that the proposed
CIM-based design outperforms classical counterparts and achieves near-optimal
performance (similar to exponentially complex exhaustive searching) while
ensuring polynomial complexity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimal Single-Shot Decoding of Quantum Codes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.18138v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.18138v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aldo Cumitini, Stefano Tinelli, Balázs Matuz, Francisco Lázaro, Luca Barletta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We discuss single-shot decoding of quantum Calderbank-Shor-Steane codes with
faulty syndrome measurements. We state the problem as a joint source-channel
coding problem. By adding redundant rows to the code's parity-check matrix we
obtain an additional syndrome error correcting code which addresses faulty
syndrome measurements. Thereby, the redundant rows are chosen to obtain good
syndrome error correcting capabilities while keeping the stabilizer weights
low. Optimal joint decoding rules are derived which, though too complex for
general codes, can be evaluated for short quantum codes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Complex-Valued Neural Network based Federated Learning for Multi-user
  Indoor Positioning Performance Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.00665v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.00665v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanzhi Yu, Yuchen Liu, Mingzhe Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this article, the use of channel state information (CSI) for indoor
positioning is studied. In the considered model, a server equipped with several
antennas sends pilot signals to users, while each user uses the received pilot
signals to estimate channel states for user positioning. To this end, we
formulate the positioning problem as an optimization problem aiming to minimize
the gap between the estimated positions and the ground truth positions of
users. To solve this problem, we design a complex-valued neural network (CVNN)
model based federated learning (FL) algorithm. Compared to standard real-valued
centralized machine learning (ML) methods, our proposed algorithm has two main
advantages. First, our proposed algorithm can directly process complex-valued
CSI data without data transformation. Second, our proposed algorithm is a
distributed ML method that does not require users to send their CSI data to the
server. Since the output of our proposed algorithm is complex-valued which
consists of the real and imaginary parts, we study the use of the CVNN to
implement two learning tasks. First, the proposed algorithm directly outputs
the estimated positions of a user. Here, the real and imaginary parts of an
output neuron represent the 2D coordinates of the user. Second, the proposed
method can output two CSI features (i.e., line-of-sight/non-line-of-sight
transmission link classification and time of arrival (TOA) prediction) which
can be used in traditional positioning algorithms. Simulation results
demonstrate that our designed CVNN based FL can reduce the mean positioning
error between the estimated position and the actual position by up to 36%,
compared to a RVNN based FL which requires to transform CSI data into
real-valued data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Fast and Provable Algorithm for Sparse Phase Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.02046v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.02046v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jian-Feng Cai, Yu Long, Ruixue Wen, Jiaxi Ying
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the sparse phase retrieval problem, which seeks to recover a sparse
signal from a limited set of magnitude-only measurements. In contrast to
prevalent sparse phase retrieval algorithms that primarily use first-order
methods, we propose an innovative second-order algorithm that employs a
Newton-type method with hard thresholding. This algorithm overcomes the linear
convergence limitations of first-order methods while preserving their hallmark
per-iteration computational efficiency. We provide theoretical guarantees that
our algorithm converges to the $s$-sparse ground truth signal
$\mathbf{x}^{\natural} \in \mathbb{R}^n$ (up to a global sign) at a quadratic
convergence rate after at most $O(\log (\Vert\mathbf{x}^{\natural} \Vert
/x_{\min}^{\natural}))$ iterations, using $\Omega(s^2\log n)$ Gaussian random
samples. Numerical experiments show that our algorithm achieves a significantly
faster convergence rate than state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SplitMAC: Wireless Split Learning over Multiple Access Channels 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.02405v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.02405v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seonjung Kim, Yongjeong Oh, Yo-Seb Jeon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a novel split learning (SL) framework, referred to as
SplitMAC, which reduces the latency of SL by leveraging simultaneous uplink
transmission over multiple access channels. The key strategy is to divide
devices into multiple groups and allow the devices within the same group to
simultaneously transmit their smashed data and device-side models over the
multiple access channels. The optimization problem of device grouping to
minimize SL latency is formulated, and the benefit of device grouping in
reducing the uplink latency of SL is theoretically derived. By examining a
two-device grouping case, two asymptotically-optimal algorithms are devised for
device grouping in low and high signal-to-noise ratio (SNR) scenarios,
respectively, while providing proofs of their optimality. By merging these
algorithms, a near-optimal device grouping algorithm is proposed to cover a
wide range of SNR. Our SL framework is also extended to consider practical
fading channels and to support a general group size. Simulation results
demonstrate that our SL framework with the proposed device grouping algorithm
is superior to existing SL frameworks in reducing SL latency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Approximation and bounding techniques for the Fisher-Rao distances 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10089v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10089v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Frank Nielsen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Fisher-Rao distance between two probability distributions of a
statistical model is defined as the Riemannian geodesic distance induced by the
Fisher information metric. In order to calculate the Fisher-Rao distance in
closed-form, we need (1) to elicit a formula for the Fisher-Rao geodesics, and
(2) to integrate the Fisher length element along those geodesics. We consider
several numerically robust approximation and bounding techniques for the
Fisher-Rao distances: First, we report generic upper bounds on Fisher-Rao
distances based on closed-form 1D Fisher-Rao distances of submodels. Second, we
describe several generic approximation schemes depending on whether the
Fisher-Rao geodesics or pregeodesics are available in closed-form or not. In
particular, we obtain a generic method to guarantee an arbitrarily small
additive error on the approximation provided that Fisher-Rao pregeodesics and
tight lower and upper bounds are available. Third, we consider the case of
Fisher metrics being Hessian metrics, and report generic tight upper bounds on
the Fisher-Rao distances using techniques of information geometry.
Uniparametric and biparametric statistical models always have Fisher Hessian
metrics, and in general a simple test allows to check whether the Fisher
information matrix yields a Hessian metric or not. Fourth, we consider
elliptical distribution families and show how to apply the above techniques to
these models. We also propose two new distances based either on the Fisher-Rao
lengths of curves serving as proxies of Fisher-Rao geodesics, or based on the
Birkhoff/Hilbert projective cone distance. Last, we consider an alternative
group-theoretic approach for statistical transformation models based on the
notion of maximal invariant which yields insights on the structures of the
Fisher-Rao distance formula which may be used fruitfully in applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>43 pages</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-18T00:00:00Z">2024-03-18</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Distributed, Parallel, and Cluster Computing
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fair Distributed Cooperative Bandit Learning on Networks for Intelligent
  Internet of Things Systems (Technical Report) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11603v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11603v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziqun Chen, Kechao Cai, Jinbei Zhang, Zhigang Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In intelligent Internet of Things (IoT) systems, edge servers within a
network exchange information with their neighbors and collect data from sensors
to complete delivered tasks. In this paper, we propose a multiplayer
multi-armed bandit model for intelligent IoT systems to facilitate data
collection and incorporate fairness considerations. In our model, we establish
an effective communication protocol that helps servers cooperate with their
neighbors. Then we design a distributed cooperative bandit algorithm, DC-ULCB,
enabling servers to collaboratively select sensors to maximize data rates while
maintaining fairness in their choices. We conduct an analysis of the reward
regret and fairness regret of DC-ULCB, and prove that both regrets have
logarithmic instance-dependent upper bounds. Additionally, through extensive
simulations, we validate that DC-ULCB outperforms existing algorithms in
maximizing reward and ensuring fairness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 8 figures, conference technical report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LOOPer: A Learned Automatic Code Optimizer For Polyhedral Compilers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11522v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11522v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Massinissa Merouani, Khaled Afif Boudaoud, Iheb Nassim Aouadj, Nassim Tchoulak, Islam Kara Bernou, Hamza Benyamina, Fatima Benbouzid-Si Tayeb, Karima Benatchba, Hugh Leather, Riyadh Baghdadi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While polyhedral compilers have shown success in implementing advanced code
transformations, they still have challenges in selecting the most profitable
transformations that lead to the best speedups. This has motivated the use of
machine learning to build cost models to guide the search for polyhedral
optimizations. State-of-the-art polyhedral compilers have demonstrated a viable
proof-of-concept of this approach. While such a proof-of-concept has shown
promise, it still has significant limitations. State-of-the-art polyhedral
compilers that use a deep-learning cost model only support a small subset of
affine transformations, limiting their ability to apply complex code
transformations. They also only support simple programs that have a single loop
nest and a rectangular iteration domain, limiting their applicability to many
programs. These limitations significantly impact the generality of such
compilers and autoschedulers and put into question the whole approach. In this
paper, we introduce LOOPer, the first polyhedral autoscheduler that uses a
deep-learning based cost model and covers a large set of affine transformations
and programs. It supports the exploration of a large set of affine
transformations, allowing the application of complex sequences of polyhedral
transformations. It also supports the optimization of programs with multiple
loop nests and with rectangular and non-rectangular iteration domains, allowing
the optimization of an extensive set of programs. We implement and evaluate
LOOPer and show that it achieves speedups over the state-of-the-art. On the
Polybench benchmark, LOOPer achieves a geometric mean speedup of 1.59x over
Tiramisu. LOOPer also achieves competitive speedups with a geometric mean
speedup of 1.34x over Pluto, a state-of-the-art polyhedral compiler that does
not use a machine-learning based cost model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Earth+: on-board satellite imagery compression leveraging historical
  earth observations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11434v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11434v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kuntai Du, Yihua Cheng, Peder Olsen, Shadi Noghabi, Ranveer Chandra, Junchen Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the increasing deployment of earth observation satellite constellations,
the downlink (satellite-to-ground) capacity often limits the freshness,
quality, and coverage of the imagery data available to applications on the
ground. To overcome the downlink limitation, we present Earth+, a new satellite
imagery compression system that, instead of compressing each image
individually, pinpoints and downloads only recent imagery changes with respect
to the history reference images. To minimize the amount of changes, it is
critical to make reference images as fresh as possible. Earth+ enables each
satellite to choose fresh reference images from not only its own history images
but also past images of other satellites from an entire satellite
constellation. To share reference images across satellites, Earth+ utilizes the
limited capacity of the existing uplink (ground-to-satellite) by judiciously
selecting and compressing reference images while still allowing accurate change
detection. In short, Earth+ is the first to make reference-based compression
efficient, by enabling constellation-wide sharing of fresh reference images
across satellites. Our evaluation shows that Earth+ can reduce the downlink
usage by a factor of 3.3 compared to state-of-the-art on-board image
compression techniques while not sacrificing image quality, or using more
on-board computing or storage resources, or more uplink bandwidth than
currently available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FastDecode: High-Throughput GPU-Efficient LLM Serving using
  Heterogeneous Pipelines 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11421v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11421v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaao He, Jidong Zhai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cost of serving large language models (LLM) is high, but the expensive and
scarce GPUs are poorly efficient when generating tokens sequentially, unless
the batch of sequences is enlarged. However, the batch size is limited by some
constantly reused intermediate results, namely KV-Cache. They occupy too much
memory to fit more sequences into a GPU simultaneously. While they could be
offloaded to host memory, the CPU-GPU bandwidth is an inevitable bottleneck.
  We find a way to decompose the transformer models into two parts of different
characteristics, one of which includes the memory-bound KV-Cache accessing. Our
key insight is that the aggregated memory capacity, bandwidth, and computing
power of CPUs across multiple nodes is an efficient option to process this
part. Performance improvement comes from reduced data transmission overhead and
boosted GPU throughput to process the other model part. Moreover, we address
efficiency challenges brought by heterogeneity at both temporal and
inter-device scopes using scheduling and performance modeling techniques.
Evaluation results show that our system achieves 1.88x - 5.04x the throughput
of vLLM when serving modern LLMs with the same GPU.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 15 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving LoRA in Privacy-preserving Federated Learning <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12313v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12313v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Youbang Sun, Zitao Li, Yaliang Li, Bolin Ding
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low-rank adaptation (LoRA) is one of the most popular task-specific
parameter-efficient fine-tuning (PEFT) methods on pre-trained language models
for its good performance and computational efficiency. LoRA injects a product
of two trainable rank decomposition matrices over the top of each frozen
pre-trained model module. However, when applied in the setting of
privacy-preserving federated learning (FL), LoRA may become unstable due to the
following facts: 1) the effects of data heterogeneity and multi-step local
updates are non-negligible, 2) additive noise enforced on updating gradients to
guarantee differential privacy (DP) can be amplified and 3) the final
performance is susceptible to hyper-parameters. A key factor leading to these
phenomena is the discordance between jointly optimizing the two low-rank
matrices by local clients and separately aggregating them by the central
server. Thus, this paper proposes an efficient and effective version of LoRA,
Federated Freeze A LoRA (FFA-LoRA), to alleviate these challenges and further
halve the communication cost of federated fine-tuning LLMs. The core idea of
FFA-LoRA is to fix the randomly initialized non-zero matrices and only
fine-tune the zero-initialized matrices. Compared to LoRA, FFA-LoRA is
motivated by practical and theoretical benefits in privacy-preserved FL. Our
experiments demonstrate that FFA-LoRA provides more consistent performance with
better computational efficiency over vanilla LoRA in various FL tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>published at ICLR 2024, full paper 17 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BeRGeR: Byzantine-Robust Geometric Routing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12256v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12256v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brown Zaz, Mikhail Nesterenko, Gokarna Sharma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present BeRGeR: the first asynchronous geometric routing algorithm that
guarantees delivery of a message despite a Byzantine fault without relying on
cryptographic primitives or randomization. The communication graph is a planar
embedding that remains three-connected if all edges intersecting the
source-target line segment are removed. We prove the algorithm correct and
estimate its message complexity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large language models in 6G security: challenges and opportunities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12239v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12239v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tri Nguyen, Huong Nguyen, Ahmad Ijaz, Saeid Sheikhi, Athanasios V. Vasilakos, Panos Kostakos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid integration of Generative AI (GenAI) and Large Language Models
(LLMs) in sectors such as education and healthcare have marked a significant
advancement in technology. However, this growth has also led to a largely
unexplored aspect: their security vulnerabilities. As the ecosystem that
includes both offline and online models, various tools, browser plugins, and
third-party applications continues to expand, it significantly widens the
attack surface, thereby escalating the potential for security breaches. These
expansions in the 6G and beyond landscape provide new avenues for adversaries
to manipulate LLMs for malicious purposes. We focus on the security aspects of
LLMs from the viewpoint of potential adversaries. We aim to dissect their
objectives and methodologies, providing an in-depth analysis of known security
weaknesses. This will include the development of a comprehensive threat
taxonomy, categorizing various adversary behaviors. Also, our research will
concentrate on how LLMs can be integrated into cybersecurity efforts by defense
teams, also known as blue teams. We will explore the potential synergy between
LLMs and blockchain technology, and how this combination could lead to the
development of next-generation, fully autonomous security solutions. This
approach aims to establish a unified cybersecurity strategy across the entire
computing continuum, enhancing overall digital security infrastructure.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Edge-Disjoint Spanning Trees on Star-Product Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12231v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12231v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aleyah Dawkins, Kelly Isham, Ales Kubicek, Kartik Lakhotia, Laura Monroe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Star-product graphs are a natural extension of the Cartesian product, but
have not been well-studied. We show that many important established and
emerging network topologies, including HyperX, SlimFly, BundleFly, PolarStar,
mesh, and torus, are in fact star-product graphs. While this connection was
known for BundleFly and PolarStar, it was not for the others listed.
  We extend a method of constructing maximal and near-maximal sets of
edge-disjoint spanning trees on Cartesian products to the star product, thus
obtain maximal or near-maximal sets of edge-disjoint spanning trees on new
networks of importance, where such sets can improve bandwidth of collective
operations and therefore accelerate many important workloads in
high-performance computing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AMReX and pyAMReX: Looking Beyond ECP 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12179v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12179v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrew Myers, Weiqun Zhang, Ann Almgren, Thierry Antoun, John Bell, Axel Huebl, Alexander Sinn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  AMReX is a software framework for the development of block-structured mesh
applications with adaptive mesh refinement (AMR). AMReX was initially developed
and supported by the AMReX Co-Design Center as part of the U.S. DOE Exascale
Computing Project, and is continuing to grow post-ECP. In addition to adding
new functionality and performance improvements to the core AMReX framework, we
have also developed a Python binding, pyAMReX, that provides a bridge between
AMReX-based application codes and the data science ecosystem. pyAMReX provides
zero-copy application GPU data access for AI/ML, in situ analysis and
application coupling, and enables rapid, massively parallel prototyping. In
this paper we review the overall functionality of AMReX and pyAMReX, focusing
on new developments, new functionality, and optimizations of key operations. We
also summarize capabilities of ECP projects that used AMReX and provide an
overview of new, non-ECP applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 1 figure, submitted to the International Journal of High
  Performance Computing Applications</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ KnFu: Effective Knowledge Fusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11892v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11892v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        S. Jamal Seyedmohammadi, S. Kawa Atapour, Jamshid Abouei, Arash Mohammadi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) has emerged as a prominent alternative to the
traditional centralized learning approach. Generally speaking, FL is a
decentralized approach that allows for collaborative training of Machine
Learning (ML) models across multiple local nodes, ensuring data privacy and
security while leveraging diverse datasets. Conventional FL, however, is
susceptible to gradient inversion attacks, restrictively enforces a uniform
architecture on local models, and suffers from model heterogeneity (model
drift) due to non-IID local datasets. To mitigate some of these challenges, the
new paradigm of Federated Knowledge Distillation (FKD) has emerged. FDK is
developed based on the concept of Knowledge Distillation (KD), which involves
extraction and transfer of a large and well-trained teacher model's knowledge
to lightweight student models. FKD, however, still faces the model drift issue.
Intuitively speaking, not all knowledge is universally beneficial due to the
inherent diversity of data among local nodes. This calls for innovative
mechanisms to evaluate the relevance and effectiveness of each client's
knowledge for others, to prevent propagation of adverse knowledge. In this
context, the paper proposes Effective Knowledge Fusion (KnFu) algorithm that
evaluates knowledge of local models to only fuse semantic neighbors' effective
knowledge for each client. The KnFu is a personalized effective knowledge
fusion scheme for each client, that analyzes effectiveness of different local
models' knowledge prior to the aggregation phase. Comprehensive experiments
were performed on MNIST and CIFAR10 datasets illustrating effectiveness of the
proposed KnFu in comparison to its state-of-the-art counterparts. A key
conclusion of the work is that in scenarios with large and highly heterogeneous
local datasets, local training could be preferable to knowledge fusion-based
solutions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Low-Cost Privacy-Aware Decentralized Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11795v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11795v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sayan Biswas, Davide Frey, Romaric Gaudel, Anne-Marie Kermarrec, Dimitri Lerévérend, Rafael Pires, Rishi Sharma, François Taïani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces ZIP-DL, a novel privacy-aware decentralized learning
(DL) algorithm that relies on adding correlated noise to each model update
during the model training process. This technique ensures that the added noise
almost neutralizes itself during the aggregation process due to its
correlation, thus minimizing the impact on model accuracy. In addition, ZIP-DL
does not require multiple communication rounds for noise cancellation,
addressing the common trade-off between privacy protection and communication
overhead. We provide theoretical guarantees for both convergence speed and
privacy guarantees, thereby making ZIP-DL applicable to practical scenarios.
Our extensive experimental study shows that ZIP-DL achieves the best trade-off
between vulnerability and accuracy. In particular, ZIP-DL (i) reduces the
effectiveness of a linkability attack by up to 52 points compared to baseline
DL, and (ii) achieves up to 37 more accuracy points for the same vulnerability
under membership inference attacks against a privacy-preserving competitor
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-GPU-Enabled Hybrid Quantum-Classical Workflow in Quantum-HPC
  Middleware: Applications in Quantum Simulations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.05828v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.05828v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kuan-Cheng Chen, Xiaoren Li, Xiaotian Xu, Yun-Yuan Wang, Chen-Yu Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Achieving high-performance computation on quantum systems presents a
formidable challenge that necessitates bridging the capabilities between
quantum hardware and classical computing resources. This study introduces an
innovative distribution-aware Quantum-Classical-Quantum (QCQ) architecture,
which integrates cutting-edge quantum software framework works with
high-performance classical computing resources to address challenges in quantum
simulation for materials and condensed matter physics. At the heart of this
architecture is the seamless integration of VQE algorithms running on QPUs for
efficient quantum state preparation, Tensor Network states, and QCNNs for
classifying quantum states on classical hardware.
  For benchmarking quantum simulators, the QCQ architecture utilizes the
cuQuantum SDK to leverage multi-GPU acceleration, integrated with PennyLane's
Lightning plugin, demonstrating up to tenfold increases in computational speed
for complex phase transition classification tasks compared to traditional
CPU-based methods. This significant acceleration enables models such as the
transverse field Ising and XXZ systems to accurately predict phase transitions
with a 99.5% accuracy. The architecture's ability to distribute computation
between QPUs and classical resources addresses critical bottlenecks in
Quantum-HPC, paving the way for scalable quantum simulation.
  The QCQ framework embodies a synergistic combination of quantum algorithms,
machine learning, and Quantum-HPC capabilities, enhancing its potential to
provide transformative insights into the behavior of quantum systems across
different scales. As quantum hardware continues to improve, this hybrid
distribution-aware framework will play a crucial role in realizing the full
potential of quantum computing by seamlessly integrating distributed quantum
resources with the state-of-the-art classical computing infrastructure.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PULSAR: Simultaneous Many-Row Activation for Reliable and
  High-Performance Computing in Off-the-Shelf DRAM Chips 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.02880v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.02880v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ismail Emir Yuksel, Yahya Can Tugrul, F. Nisa Bostanci, Abdullah Giray Yaglikci, Ataberk Olgun, Geraldo F. Oliveira, Melina Soysal, Haocong Luo, Juan Gomez Luna, Mohammad Sadrosadati, Onur Mutlu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data movement between the processor and the main memory is a first-order
obstacle against improving performance and energy efficiency in modern systems.
To address this obstacle, Processing-using-Memory (PuM) is a promising approach
where bulk-bitwise operations are performed leveraging intrinsic analog
properties within the DRAM array and massive parallelism across DRAM columns.
Unfortunately, 1) modern off-the-shelf DRAM chips do not officially support PuM
operations, and 2) existing techniques of performing PuM operations on
off-the-shelf DRAM chips suffer from two key limitations. First, these
techniques have low success rates, i.e., only a small fraction of DRAM columns
can correctly execute PuM operations because they operate beyond
manufacturer-recommended timing constraints, causing these operations to be
highly susceptible to noise and process variation. Second, these techniques
have limited compute primitives, preventing them from fully leveraging
parallelism across DRAM columns and thus hindering their performance benefits.
  We propose PULSAR, a new technique to enable high-success-rate and
high-performance PuM operations in off-the-shelf DRAM chips. PULSAR leverages
our new observation that a carefully crafted sequence of DRAM commands
simultaneously activates up to 32 DRAM rows. PULSAR overcomes the limitations
of existing techniques by 1) replicating the input data to improve the success
rate and 2) enabling new bulk bitwise operations (e.g., many-input majority,
Multi-RowInit, and Bulk-Write) to improve the performance.
  Our analysis on 120 off-the-shelf DDR4 chips from two major manufacturers
shows that PULSAR achieves a 24.18% higher success rate and 121% higher
performance over seven arithmetic-logic operations compared to FracDRAM, a
state-of-the-art off-the-shelf DRAM-based PuM technique.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pollen: High-throughput Simulation of Federated Learning via
  Resource-Aware Client Placement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.17453v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.17453v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lorenzo Sani, Pedro Porto Buarque de Gusmão, Alex Iacob, Wanru Zhao, Xinchi Qiu, Yan Gao, Javier Fernandez-Marques, Nicholas Donald Lane
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) is a privacy-focused machine learning paradigm that
collaboratively trains models directly on edge devices. Simulated environments
are crucial for large-scale FL research, allowing scientists to quickly test
new ideas without acquiring millions of devices. However, current simulators
cannot match the scale necessary to emulate production systems or push the
boundaries of research in a time-efficient manner. This work proposes
\emph{Pollen}, a novel resource-aware system for speeding up simulations.
\emph{Pollen} addresses two limiting factors from previous systems: (a)
communication inefficiency in pull-based client execution and (b) ignoring
system inefficiencies from simulation-hardware diversity. \emph{Pollen}
executes high-throughput FL simulations at scale by (a) using a push-based
client placement system and (b) balancing clients across servers and their GPUs
with a novel online machine learning model. Furthermore, \emph{Pollen}'s
placement model reduces GPU idle time by up to 50\% by providing accurate
training time predictions, allowing researchers to run extensive experiments
sampling from millions of clients. Our experiments evaluate \pollen on four
representative FL tasks. We compare \emph{Pollen} to ad-hoc FL frameworks,
\emph{Flower}, \emph{Flute}, \emph{FedScale}, and \emph{Parrot}, and show
experimental speed-ups of days or weeks.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Performance
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FastDecode: High-Throughput GPU-Efficient LLM Serving using
  Heterogeneous Pipelines 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11421v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11421v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaao He, Jidong Zhai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cost of serving large language models (LLM) is high, but the expensive and
scarce GPUs are poorly efficient when generating tokens sequentially, unless
the batch of sequences is enlarged. However, the batch size is limited by some
constantly reused intermediate results, namely KV-Cache. They occupy too much
memory to fit more sequences into a GPU simultaneously. While they could be
offloaded to host memory, the CPU-GPU bandwidth is an inevitable bottleneck.
  We find a way to decompose the transformer models into two parts of different
characteristics, one of which includes the memory-bound KV-Cache accessing. Our
key insight is that the aggregated memory capacity, bandwidth, and computing
power of CPUs across multiple nodes is an efficient option to process this
part. Performance improvement comes from reduced data transmission overhead and
boosted GPU throughput to process the other model part. Moreover, we address
efficiency challenges brought by heterogeneity at both temporal and
inter-device scopes using scheduling and performance modeling techniques.
Evaluation results show that our system achieves 1.88x - 5.04x the throughput
of vLLM when serving modern LLMs with the same GPU.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 15 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Artificial Intelligence
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLaVA-UHD: an LMM Perceiving Any Aspect Ratio and High-Resolution Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11703v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11703v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruyi Xu, Yuan Yao, Zonghao Guo, Junbo Cui, Zanlin Ni, Chunjiang Ge, Tat-Seng Chua, Zhiyuan Liu, Maosong Sun, Gao Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual encoding constitutes the basis of large multimodal models (LMMs) in
understanding the visual world. Conventional LMMs process images in fixed sizes
and limited resolutions, while recent explorations in this direction are
limited in adaptivity, efficiency, and even correctness. In this work, we first
take GPT-4V and LLaVA-1.5 as representative examples and expose systematic
flaws rooted in their visual encoding strategy. To address the challenges, we
present LLaVA-UHD, a large multimodal model that can efficiently perceive
images in any aspect ratio and high resolution. LLaVA-UHD includes three key
components: (1) An image modularization strategy that divides native-resolution
images into smaller variable-sized slices for efficient and extensible
encoding, (2) a compression module that further condenses image tokens from
visual encoders, and (3) a spatial schema to organize slice tokens for LLMs.
Comprehensive experiments show that LLaVA-UHD outperforms established LMMs
trained with 2-3 orders of magnitude more data on 9 benchmarks. Notably, our
model built on LLaVA-1.5 336x336 supports 6 times larger (i.e., 672x1088)
resolution images using only 94% inference computation, and achieves 6.4
accuracy improvement on TextVQA. Moreover, the model can be efficiently trained
in academic settings, within 23 hours on 8 A100 GPUs (vs. 26 hours of
LLaVA-1.5). We make the data and code publicly available at
https://github.com/thunlp/LLaVA-UHD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HDLdebugger: Streamlining HDL debugging with Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11671v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11671v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xufeng Yao, Haoyang Li, Tsz Ho Chan, Wenyi Xiao, Mingxuan Yuan, Yu Huang, Lei Chen, Bei Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the domain of chip design, Hardware Description Languages (HDLs) play a
pivotal role. However, due to the complex syntax of HDLs and the limited
availability of online resources, debugging HDL codes remains a difficult and
time-intensive task, even for seasoned engineers. Consequently, there is a
pressing need to develop automated HDL code debugging models, which can
alleviate the burden on hardware engineers. Despite the strong capabilities of
Large Language Models (LLMs) in generating, completing, and debugging software
code, their utilization in the specialized field of HDL debugging has been
limited and, to date, has not yielded satisfactory results. In this paper, we
propose an LLM-assisted HDL debugging framework, namely HDLdebugger, which
consists of HDL debugging data generation via a reverse engineering approach, a
search engine for retrieval-augmented generation, and a retrieval-augmented LLM
fine-tuning approach. Through the integration of these components, HDLdebugger
can automate and streamline HDL debugging for chip design. Our comprehensive
experiments, conducted on an HDL code dataset sourced from Huawei, reveal that
HDLdebugger outperforms 13 cutting-edge LLM baselines, displaying exceptional
effectiveness in HDL code debugging.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages,5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Guiding the generation of counterfactual explanations through temporal
  background knowledge for Predictive Process Monitoring 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11642v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11642v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrei Buliga, Chiara Di Francescomarino, Chiara Ghidini, Ivan Donadello, Fabrizio Maria Maggi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Counterfactual explanations suggest what should be different in the input
instance to change the outcome of an AI system. When dealing with
counterfactual explanations in the field of Predictive Process Monitoring,
however, control flow relationships among events have to be carefully
considered. A counterfactual, indeed, should not violate control flow
relationships among activities (temporal background knowledege). Within the
field of Explainability in Predictive Process Monitoring, there have been a
series of works regarding counterfactual explanations for outcome-based
predictions. However, none of them consider the inclusion of temporal
background knowledge when generating these counterfactuals. In this work, we
adapt state-of-the-art techniques for counterfactual generation in the domain
of XAI that are based on genetic algorithms to consider a series of temporal
constraints at runtime. We assume that this temporal background knowledge is
given, and we adapt the fitness function, as well as the crossover and mutation
operators, to maintain the satisfaction of the constraints. The proposed
methods are evaluated with respect to state-of-the-art genetic algorithms for
counterfactual generation and the results are presented. We showcase that the
inclusion of temporal background knowledge allows the generation of
counterfactuals more conformant to the temporal background knowledge, without
however losing in terms of the counterfactual traditional quality metrics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ QEAN: Quaternion-Enhanced Attention Network for Visual Dance Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11626v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11626v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhizhen Zhou, Yejing Huo, Guoheng Huang, An Zeng, Xuhang Chen, Lian Huang, Zinuo Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The study of music-generated dance is a novel and challenging Image
generation task. It aims to input a piece of music and seed motions, then
generate natural dance movements for the subsequent music. Transformer-based
methods face challenges in time series prediction tasks related to human
movements and music due to their struggle in capturing the nonlinear
relationship and temporal aspects. This can lead to issues like joint
deformation, role deviation, floating, and inconsistencies in dance movements
generated in response to the music. In this paper, we propose a
Quaternion-Enhanced Attention Network (QEAN) for visual dance synthesis from a
quaternion perspective, which consists of a Spin Position Embedding (SPE)
module and a Quaternion Rotary Attention (QRA) module. First, SPE embeds
position information into self-attention in a rotational manner, leading to
better learning of features of movement sequences and audio sequences, and
improved understanding of the connection between music and dance. Second, QRA
represents and fuses 3D motion features and audio features in the form of a
series of quaternions, enabling the model to better learn the temporal
coordination of music and dance under the complex temporal cycle conditions of
dance generation. Finally, we conducted experiments on the dataset AIST++, and
the results show that our approach achieves better and more robust performance
in generating accurate, high-quality dance movements. Our source code and
dataset can be available from https://github.com/MarasyZZ/QEAN and
https://google.github.io/aistplusplus_dataset respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by The Visual Computer Journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimal Layout Synthesis for Deep Quantum Circuits on NISQ Processors
  with 100+ Qubits 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11598v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11598v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Irfansha Shaik, Jaco van de Pol
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Layout synthesis is mapping a quantum circuit to a quantum processor. SWAP
gate insertions are needed for scheduling 2-qubit gates only on connected
physical qubits. With the ever-increasing number of qubits in NISQ processors,
scalable layout synthesis is of utmost importance. With large optimality gaps
observed in heuristic approaches, scalable exact methods are needed. While
recent exact and near-optimal approaches scale to moderate circuits, large deep
circuits are still out of scope.
  In this work, we propose a SAT encoding based on parallel plans that apply 1
SWAP and a group of CNOTs at each time step. Using domain-specific information,
we maintain optimality in parallel plans while scaling to large and deep
circuits. From our results, we show the scalability of our approach which
significantly outperforms leading exact and near-optimal approaches (up to
100x). For the first time, we can optimally map several 8, 14, and 16 qubit
circuits onto 54, 80, and 127 qubit platforms with up to 17 SWAPs. While adding
optimal SWAPs, we also report near-optimal depth in our mapped circuits.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 Figures, 4 Tables, 1 Listing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Linguacodus: A Synergistic Framework for Transformative Code Generation
  in Machine Learning Pipelines 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11585v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11585v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ekaterina Trofimova, Emil Sataev, Andrey E. Ustyuzhanin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the ever-evolving landscape of machine learning, seamless translation of
natural language descriptions into executable code remains a formidable
challenge. This paper introduces Linguacodus, an innovative framework designed
to tackle this challenge by deploying a dynamic pipeline that iteratively
transforms natural language task descriptions into code through high-level
data-shaping instructions. The core of Linguacodus is a fine-tuned large
language model (LLM), empowered to evaluate diverse solutions for various
problems and select the most fitting one for a given task. This paper details
the fine-tuning process, and sheds light on how natural language descriptions
can be translated into functional code. Linguacodus represents a substantial
leap towards automated code generation, effectively bridging the gap between
task descriptions and executable code. It holds great promise for advancing
machine learning applications across diverse domains. Additionally, we propose
an algorithm capable of transforming a natural description of an ML task into
code with minimal human interaction. In extensive experiments on a vast machine
learning code dataset originating from Kaggle, we showcase the effectiveness of
Linguacodus. The investigations highlight its potential applications across
diverse domains, emphasizing its impact on applied machine learning in various
scientific fields.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reinforcement Learning with Token-level Feedback for Controllable Text
  Generation <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11558v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11558v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wendi Li, Wei Wei, Kaihe Xu, Wenfeng Xie, Dangyang Chen, Yu Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To meet the requirements of real-world applications, it is essential to
control generations of large language models (LLMs). Prior research has tried
to introduce reinforcement learning (RL) into controllable text generation
while most existing methods suffer from overfitting issues (finetuning-based
methods) or semantic collapse (post-processing methods). However, current RL
methods are generally guided by coarse-grained (sentence/paragraph-level)
feedback, which may lead to suboptimal performance owing to semantic twists or
progressions within sentences. To tackle that, we propose a novel reinforcement
learning algorithm named TOLE which formulates TOken-LEvel rewards for
controllable text generation, and employs a "first-quantize-then-noise"
paradigm to enhance the robustness of the RL algorithm.Furthermore, TOLE can be
flexibly extended to multiple constraints with little computational expense.
Experimental results show that our algorithm can achieve superior performance
on both single-attribute and multi-attribute control tasks. We have released
our codes at https://github.com/WindyLee0822/CTG
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NAACL 2024 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLM^3:Large Language Model-based Task and Motion Planning with Motion
  Failure Reasoning <span class="chip">IROS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11552v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11552v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shu Wang, Muzhi Han, Ziyuan Jiao, Zeyu Zhang, Ying Nian Wu, Song-Chun Zhu, Hangxin Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conventional Task and Motion Planning (TAMP) approaches rely on manually
crafted interfaces connecting symbolic task planning with continuous motion
generation. These domain-specific and labor-intensive modules are limited in
addressing emerging tasks in real-world settings. Here, we present LLM^3, a
novel Large Language Model (LLM)-based TAMP framework featuring a
domain-independent interface. Specifically, we leverage the powerful reasoning
and planning capabilities of pre-trained LLMs to propose symbolic action
sequences and select continuous action parameters for motion planning.
Crucially, LLM^3 incorporates motion planning feed- back through prompting,
allowing the LLM to iteratively refine its proposals by reasoning about motion
failure. Consequently, LLM^3 interfaces between task planning and motion
planning, alleviating the intricate design process of handling domain- specific
messages between them. Through a series of simulations in a box-packing domain,
we quantitatively demonstrate the effectiveness of LLM^3 in solving TAMP
problems and the efficiency in selecting action parameters. Ablation studies
un- derscore the significant contribution of motion failure reasoning to the
success of LLM^3. Furthermore, we conduct qualitative experiments on a physical
manipulator, demonstrating the practical applicability of our approach in
real-world settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IROS 2024. Codes available:
  https://github.com/AssassinWS/LLM-TAMP</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OCR is All you need: Importing Multi-Modality into Image-based Defect
  Detection System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11536v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11536v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chih-Chung Hsu, Chia-Ming Lee, Chun-Hung Sun, Kuang-Ming Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic optical inspection (AOI) plays a pivotal role in the manufacturing
process, predominantly leveraging high-resolution imaging instruments for
scanning purposes. It detects anomalies by analyzing image textures or
patterns, making it an essential tool in industrial manufacturing and quality
control. Despite its importance, the deployment of models for AOI often faces
challenges. These include limited sample sizes, which hinder effective feature
learning, variations among source domains, and sensitivities to changes in
lighting and camera positions during imaging. These factors collectively
compromise the accuracy of model predictions. Traditional AOI often fails to
capitalize on the rich mechanism-parameter information from machines or inside
images, including statistical parameters, which typically benefit AOI
classification. To address this, we introduce an external modality-guided data
mining framework, primarily rooted in optical character recognition (OCR), to
extract statistical features from images as a second modality to enhance
performance, termed OANet (Ocr-Aoi-Net). A key aspect of our approach is the
alignment of external modality features, extracted using a single
modality-aware model, with image features encoded by a convolutional neural
network. This synergy enables a more refined fusion of semantic representations
from different modalities. We further introduce feature refinement and a gating
function in our OANet to optimize the combination of these features, enhancing
inference and decision-making capabilities. Experimental outcomes show that our
methodology considerably boosts the recall rate of the defect detection model
and maintains high robustness even in challenging scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ End-To-End Underwater Video Enhancement: <span class="highlight-title">Dataset</span> and Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11506v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11506v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dazhao Du, Enhan Li, Lingyu Si, Fanjiang Xu, Jianwei Niu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Underwater video enhancement (UVE) aims to improve the visibility and frame
quality of underwater videos, which has significant implications for marine
research and exploration. However, existing methods primarily focus on
developing image enhancement algorithms to enhance each frame independently.
There is a lack of supervised datasets and models specifically tailored for UVE
tasks. To fill this gap, we construct the Synthetic Underwater Video
Enhancement (SUVE) dataset, comprising 840 diverse underwater-style videos
paired with ground-truth reference videos. Based on this dataset, we train a
novel underwater video enhancement model, UVENet, which utilizes inter-frame
relationships to achieve better enhancement performance. Through extensive
experiments on both synthetic and real underwater videos, we demonstrate the
effectiveness of our approach. This study represents the first comprehensive
exploration of UVE to our knowledge. The code is available at
https://anonymous.4open.science/r/UVENet.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MLVICX: Multi-Level Variance-Covariance Exploration for Chest X-ray
  <span class="highlight-title">Self-Supervised</span> Representation Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11504v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11504v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Azad Singh, Vandan Gorade, Deepak Mishra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised learning (SSL) is potentially useful in reducing the need for
manual annotation and making deep learning models accessible for medical image
analysis tasks. By leveraging the representations learned from unlabeled data,
self-supervised models perform well on tasks that require little to no
fine-tuning. However, for medical images, like chest X-rays, which are
characterized by complex anatomical structures and diverse clinical conditions,
there arises a need for representation learning techniques that can encode
fine-grained details while preserving the broader contextual information. In
this context, we introduce MLVICX (Multi-Level Variance-Covariance Exploration
for Chest X-ray Self-Supervised Representation Learning), an approach to
capture rich representations in the form of embeddings from chest X-ray images.
Central to our approach is a novel multi-level variance and covariance
exploration strategy that empowers the model to detect diagnostically
meaningful patterns while reducing redundancy effectively. By enhancing the
variance and covariance of the learned embeddings, MLVICX promotes the
retention of critical medical insights by adapting both global and local
contextual details. We demonstrate the performance of MLVICX in advancing
self-supervised chest X-ray representation learning through comprehensive
experiments. The performance enhancements we observe across various downstream
tasks highlight the significance of the proposed approach in enhancing the
utility of chest X-ray embeddings for precision medical diagnosis and
comprehensive image analysis. For pertaining, we used the NIH-Chest X-ray
dataset, while for downstream tasks, we utilized NIH-Chest X-ray, Vinbig-CXR,
RSNA pneumonia, and SIIM-ACR Pneumothorax datasets. Overall, we observe more
than 3% performance gains over SOTA SSL approaches in various downstream tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MCD: Diverse Large-Scale Multi-Campus <span class="highlight-title">Dataset</span> for Robot Perception 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11496v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11496v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thien-Minh Nguyen, Shenghai Yuan, Thien Hoang Nguyen, Pengyu Yin, Haozhi Cao, Lihua Xie, Maciej Wozniak, Patric Jensfelt, Marko Thiel, Justin Ziegenbein, Noel Blunder
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Perception plays a crucial role in various robot applications. However,
existing well-annotated datasets are biased towards autonomous driving
scenarios, while unlabelled SLAM datasets are quickly over-fitted, and often
lack environment and domain variations. To expand the frontier of these fields,
we introduce a comprehensive dataset named MCD (Multi-Campus Dataset),
featuring a wide range of sensing modalities, high-accuracy ground truth, and
diverse challenging environments across three Eurasian university campuses. MCD
comprises both CCS (Classical Cylindrical Spinning) and NRE (Non-Repetitive
Epicyclic) lidars, high-quality IMUs (Inertial Measurement Units), cameras, and
UWB (Ultra-WideBand) sensors. Furthermore, in a pioneering effort, we introduce
semantic annotations of 29 classes over 59k sparse NRE lidar scans across three
domains, thus providing a novel challenge to existing semantic segmentation
research upon this largely unexplored lidar modality. Finally, we propose, for
the first time to the best of our knowledge, continuous-time ground truth based
on optimization-based registration of lidar-inertial data on large survey-grade
prior maps, which are also publicly released, each several times the size of
existing ones. We conduct a rigorous evaluation of numerous state-of-the-art
algorithms on MCD, report their performance, and highlight the challenges
awaiting solutions from the research community.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by The IEEE/CVF Conference on Computer Vision and Pattern
  Recognition 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semantic-Enhanced Representation Learning for Road Networks with
  Temporal Dynamics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11495v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11495v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yile Chen, Xiucheng Li, Gao Cong, Zhifeng Bao, Cheng Long
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we introduce a novel framework called Toast for learning
general-purpose representations of road networks, along with its advanced
counterpart DyToast, designed to enhance the integration of temporal dynamics
to boost the performance of various time-sensitive downstream tasks.
Specifically, we propose to encode two pivotal semantic characteristics
intrinsic to road networks: traffic patterns and traveling semantics. To
achieve this, we refine the skip-gram module by incorporating auxiliary
objectives aimed at predicting the traffic context associated with a target
road segment. Moreover, we leverage trajectory data and design pre-training
strategies based on Transformer to distill traveling semantics on road
networks. DyToast further augments this framework by employing unified
trigonometric functions characterized by their beneficial properties, enabling
the capture of temporal evolution and dynamic nature of road networks more
effectively. With these proposed techniques, we can obtain representations that
encode multi-faceted aspects of knowledge within road networks, applicable
across both road segment-based applications and trajectory-based applications.
Extensive experiments on two real-world datasets across three tasks demonstrate
that our proposed framework consistently outperforms the state-of-the-art
baselines by a significant margin.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SmartRefine: An Scenario-Adaptive Refinement Framework for Efficient
  Motion Prediction <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11492v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11492v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Zhou, Hao Shao, Letian Wang, Steven L. Waslander, Hongsheng Li, Yu Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Predicting the future motion of surrounding agents is essential for
autonomous vehicles (AVs) to operate safely in dynamic, human-robot-mixed
environments. Context information, such as road maps and surrounding agents'
states, provides crucial geometric and semantic information for motion behavior
prediction. To this end, recent works explore two-stage prediction frameworks
where coarse trajectories are first proposed, and then used to select critical
context information for trajectory refinement. However, they either incur a
large amount of computation or bring limited improvement, if not both. In this
paper, we introduce a novel scenario-adaptive refinement strategy, named
SmartRefine, to refine prediction with minimal additional computation.
Specifically, SmartRefine can comprehensively adapt refinement configurations
based on each scenario's properties, and smartly chooses the number of
refinement iterations by introducing a quality score to measure the prediction
quality and remaining refinement potential of each scenario. SmartRefine is
designed as a generic and flexible approach that can be seamlessly integrated
into most state-of-the-art motion prediction models. Experiments on Argoverse
(1 & 2) show that our method consistently improves the prediction accuracy of
multiple state-of-the-art prediction models. Specifically, by adding
SmartRefine to QCNet, we outperform all published ensemble-free works on the
Argoverse 2 leaderboard (single agent track) at submission. Comprehensive
studies are also conducted to ablate design choices and explore the mechanism
behind multi-iteration refinement. Codes are available at
https://github.com/opendilab/SmartRefine/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Camera-ready version for CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can LLMs Generate Human-Like Wayfinding Instructions? Towards
  Platform-Agnostic Embodied Instruction Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11487v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11487v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vishnu Sashank Dorbala, Sanjoy Chowdhury, Dinesh Manocha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel approach to automatically synthesize "wayfinding
instructions" for an embodied robot agent. In contrast to prior approaches that
are heavily reliant on human-annotated datasets designed exclusively for
specific simulation platforms, our algorithm uses in-context learning to
condition an LLM to generate instructions using just a few references. Using an
LLM-based Visual Question Answering strategy, we gather detailed information
about the environment which is used by the LLM for instruction synthesis. We
implement our approach on multiple simulation platforms including Matterport3D,
AI Habitat and ThreeDWorld, thereby demonstrating its platform-agnostic nature.
We subjectively evaluate our approach via a user study and observe that 83.3%
of users find the synthesized instructions accurately capture the details of
the environment and show characteristics similar to those of human-generated
instructions. Further, we conduct zero-shot navigation with multiple approaches
on the REVERIE dataset using the generated instructions, and observe very close
correlation with the baseline on standard success metrics (< 1% change in SR),
quantifying the viability of generated instructions in replacing
human-annotated data. To the best of our knowledge, ours is the first
LLM-driven approach capable of generating "human-like" instructions in a
platform-agnostic manner, without requiring any form of training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 Pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Open-World Semi-Supervised Learning for Node Classification <span class="chip">ICDE 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11483v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11483v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanling Wang, Jing Zhang, Lingxi Zhang, Lixin Liu, Yuxiao Dong, Cuiping Li, Hong Chen, Hongzhi Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Open-world semi-supervised learning (Open-world SSL) for node classification,
that classifies unlabeled nodes into seen classes or multiple novel classes, is
a practical but under-explored problem in the graph community. As only seen
classes have human labels, they are usually better learned than novel classes,
and thus exhibit smaller intra-class variances within the embedding space
(named as imbalance of intra-class variances between seen and novel classes).
Based on empirical and theoretical analysis, we find the variance imbalance can
negatively impact the model performance. Pre-trained feature encoders can
alleviate this issue via producing compact representations for novel classes.
However, creating general pre-trained encoders for various types of graph data
has been proven to be challenging. As such, there is a demand for an effective
method that does not rely on pre-trained graph encoders. In this paper, we
propose an IMbalance-Aware method named OpenIMA for Open-world semi-supervised
node classification, which trains the node classification model from scratch
via contrastive learning with bias-reduced pseudo labels. Extensive experiments
on seven popular graph benchmarks demonstrate the effectiveness of OpenIMA, and
the source code has been available on GitHub.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICDE 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Word Order's Impacts: Insights from Reordering and Generation Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11473v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11473v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qinghua Zhao, Jiaang Li, Lei Li, Zenghui Zhou, Junfeng Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing works have studied the impacts of the order of words within natural
text. They usually analyze it by destroying the original order of words to
create a scrambled sequence, and then comparing the models' performance between
the original and scrambled sequences. The experimental results demonstrate
marginal drops. Considering this findings, different hypothesis about word
order is proposed, including ``the order of words is redundant with lexical
semantics'', and ``models do not rely on word order''. In this paper, we
revisit the aforementioned hypotheses by adding a order reconstruction
perspective, and selecting datasets of different spectrum. Specifically, we
first select four different datasets, and then design order reconstruction and
continuing generation tasks. Empirical findings support that ChatGPT relies on
word order to infer, but cannot support or negate the redundancy relations
between word order lexical semantics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Collage <span class="highlight-title">Prompt</span>ing: Budget-Friendly Visual Recognition with <span class="highlight-title">GPT</span>-4V 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11468v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11468v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siyu Xu, Yunke Wang, Daochang Liu, Chang Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in generative AI have suggested that by taking visual
prompt, GPT-4V can demonstrate significant proficiency in image recognition
task. Despite its impressive capabilities, the financial cost associated with
GPT-4V's inference presents a substantial barrier for its wide use. To address
this challenge, our work introduces Collage Prompting, a budget-friendly
prompting approach that concatenates multiple images into a single visual
input. With collage prompt, GPT-4V is able to perform image recognition on
several images simultaneously. Based on the observation that the accuracy of
GPT-4V's image recognition varies significantly with the order of images within
the collage prompt, our method further learns to optimize the arrangement of
images for maximum recognition accuracy. A graph predictor is trained to
indicate the accuracy of each collage prompt, then we propose an optimization
method to navigate the search space of possible image arrangements. Experiment
results across various datasets demonstrate the cost-efficiency score of
collage prompt is much larger than standard prompt. Additionally, collage
prompt with learned arrangement achieves clearly better accuracy than collage
prompt with random arrangement in GPT-4V's visual recognition.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HateCOT: An Explanation-Enhanced <span class="highlight-title">Dataset</span> for Generalizable Offensive
  Speech Detection via Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11456v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11456v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huy Nghiem, Hal Daumé III
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ubiquitousness of social media has led to the need for reliable and
efficient detection of offensive content to limit harmful effects. This has led
to a proliferation of datasets and models related to detecting offensive
content. While sophisticated models have attained strong performance on
individual datasets, these models often do not generalize due to differences
between how "offensive content" is conceptualized, and the resulting
differences in how these datasets are labeled. In this paper, we introduce
HateCOT, a dataset of 52,000 samples drawn from diverse existing sources with
explanations generated by GPT-3.5-Turbo and human-curated. We show that
pre-training models for the detection of offensive content on HateCOT
significantly boots open-sourced Language Models on three benchmark datasets in
both zero and few-shot settings, despite differences in domain and task.} We
further find that HateCOT enables effective K-shot fine-tuning in the
low-resource settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Demystifying Deep Reinforcement Learning-Based Autonomous Vehicle
  Decision-Making 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11432v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11432v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanxi Wan, Pei Li, Arpan Kusari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the advent of universal function approximators in the domain of
reinforcement learning, the number of practical applications leveraging deep
reinforcement learning (DRL) has exploded. Decision-making in automated driving
tasks has emerged as a chief application among them, taking the sensor data or
the higher-order kinematic variables as the input and providing a discrete
choice or continuous control output. However, the black-box nature of the
models presents an overwhelming limitation that restricts the real-world
deployment of DRL in autonomous vehicles (AVs). Therefore, in this research
work, we focus on the interpretability of an attention-based DRL framework. We
use a continuous proximal policy optimization-based DRL algorithm as the
baseline model and add a multi-head attention framework in an open-source AV
simulation environment. We provide some analytical techniques for discussing
the interpretability of the trained models in terms of explainability and
causality for spatial and temporal correlations. We show that the weights in
the first head encode the positions of the neighboring vehicles while the
second head focuses on the leader vehicle exclusively. Also, the ego vehicle's
action is causally dependent on the vehicles in the target lane spatially and
temporally. Through these findings, we reliably show that these techniques can
help practitioners decipher the results of the DRL algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted for peer-review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural network representation of quantum systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11420v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11420v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Koji Hashimoto, Yuji Hirono, Jun Maeda, Jojiro Totsuka-Yoshinaka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It has been proposed that random wide neural networks near Gaussian process
are quantum field theories around Gaussian fixed points. In this paper, we
provide a novel map with which a wide class of quantum mechanical systems can
be cast into the form of a neural network with a statistical summation over
network parameters. Our simple idea is to use the universal approximation
theorem of neural networks to generate arbitrary paths in the Feynman's path
integral. The map can be applied to interacting quantum systems / field
theories, even away from the Gaussian limit. Our findings bring machine
learning closer to the quantum world.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Measuring Meaning Composition in the Human Brain with Composition Scores
  from Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.04325v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.04325v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changjiang Gao, Jixing Li, Jiajun Chen, Shujian Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The process of meaning composition, wherein smaller units like morphemes or
words combine to form the meaning of phrases and sentences, is essential for
human sentence comprehension. Despite extensive neurolinguistic research into
the brain regions involved in meaning composition, a computational metric to
quantify the extent of composition is still lacking. Drawing on the key-value
memory interpretation of transformer feed-forward network blocks, we introduce
the Composition Score, a novel model-based metric designed to quantify the
degree of meaning composition during sentence comprehension. Experimental
findings show that this metric correlates with brain clusters associated with
word frequency, structural processing, and general sensitivity to words,
suggesting the multifaceted nature of meaning composition during human sentence
comprehension.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Stop Reasoning! When Multimodal LLMs with Chain-of-Thought Reasoning
  Meets Adversarial Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.14899v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.14899v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zefeng Wang, Zhen Han, Shuo Chen, Fan Xue, Zifeng Ding, Xun Xiao, Volker Tresp, Philip Torr, Jindong Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, Multimodal LLMs (MLLMs) have shown a great ability to understand
images. However, like traditional vision models, they are still vulnerable to
adversarial images. Meanwhile, Chain-of-Thought (CoT) reasoning has been widely
explored on MLLMs, which not only improves model's performance, but also
enhances model's explainability by giving intermediate reasoning steps.
Nevertheless, there is still a lack of study regarding MLLMs' adversarial
robustness with CoT and an understanding of what the rationale looks like when
MLLMs infer wrong answers with adversarial images. Our research evaluates the
adversarial robustness of MLLMs when employing CoT reasoning, finding that CoT
marginally improves adversarial robustness against existing attack methods.
Moreover, we introduce a novel stop-reasoning attack technique that effectively
bypasses the CoT-induced robustness enhancements. Finally, we demonstrate the
alterations in CoT reasoning when MLLMs confront adversarial images, shedding
light on their reasoning process under adversarial attacks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unconstrained Stochastic CCA: Unifying Multiview and <span class="highlight-title">Self-Supervised</span>
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.01012v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.01012v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        James Chapman, Lennie Wells, Ana Lawry Aguila
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Canonical Correlation Analysis (CCA) family of methods is foundational in
multiview learning. Regularised linear CCA methods can be seen to generalise
Partial Least Squares (PLS) and be unified with a Generalized Eigenvalue
Problem (GEP) framework. However, classical algorithms for these linear methods
are computationally infeasible for large-scale data. Extensions to Deep CCA
show great promise, but current training procedures are slow and complicated.
First we propose a novel unconstrained objective that characterizes the top
subspace of GEPs. Our core contribution is a family of fast algorithms for
stochastic PLS, stochastic CCA, and Deep CCA, simply obtained by applying
stochastic gradient descent (SGD) to the corresponding CCA objectives. Our
algorithms show far faster convergence and recover higher correlations than the
previous state-of-the-art on all standard CCA and Deep CCA benchmarks. These
improvements allow us to perform a first-of-its-kind PLS analysis of an
extremely large biomedical dataset from the UK Biobank, with over 33,000
individuals and 500,000 features. Finally, we apply our algorithms to match the
performance of `CCA-family' Self-Supervised Learning (SSL) methods on CIFAR-10
and CIFAR-100 with minimal hyper-parameter tuning, and also present theory to
clarify the links between these methods and classical CCA, laying the
groundwork for future insights.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Matrix-Transformation Based Low-Rank Adaptation (MTLoRA): A
  Brain-Inspired Method for Parameter-Efficient Fine-Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07440v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07440v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yao Liang, Yuwei Wang, Yang Li, Yi Zeng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning techniques based on Large Pretrained Language Models (LPLMs) have
been proven to significantly enhance model performance on a variety of
downstream tasks and effectively control the output behaviors of LPLMs. Recent
studies have proposed numerous methods for fine-tuning a small number of
parameters based on open-source LPLMs, reducing the demand for computational
and storage resources. Among these, reparameterization fine-tuning methods
represented by LoRA (Low-Rank Adaptation) have gained popularity. We find that
although these methods perform well in many aspects, there is still
considerable room for improvement in terms of complex task adaptability,
performance, stability, and algorithm complexity. In response to this, inspired
by the idea that the functions of the brain are shaped by its geometric
structure, this paper integrates this idea into LoRA technology and proposes a
new matrix transformation-based reparameterization method for efficient
fine-tuning, named Matrix-Transformation based Low-Rank Adaptation (MTLoRA).
MTLoRA aims to dynamically alter its spatial geometric structure by applying a
transformation-matrix T to perform linear transformations, such as rotation,
scaling, and translation, on the task-specific parameter matrix, generating new
matrix feature patterns (eigenvectors) to mimic the fundamental influence of
complex geometric structure feature patterns in the brain on functions, thereby
enhancing the model's performance in downstream tasks. In Natural Language
Understanding (NLU) tasks, it is evaluated using the GLUE benchmark test, and
the results reveal that MTLoRA achieves an overall performance increase of
about 1.0% across eight tasks; in Natural Language Generation (NLG) tasks,
MTLoRA improves performance by an average of 0.95% and 0.56% in the DART and
WebNLG tasks, respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hybrid Reasoning Based on Large Language Models for Autonomous Car
  Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.13602v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.13602v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mehdi Azarafza, Mojtaba Nayyeri, Charles Steinmetz, Steffen Staab, Achim Rettberg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have garnered significant attention for their
ability to understand text and images, generate human-like text, and perform
complex reasoning tasks. However, their ability to generalize this advanced
reasoning with a combination of natural language text for decision-making in
dynamic situations requires further exploration. In this study, we investigate
how well LLMs can adapt and apply a combination of arithmetic and common-sense
reasoning, particularly in autonomous driving scenarios. We hypothesize that
LLMs hybrid reasoning abilities can improve autonomous driving by enabling them
to analyze detected object and sensor data, understand driving regulations and
physical laws, and offer additional context. This addresses complex scenarios,
like decisions in low visibility (due to weather conditions), where traditional
methods might fall short. We evaluated Large Language Models (LLMs) based on
accuracy by comparing their answers with human-generated ground truth inside
CARLA. The results showed that when a combination of images (detected objects)
and sensor data is fed into the LLM, it can offer precise information for brake
and throttle control in autonomous vehicles across various weather conditions.
This formulation and answers can assist in decision-making for auto-pilot
systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unveiling the Significance of Toddler-Inspired Reward Transition in
  Goal-Oriented Reinforcement Learning <span class="chip">AAAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06880v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06880v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junseok Park, Yoonsung Kim, Hee Bin Yoo, Min Whoo Lee, Kibeom Kim, Won-Seok Choi, Minsu Lee, Byoung-Tak Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Toddlers evolve from free exploration with sparse feedback to exploiting
prior experiences for goal-directed learning with denser rewards. Drawing
inspiration from this Toddler-Inspired Reward Transition, we set out to explore
the implications of varying reward transitions when incorporated into
Reinforcement Learning (RL) tasks. Central to our inquiry is the transition
from sparse to potential-based dense rewards, which share optimal strategies
regardless of reward changes. Through various experiments, including those in
egocentric navigation and robotic arm manipulation tasks, we found that proper
reward transitions significantly influence sample efficiency and success rates.
Of particular note is the efficacy of the toddler-inspired Sparse-to-Dense
(S2D) transition. Beyond these performance metrics, using Cross-Density
Visualizer technique, we observed that transitions, especially the S2D, smooth
the policy loss landscape, promoting wide minima that enhance generalization in
RL models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as a full paper at AAAI 2024 (Oral presentation): 7 pages
  (main paper), 2 pages (references), 17 pages (appendix) each</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Homography Estimation for Visual Place Recognition <span class="chip">AAAI2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16086v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16086v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Feng Lu, Shuting Dong, Lijun Zhang, Bingxi Liu, Xiangyuan Lan, Dongmei Jiang, Chun Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual place recognition (VPR) is a fundamental task for many applications
such as robot localization and augmented reality. Recently, the hierarchical
VPR methods have received considerable attention due to the trade-off between
accuracy and efficiency. They usually first use global features to retrieve the
candidate images, then verify the spatial consistency of matched local features
for re-ranking. However, the latter typically relies on the RANSAC algorithm
for fitting homography, which is time-consuming and non-differentiable. This
makes existing methods compromise to train the network only in global feature
extraction. Here, we propose a transformer-based deep homography estimation
(DHE) network that takes the dense feature map extracted by a backbone network
as input and fits homography for fast and learnable geometric verification.
Moreover, we design a re-projection error of inliers loss to train the DHE
network without additional homography labels, which can also be jointly trained
with the backbone network to help it extract the features that are more
suitable for local matching. Extensive experiments on benchmark datasets show
that our method can outperform several state-of-the-art methods. And it is more
than one order of magnitude faster than the mainstream hierarchical VPR methods
using RANSAC. The code is released at https://github.com/Lu-Feng/DHE-VPR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CMMMU: A Chinese Massive Multi-discipline Multimodal Understanding
  Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.11944v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.11944v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ge Zhang, Xinrun Du, Bei Chen, Yiming Liang, Tongxu Luo, Tianyu Zheng, Kang Zhu, Yuyang Cheng, Chunpu Xu, Shuyue Guo, Haoran Zhang, Xingwei Qu, Junjie Wang, Ruibin Yuan, Yizhi Li, Zekun Wang, Yudong Liu, Yu-Hsuan Tsai, Fengji Zhang, Chenghua Lin, Wenhao Huang, Wenhu Chen, Jie Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the capabilities of large multimodal models (LMMs) continue to advance,
evaluating the performance of LMMs emerges as an increasing need. Additionally,
there is an even larger gap in evaluating the advanced knowledge and reasoning
abilities of LMMs in non-English contexts such as Chinese. We introduce CMMMU,
a new Chinese Massive Multi-discipline Multimodal Understanding benchmark
designed to evaluate LMMs on tasks demanding college-level subject knowledge
and deliberate reasoning in a Chinese context. CMMMU is inspired by and
strictly follows the annotation and analysis pattern of MMMU.
  CMMMU includes 12k manually collected multimodal questions from college
exams, quizzes, and textbooks, covering six core disciplines: Art & Design,
Business, Science, Health & Medicine, Humanities & Social Science, and Tech &
Engineering, like its companion, MMMU. These questions span 30 subjects and
comprise 39 highly heterogeneous image types, such as charts, diagrams, maps,
tables, music sheets, and chemical structures.
  CMMMU focuses on complex perception and reasoning with domain-specific
knowledge in the Chinese context. We evaluate 11 open-source LLMs and one
proprietary GPT-4V(ision). Even GPT-4V only achieves accuracies of 42%,
indicating a large space for improvement. CMMMU will boost the community to
build the next-generation LMMs towards expert artificial intelligence and
promote the democratization of LMMs by providing diverse language contexts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Is it Really Negative? Evaluating Natural Language Video Localization
  Performance on Multiple Reliable Videos Pool 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.16701v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.16701v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nakyeong Yang, Minsung Kim, Seunghyun Yoon, Joongbo Shin, Kyomin Jung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the explosion of multimedia content in recent years, Video Corpus Moment
Retrieval (VCMR), which aims to detect a video moment that matches a given
natural language query from multiple videos, has become a critical problem.
However, existing VCMR studies have a significant limitation since they have
regarded all videos not paired with a specific query as negative, neglecting
the possibility of including false negatives when constructing the negative
video set. In this paper, we propose an MVMR (Massive Videos Moment Retrieval)
task that aims to localize video frames within a massive video set, mitigating
the possibility of falsely distinguishing positive and negative videos. For
this task, we suggest an automatic dataset construction framework by employing
textual and visual semantic matching evaluation methods on the existing video
moment search datasets and introduce three MVMR datasets. To solve MVMR task,
we further propose a strong method, CroCs, which employs cross-directional
contrastive learning that selectively identifies the reliable and informative
negatives, enhancing the robustness of a model on MVMR task. Experimental
results on the introduced datasets reveal that existing video moment search
models are easily distracted by negative video frames, whereas our model shows
significant performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-GPU-Enabled Hybrid Quantum-Classical Workflow in Quantum-HPC
  Middleware: Applications in Quantum Simulations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.05828v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.05828v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kuan-Cheng Chen, Xiaoren Li, Xiaotian Xu, Yun-Yuan Wang, Chen-Yu Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Achieving high-performance computation on quantum systems presents a
formidable challenge that necessitates bridging the capabilities between
quantum hardware and classical computing resources. This study introduces an
innovative distribution-aware Quantum-Classical-Quantum (QCQ) architecture,
which integrates cutting-edge quantum software framework works with
high-performance classical computing resources to address challenges in quantum
simulation for materials and condensed matter physics. At the heart of this
architecture is the seamless integration of VQE algorithms running on QPUs for
efficient quantum state preparation, Tensor Network states, and QCNNs for
classifying quantum states on classical hardware.
  For benchmarking quantum simulators, the QCQ architecture utilizes the
cuQuantum SDK to leverage multi-GPU acceleration, integrated with PennyLane's
Lightning plugin, demonstrating up to tenfold increases in computational speed
for complex phase transition classification tasks compared to traditional
CPU-based methods. This significant acceleration enables models such as the
transverse field Ising and XXZ systems to accurately predict phase transitions
with a 99.5% accuracy. The architecture's ability to distribute computation
between QPUs and classical resources addresses critical bottlenecks in
Quantum-HPC, paving the way for scalable quantum simulation.
  The QCQ framework embodies a synergistic combination of quantum algorithms,
machine learning, and Quantum-HPC capabilities, enhancing its potential to
provide transformative insights into the behavior of quantum systems across
different scales. As quantum hardware continues to improve, this hybrid
distribution-aware framework will play a crucial role in realizing the full
potential of quantum computing by seamlessly integrating distributed quantum
resources with the state-of-the-art classical computing infrastructure.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generative <span class="highlight-title">Pretrain</span>ed Structured <span class="highlight-title">Transformer</span>s: Unsupervised Syntactic
  Language Models at Scale 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08293v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08293v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Hu, Pengyu Ji, Qingyang Zhu, Wei Wu, Kewei Tu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A syntactic language model (SLM) incrementally generates a sentence with its
syntactic tree in a left-to-right manner. We present Generative Pretrained
Structured Transformers (GPST), an unsupervised SLM at scale capable of being
pre-trained from scratch on raw texts with high parallelism. GPST circumvents
the limitations of previous SLMs such as relying on gold trees and sequential
training. It consists of two components, a usual SLM supervised by a
uni-directional language modeling loss, and an additional composition model,
which induces syntactic parse trees and computes constituent representations,
supervised by a bi-directional language modeling loss. We propose a
representation surrogate to enable joint parallel training of the two models in
a hard-EM fashion. We pre-train GPST on OpenWebText, a corpus with $9$ billion
tokens, and demonstrate the superiority of GPST over GPT-2 with a comparable
size in numerous tasks covering both language understanding and language
generation. Meanwhile, GPST also significantly outperforms existing
unsupervised SLMs on left-to-right grammar induction, while holding a
substantial acceleration on training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multimodal Pathway: Improve <span class="highlight-title">Transformer</span>s with Irrelevant Data from Other
  Modalities <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.14405v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.14405v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiyuan Zhang, Xiaohan Ding, Kaixiong Gong, Yixiao Ge, Ying Shan, Xiangyu Yue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose to improve transformers of a specific modality with irrelevant
data from other modalities, e.g., improve an ImageNet model with audio or point
cloud datasets. We would like to highlight that the data samples of the target
modality are irrelevant to the other modalities, which distinguishes our method
from other works utilizing paired (e.g., CLIP) or interleaved data of different
modalities. We propose a methodology named Multimodal Pathway - given a target
modality and a transformer designed for it, we use an auxiliary transformer
trained with data of another modality and construct pathways to connect
components of the two models so that data of the target modality can be
processed by both models. In this way, we utilize the universal
sequence-to-sequence modeling abilities of transformers obtained from two
modalities. As a concrete implementation, we use a modality-specific tokenizer
and task-specific head as usual but utilize the transformer blocks of the
auxiliary model via a proposed method named Cross-Modal Re-parameterization,
which exploits the auxiliary weights without any inference costs. On the image,
point cloud, video, and audio recognition tasks, we observe significant and
consistent performance improvements with irrelevant data from other modalities.
The code and models are available at https://github.com/AILab-CVC/M2PT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024. Code and models are available at
  https://github.com/AILab-CVC/M2PT</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ProMISe: <span class="highlight-title">Prompt</span>able Medical Image Segmentation using SAM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.04164v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.04164v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinfeng Wang, Sifan Song, Xinkun Wang, Yiyi Wang, Yiyi Miao, Jionglong Su, S. Kevin Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the proposal of the Segment Anything Model (SAM), fine-tuning SAM for
medical image segmentation (MIS) has become popular. However, due to the large
size of the SAM model and the significant domain gap between natural and
medical images, fine-tuning-based strategies are costly with potential risk of
instability, feature damage and catastrophic forgetting. Furthermore, some
methods of transferring SAM to a domain-specific MIS through fine-tuning
strategies disable the model's prompting capability, severely limiting its
utilization scenarios. In this paper, we propose an Auto-Prompting Module
(APM), which provides SAM-based foundation model with Euclidean adaptive
prompts in the target domain. Our experiments demonstrate that such adaptive
prompts significantly improve SAM's non-fine-tuned performance in MIS. In
addition, we propose a novel non-invasive method called Incremental Pattern
Shifting (IPS) to adapt SAM to specific medical domains. Experimental results
show that the IPS enables SAM to achieve state-of-the-art or competitive
performance in MIS without the need for fine-tuning. By coupling these two
methods, we propose ProMISe, an end-to-end non-fine-tuned framework for
Promptable Medical Image Segmentation. Our experiments demonstrate that both
using our methods individually or in combination achieves satisfactory
performance in low-cost pattern shifting, with all of SAM's parameters frozen.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UniRepLKNet: A Universal Perception Large-Kernel ConvNet for Audio,
  Video, Point Cloud, Time-Series and Image Recognition <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.15599v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.15599v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaohan Ding, Yiyuan Zhang, Yixiao Ge, Sijie Zhao, Lin Song, Xiangyu Yue, Ying Shan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-kernel convolutional neural networks (ConvNets) have recently received
extensive research attention, but two unresolved and critical issues demand
further investigation. 1) The architectures of existing large-kernel ConvNets
largely follow the design principles of conventional ConvNets or transformers,
while the architectural design for large-kernel ConvNets remains
under-addressed. 2) As transformers have dominated multiple modalities, it
remains to be investigated whether ConvNets also have a strong universal
perception ability in domains beyond vision. In this paper, we contribute from
two aspects. 1) We propose four architectural guidelines for designing
large-kernel ConvNets, the core of which is to exploit the essential
characteristics of large kernels that distinguish them from small kernels -
they can see wide without going deep. Following such guidelines, our proposed
large-kernel ConvNet shows leading performance in image recognition (ImageNet
accuracy of 88.0%, ADE20K mIoU of 55.6%, and COCO box AP of 56.4%),
demonstrating better performance and higher speed than the recent powerful
competitors. 2) We discover large kernels are the key to unlocking the
exceptional performance of ConvNets in domains where they were originally not
proficient. With certain modality-related preprocessing approaches, the
proposed model achieves state-of-the-art performance on time-series forecasting
and audio recognition tasks even without modality-specific customization to the
architecture. All the code and models are publicly available on GitHub and
Huggingface.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024. Code, all the models, reproducible training scripts at
  https://github.com/AILab-CVC/UniRepLKNet</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Quiet-STaR: Language Models Can Teach Themselves to Think Before
  Speaking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09629v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09629v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eric Zelikman, Georges Harik, Yijia Shao, Varuna Jayasiri, Nick Haber, Noah D. Goodman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When writing and talking, people sometimes pause to think. Although
reasoning-focused works have often framed reasoning as a method of answering
questions or completing agentic tasks, reasoning is implicit in almost all
written text. For example, this applies to the steps not stated between the
lines of a proof or to the theory of mind underlying a conversation. In the
Self-Taught Reasoner (STaR, Zelikman et al. 2022), useful thinking is learned
by inferring rationales from few-shot examples in question-answering and
learning from those that lead to a correct answer. This is a highly constrained
setting -- ideally, a language model could instead learn to infer unstated
rationales in arbitrary text. We present Quiet-STaR, a generalization of STaR
in which LMs learn to generate rationales at each token to explain future text,
improving their predictions. We address key challenges, including 1) the
computational cost of generating continuations, 2) the fact that the LM does
not initially know how to generate or use internal thoughts, and 3) the need to
predict beyond individual next tokens. To resolve these, we propose a tokenwise
parallel sampling algorithm, using learnable tokens indicating a thought's
start and end, and an extended teacher-forcing technique. Encouragingly,
generated rationales disproportionately help model difficult-to-predict tokens
and improve the LM's ability to directly answer difficult questions. In
particular, after continued pretraining of an LM on a corpus of internet text
with Quiet-STaR, we find zero-shot improvements on GSM8K
(5.9%$\rightarrow$10.9%) and CommonsenseQA (36.3%$\rightarrow$47.2%) and
observe a perplexity improvement of difficult tokens in natural text.
Crucially, these improvements require no fine-tuning on these tasks. Quiet-STaR
marks a step towards LMs that can learn to reason in a more general and
scalable way.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Global $\mathcal{L}^2$ minimization at uniform exponential rate via
  geometrically adapted gradient descent in Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.15487v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.15487v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the gradient descent flow widely used for the minimization of the
$\mathcal{L}^2$ cost function in Deep Learning networks, and introduce two
modified versions; one adapted for the overparametrized setting, and the other
for the underparametrized setting. Both have a clear and natural invariant
geometric meaning, taking into account the pullback vector bundle structure in
the overparametrized, and the pushforward vector bundle structure in the
underparametrized setting. In the overparametrized case, we prove that,
provided that a rank condition holds, all orbits of the modified gradient
descent drive the $\mathcal{L}^2$ cost to its global minimum at a uniform
exponential convergence rate; one thereby obtains an a priori stopping time for
any prescribed proximity to the global minimum. We point out relations of the
latter to sub-Riemannian geometry.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AMS Latex, 16 pages. Section 2.1 on rank condition, and Section 2.4
  on the trapping of orbits in the standard gradient descent flow added. Title
  changed</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mastering Text, Code and Math Simultaneously via Fusing Highly
  Specialized Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08281v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08281v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ning Ding, Yulin Chen, Ganqu Cui, Xingtai Lv, Weilin Zhao, Ruobing Xie, Bowen Zhou, Zhiyuan Liu, Maosong Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Underlying data distributions of natural language, programming code, and
mathematical symbols vary vastly, presenting a complex challenge for large
language models (LLMs) that strive to achieve high performance across all three
domains simultaneously. Achieving a very high level of proficiency for an LLM
within a specific domain often requires extensive training with relevant
corpora, which is typically accompanied by a sacrifice in performance in other
domains. In this paper, we propose to fuse models that are already
highly-specialized directly. The proposed fusing framework, UltraFuser,
consists of three distinct specialists that are already sufficiently trained on
language, coding, and mathematics. A token-level gating mechanism is introduced
to blend the specialists' outputs. A two-stage training strategy accompanied by
balanced sampling is designed to ensure stability. To effectively train the
fused model, we further construct a high-quality supervised instruction tuning
dataset, UltraChat 2, which includes text, code, and mathematical content. This
dataset comprises approximately 300,000 instructions and covers a wide range of
topics in each domain. Experiments show that our model could simultaneously
achieve mastery of the three crucial domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Effectiveness Assessment of Recent Large Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.04306v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.04306v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yao Jiang, Xinyu Yan, Ge-Peng Ji, Keren Fu, Meijun Sun, Huan Xiong, Deng-Ping Fan, Fahad Shahbaz Khan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of large vision-language models (LVLMs) represents a noteworthy
advancement towards the pursuit of artificial general intelligence. However,
the extent of their efficacy across both specialized and general tasks warrants
further investigation. This article endeavors to evaluate the competency of
popular LVLMs in specialized and general tasks, respectively, aiming to offer a
comprehensive comprehension of these innovative methodologies. To gauge their
efficacy in specialized tasks, we tailor a comprehensive testbed comprising
three distinct scenarios: natural, healthcare, and industrial, encompassing six
challenging tasks. These tasks include salient, camouflaged, and transparent
object detection, as well as polyp and skin lesion detection, alongside
industrial anomaly detection. We examine the performance of three recent
open-source LVLMs -- MiniGPT-v2, LLaVA-1.5, and Shikra -- in the realm of
visual recognition and localization. Moreover, we conduct empirical
investigations utilizing the aforementioned models alongside GPT-4V, assessing
their multi-modal understanding capacities in general tasks such as object
counting, absurd question answering, affordance reasoning, attribute
recognition, and spatial relation reasoning. Our investigations reveal that
these models demonstrate limited proficiency not only in specialized tasks but
also in general tasks. We delve deeper into this inadequacy and suggest several
potential factors, including limited cognition in specialized tasks, object
hallucination, text-to-image interference, and decreased robustness in complex
problems. We hope this study would provide valuable insights for the future
development of LVLMs, augmenting their power in coping with both general and
specialized applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LeTO: Learning Constrained Visuomotor Policy with Differentiable
  Trajectory Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.17500v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.17500v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengtong Xu, Yu She
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces LeTO, a method for learning constrained visuomotor
policy via differentiable trajectory optimization. Our approach uniquely
integrates a differentiable optimization layer into the neural network. By
formulating the optimization layer as a trajectory optimization problem, we
enable the model to end-to-end generate actions in a safe and controlled
fashion without extra modules. Our method allows for the introduction of
constraints information during the training process, thereby balancing the
training objectives of satisfying constraints, smoothing the trajectories, and
minimizing errors with demonstrations. This "gray box" method marries the
optimization-based safety and interpretability with the powerful
representational abilities of neural networks. We quantitatively evaluate LeTO
in simulation and on the real robot. In simulation, LeTO achieves a success
rate comparable to state-of-the-art imitation learning methods, but the
generated trajectories are of less uncertainty, higher quality, and smoother.
In real-world experiments, we deployed LeTO to handle constraints-critical
tasks. The results show the effectiveness of LeTO comparing with
state-of-the-art imitation learning approaches. We release our code at
https://github.com/ZhengtongXu/LeTO.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Controllable Data Generation by Deep Learning: A <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09542v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09542v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiyu Wang, Yuanqi Du, Xiaojie Guo, Bo Pan, Zhaohui Qin, Liang Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Designing and generating new data under targeted properties has been
attracting various critical applications such as molecule design, image editing
and speech synthesis. Traditional hand-crafted approaches heavily rely on
expertise experience and intensive human efforts, yet still suffer from the
insufficiency of scientific knowledge and low throughput to support effective
and efficient data generation. Recently, the advancement of deep learning has
created the opportunity for expressive methods to learn the underlying
representation and properties of data. Such capability provides new ways of
determining the mutual relationship between the structural patterns and
functional properties of the data and leveraging such relationships to generate
structural data, given the desired properties. This article is a systematic
review that explains this promising research area, commonly known as
controllable deep data generation. First, the article raises the potential
challenges and provides preliminaries. Then the article formally defines
controllable deep data generation, proposes a taxonomy on various techniques
and summarizes the evaluation metrics in this specific domain. After that, the
article introduces exciting applications of controllable deep data generation,
experimentally analyzes and compares existing works. Finally, this article
highlights the promising future directions of controllable deep data generation
and identifies five potential challenges.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This survey has been accepted by ACM Computing Surveys</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ S-Agents: Self-organizing Agents in Open-ended Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.04578v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.04578v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaqi Chen, Yuxian Jiang, Jiachen Lu, Li Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Leveraging large language models (LLMs), autonomous agents have significantly
improved, gaining the ability to handle a variety of tasks. In open-ended
settings, optimizing collaboration for efficiency and effectiveness demands
flexible adjustments. Despite this, current research mainly emphasizes fixed,
task-oriented workflows and overlooks agent-centric organizational structures.
Drawing inspiration from human organizational behavior, we introduce a
self-organizing agent system (S-Agents) with a "tree of agents" structure for
dynamic workflow, an "hourglass agent architecture" for balancing information
priorities, and a "non-obstructive collaboration" method to allow asynchronous
task execution among agents. This structure can autonomously coordinate a group
of agents, efficiently addressing the challenges of open and dynamic
environments without human intervention. Our experiments demonstrate that
S-Agents proficiently execute collaborative building tasks and resource
collection in the Minecraft environment, validating their effectiveness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preview, 15 pages, 12 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cobweb: An Incremental and Hierarchical Model of Human-Like Category
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.03835v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.03835v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Lian, Sashank Varma, Christopher J. MacLellan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cobweb, a human like category learning system, differs from other incremental
categorization models in constructing hierarchically organized cognitive
tree-like structures using the category utility measure. Prior studies have
shown that Cobweb can capture psychological effects such as the basic level,
typicality, and fan effects. However, a broader evaluation of Cobweb as a model
of human categorization remains lacking. The current study addresses this gap.
It establishes Cobweb's alignment with classical human category learning
effects. It also explores Cobweb's flexibility to exhibit both exemplar and
prototype like learning within a single model. These findings set the stage for
future research on Cobweb as a comprehensive model of human category learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HyperAgent: A Simple, Scalable, Efficient and Provable Reinforcement
  Learning Framework for Complex Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.10228v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.10228v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingru Li, Jiawei Xu, Lei Han, Zhi-Quan Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To solve complex tasks under resource constraints, reinforcement learning
(RL) agents need to be simple, efficient, and scalable, addressing (1) large
state spaces and (2) the continuous accumulation of interaction data. We
propose HyperAgent, an RL framework featuring the hypermodel and index sampling
schemes that enable computation-efficient incremental approximation for the
posteriors associated with general value functions without the need for
conjugacy, and data-efficient action selection. Implementing HyperAgent is
straightforward, requiring only one additional module beyond what is necessary
for Double-DQN. HyperAgent stands out as the first method to offer robust
performance in large-scale deep RL benchmarks while achieving provably scalable
per-step computational complexity and attaining sublinear regret under tabular
assumptions. HyperAgent can solve Deep Sea hard exploration problems with
episodes that optimally scale with problem size and exhibits significant
efficiency gains in both data and computation under the Atari benchmark. The
core of our theoretical analysis is the sequential posterior approximation
argument, enabled by the first analytical tool for sequential random projection
-- a non-trivial martingale extension of the Johnson-Lindenstrauss. This work
bridges the theoretical and practical realms of RL, establishing a new
benchmark for RL algorithm design.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Bridging the theory and practice! Invited talk in Informs
  Optimization Conference 2024 and International Symposium on Mathematical
  Programming 2024!</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Motion<span class="highlight-title">GPT</span>: Finetuned LLMs Are General-Purpose Motion Generators <span class="chip">AAAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.10900v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.10900v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yaqi Zhang, Di Huang, Bin Liu, Shixiang Tang, Yan Lu, Lu Chen, Lei Bai, Qi Chu, Nenghai Yu, Wanli Ouyang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating realistic human motion from given action descriptions has
experienced significant advancements because of the emerging requirement of
digital humans. While recent works have achieved impressive results in
generating motion directly from textual action descriptions, they often support
only a single modality of the control signal, which limits their application in
the real digital human industry. This paper presents a Motion General-Purpose
generaTor (MotionGPT) that can use multimodal control signals, e.g., text and
single-frame poses, for generating consecutive human motions by treating
multimodal signals as special input tokens in large language models (LLMs).
Specifically, we first quantize multimodal control signals into discrete codes
and then formulate them in a unified prompt instruction to ask the LLMs to
generate the motion answer. Our MotionGPT demonstrates a unified human motion
generation model with multimodal control signals by tuning a mere 0.4% of LLM
parameters. To the best of our knowledge, MotionGPT is the first method to
generate human motion by multimodal control signals, which we hope can shed
light on this new direction. Visit our webpage at
https://qiqiapink.github.io/MotionGPT/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 8 figures, accepted by AAAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Shifting to Machine Supervision: Annotation-Efficient Semi and
  <span class="highlight-title">Self-Supervised</span> Learning for Automatic Medical Image Segmentation and
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10319v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10319v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pranav Singh, Raviteja Chukkapalli, Shravan Chaudhari, Luoyao Chen, Mei Chen, Jinqian Pan, Craig Smuda, Jacopo Cirrone
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advancements in clinical treatment are increasingly constrained by the
limitations of supervised learning techniques, which depend heavily on large
volumes of annotated data. The annotation process is not only costly but also
demands substantial time from clinical specialists. Addressing this issue, we
introduce the S4MI (Self-Supervision and Semi-Supervision for Medical Imaging)
pipeline, a novel approach that leverages the advancements in self-supervised
and semi-supervised learning. These techniques engage in auxiliary tasks that
do not require labeling, thus simplifying the scaling of machine supervision
compared to fully-supervised methods. Our study benchmarks these techniques on
three distinct medical imaging datasets to evaluate their effectiveness in
classification and segmentation tasks. Notably, we observed that
self-supervised learning significantly surpassed the performance of supervised
methods in the classification of all evaluated datasets. Remarkably, the
semi-supervised approach demonstrated superior outcomes in segmentation,
outperforming fully-supervised methods while using 50% fewer labels across all
datasets. In line with our commitment to contributing to the scientific
community, we have made the S4MI code openly accessible, allowing for broader
application and further development of these methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Seventeen pages (incl. references), five figures, and one table.
  (Under Review)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLM-CXR: Instruction-Finetuned LLM for CXR Image Understanding and
  Generation <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.11490v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.11490v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Suhyeon Lee, Won Jun Kim, Jinho Chang, Jong Chul Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Following the impressive development of LLMs, vision-language alignment in
LLMs is actively being researched to enable multimodal reasoning and visual IO.
This direction of research is particularly relevant to medical imaging because
medical image analysis and generation consist of reasoning based on a
combination of visual features and prior knowledge. Many recent works have
focused on training adapter networks that serve as an information bridge
between image processing networks and LLMs; but presumably, in order to achieve
maximum reasoning potential of LLMs on visual information as well, visual and
language features should be allowed to interact more freely. This is especially
important in the medical domain because understanding and generating medical
images such as chest X-rays (CXR) require not only accurate visual and
language-based reasoning but also a more intimate mapping between the two
modalities. Thus, taking inspiration from previous work on the transformer and
VQ-GAN combination for bidirectional image and text generation, we build upon
this approach and develop a method for instruction-tuning an LLM pre-trained
only on text to gain vision-language capabilities for medical images.
Specifically, we leverage a pretrained LLM's existing question-answering and
instruction-following abilities to teach it to understand visual inputs by
instructing it to answer questions about image inputs and, symmetrically,
output both text and image responses appropriate to a given query by tuning the
LLM with diverse tasks that encompass image-based text-generation and
text-based image-generation. We show that our model, LLM-CXR, trained in this
approach shows better image-text alignment in both CXR understanding and
generation tasks while being smaller in size compared to previously developed
models that perform a narrower range of tasks. The code is at
https://github.com/hyn2028/llm-cxr.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 8 figures; ICLR 2024 (poster)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Value of Assistance for Grasping 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.14402v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.14402v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Masarwy, Yuval Goshen, David Dovrat, Sarah Keren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In multiple realistic settings, a robot is tasked with grasping an object
without knowing its exact pose and relies on a probabilistic estimation of the
pose to decide how to attempt the grasp. We support settings in which it is
possible to provide the robot with an observation of the object before a grasp
is attempted but this possibility is limited and there is a need to decide
which sensing action would be most beneficial. We support this decision by
offering a novel Value of Assistance (VOA) measure for assessing the expected
effect a specific observation will have on the robot's ability to complete its
task. We evaluate our suggested measure in simulated and real-world
collaborative grasping settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Toolink: Linking Toolkit Creation and Using through Chain-of-Solving on
  Open-Source Model <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05155v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05155v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cheng Qian, Chenyan Xiong, Zhenghao Liu, Zhiyuan Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated remarkable progress in
utilizing tools, but their closed-source nature and high inference costs pose
limitations on their adaptability, necessitating a valid method that leverages
smaller, open-sourced models. In this paper, we introduce Toolink, a
comprehensive framework that performs task-solving by first creating a toolkit
and then integrating the planning and calling of tools through a
chain-of-solving (CoS) approach. We first validate the efficacy of Toolink in
harnessing the model's creativity and CoS ability on ChatGPT. Subsequently, we
curate CoS-GPT, a chain-of-solving dataset designed for tool-using, and
finetune the LLaMA-7B model. It results in LLaMA-CoS, a powerful open-source
model with advanced tool-planning and tool-calling capabilities. Evaluation of
diverse tasks from BIG-bench demonstrates its CoS ability matches that of
ChatGPT while its performance surpasses the chain-of-thought approach. Further
studies highlight the generalization of LLaMA-CoS to unseen tasks and showcase
its capability in using toolkits not explicitly tailored for the target task,
affirming its robustness in real-world scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2024 Main</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Expressive Power of Low-Rank Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.17513v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.17513v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuchen Zeng, Kangwook Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low-Rank Adaptation (LoRA), a parameter-efficient fine-tuning method that
leverages low-rank adaptation of weight matrices, has emerged as a prevalent
technique for fine-tuning pre-trained models such as large language models and
diffusion models. Despite its huge success in practice, the theoretical
underpinnings of LoRA have largely remained unexplored. This paper takes the
first step to bridge this gap by theoretically analyzing the expressive power
of LoRA. We prove that, for fully connected neural networks, LoRA can adapt any
model $f$ to accurately represent any smaller target model $\overline{f}$ if
LoRA-rank $\geq(\text{width of }f) \times \frac{\text{depth of
}\overline{f}}{\text{depth of }f}$. We also quantify the approximation error
when LoRA-rank is lower than the threshold. For Transformer networks, we show
any model can be adapted to a target model of the same size with
rank-$(\frac{\text{embedding size}}{2})$ LoRA adapters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>40 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Databases
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Models for Storage in Database Backends <span class="chip">EuroSys</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11716v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11716v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Edgard Schiebelbein, Saalik Hatia, Annette Bieniusa, Gustavo Petri, Carla Ferreira, Marc Shapiro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper describes ongoing work on developing a formal specification of a
database backend. We present the formalisation of the expected behaviour of a
basic transactional system that calls into a simple store API, and instantiate
in two semantic models. The first one is a map-based, classical versioned
key-value store; the second one, journal-based, appends individual transaction
effects to a journal. We formalise a significant part of the specification in
the Coq proof assistant. This work will form the basis for a formalisation of a
full-fledged backend store with features such as caching or write-ahead
logging, as variations on maps and journals.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Workshop on Principles and Practice of Consistency for Distributed
  Data (PaPoC), EuroSys (ACM), Apr 2024, Ath{\`e}nes, Greece</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Accelerating String-Key Learned Index Structures via Memoization-based
  Incremental Training <span class="chip">VLDB '24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11472v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11472v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minsu Kim, Jinwoo Hwang, Guseul Heo, Seiyeon Cho, Divya Mahajan, Jongse Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learned indexes use machine learning models to learn the mappings between
keys and their corresponding positions in key-value indexes. These indexes use
the mapping information as training data. Learned indexes require frequent
retrainings of their models to incorporate the changes introduced by update
queries. To efficiently retrain the models, existing learned index systems
often harness a linear algebraic QR factorization technique that performs
matrix decomposition. This factorization approach processes all key-position
pairs during each retraining, resulting in compute operations that grow
linearly with the total number of keys and their lengths. Consequently, the
retrainings create a severe performance bottleneck, especially for
variable-length string keys, while the retrainings are crucial for maintaining
high prediction accuracy and in turn, ensuring low query service latency.
  To address this performance problem, we develop an algorithm-hardware
co-designed string-key learned index system, dubbed SIA. In designing SIA, we
leverage a unique algorithmic property of the matrix decomposition-based
training method. Exploiting the property, we develop a memoization-based
incremental training scheme, which only requires computation over updated keys,
while decomposition results of non-updated keys from previous computations can
be reused. We further enhance SIA to offload a portion of this training process
to an FPGA accelerator to not only relieve CPU resources for serving index
queries (i.e., inference), but also accelerate the training itself. Our
evaluation shows that compared to ALEX, LIPP, and SIndex, a state-of-the-art
learned index systems, SIA-accelerated learned indexes offer 2.6x and 3.4x
higher throughput on the two real-world benchmark suites, YCSB and Twitter
cache trace, respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at VLDB '24; 12 pages + 2 pages (ref), 18 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automated data processing and feature engineering for deep learning and
  big data applications: a <span class="highlight-title">survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11395v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11395v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alhassan Mumuni amd Fuseini Mumuni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern approach to artificial intelligence (AI) aims to design algorithms
that learn directly from data. This approach has achieved impressive results
and has contributed significantly to the progress of AI, particularly in the
sphere of supervised deep learning. It has also simplified the design of
machine learning systems as the learning process is highly automated. However,
not all data processing tasks in conventional deep learning pipelines have been
automated. In most cases data has to be manually collected, preprocessed and
further extended through data augmentation before they can be effective for
training. Recently, special techniques for automating these tasks have emerged.
The automation of data processing tasks is driven by the need to utilize large
volumes of complex, heterogeneous data for machine learning and big data
applications. Today, end-to-end automated data processing systems based on
automated machine learning (AutoML) techniques are capable of taking raw data
and transforming them into useful features for Big Data tasks by automating all
intermediate processing stages. In this work, we present a thorough review of
approaches for automating data processing tasks in deep learning pipelines,
including automated data preprocessing--e.g., data cleaning, labeling, missing
data imputation, and categorical data encoding--as well as data augmentation
(including synthetic data generation using generative AI methods) and feature
engineering--specifically, automated feature extraction, feature construction
and feature selection. In addition to automating specific data processing
tasks, we discuss the use of AutoML methods and tools to simultaneously
optimize all stages of the machine learning pipeline.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Journal of Information and Intelligence (2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Estonia's Open Government Data Development as a Journey
  towards Excellence: Unveiling the Progress of Local Governments in Open Data
  Provision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11952v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11952v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Katrin Rajamäe-Soosaar, Anastasija Nikiforova
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Estonia has a global reputation of a digital state or e-country. However,
despite the success in digital governance, the country has faced challenges in
the realm of Open Government Data (OGD) area, with significant advancements in
its OGD ecosystem, as reflected in various open data rankings from 2020 and
onwards, in the recent years being recognized among trend-setters. This paper
aims to explore the evolution and positioning of Estonia's OGD development,
encompassing national and local levels, through an integrated analysis of
various indices, primary data from the Estonian OGD portal, and a thorough
literature review. The research shows that Estonia has made progress in the
national level open data ecosystem, primarily due to improvements in the OGD
portal usability and legislation amendments. However, the local level is not as
developed, with local governments lagging behind in OGD provision. The
literature review highlights the lack of previous research focusing on Estonian
and European local open data, emphasizing the need for future studies to
explore the barriers and enablers of municipal OGD. This study contributes to a
nuanced understanding of Estonia's dynamic journey in the OGD landscape,
shedding light on both achievements and areas warranting further attention for
establishing a sustainable open data ecosystem.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted for publication in Proceedings of the
  25th Annual International Conference on Digital Government Research and this
  is a pre-print version of the manuscript. It is posted here for your personal
  use. Not for redistribution</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Benchmarking Analytical Query Processing in Intel SGXv2 <span class="chip">VLDB 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11874v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11874v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adrian Lutsch, Muhammad El-Hindi, Matthias Heinrich, Daniel Ritter, Zsolt István, Carsten Binnig
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recently introduced second generation of Intel SGX (SGXv2) lifts memory
size limitations of the first generation. Theoretically, this promises to
enable secure and highly efficient analytical DBMSs in the cloud. To validate
this promise, in this paper, we conduct the first in-depth evaluation study of
running analytical query processing algorithms inside SGXv2. Our study reveals
that state-of-the-art query operators like radix joins and SIMD-based scans can
indeed achieve high performance inside SGXv2 enclaves. These operations are
orders of magnitude faster than joins optimized for the discontinued SGXv1
hardware. However, substantial performance overheads are still caused by subtle
hardware and software differences influencing code execution inside an SGX
enclave. We investigate these differences and propose new optimizations to
bring the performance inside the enclave on par with native code execution
outside an enclave.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 17 figures, submitted for VLDB 2024 in the EA&B category,
  associated code is available under
  https://github.com/DataManagementLab/sgxv2-analytical-query-processing-benchmarks</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LIST: Learning to Index Spatio-Textual Data for Embedding based Spatial
  Keyword Queries 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07331v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07331v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziqi Yin, Shanshan Feng, Shang Liu, Gao Cong, Yew Soon Ong, Bin Cui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the proliferation of spatio-textual data, Top-k KNN spatial keyword
queries (TkQs), which return a list of objects based on a ranking function that
evaluates both spatial and textual relevance, have found many real-life
applications. Existing geo-textual indexes for TkQs use traditional retrieval
models like BM25 to compute text relevance and usually exploit a simple linear
function to compute spatial relevance, but its effectiveness is limited. To
improve effectiveness, several deep learning models have recently been
proposed, but they suffer severe efficiency issues. To the best of our
knowledge, there are no efficient indexes specifically designed to accelerate
the top-k search process for these deep learning models.
  To tackle these issues, we propose a novel technique, which Learns to Index
the Spatio-Textual data for answering embedding based spatial keyword queries
(called LIST). LIST is featured with two novel components. Firstly, we propose
a lightweight and effective relevance model that is capable of learning both
textual and spatial relevance. Secondly, we introduce a novel machine learning
based Approximate Nearest Neighbor Search (ANNS) index, which utilizes a new
learning-to-cluster technique to group relevant queries and objects together
while separating irrelevant queries and objects. Two key challenges in building
an effective and efficient index are the absence of high-quality labels and
unbalanced clustering results. We develop a novel pseudo-label generation
technique to address the two challenges. Experimental results show that LIST
significantly outperforms state-of-the-art methods on effectiveness, with
improvements up to 19.21% and 12.79% in terms of NDCG@1 and Recall@10, and is
three orders of magnitude faster than the most effective baseline.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adaptive Recursive Query Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.04282v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.04282v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anna Herlihy, Guillaume Martres, Anastasia Ailamaki, Martin Odersky
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Performance-critical industrial applications, including large-scale program,
network, and distributed system analyses, are increasingly reliant on recursive
queries for data analysis. Yet traditional relational algebra-based query
optimization techniques do not scale well to recursive query processing due to
the iterative nature of query evaluation, where relation cardinalities can
change unpredictably during the course of a single query execution. To avoid
error-prone cardinality estimation, adaptive query processing techniques use
runtime information to inform query optimization, but these systems are not
optimized for the specific needs of recursive query processing. In this paper,
we introduce Adaptive Metaprogramming, an innovative technique that shifts
recursive query optimization and code generation from compile-time to runtime
using principled metaprogramming, enabling dynamic optimization and
re-optimization before and after query execution has begun. We present a custom
join-ordering optimization applicable at multiple stages during query
compilation and execution. Through Carac, a custom Datalog engine, we evaluate
the optimization potential of Adaptive Metaprogramming and show unoptimized
recursive query execution time can be improved by three orders of magnitude and
hand-optimized queries by 6x.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Time Series Compression using Quaternion Valued Neural Networks and
  Quaternion Backpropagation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11722v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11722v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Johannes Pöppelbaum, Andreas Schwung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel quaternionic time-series compression methodology where we
divide a long time-series into segments of data, extract the min, max, mean and
standard deviation of these chunks as representative features and encapsulate
them in a quaternion, yielding a quaternion valued time-series. This
time-series is processed using quaternion valued neural network layers, where
we aim to preserve the relation between these features through the usage of the
Hamilton product. To train this quaternion neural network, we derive quaternion
backpropagation employing the GHR calculus, which is required for a valid
product and chain rule in quaternion space. Furthermore, we investigate the
connection between the derived update rules and automatic differentiation. We
apply our proposed compression method on the Tennessee Eastman Dataset, where
we perform fault classification using the compressed data in two settings: a
fully supervised one and in a semi supervised, contrastive learning setting.
Both times, we were able to outperform real valued counterparts as well as two
baseline models: one with the uncompressed time-series as the input and the
other with a regular downsampling using the mean. Further, we could improve the
classification benchmark set by SimCLR-TS from 81.43% to 83.90%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generalized Multi-Source Inference for Text Conditioned Music Diffusion
  Models <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11706v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11706v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emilian Postolache, Giorgio Mariani, Luca Cosmo, Emmanouil Benetos, Emanuele Rodolà
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-Source Diffusion Models (MSDM) allow for compositional musical
generation tasks: generating a set of coherent sources, creating
accompaniments, and performing source separation. Despite their versatility,
they require estimating the joint distribution over the sources, necessitating
pre-separated musical data, which is rarely available, and fixing the number
and type of sources at training time. This paper generalizes MSDM to arbitrary
time-domain diffusion models conditioned on text embeddings. These models do
not require separated data as they are trained on mixtures, can parameterize an
arbitrary number of sources, and allow for rich semantic control. We propose an
inference procedure enabling the coherent generation of sources and
accompaniments. Additionally, we adapt the Dirac separator of MSDM to perform
source separation. We experiment with diffusion models trained on Slakh2100 and
MTG-Jamendo, showcasing competitive generation and separation results in a
relaxed data setting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICASSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Coarsening of chiral domains in itinerant electron magnets: A machine
  learning force field approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11705v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11705v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunhao Fan, Sheng Zhang, Gia-Wei Chern
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Frustrated itinerant magnets often exhibit complex noncollinear or
noncoplanar magnetic orders which support topological electronic structures. A
canonical example is the anomalous quantum Hall state with a chiral spin order
stabilized by electron-spin interactions on a triangular lattice. While a
long-range magnetic order cannot survive thermal fluctuations in two
dimensions, the chiral order which results from the breaking of a discrete
Ising symmetry persists even at finite temperatures. We present a scalable
machine learning (ML) framework to model the complex electron-mediated
spin-spin interactions that stabilize the chiral magnetic domains in a
triangular lattice. Large-scale dynamical simulations, enabled by the ML
force-field models, are performed to investigate the coarsening of chiral
domains after a thermal quench. While the chiral phase is described by a broken
$Z_2$ Ising-type symmetry, we find that the characteristic size of chiral
domains increases linearly with time, in stark contrast to the expected
Allen-Cahn domain growth law for a non-conserved Ising order parameter field.
The linear growth of the chiral domains is attributed to the orientational
anisotropy of domain boundaries. Our work also demonstrates the promising
potential of ML models for large-scale spin dynamics of itinerant magnets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generalization error of spectral algorithms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11696v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11696v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maksim Velikanov, Maxim Panov, Dmitry Yarotsky
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The asymptotically precise estimation of the generalization of kernel methods
has recently received attention due to the parallels between neural networks
and their associated kernels. However, prior works derive such estimates for
training by kernel ridge regression (KRR), whereas neural networks are
typically trained with gradient descent (GD). In the present work, we consider
the training of kernels with a family of $\textit{spectral algorithms}$
specified by profile $h(\lambda)$, and including KRR and GD as special cases.
Then, we derive the generalization error as a functional of learning profile
$h(\lambda)$ for two data models: high-dimensional Gaussian and low-dimensional
translation-invariant model. Under power-law assumptions on the spectrum of the
kernel and target, we use our framework to (i) give full loss asymptotics for
both noisy and noiseless observations (ii) show that the loss localizes on
certain spectral scales, giving a new perspective on the KRR saturation
phenomenon (iii) conjecture, and demonstrate for the considered data models,
the universality of the loss w.r.t. non-spectral details of the problem, but
only in case of noisy observation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Nonsmooth Implicit Differentiation: Deterministic and Stochastic
  Convergence Rates 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11687v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11687v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Riccardo Grazzi, Massimiliano Pontil, Saverio Salzo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the problem of efficiently computing the derivative of the
fixed-point of a parametric non-differentiable contraction map. This problem
has wide applications in machine learning, including hyperparameter
optimization, meta-learning and data poisoning attacks. We analyze two popular
approaches: iterative differentiation (ITD) and approximate implicit
differentiation (AID). A key challenge behind the nonsmooth setting is that the
chain rule does not hold anymore. Building upon the recent work by Bolte et al.
(2022), who proved the linear convergence of non-differentiable ITD, we provide
refined linear convergence rates for both ITD and AID in the deterministic
case. We further introduce NSID, a new method to compute the implicit
derivative when the fixed point is defined as the composition of an outer map
and an inner map which is accessible only through a stochastic unbiased
estimator. We establish rates for the convergence of NSID to the true
derivative, encompassing the best available rates in the smooth setting. We
present illustrative experiments confirming our analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Crystalformer: Infinitely Connected Attention for Periodic Structure
  Encoding <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11686v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11686v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tatsunori Taniai, Ryo Igarashi, Yuta Suzuki, Naoya Chiba, Kotaro Saito, Yoshitaka Ushiku, Kanta Ono
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Predicting physical properties of materials from their crystal structures is
a fundamental problem in materials science. In peripheral areas such as the
prediction of molecular properties, fully connected attention networks have
been shown to be successful. However, unlike these finite atom arrangements,
crystal structures are infinitely repeating, periodic arrangements of atoms,
whose fully connected attention results in infinitely connected attention. In
this work, we show that this infinitely connected attention can lead to a
computationally tractable formulation, interpreted as neural potential
summation, that performs infinite interatomic potential summations in a deeply
learned feature space. We then propose a simple yet effective Transformer-based
encoder architecture for crystal structures called Crystalformer. Compared to
an existing Transformer-based model, the proposed model requires only 29.4% of
the number of parameters, with minimal modifications to the original
Transformer architecture. Despite the architectural simplicity, the proposed
method outperforms state-of-the-art methods for various property regression
tasks on the Materials Project and JARVIS-DFT datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 main pages, 3 figures, 4 tables, 10 appendix pages. Published as a
  conference paper at ICLR 2024. For more information, see
  https://omron-sinicx.github.io/crystalformer/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring 3D-aware Latent Spaces for Efficiently Learning Numerous
  Scenes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11678v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11678v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antoine Schnepf, Karim Kassab, Jean-Yves Franceschi, Laurent Caraffa, Flavian Vasile, Jeremie Mary, Andrew Comport, Valérie Gouet-Brunet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a method enabling the scaling of NeRFs to learn a large number of
semantically-similar scenes. We combine two techniques to improve the required
training time and memory cost per scene. First, we learn a 3D-aware latent
space in which we train Tri-Plane scene representations, hence reducing the
resolution at which scenes are learned. Moreover, we present a way to share
common information across scenes, hence allowing for a reduction of model
complexity to learn a particular scene. Our method reduces effective per-scene
memory costs by 44% and per-scene time costs by 86% when training 1000 scenes.
Our project page can be found at https://3da-ae.github.io .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HDLdebugger: Streamlining HDL debugging with Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11671v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11671v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xufeng Yao, Haoyang Li, Tsz Ho Chan, Wenyi Xiao, Mingxuan Yuan, Yu Huang, Lei Chen, Bei Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the domain of chip design, Hardware Description Languages (HDLs) play a
pivotal role. However, due to the complex syntax of HDLs and the limited
availability of online resources, debugging HDL codes remains a difficult and
time-intensive task, even for seasoned engineers. Consequently, there is a
pressing need to develop automated HDL code debugging models, which can
alleviate the burden on hardware engineers. Despite the strong capabilities of
Large Language Models (LLMs) in generating, completing, and debugging software
code, their utilization in the specialized field of HDL debugging has been
limited and, to date, has not yielded satisfactory results. In this paper, we
propose an LLM-assisted HDL debugging framework, namely HDLdebugger, which
consists of HDL debugging data generation via a reverse engineering approach, a
search engine for retrieval-augmented generation, and a retrieval-augmented LLM
fine-tuning approach. Through the integration of these components, HDLdebugger
can automate and streamline HDL debugging for chip design. Our comprehensive
experiments, conducted on an HDL code dataset sourced from Huawei, reveal that
HDLdebugger outperforms 13 cutting-edge LLM baselines, displaying exceptional
effectiveness in HDL code debugging.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages,5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diffusion-Based Environment-Aware Trajectory Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11643v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11643v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Theodor Westny, Björn Olofsson, Erik Frisk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability to predict the future trajectories of traffic participants is
crucial for the safe and efficient operation of autonomous vehicles. In this
paper, a diffusion-based generative model for multi-agent trajectory prediction
is proposed. The model is capable of capturing the complex interactions between
traffic participants and the environment, accurately learning the multimodal
nature of the data. The effectiveness of the approach is assessed on
large-scale datasets of real-world traffic scenarios, showing that our model
outperforms several well-established methods in terms of prediction accuracy.
By the incorporation of differential motion constraints on the model output, we
illustrate that our model is capable of generating a diverse set of realistic
future trajectories. Through the use of an interaction-aware guidance signal,
we further demonstrate that the model can be adapted to predict the behavior of
less cooperative agents, emphasizing its practical applicability under
uncertain traffic conditions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Guiding the generation of counterfactual explanations through temporal
  background knowledge for Predictive Process Monitoring 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11642v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11642v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrei Buliga, Chiara Di Francescomarino, Chiara Ghidini, Ivan Donadello, Fabrizio Maria Maggi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Counterfactual explanations suggest what should be different in the input
instance to change the outcome of an AI system. When dealing with
counterfactual explanations in the field of Predictive Process Monitoring,
however, control flow relationships among events have to be carefully
considered. A counterfactual, indeed, should not violate control flow
relationships among activities (temporal background knowledege). Within the
field of Explainability in Predictive Process Monitoring, there have been a
series of works regarding counterfactual explanations for outcome-based
predictions. However, none of them consider the inclusion of temporal
background knowledge when generating these counterfactuals. In this work, we
adapt state-of-the-art techniques for counterfactual generation in the domain
of XAI that are based on genetic algorithms to consider a series of temporal
constraints at runtime. We assume that this temporal background knowledge is
given, and we adapt the fitness function, as well as the crossover and mutation
operators, to maintain the satisfaction of the constraints. The proposed
methods are evaluated with respect to state-of-the-art genetic algorithms for
counterfactual generation and the results are presented. We showcase that the
inclusion of temporal background knowledge allows the generation of
counterfactuals more conformant to the temporal background knowledge, without
however losing in terms of the counterfactual traditional quality metrics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Value of Reward Lookahead in Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11637v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11637v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nadav Merlis, Dorian Baudry, Vianney Perchet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In reinforcement learning (RL), agents sequentially interact with changing
environments while aiming to maximize the obtained rewards. Usually, rewards
are observed only after acting, and so the goal is to maximize the expected
cumulative reward. Yet, in many practical settings, reward information is
observed in advance -- prices are observed before performing transactions;
nearby traffic information is partially known; and goals are oftentimes given
to agents prior to the interaction. In this work, we aim to quantifiably
analyze the value of such future reward information through the lens of
competitive analysis. In particular, we measure the ratio between the value of
standard RL agents and that of agents with partial future-reward lookahead. We
characterize the worst-case reward distribution and derive exact ratios for the
worst-case reward expectations. Surprisingly, the resulting ratios relate to
known quantities in offline RL and reward-free exploration. We further provide
tight bounds for the ratio given the worst-case dynamics. Our results cover the
full spectrum between observing the immediate rewards before acting to
observing all the rewards before the interaction starts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dual-Channel Multiplex Graph Neural Networks for Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11624v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11624v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Li, Chaofan Fu, Zhongying Zhao, Guanjie Zheng, Chao Huang, Junyu Dong, Yanwei Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficient recommender systems play a crucial role in accurately capturing
user and item attributes that mirror individual preferences. Some existing
recommendation techniques have started to shift their focus towards modeling
various types of interaction relations between users and items in real-world
recommendation scenarios, such as clicks, marking favorites, and purchases on
online shopping platforms. Nevertheless, these approaches still grapple with
two significant shortcomings: (1) Insufficient modeling and exploitation of the
impact of various behavior patterns formed by multiplex relations between users
and items on representation learning, and (2) ignoring the effect of different
relations in the behavior patterns on the target relation in recommender system
scenarios. In this study, we introduce a novel recommendation framework,
Dual-Channel Multiplex Graph Neural Network (DCMGNN), which addresses the
aforementioned challenges. It incorporates an explicit behavior pattern
representation learner to capture the behavior patterns composed of multiplex
user-item interaction relations, and includes a relation chain representation
learning and a relation chain-aware encoder to discover the impact of various
auxiliary relations on the target relation, the dependencies between different
relations, and mine the appropriate order of relations in a behavior pattern.
Extensive experiments on three real-world datasets demonstrate that our \model
surpasses various state-of-the-art recommendation methods. It outperforms the
best baselines by 10.06\% and 12.15\% on average across all datasets in terms
of R@10 and N@10 respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fair Distributed Cooperative Bandit Learning on Networks for Intelligent
  Internet of Things Systems (Technical Report) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11603v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11603v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziqun Chen, Kechao Cai, Jinbei Zhang, Zhigang Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In intelligent Internet of Things (IoT) systems, edge servers within a
network exchange information with their neighbors and collect data from sensors
to complete delivered tasks. In this paper, we propose a multiplayer
multi-armed bandit model for intelligent IoT systems to facilitate data
collection and incorporate fairness considerations. In our model, we establish
an effective communication protocol that helps servers cooperate with their
neighbors. Then we design a distributed cooperative bandit algorithm, DC-ULCB,
enabling servers to collaboratively select sensors to maximize data rates while
maintaining fairness in their choices. We conduct an analysis of the reward
regret and fairness regret of DC-ULCB, and prove that both regrets have
logarithmic instance-dependent upper bounds. Additionally, through extensive
simulations, we validate that DC-ULCB outperforms existing algorithms in
maximizing reward and ensuring fairness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 8 figures, conference technical report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A physics-informed neural network method for the approximation of slow
  invariant manifolds for the general class of stiff systems of ODEs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11591v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11591v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dimitrios G. Patsatzis, Lucia Russo, Constantinos Siettos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a physics-informed neural network (PINN) approach for the
discovery of slow invariant manifolds (SIMs), for the most general class of
fast/slow dynamical systems of ODEs. In contrast to other machine learning (ML)
approaches that construct reduced order black box surrogate models using simple
regression, and/or require a priori knowledge of the fast and slow variables,
our approach, simultaneously decomposes the vector field into fast and slow
components and provides a functional of the underlying SIM in a closed form.
The decomposition is achieved by finding a transformation of the state
variables to the fast and slow ones, which enables the derivation of an
explicit, in terms of fast variables, SIM functional. The latter is obtained by
solving a PDE corresponding to the invariance equation within the Geometric
Singular Perturbation Theory (GSPT) using a single-layer feedforward neural
network with symbolic differentiation. The performance of the proposed
physics-informed ML framework is assessed via three benchmark problems: the
Michaelis-Menten, the target mediated drug disposition (TMDD) reaction model
and a fully competitive substrate-inhibitor(fCSI) mechanism. We also provide a
comparison with other GPST methods, namely the quasi steady state approximation
(QSSA), the partial equilibrium approximation (PEA) and CSP with one and two
iterations. We show that the proposed PINN scheme provides SIM approximations,
of equivalent or even higher accuracy, than those provided by QSSA, PEA and
CSP, especially close to the boundaries of the underlying SIMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Linguacodus: A Synergistic Framework for Transformative Code Generation
  in Machine Learning Pipelines 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11585v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11585v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ekaterina Trofimova, Emil Sataev, Andrey E. Ustyuzhanin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the ever-evolving landscape of machine learning, seamless translation of
natural language descriptions into executable code remains a formidable
challenge. This paper introduces Linguacodus, an innovative framework designed
to tackle this challenge by deploying a dynamic pipeline that iteratively
transforms natural language task descriptions into code through high-level
data-shaping instructions. The core of Linguacodus is a fine-tuned large
language model (LLM), empowered to evaluate diverse solutions for various
problems and select the most fitting one for a given task. This paper details
the fine-tuning process, and sheds light on how natural language descriptions
can be translated into functional code. Linguacodus represents a substantial
leap towards automated code generation, effectively bridging the gap between
task descriptions and executable code. It holds great promise for advancing
machine learning applications across diverse domains. Additionally, we propose
an algorithm capable of transforming a natural description of an ML task into
code with minimal human interaction. In extensive experiments on a vast machine
learning code dataset originating from Kaggle, we showcase the effectiveness of
Linguacodus. The investigations highlight its potential applications across
diverse domains, emphasizing its impact on applied machine learning in various
scientific fields.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Offline Multitask Representation Learning for Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11574v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11574v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haque Ishfaq, Thanh Nguyen-Tang, Songtao Feng, Raman Arora, Mengdi Wang, Ming Yin, Doina Precup
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study offline multitask representation learning in reinforcement learning
(RL), where a learner is provided with an offline dataset from different tasks
that share a common representation and is asked to learn the shared
representation. We theoretically investigate offline multitask low-rank RL, and
propose a new algorithm called MORL for offline multitask representation
learning. Furthermore, we examine downstream RL in reward-free, offline and
online scenarios, where a new task is introduced to the agent that shares the
same representation as the upstream offline tasks. Our theoretical results
demonstrate the benefits of using the learned representation from the upstream
offline task instead of directly learning the representation of the low-rank
model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Decentralized Stochastic Subgradient Methods for Nonsmooth Nonconvex
  Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11565v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11565v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siyuan Zhang, Nachuan Xiao, Xin Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we concentrate on decentralized optimization problems with
nonconvex and nonsmooth objective functions, especially on the decentralized
training of nonsmooth neural networks. We introduce a unified framework, named
DSM, to analyze the global convergence of decentralized stochastic subgradient
methods. We prove the global convergence of our proposed framework under mild
conditions, by establishing that the generated sequence asymptotically
approximates the trajectories of its associated differential inclusion.
Furthermore, we establish that our proposed framework encompasses a wide range
of existing efficient decentralized subgradient methods, including
decentralized stochastic subgradient descent (DSGD), DSGD with
gradient-tracking technique (DSGD-T), and DSGD with momentum (DSGDm). In
addition, we introduce SignSGD employing the sign map to regularize the update
directions in DSGDm, and show it is enclosed in our proposed framework.
Consequently, our convergence results establish, for the first time, global
convergence of these methods when applied to nonsmooth nonconvex objectives.
Preliminary numerical experiments demonstrate that our proposed framework
yields highly efficient decentralized subgradient methods with convergence
guarantees in the training of nonsmooth neural networks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Advancing Neuromorphic Computing: Mixed-Signal Design Techniques
  Leveraging Brain Code Units and Fundamental Code Units 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11563v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11563v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Murat Isik, Sols Miziev, Wiktoria Pawlak, Newton Howard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a groundbreaking digital neuromorphic architecture that
innovatively integrates Brain Code Unit (BCU) and Fundamental Code Unit (FCU)
using mixedsignal design methodologies. Leveraging open-source datasets and the
latest advances in materials science, our research focuses on enhancing the
computational efficiency, accuracy, and adaptability of neuromorphic systems.
The core of our approach lies in harmonizing the precision and scalability of
digital systems with the robustness and energy efficiency of analog processing.
Through experimentation, we demonstrate the effectiveness of our system across
various metrics. The BCU achieved an accuracy of 88.0% and a power efficiency
of 20.0 GOP/s/W, while the FCU recorded an accuracy of 86.5% and a power
efficiency of 18.5 GOP/s/W. Our mixed-signal design approach significantly
improved latency and throughput, achieving a latency as low as 0.75 ms and
throughput up to 213 TOP/s. These results firmly establish the potential of our
architecture in neuromorphic computing, providing a solid foundation for future
developments in this domain. Our study underscores the feasibility of
mixedsignal neuromorphic systems and their promise in advancing the field,
particularly in applications requiring high efficiency and adaptability
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at 2024 International Joint Conference on Neural Networks</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RL en Markov Games with Independent Function Approximation: Improved
  Sample Complexity Bound under the Local Access Model <span class="chip">AISTATS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11544v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11544v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junyi Fan, Yuxuan Han, Jialin Zeng, Jian-Feng Cai, Yang Wang, Yang Xiang, Jiheng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficiently learning equilibria with large state and action spaces in
general-sum Markov games while overcoming the curse of multi-agency is a
challenging problem. Recent works have attempted to solve this problem by
employing independent linear function classes to approximate the marginal
$Q$-value for each agent. However, existing sample complexity bounds under such
a framework have a suboptimal dependency on the desired accuracy $\varepsilon$
or the action space. In this work, we introduce a new algorithm,
Lin-Confident-FTRL, for learning coarse correlated equilibria (CCE) with local
access to the simulator, i.e., one can interact with the underlying environment
on the visited states. Up to a logarithmic dependence on the size of the state
space, Lin-Confident-FTRL learns $\epsilon$-CCE with a provable optimal
accuracy bound $O(\epsilon^{-2})$ and gets rids of the linear dependency on the
action space, while scaling polynomially with relevant problem parameters (such
as the number of agents and time horizon). Moreover, our analysis of
Linear-Confident-FTRL generalizes the virtual policy iteration technique in the
single-agent local planning literature, which yields a new computationally
efficient algorithm with a tighter sample complexity bound when assuming random
access to the simulator.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the 27th International Conference on Artificial
  Intelligence and Statistics (AISTATS 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semantic <span class="highlight-title">Prompt</span>ing with Image-Token for Continual Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11537v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11537v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jisu Han, Jaemin Na, Wonjun Hwang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Continual learning aims to refine model parameters for new tasks while
retaining knowledge from previous tasks. Recently, prompt-based learning has
emerged to leverage pre-trained models to be prompted to learn subsequent tasks
without the reliance on the rehearsal buffer. Although this approach has
demonstrated outstanding results, existing methods depend on preceding
task-selection process to choose appropriate prompts. However, imperfectness in
task-selection may lead to negative impacts on the performance particularly in
the scenarios where the number of tasks is large or task distributions are
imbalanced. To address this issue, we introduce I-Prompt, a task-agnostic
approach focuses on the visual semantic information of image tokens to
eliminate task prediction. Our method consists of semantic prompt matching,
which determines prompts based on similarities between tokens, and image
token-level prompting, which applies prompts directly to image tokens in the
intermediate layers. Consequently, our method achieves competitive performance
on four benchmarks while significantly reducing training time compared to
state-of-the-art methods. Moreover, we demonstrate the superiority of our
method across various scenarios through extensive experiments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OCR is All you need: Importing Multi-Modality into Image-based Defect
  Detection System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11536v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11536v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chih-Chung Hsu, Chia-Ming Lee, Chun-Hung Sun, Kuang-Ming Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic optical inspection (AOI) plays a pivotal role in the manufacturing
process, predominantly leveraging high-resolution imaging instruments for
scanning purposes. It detects anomalies by analyzing image textures or
patterns, making it an essential tool in industrial manufacturing and quality
control. Despite its importance, the deployment of models for AOI often faces
challenges. These include limited sample sizes, which hinder effective feature
learning, variations among source domains, and sensitivities to changes in
lighting and camera positions during imaging. These factors collectively
compromise the accuracy of model predictions. Traditional AOI often fails to
capitalize on the rich mechanism-parameter information from machines or inside
images, including statistical parameters, which typically benefit AOI
classification. To address this, we introduce an external modality-guided data
mining framework, primarily rooted in optical character recognition (OCR), to
extract statistical features from images as a second modality to enhance
performance, termed OANet (Ocr-Aoi-Net). A key aspect of our approach is the
alignment of external modality features, extracted using a single
modality-aware model, with image features encoded by a convolutional neural
network. This synergy enables a more refined fusion of semantic representations
from different modalities. We further introduce feature refinement and a gating
function in our OANet to optimize the combination of these features, enhancing
inference and decision-making capabilities. Experimental outcomes show that our
methodology considerably boosts the recall rate of the defect detection model
and maintains high robustness even in challenging scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Out-of-Distribution Detection Should Use Conformal Prediction (and
  Vice-versa?) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11532v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11532v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paul Novello, Joseba Dalmau, Léo Andeol
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Research on Out-Of-Distribution (OOD) detection focuses mainly on building
scores that efficiently distinguish OOD data from In Distribution (ID) data. On
the other hand, Conformal Prediction (CP) uses non-conformity scores to
construct prediction sets with probabilistic coverage guarantees. In this work,
we propose to use CP to better assess the efficiency of OOD scores.
Specifically, we emphasize that in standard OOD benchmark settings, evaluation
metrics can be overly optimistic due to the finite sample size of the test
dataset. Based on the work of (Bates et al., 2022), we define new conformal
AUROC and conformal FRP@TPR95 metrics, which are corrections that provide
probabilistic conservativeness guarantees on the variability of these metrics.
We show the effect of these corrections on two reference OOD and anomaly
detection benchmarks, OpenOOD (Yang et al., 2022) and ADBench (Han et al.,
2022). We also show that the benefits of using OOD together with CP apply the
other way around by using OOD scores as non-conformity scores, which results in
improving upon current CP methods. One of the key messages of these
contributions is that since OOD is concerned with designing scores and CP with
interpreting these scores, the two fields may be inherently intertwined.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LOOPer: A Learned Automatic Code Optimizer For Polyhedral Compilers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11522v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11522v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Massinissa Merouani, Khaled Afif Boudaoud, Iheb Nassim Aouadj, Nassim Tchoulak, Islam Kara Bernou, Hamza Benyamina, Fatima Benbouzid-Si Tayeb, Karima Benatchba, Hugh Leather, Riyadh Baghdadi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While polyhedral compilers have shown success in implementing advanced code
transformations, they still have challenges in selecting the most profitable
transformations that lead to the best speedups. This has motivated the use of
machine learning to build cost models to guide the search for polyhedral
optimizations. State-of-the-art polyhedral compilers have demonstrated a viable
proof-of-concept of this approach. While such a proof-of-concept has shown
promise, it still has significant limitations. State-of-the-art polyhedral
compilers that use a deep-learning cost model only support a small subset of
affine transformations, limiting their ability to apply complex code
transformations. They also only support simple programs that have a single loop
nest and a rectangular iteration domain, limiting their applicability to many
programs. These limitations significantly impact the generality of such
compilers and autoschedulers and put into question the whole approach. In this
paper, we introduce LOOPer, the first polyhedral autoscheduler that uses a
deep-learning based cost model and covers a large set of affine transformations
and programs. It supports the exploration of a large set of affine
transformations, allowing the application of complex sequences of polyhedral
transformations. It also supports the optimization of programs with multiple
loop nests and with rectangular and non-rectangular iteration domains, allowing
the optimization of an extensive set of programs. We implement and evaluate
LOOPer and show that it achieves speedups over the state-of-the-art. On the
Polybench benchmark, LOOPer achieves a geometric mean speedup of 1.59x over
Tiramisu. LOOPer also achieves competitive speedups with a geometric mean
speedup of 1.34x over Pluto, a state-of-the-art polyhedral compiler that does
not use a machine-learning based cost model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Data-driven Approach for Rapid Detection of Aeroelastic Modes from
  Flutter Flight Test Based on Limited Sensor Measurements 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11521v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11521v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arpan Das, Pier Marzocca, Giuliano Coppotelli, Oleg Levinski, Paul Taylor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Flutter flight test involves the evaluation of the airframes aeroelastic
stability by applying artificial excitation on the aircraft lifting surfaces.
The subsequent responses are captured and analyzed to extract the frequencies
and damping characteristics of the system. However, noise contamination,
turbulence, non-optimal excitation of modes, and sensor malfunction in one or
more sensors make it time-consuming and corrupt the extraction process. In
order to expedite the process of identifying and analyzing aeroelastic modes,
this study implements a time-delay embedded Dynamic Mode Decomposition
technique. This approach is complemented by Robust Principal Component Analysis
methodology, and a sparsity promoting criterion which enables the automatic and
optimal selection of sparse modes. The anonymized flutter flight test data,
provided by the fifth author of this research paper, is utilized in this
implementation. The methodology assumes no knowledge of the input excitation,
only deals with the responses captured by accelerometer channels, and rapidly
identifies the aeroelastic modes. By incorporating a compressed sensing
algorithm, the methodology gains the ability to identify aeroelastic modes,
even when the number of available sensors is limited. This augmentation greatly
enhances the methodology's robustness and effectiveness, making it an excellent
choice for real-time implementation during flutter test campaigns.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages, 12 figures, submitted in 'Mechanical Systems and Signal
  processing' journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ State-Separated SARSA: A Practical Sequential Decision-Making Algorithm
  with Recovering Rewards 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11520v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11520v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuto Tanimoto, Kenji Fukumizu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While many multi-armed bandit algorithms assume that rewards for all arms are
constant across rounds, this assumption does not hold in many real-world
scenarios. This paper considers the setting of recovering bandits (Pike-Burke &
Grunewalder, 2019), where the reward depends on the number of rounds elapsed
since the last time an arm was pulled. We propose a new reinforcement learning
(RL) algorithm tailored to this setting, named the State-Separate SARSA
(SS-SARSA) algorithm, which treats rounds as states. The SS-SARSA algorithm
achieves efficient learning by reducing the number of state combinations
required for Q-learning/SARSA, which often suffers from combinatorial issues
for large-scale RL problems. Additionally, it makes minimal assumptions about
the reward structure and offers lower computational complexity. Furthermore, we
prove asymptotic convergence to an optimal policy under mild assumptions.
Simulation studies demonstrate the superior performance of our algorithm across
various settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Temporal Bias Correction using a Machine Learning Attention model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.14169v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.14169v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Omer Nivron, Damon J. Wischik, Mathieu Vrac, Emily Shuckburgh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Climate models are biased with respect to real world observations and usually
need to be calibrated prior to impact studies. The suite of statistical methods
that enable such calibrations is called bias correction (BC). However, current
BC methods struggle to adjust for temporal biases, because they disregard the
dependence between consecutive time-points. As a result, climate statistics
with long-range temporal properties, such as heatwave duration and frequency,
cannot be corrected accurately, making it more difficult to produce reliable
impact studies on such climate statistics. In this paper, we offer a novel BC
methodology to correct for temporal biases. This is made possible by i)
re-thinking BC as a probability model rather than an algorithmic procedure, and
ii) adapting state-of-the-art machine-learning (ML) probabilistic attention
models to fit the BC task. With a case study of heatwave duration statistics in
Abuja, Nigeria, and Tokyo, Japan, we show striking results compared to current
climate model outputs and alternative BC methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 15 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Truly No-Regret Learning in Constrained MDPs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.15776v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.15776v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adrian Müller, Pragnya Alatur, Volkan Cevher, Giorgia Ramponi, Niao He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Constrained Markov decision processes (CMDPs) are a common way to model
safety constraints in reinforcement learning. State-of-the-art methods for
efficiently solving CMDPs are based on primal-dual algorithms. For these
algorithms, all currently known regret bounds allow for error cancellations --
one can compensate for a constraint violation in one round with a strict
constraint satisfaction in another. This makes the online learning process
unsafe since it only guarantees safety for the final (mixture) policy but not
during learning. As Efroni et al. (2020) pointed out, it is an open question
whether primal-dual algorithms can provably achieve sublinear regret if we do
not allow error cancellations. In this paper, we give the first affirmative
answer. We first generalize a result on last-iterate convergence of regularized
primal-dual schemes to CMDPs with multiple constraints. Building upon this
insight, we propose a model-based primal-dual algorithm to learn in an unknown
CMDP. We prove that our algorithm achieves sublinear regret without error
cancellations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Stop Reasoning! When Multimodal LLMs with Chain-of-Thought Reasoning
  Meets Adversarial Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.14899v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.14899v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zefeng Wang, Zhen Han, Shuo Chen, Fan Xue, Zifeng Ding, Xun Xiao, Volker Tresp, Philip Torr, Jindong Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, Multimodal LLMs (MLLMs) have shown a great ability to understand
images. However, like traditional vision models, they are still vulnerable to
adversarial images. Meanwhile, Chain-of-Thought (CoT) reasoning has been widely
explored on MLLMs, which not only improves model's performance, but also
enhances model's explainability by giving intermediate reasoning steps.
Nevertheless, there is still a lack of study regarding MLLMs' adversarial
robustness with CoT and an understanding of what the rationale looks like when
MLLMs infer wrong answers with adversarial images. Our research evaluates the
adversarial robustness of MLLMs when employing CoT reasoning, finding that CoT
marginally improves adversarial robustness against existing attack methods.
Moreover, we introduce a novel stop-reasoning attack technique that effectively
bypasses the CoT-induced robustness enhancements. Finally, we demonstrate the
alterations in CoT reasoning when MLLMs confront adversarial images, shedding
light on their reasoning process under adversarial attacks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Identifying Three-Dimensional Radiative Patterns Associated with Early
  Tropical Cyclone Intensification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.09493v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.09493v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Frederick Iat-Hin Tam, Tom Beucler, James H. Ruppert Jr
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cloud radiative feedback impacts early tropical cyclone (TC) intensification,
but limitations in existing diagnostic frameworks make them unsuitable for
studying asymmetric or transient radiative heating. We propose a linear
Variational Encoder-Decoder (VED) to learn the hidden relationship between
radiation and the surface intensification of realistic simulated TCs. Limiting
VED model inputs enables using its uncertainty to identify periods when
radiation has more importance for intensification. A close examination of the
extracted 3D radiative structures suggests that longwave radiative forcing from
inner core deep convection and shallow clouds both contribute to
intensification, with the deep convection having the most impact overall. We
find that deep convection downwind of the shallow clouds is critical to the
intensification of Haiyan. Our work demonstrates that machine learning can
discover thermodynamic-kinematic relationships without relying on axisymmetric
or deterministic assumptions, paving the way towards the objective discovery of
processes leading to TC intensification in realistic conditions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 6 figures (main text)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unconstrained Stochastic CCA: Unifying Multiview and <span class="highlight-title">Self-Supervised</span>
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.01012v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.01012v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        James Chapman, Lennie Wells, Ana Lawry Aguila
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Canonical Correlation Analysis (CCA) family of methods is foundational in
multiview learning. Regularised linear CCA methods can be seen to generalise
Partial Least Squares (PLS) and be unified with a Generalized Eigenvalue
Problem (GEP) framework. However, classical algorithms for these linear methods
are computationally infeasible for large-scale data. Extensions to Deep CCA
show great promise, but current training procedures are slow and complicated.
First we propose a novel unconstrained objective that characterizes the top
subspace of GEPs. Our core contribution is a family of fast algorithms for
stochastic PLS, stochastic CCA, and Deep CCA, simply obtained by applying
stochastic gradient descent (SGD) to the corresponding CCA objectives. Our
algorithms show far faster convergence and recover higher correlations than the
previous state-of-the-art on all standard CCA and Deep CCA benchmarks. These
improvements allow us to perform a first-of-its-kind PLS analysis of an
extremely large biomedical dataset from the UK Biobank, with over 33,000
individuals and 500,000 features. Finally, we apply our algorithms to match the
performance of `CCA-family' Self-Supervised Learning (SSL) methods on CIFAR-10
and CIFAR-100 with minimal hyper-parameter tuning, and also present theory to
clarify the links between these methods and classical CCA, laying the
groundwork for future insights.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Comprehensive Evaluation of Augmentations for Robust OOD
  <span class="highlight-title">Self-Supervised</span> Contrastive Phonocardiogram Representation Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.00502v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.00502v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aristotelis Ballas, Vasileios Papapanagiotou, Christos Diou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the recent increase in research activity, deep-learning models have
not yet been widely accepted in several real-world settings, such as medicine.
The shortage of high-quality annotated data often hinders the development of
robust and generalizable models, which do not suffer from degraded
effectiveness when presented with newly-collected, out-of-distribution (OOD)
datasets. Contrastive Self-Supervised Learning (SSL) offers a potential
solution to labeled data scarcity, as it takes advantage of unlabeled data to
increase model effectiveness and robustness. In this research, we propose
applying contrastive SSL for detecting abnormalities in 1D phonocardiogram
(PCG) samples by learning a generalized representation of the signal.
Specifically, we perform an extensive comparative evaluation of a wide range of
audio-based augmentations, evaluate trained classifiers on multiple datasets
across different downstream tasks, and finally report on the impact of each
augmentation in model training. We experimentally demonstrate that, depending
on its training distribution, the effectiveness of a fully-supervised model can
degrade up to 32% when evaluated on unseen data, while SSL models only lose up
to 10% or even improve in some cases. We argue and experimentally demonstrate
that, contrastive SSL pretraining can assist in providing robust classifiers
which can generalize to unseen, OOD data, without relying on time- and
labor-intensive annotation processes by medical experts. Furthermore, the
proposed extensive evaluation protocol sheds light on the most promising and
appropriate augmentations for robust PCG signal processing, by calculating
their effect size on model training. Finally, we provide researchers and
practitioners with a roadmap towards producing robust models for PCG
classification, in addition to an open-source codebase for developing novel
approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>PREPRINT Manuscript under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Properties of Discrete Sliced Wasserstein Losses 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.10352v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.10352v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eloi Tanguy, Rémi Flamary, Julie Delon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Sliced Wasserstein (SW) distance has become a popular alternative to the
Wasserstein distance for comparing probability measures. Widespread
applications include image processing, domain adaptation and generative
modelling, where it is common to optimise some parameters in order to minimise
SW, which serves as a loss function between discrete probability measures
(since measures admitting densities are numerically unattainable). All these
optimisation problems bear the same sub-problem, which is minimising the Sliced
Wasserstein energy. In this paper we study the properties of $\mathcal{E}: Y
\longmapsto \mathrm{SW}_2^2(\gamma_Y, \gamma_Z)$, i.e. the SW distance between
two uniform discrete measures with the same amount of points as a function of
the support $Y \in \mathbb{R}^{n \times d}$ of one of the measures. We
investigate the regularity and optimisation properties of this energy, as well
as its Monte-Carlo approximation $\mathcal{E}_p$ (estimating the expectation in
SW using only $p$ samples) and show convergence results on the critical points
of $\mathcal{E}_p$ to those of $\mathcal{E}$, as well as an almost-sure uniform
convergence. Finally, we show that in a certain sense, Stochastic Gradient
Descent methods minimising $\mathcal{E}$ and $\mathcal{E}_p$ converge towards
(Clarke) critical points of these energies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Convergence of SGD for Training Neural Networks with Sliced Wasserstein
  Losses 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.11714v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.11714v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eloi Tanguy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Optimal Transport has sparked vivid interest in recent years, in particular
thanks to the Wasserstein distance, which provides a geometrically sensible and
intuitive way of comparing probability measures. For computational reasons, the
Sliced Wasserstein (SW) distance was introduced as an alternative to the
Wasserstein distance, and has seen uses for training generative Neural Networks
(NNs). While convergence of Stochastic Gradient Descent (SGD) has been observed
practically in such a setting, there is to our knowledge no theoretical
guarantee for this observation. Leveraging recent works on convergence of SGD
on non-smooth and non-convex functions by Bianchi et al. (2022), we aim to
bridge that knowledge gap, and provide a realistic context under which
fixed-step SGD trajectories for the SW loss on NN parameters converge. More
precisely, we show that the trajectories approach the set of (sub)-gradient
flow equations as the step decreases. Under stricter assumptions, we show a
much stronger convergence result for noised and projected SGD schemes, namely
that the long-run limits of the trajectories approach a set of generalised
critical points of the loss function.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Identifying Policy Gradient Subspaces <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.06604v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.06604v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jan Schneider, Pierre Schumacher, Simon Guist, Le Chen, Daniel Häufle, Bernhard Schölkopf, Dieter Büchler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Policy gradient methods hold great potential for solving complex continuous
control tasks. Still, their training efficiency can be improved by exploiting
structure within the optimization problem. Recent work indicates that
supervised learning can be accelerated by leveraging the fact that gradients
lie in a low-dimensional and slowly-changing subspace. In this paper, we
conduct a thorough evaluation of this phenomenon for two popular deep policy
gradient methods on various simulated benchmark tasks. Our results demonstrate
the existence of such gradient subspaces despite the continuously changing data
distribution inherent to reinforcement learning. These findings reveal
promising directions for future work on more efficient reinforcement learning,
e.g., through improving parameter-space exploration or enabling second-order
optimization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as conference paper at ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EcoVal: An Efficient Data Valuation Framework for Machine Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.09288v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.09288v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ayush K Tarun, Vikram S Chundawat, Murari Mandal, Hong Ming Tan, Bowei Chen, Mohan Kankanhalli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantifying the value of data within a machine learning workflow can play a
pivotal role in making more strategic decisions in machine learning
initiatives. The existing Shapley value based frameworks for data valuation in
machine learning are computationally expensive as they require considerable
amount of repeated training of the model to obtain the Shapley value. In this
paper, we introduce an efficient data valuation framework EcoVal, to estimate
the value of data for machine learning models in a fast and practical manner.
Instead of directly working with individual data sample, we determine the value
of a cluster of similar data points. This value is further propagated amongst
all the member cluster points. We show that the overall data value can be
determined by estimating the intrinsic and extrinsic value of each data. This
is enabled by formulating the performance of a model as a \textit{production
function}, a concept which is popularly used to estimate the amount of output
based on factors like labor and capital in a traditional free economic market.
We provide a formal proof of our valuation technique and elucidate the
principles and mechanisms that enable its accelerated performance. We
demonstrate the real-world applicability of our method by showcasing its
effectiveness for both in-distribution and out-of-sample data. This work
addresses one of the core challenges of efficient data valuation at scale in
machine learning models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unveiling the Significance of Toddler-Inspired Reward Transition in
  Goal-Oriented Reinforcement Learning <span class="chip">AAAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06880v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06880v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junseok Park, Yoonsung Kim, Hee Bin Yoo, Min Whoo Lee, Kibeom Kim, Won-Seok Choi, Minsu Lee, Byoung-Tak Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Toddlers evolve from free exploration with sparse feedback to exploiting
prior experiences for goal-directed learning with denser rewards. Drawing
inspiration from this Toddler-Inspired Reward Transition, we set out to explore
the implications of varying reward transitions when incorporated into
Reinforcement Learning (RL) tasks. Central to our inquiry is the transition
from sparse to potential-based dense rewards, which share optimal strategies
regardless of reward changes. Through various experiments, including those in
egocentric navigation and robotic arm manipulation tasks, we found that proper
reward transitions significantly influence sample efficiency and success rates.
Of particular note is the efficacy of the toddler-inspired Sparse-to-Dense
(S2D) transition. Beyond these performance metrics, using Cross-Density
Visualizer technique, we observed that transitions, especially the S2D, smooth
the policy loss landscape, promoting wide minima that enhance generalization in
RL models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as a full paper at AAAI 2024 (Oral presentation): 7 pages
  (main paper), 2 pages (references), 17 pages (appendix) each</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Ricci flow-based brain surface covariance descriptors for diagnosing
  Alzheimer's disease 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06645v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06645v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fatemeh Ahmadi, Mohamad Ebrahim Shiri, Behroz Bidabad, Maral Sedaghat, Pooran Memari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automated feature extraction from MRI brain scans and diagnosis of
Alzheimer's disease are ongoing challenges. With advances in 3D imaging
technology, 3D data acquisition is becoming more viable and efficient than its
2D counterpart. Rather than using feature-based vectors, in this paper, for the
first time, we suggest a pipeline to extract novel covariance-based descriptors
from the cortical surface using the Ricci energy optimization. The covariance
descriptors are components of the nonlinear manifold of symmetric
positive-definite matrices, thus we focus on using the Gaussian radial basis
function to apply manifold-based classification to the 3D shape problem.
Applying this novel signature to the analysis of abnormal cortical brain
morphometry allows for diagnosing Alzheimer's disease. Experimental studies
performed on about two hundred 3D MRI brain models, gathered from Alzheimer's
Disease Neuroimaging Initiative (ADNI) dataset demonstrate the effectiveness of
our descriptors in achieving remarkable classification accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in Biomedical Signal Processing and Control
  journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bongard-OpenWorld: Few-Shot Reasoning for Free-form Visual Concepts in
  the Real World <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.10207v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.10207v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rujie Wu, Xiaojian Ma, Zhenliang Zhang, Wei Wang, Qing Li, Song-Chun Zhu, Yizhou Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Bongard-OpenWorld, a new benchmark for evaluating real-world
few-shot reasoning for machine vision. It originates from the classical Bongard
Problems (BPs): Given two sets of images (positive and negative), the model
needs to identify the set that query images belong to by inducing the visual
concepts, which is exclusively depicted by images from the positive set. Our
benchmark inherits the few-shot concept induction of the original BPs while
adding the two novel layers of challenge: 1) open-world free-form concepts, as
the visual concepts in Bongard-OpenWorld are unique compositions of terms from
an open vocabulary, ranging from object categories to abstract visual
attributes and commonsense factual knowledge; 2) real-world images, as opposed
to the synthetic diagrams used by many counterparts. In our exploration,
Bongard-OpenWorld already imposes a significant challenge to current few-shot
reasoning algorithms. We further investigate to which extent the recently
introduced Large Language Models (LLMs) and Vision-Language Models (VLMs) can
solve our task, by directly probing VLMs, and combining VLMs and LLMs in an
interactive reasoning scheme. We even conceived a neuro-symbolic reasoning
approach that reconciles LLMs & VLMs with logical reasoning to emulate the
human problem-solving process for Bongard Problems. However, none of these
approaches manage to close the human-machine gap, as the best learner achieves
64% accuracy while human participants easily reach 91%. We hope
Bongard-OpenWorld can help us better understand the limitations of current
visual intelligence and facilitate future research on visual agents with
stronger few-shot visual reasoning capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Expressive Power of Spectral Graph Neural Networks with
  Eigenvalue Correction <span class="chip">AAAI-24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.15603v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.15603v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kangkang Lu, Yanhua Yu, Hao Fei, Xuan Li, Zixuan Yang, Zirui Guo, Meiyu Liang, Mengran Yin, Tat-Seng Chua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, spectral graph neural networks, characterized by polynomial
filters, have garnered increasing attention and have achieved remarkable
performance in tasks such as node classification. These models typically assume
that eigenvalues for the normalized Laplacian matrix are distinct from each
other, thus expecting a polynomial filter to have a high fitting ability.
However, this paper empirically observes that normalized Laplacian matrices
frequently possess repeated eigenvalues. Moreover, we theoretically establish
that the number of distinguishable eigenvalues plays a pivotal role in
determining the expressive power of spectral graph neural networks. In light of
this observation, we propose an eigenvalue correction strategy that can free
polynomial filters from the constraints of repeated eigenvalue inputs.
Concretely, the proposed eigenvalue correction strategy enhances the uniform
distribution of eigenvalues, thus mitigating repeated eigenvalues, and
improving the fitting capacity and expressive power of polynomial filters.
Extensive experimental results on both synthetic and real-world datasets
demonstrate the superiority of our method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI-24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multimodal Pathway: Improve <span class="highlight-title">Transformer</span>s with Irrelevant Data from Other
  Modalities <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.14405v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.14405v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiyuan Zhang, Xiaohan Ding, Kaixiong Gong, Yixiao Ge, Ying Shan, Xiangyu Yue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose to improve transformers of a specific modality with irrelevant
data from other modalities, e.g., improve an ImageNet model with audio or point
cloud datasets. We would like to highlight that the data samples of the target
modality are irrelevant to the other modalities, which distinguishes our method
from other works utilizing paired (e.g., CLIP) or interleaved data of different
modalities. We propose a methodology named Multimodal Pathway - given a target
modality and a transformer designed for it, we use an auxiliary transformer
trained with data of another modality and construct pathways to connect
components of the two models so that data of the target modality can be
processed by both models. In this way, we utilize the universal
sequence-to-sequence modeling abilities of transformers obtained from two
modalities. As a concrete implementation, we use a modality-specific tokenizer
and task-specific head as usual but utilize the transformer blocks of the
auxiliary model via a proposed method named Cross-Modal Re-parameterization,
which exploits the auxiliary weights without any inference costs. On the image,
point cloud, video, and audio recognition tasks, we observe significant and
consistent performance improvements with irrelevant data from other modalities.
The code and models are available at https://github.com/AILab-CVC/M2PT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024. Code and models are available at
  https://github.com/AILab-CVC/M2PT</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Lipschitz-Variance-Margin Tradeoff for Enhanced Randomized Smoothing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.16883v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.16883v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Blaise Delattre, Alexandre Araujo, Quentin Barthélemy, Alexandre Allauzen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-life applications of deep neural networks are hindered by their unsteady
predictions when faced with noisy inputs and adversarial attacks. The certified
radius in this context is a crucial indicator of the robustness of models.
However how to design an efficient classifier with an associated certified
radius? Randomized smoothing provides a promising framework by relying on noise
injection into the inputs to obtain a smoothed and robust classifier. In this
paper, we first show that the variance introduced by the Monte-Carlo sampling
in the randomized smoothing procedure estimate closely interacts with two other
important properties of the classifier, \textit{i.e.} its Lipschitz constant
and margin. More precisely, our work emphasizes the dual impact of the
Lipschitz constant of the base classifier, on both the smoothed classifier and
the empirical variance. To increase the certified robust radius, we introduce a
different way to convert logits to probability vectors for the base classifier
to leverage the variance-margin trade-off. We leverage the use of Bernstein's
concentration inequality along with enhanced Lipschitz bounds for randomized
smoothing. Experimental results show a significant improvement in certified
accuracy compared to current state-of-the-art methods. Our novel certification
procedure allows us to use pre-trained models with randomized smoothing,
effectively improving the current certification radius in a zero-shot manner.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UniRepLKNet: A Universal Perception Large-Kernel ConvNet for Audio,
  Video, Point Cloud, Time-Series and Image Recognition <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.15599v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.15599v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaohan Ding, Yiyuan Zhang, Yixiao Ge, Sijie Zhao, Lin Song, Xiangyu Yue, Ying Shan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-kernel convolutional neural networks (ConvNets) have recently received
extensive research attention, but two unresolved and critical issues demand
further investigation. 1) The architectures of existing large-kernel ConvNets
largely follow the design principles of conventional ConvNets or transformers,
while the architectural design for large-kernel ConvNets remains
under-addressed. 2) As transformers have dominated multiple modalities, it
remains to be investigated whether ConvNets also have a strong universal
perception ability in domains beyond vision. In this paper, we contribute from
two aspects. 1) We propose four architectural guidelines for designing
large-kernel ConvNets, the core of which is to exploit the essential
characteristics of large kernels that distinguish them from small kernels -
they can see wide without going deep. Following such guidelines, our proposed
large-kernel ConvNet shows leading performance in image recognition (ImageNet
accuracy of 88.0%, ADE20K mIoU of 55.6%, and COCO box AP of 56.4%),
demonstrating better performance and higher speed than the recent powerful
competitors. 2) We discover large kernels are the key to unlocking the
exceptional performance of ConvNets in domains where they were originally not
proficient. With certain modality-related preprocessing approaches, the
proposed model achieves state-of-the-art performance on time-series forecasting
and audio recognition tasks even without modality-specific customization to the
architecture. All the code and models are publicly available on GitHub and
Huggingface.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024. Code, all the models, reproducible training scripts at
  https://github.com/AILab-CVC/UniRepLKNet</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CRISP: Hybrid Structured Sparsity for Class-aware Model Pruning <span class="chip">DATE</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.14272v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.14272v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shivam Aggarwal, Kuluhan Binici, Tulika Mitra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning pipelines for classification tasks often train a universal
model to achieve accuracy across a broad range of classes. However, a typical
user encounters only a limited selection of classes regularly. This disparity
provides an opportunity to enhance computational efficiency by tailoring models
to focus on user-specific classes. Existing works rely on unstructured pruning,
which introduces randomly distributed non-zero values in the model, making it
unsuitable for hardware acceleration. Alternatively, some approaches employ
structured pruning, such as channel pruning, but these tend to provide only
minimal compression and may lead to reduced model accuracy. In this work, we
propose CRISP, a novel pruning framework leveraging a hybrid structured
sparsity pattern that combines both fine-grained N:M structured sparsity and
coarse-grained block sparsity. Our pruning strategy is guided by a
gradient-based class-aware saliency score, allowing us to retain weights
crucial for user-specific classes. CRISP achieves high accuracy with minimal
memory consumption for popular models like ResNet-50, VGG-16, and MobileNetV2
on ImageNet and CIFAR-100 datasets. Moreover, CRISP delivers up to 14$\times$
reduction in latency and energy consumption compared to existing pruning
methods while maintaining comparable accuracy. Our code is available at
https://github.com/shivmgg/CRISP/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, accepted in Design, Automation & Test in Europe Conference &
  Exhibition (DATE) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Energy-guided Entropic Neural Optimal Transport 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.06094v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.06094v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Petr Mokrov, Alexander Korotin, Alexander Kolesov, Nikita Gushchin, Evgeny Burnaev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Energy-based models (EBMs) are known in the Machine Learning community for
decades. Since the seminal works devoted to EBMs dating back to the noughties,
there have been a lot of efficient methods which solve the generative modelling
problem by means of energy potentials (unnormalized likelihood functions). In
contrast, the realm of Optimal Transport (OT) and, in particular, neural OT
solvers is much less explored and limited by few recent works (excluding
WGAN-based approaches which utilize OT as a loss function and do not model OT
maps themselves). In our work, we bridge the gap between EBMs and
Entropy-regularized OT. We present a novel methodology which allows utilizing
the recent developments and technical improvements of the former in order to
enrich the latter. From the theoretical perspective, we prove generalization
bounds for our technique. In practice, we validate its applicability in toy 2D
and image domains. To showcase the scalability, we empower our method with a
pre-trained StyleGAN and apply it to high-res AFHQ $512\times 512$ unpaired I2I
translation. For simplicity, we choose simple short- and long-run EBMs as a
backbone of our Energy-guided Entropic OT approach, leaving the application of
more sophisticated EBMs for future research. Our code is available at:
https://github.com/PetrMokrov/Energy-guided-Entropic-OT
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Quiet-STaR: Language Models Can Teach Themselves to Think Before
  Speaking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09629v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09629v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eric Zelikman, Georges Harik, Yijia Shao, Varuna Jayasiri, Nick Haber, Noah D. Goodman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When writing and talking, people sometimes pause to think. Although
reasoning-focused works have often framed reasoning as a method of answering
questions or completing agentic tasks, reasoning is implicit in almost all
written text. For example, this applies to the steps not stated between the
lines of a proof or to the theory of mind underlying a conversation. In the
Self-Taught Reasoner (STaR, Zelikman et al. 2022), useful thinking is learned
by inferring rationales from few-shot examples in question-answering and
learning from those that lead to a correct answer. This is a highly constrained
setting -- ideally, a language model could instead learn to infer unstated
rationales in arbitrary text. We present Quiet-STaR, a generalization of STaR
in which LMs learn to generate rationales at each token to explain future text,
improving their predictions. We address key challenges, including 1) the
computational cost of generating continuations, 2) the fact that the LM does
not initially know how to generate or use internal thoughts, and 3) the need to
predict beyond individual next tokens. To resolve these, we propose a tokenwise
parallel sampling algorithm, using learnable tokens indicating a thought's
start and end, and an extended teacher-forcing technique. Encouragingly,
generated rationales disproportionately help model difficult-to-predict tokens
and improve the LM's ability to directly answer difficult questions. In
particular, after continued pretraining of an LM on a corpus of internet text
with Quiet-STaR, we find zero-shot improvements on GSM8K
(5.9%$\rightarrow$10.9%) and CommonsenseQA (36.3%$\rightarrow$47.2%) and
observe a perplexity improvement of difficult tokens in natural text.
Crucially, these improvements require no fine-tuning on these tasks. Quiet-STaR
marks a step towards LMs that can learn to reason in a more general and
scalable way.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Global $\mathcal{L}^2$ minimization at uniform exponential rate via
  geometrically adapted gradient descent in Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.15487v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.15487v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the gradient descent flow widely used for the minimization of the
$\mathcal{L}^2$ cost function in Deep Learning networks, and introduce two
modified versions; one adapted for the overparametrized setting, and the other
for the underparametrized setting. Both have a clear and natural invariant
geometric meaning, taking into account the pullback vector bundle structure in
the overparametrized, and the pushforward vector bundle structure in the
underparametrized setting. In the overparametrized case, we prove that,
provided that a rank condition holds, all orbits of the modified gradient
descent drive the $\mathcal{L}^2$ cost to its global minimum at a uniform
exponential convergence rate; one thereby obtains an a priori stopping time for
any prescribed proximity to the global minimum. We point out relations of the
latter to sub-Riemannian geometry.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AMS Latex, 16 pages. Section 2.1 on rank condition, and Section 2.4
  on the trapping of orbits in the standard gradient descent flow added. Title
  changed</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Flooding Regularization for Stable Training of Generative Adversarial
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.00318v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.00318v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Iu Yahiro, Takashi Ishida, Naoto Yokoya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative Adversarial Networks (GANs) have shown remarkable performance in
image generation. However, GAN training suffers from the problem of
instability. One of the main approaches to address this problem is to modify
the loss function, often using regularization terms in addition to changing the
type of adversarial losses. This paper focuses on directly regularizing the
adversarial loss function. We propose a method that applies flooding, an
overfitting suppression method in supervised learning, to GANs to directly
prevent the discriminator's loss from becoming excessively low. Flooding
requires tuning the flood level, but when applied to GANs, we propose that the
appropriate range of flood level settings is determined by the adversarial loss
function, supported by theoretical analysis of GANs using the binary cross
entropy loss. We experimentally verify that flooding stabilizes GAN training
and can be combined with other stabilization techniques. We also show that by
restricting the discriminator's loss to be no less than the flood level, the
training proceeds stably even when the flood level is somewhat high.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 9 figures, 18 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Effectiveness Assessment of Recent Large Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.04306v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.04306v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yao Jiang, Xinyu Yan, Ge-Peng Ji, Keren Fu, Meijun Sun, Huan Xiong, Deng-Ping Fan, Fahad Shahbaz Khan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of large vision-language models (LVLMs) represents a noteworthy
advancement towards the pursuit of artificial general intelligence. However,
the extent of their efficacy across both specialized and general tasks warrants
further investigation. This article endeavors to evaluate the competency of
popular LVLMs in specialized and general tasks, respectively, aiming to offer a
comprehensive comprehension of these innovative methodologies. To gauge their
efficacy in specialized tasks, we tailor a comprehensive testbed comprising
three distinct scenarios: natural, healthcare, and industrial, encompassing six
challenging tasks. These tasks include salient, camouflaged, and transparent
object detection, as well as polyp and skin lesion detection, alongside
industrial anomaly detection. We examine the performance of three recent
open-source LVLMs -- MiniGPT-v2, LLaVA-1.5, and Shikra -- in the realm of
visual recognition and localization. Moreover, we conduct empirical
investigations utilizing the aforementioned models alongside GPT-4V, assessing
their multi-modal understanding capacities in general tasks such as object
counting, absurd question answering, affordance reasoning, attribute
recognition, and spatial relation reasoning. Our investigations reveal that
these models demonstrate limited proficiency not only in specialized tasks but
also in general tasks. We delve deeper into this inadequacy and suggest several
potential factors, including limited cognition in specialized tasks, object
hallucination, text-to-image interference, and decreased robustness in complex
problems. We hope this study would provide valuable insights for the future
development of LVLMs, augmenting their power in coping with both general and
specialized applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Continuous Domain Adaptation with Multi-Path Transfer
  Curriculum 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16681v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16681v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanbing Liu, Jingge Wang, Xuan Zhang, Ye Guo, Yang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Addressing the large distribution gap between training and testing data has
long been a challenge in machine learning, giving rise to fields such as
transfer learning and domain adaptation. Recently, Continuous Domain Adaptation
(CDA) has emerged as an effective technique, closing this gap by utilizing a
series of intermediate domains. This paper contributes a novel CDA method,
W-MPOT, which rigorously addresses the domain ordering and error accumulation
problems overlooked by previous studies. Specifically, we construct a transfer
curriculum over the source and intermediate domains based on Wasserstein
distance, motivated by theoretical analysis of CDA. Then we transfer the source
model to the target domain through multiple valid paths in the curriculum using
a modified version of continuous optimal transport. A bidirectional path
consistency constraint is introduced to mitigate the impact of accumulated
mapping errors during continuous transfer. We extensively evaluate W-MPOT on
multiple datasets, achieving up to 54.1\% accuracy improvement on multi-session
Alzheimer MR image classification and 94.7\% MSE reduction on battery capacity
estimation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Words and Exercises to Wellness: Farsi Chatbot for Self-Attachment
  Technique 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.09362v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.09362v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sina Elahimanesh, Shayan Salehi, Sara Zahedi Movahed, Lisa Alazraki, Ruoyu Hu, Abbas Edalat
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the wake of the post-pandemic era, marked by social isolation and surging
rates of depression and anxiety, conversational agents based on digital
psychotherapy can play an influential role compared to traditional therapy
sessions. In this work, we develop a voice-capable chatbot in Farsi to guide
users through Self-Attachment (SAT), a novel, self-administered, holistic
psychological technique based on attachment theory. Our chatbot uses a dynamic
array of rule-based and classification-based modules to comprehend user input
throughout the conversation and navigates a dialogue flowchart accordingly,
recommending appropriate SAT exercises that depend on the user's emotional and
mental state. In particular, we collect a dataset of over 6,000 utterances and
develop a novel sentiment-analysis module that classifies user sentiment into
12 classes, with accuracy above 92%. To keep the conversation novel and
engaging, the chatbot's responses are retrieved from a large dataset of
utterances created with the aid of Farsi GPT-2 and a reinforcement learning
approach, thus requiring minimal human annotation. Our chatbot also offers a
question-answering module, called SAT Teacher, to answer users' questions about
the principles of Self-Attachment. Finally, we design a cross-platform
application as the bot's user interface. We evaluate our platform in a ten-day
human study with N=52 volunteers from the non-clinical population, who have had
over 2,000 dialogues in total with the chatbot. The results indicate that the
platform was engaging to most users (75%), 72% felt better after the
interactions, and 74% were satisfied with the SAT Teacher's performance.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Science and Game Theory
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cooperative Agri-Food Export under Minimum Quantity Commitments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11633v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11633v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luis A. Guardiola, Behzad Hezarkhani, Ana Meca
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  International trade can be a profitable business for agri-food communities.
However, access to international markets can be costly and thus unattainable
for small and medium sized enterprises (SMEs). This problem is exacerbated
under trade policies which require minimum quantity commitments (MQCs) on
export volumes, e.g., licensing tariff rate quota (TRQ) mechanisms.
  We show how cooperative exporting among agri-food SMEs can tackle the
barriers posed by the MQCs, and give market access to a broader range of SMEs.
We formulate a class of cooperative games associated with these situations and
find a gain-sharing mechanism that result in allocations in their corresponding
cores. Thus, grand coalitions of cooperative exporting SMEs can form in stable
manners.
  This allocation rule shares the export surplus only among the "essential" SME
exporters, that is, the players who are sufficiently cost efficient. Thus, less
cost efficient "complimentary" SMEs whose capacities are needed to maintain
MQCs receive no benefit from collaborative exporting and their participation
have to be altruistic. We propose two modifications to our original allocation
rule to share a portion of export surplus among the complementary SMEs through
taxing the essential SMEs: the first through egalitarian, and the second
through revenue-based rates. We compare the performance of these allocations
with the numerical examples and discuss their practical implications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Expanding the Resolution Boundary of Outcome-Based Imperfect-Recall
  Abstraction in Games with Ordered Signals 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11486v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11486v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanchang Fu, Junge Zhang, Dongdong Bai, Lingyun Zhao, Jialu Song, Kaiqi Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the development of advanced Texas Hold'em AI systems, abstraction
technology has garnered widespread attention due to its significant effect in
simplifying game complexity. This study adopts a more specific model, the games
of ordered signal, to describe Texas Hold'em-style games and optimizes this
model to streamline its mathematical representation and broaden its
applicability. By transitioning from a broad imperfect information game model
to a game with ordered signals model, we have separated the previously
intertwined infoset abstraction and action abstraction into independent signal
abstraction and action abstraction. Importantly, this signal abstraction
provides a mathematical framework for the hand abstraction task, which is
emphatically discussed in this paper. Additionally, a novel common refinement
principle is introduced, revealing the limit performance of hand abstraction
algorithms. We introduce potential outcome isomorphism (POI) and pinpoint that
it suffers from the issue of excessive abstraction. Futher, We demonstrate that
POI serves as a common refinement for leading outcome-based hand abstraction
algorithms, such as E[HS] and PA\&PAEMD. Consequently, excessive abstraction
also inherently affects these algorithms, leading to suboptimal performance.
Our investigation reveals the omission of historical data as a primary
contributor to excessive abstraction. To remedy this, we propose the K-Recall
Outcome Isomorphism (KROI) to incorporate the missing information. Compared
with POI, KROI more accurately mirrors lossless isomorphism (LI), the ground
truth, offering enhanced signal abstraction resolution. Experimental results in
the Numeral211 Hold'em indicate that strategies developed through KROI
approximate the exploitability of those developed through LI more closely than
those trained through POI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>35 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Information Compression in Dynamic Information Disclosure Games 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12204v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12204v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dengwang Tang, Vijay G. Subramanian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider a two-player dynamic information design problem between a
principal and a receiver -- a game is played between the two agents on top of a
Markovian system controlled by the receiver's actions, where the principal
obtains and strategically shares some information about the underlying system
with the receiver in order to influence their actions. In our setting, both
players have long-term objectives, and the principal sequentially commits to
their strategies instead of committing at the beginning. Further, the principal
cannot directly observe the system state, but at every turn they can choose
randomized experiments to observe the system partially. The principal can share
details about the experiments to the receiver. For our analysis we impose the
truthful disclosure rule: the principal is required to truthfully announce the
details and the result of each experiment to the receiver immediately after the
experiment result is revealed. Based on the received information, the receiver
takes an action when its their turn, with the action influencing the state of
the underlying system. We show that there exist Perfect Bayesian equilibria in
this game where both agents play Canonical Belief Based (CBB) strategies using
a compressed version of their information, rather than full information, to
choose experiments (for the principal) or actions (for the receiver). We also
provide a backward inductive procedure to solve for an equilibrium in CBB
strategies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PackIt! Gamified Rectangle Packing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12195v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12195v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Garrison, Marijn J. H. Heule, Bernardo Subercaseaux
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present and analyze PackIt!, a turn-based game consisting of packing
rectangles on an $n \times n$ grid. PackIt! can be easily played on paper,
either as a competitive two-player game or in \emph{solitaire} fashion. On the
$t$-th turn, a rectangle of area $t$ or $t+1$ must be placed in the grid. In
the two-player format of PackIt! whichever player places a rectangle last wins,
whereas the goal in the solitaire variant is to perfectly pack the $n \times n$
grid. We analyze conditions for the existence of a perfect packing over $n
\times n$, then present an automated reasoning approach that allows finding
perfect games of PackIt! up to $n = 50$ which includes a novel SAT-encoding
technique of independent interest, and conclude by proving an NP-hardness
result.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 10 figures, Submitted to Fun with Algorithms</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fragile Stable Matchings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12183v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12183v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kirill Rudov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We show how fragile stable matchings are in a decentralized one-to-one
matching setting. The classical work of Roth and Vande Vate (1990) suggests
simple decentralized dynamics in which randomly-chosen blocking pairs match
successively. Such decentralized interactions guarantee convergence to a stable
matching. Our first theorem shows that, under mild conditions, any unstable
matching -- including a small perturbation of a stable matching -- can
culminate in any stable matching through these dynamics. Our second theorem
highlights another aspect of fragility: stabilization may take a long time.
Even in markets with a unique stable matching, where the dynamics always
converge to the same matching, decentralized interactions can require an
exponentially long duration to converge. A small perturbation of a stable
matching may lead the market away from stability and involve a sizable
proportion of mismatched participants for extended periods. Our results hold
for a broad class of dynamics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MAC Advice for Facility Location Mechanism Design 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12181v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12181v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zohar Barak, Anupam Gupta, Inbal Talgam-Cohen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Algorithms with predictions have attracted much attention in the last years
across various domains, including variants of facility location, as a way to
surpass traditional worst-case analyses. We study the $k$-facility location
mechanism design problem, where the $n$ agents are strategic and might
misreport their location.
  Unlike previous models, where predictions are for the $k$ optimal facility
locations, we receive $n$ predictions for the locations of each of the agents.
However, these predictions are only "mostly" and "approximately" correct (or
MAC for short) -- i.e., some $\delta$-fraction of the predicted locations are
allowed to be arbitrarily incorrect, and the remainder of the predictions are
allowed to be correct up to an $\varepsilon$-error. We make no assumption on
the independence of the errors. Can such predictions allow us to beat the
current best bounds for strategyproof facility location?
  We show that the $1$-median (geometric median) of a set of points is
naturally robust under corruptions, which leads to an algorithm for
single-facility location with MAC predictions. We extend the robustness result
to a "balanced" variant of the $k$ facilities case. Without balancedness, we
show that robustness completely breaks down, even for the setting of $k=2$
facilities on a line. For this "unbalanced" setting, we devise a truthful
random mechanism that outperforms the best known result of Lu et al. [2010],
which does not use predictions. En route, we introduce the problem of "second"
facility location (when the first facility's location is already fixed). Our
findings on the robustness of the $1$-median and more generally $k$-medians may
be of independent interest, as quantitative versions of classic breakdown-point
results in robust statistics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Simple 2-Approximation Algorithm For Minimum Manhattan Network Problem 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11811v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11811v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md. Musfiqur Rahman Sanim, Safrunnesa Saira, Fatin Faiaz Ahsan, Rajon Bardhan, S. M. Ferdous
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given a n points in two dimensional space, a Manhattan Network G is a network
that connects all n points with either horizontal or vertical edges, with the
property that for any two point in G should be connected by a Manhattan path
and distance between this two points is equal to Manhattan Distance. The
Minimum Manhattan Network problem is to find a Manhattan network with minimum
network length, i.e., summation of all line segment in network should be
minimize. In this paper, we proposed a 2-approximation algorithm with time
complexity O(|E|lgN) where |E| is the number of edges and N is the number of
nodes. Using randomly generated datasets, we compare our result with the
optimal one.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ARSSS International Conference, Dhaka, Bangladesh</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Contract-Based Distributed Synthesis in Two-Objective Parity Games <span class="chip">SC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06212v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06212v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ashwani Anand, Satya Prakash Nayak, Anne-Kathrin Schmuck
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel method to compute $\textit{assume-guarantee contracts}$ in
non-zerosum two-player games over finite graphs where each player has a
different $ \omega $-regular winning condition. Given a game graph $G$ and two
parity winning conditions $\Phi_0$ and $\Phi_1$ over $G$, we compute
$\textit{contracted strategy-masks}$ ($\texttt{csm}$) $(\Psi_{i},\Phi_{i})$ for
each Player $i$. Within a $\texttt{csm}$, $\Phi_{i}$ is a $\textit{permissive
strategy template}$ which collects an infinite number of winning strategies for
Player $i$ under the assumption that Player $1-i$ chooses any strategy from the
$\textit{permissive assumption template}$ $\Psi_{i}$. The main feature of
$\texttt{csm}$'s is their power to $\textit{fully decentralize all remaining
strategy choices}$ -- if the two player's $\texttt{csm}$'s are compatible, they
provide a pair of new local specifications $\Phi_0^\bullet$ and
$\Phi_1^\bullet$ such that Player $i$ can locally and fully independently
choose any strategy satisfying $\Phi_i^\bullet$ and the resulting strategy
profile is ensured to be winning in the original two-objective game
$(G,\Phi_0,\Phi_1)$.
  In addition, the new specifications $\Phi_i^\bullet$ are $\textit{maximally
cooperative}$, i.e., allow for the distributed synthesis of any cooperative
solution. Further, our algorithmic computation of $\texttt{csm}$'s is complete
and ensured to terminate.
  We illustrate how the unique features of our synthesis framework effectively
address multiple challenges in the context of \enquote{correct-by-design}
logical control software synthesis for cyber-physical systems and provide
empirical evidence that our approach possess desirable structural and
computational properties compared to state-of-the-art techniques.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>HSCC 2024</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Theory
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beamforming Design for Semantic-Bit Coexisting Communication System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11693v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11693v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maojun Zhang, Guangxu Zhu, Richeng Jin, Xiaoming Chen, Qingjiang Shi, Caijun Zhong, Kaibing Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantic communication (SemCom) is emerging as a key technology for future
sixth-generation (6G) systems. Unlike traditional bit-level communication
(BitCom), SemCom directly optimizes performance at the semantic level, leading
to supe- rior communication efficiency. Nevertheless, the task-oriented nature
of SemCom renders it challenging to completely replace BitCom. Consequently, it
is desired to consider a semantic-bit coexisting communication system, where a
base station (BS) serves SemCom users (sem-users) and BitCom users (bit-users)
simultaneously. Such a system faces severe and heterogeneous inter-user
interference. In this context, this paper provides a new semantic-bit
coexisting communication framework and proposes a spatial beamforming scheme to
accommodate both types of users. Specifically, we consider maximizing the
semantic rate for semantic users while ensuring the quality-of-service (QoS)
requirements for bit-users. Due to the intractability of obtaining the exact
closed-form expression of the semantic rate, a data driven method is first
applied to attain an approximated expression via data fitting. With the
resulting complex transcendental function, majorization minimization (MM) is
adopted to convert the original formulated problem into a multiple-ratio
problem, which allows fractional programming (FP) to be used to further
transform the problem into an inhomogeneous quadratically constrained quadratic
programs (QCQP) problem. Solving the problem leads to a semi-closed form
solution with undetermined Lagrangian factors that can be updated by a fixed
point algorithm. Extensive simulation results demonstrate that the proposed
beamforming scheme significantly outperforms conventional beamforming
algorithms such as zero-forcing (ZF), maximum ratio transmission (MRT), and
weighted minimum mean-square error (WMMSE).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE for possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ New Constructions of Reversible DNA Codes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11551v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11551v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xueyan Chen, Whan-Hyuk Choi, Hongwei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  DNA codes have many applications, such as in data storage, DNA computing,
etc. Good DNA codes have large sizes and satisfy some certain constraints. In
this paper, we present a new construction method for reversible DNA codes. We
show that the DNA codes obtained using our construction method can satisfy some
desired constraints and the lower bounds of the sizes of some DNA codes are
better than the known results. We also give new lower bounds on the sizes of
some DNA codes of lengths $80$, $96$ and $160$ for some fixed Hamming distance
$d$.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Span-Based Optimal Sample Complexity for Weakly Communicating and
  General Average Reward MDPs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11477v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11477v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthew Zurek, Yudong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the sample complexity of learning an $\epsilon$-optimal policy in an
average-reward Markov decision process (MDP) under a generative model. For
weakly communicating MDPs, we establish the complexity bound
$\tilde{O}(SA\frac{H}{\epsilon^2})$, where $H$ is the span of the bias function
of the optimal policy and $SA$ is the cardinality of the state-action space.
Our result is the first that is minimax optimal (up to log factors) in all
parameters $S,A,H$ and $\epsilon$, improving on existing work that either
assumes uniformly bounded mixing times for all policies or has suboptimal
dependence on the parameters. We further investigate sample complexity in
general (non-weakly-communicating) average-reward MDPs. We argue a new
transient time parameter $B$ is necessary, establish an
$\tilde{O}(SA\frac{B+H}{\epsilon^2})$ complexity bound, and prove a matching
(up to log factors) minimax lower bound. Both results are based on reducing the
average-reward MDP to a discounted MDP, which requires new ideas in the general
setting. To establish the optimality of this reduction, we develop improved
bounds for $\gamma$-discounted MDPs, showing that
$\tilde{\Omega}\left(SA\frac{H}{(1-\gamma)^2\epsilon^2}\right)$ samples suffice
to learn an $\epsilon$-optimal policy in weakly communicating MDPs under the
regime that $\gamma\geq 1-1/H$, and
$\tilde{\Omega}\left(SA\frac{B+H}{(1-\gamma)^2\epsilon^2}\right)$ samples
suffice in general MDPs when $\gamma\geq 1-\frac{1}{B+H}$. Both these results
circumvent the well-known lower bound of
$\tilde{\Omega}\left(SA\frac{1}{(1-\gamma)^3\epsilon^2}\right)$ for arbitrary
$\gamma$-discounted MDPs. Our analysis develops upper bounds on certain
instance-dependent variance parameters in terms of the span and transient time
parameters. The weakly communicating bounds are tighter than those based on the
mixing time or diameter of the MDP and may be of broader use.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>42 pages, 2 figures; this article supersedes arXiv:2311.13469</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Holes of Twisted Reed-Solomon Codes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11436v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11436v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weijun Fang, Jingke Xu, Ruiqi Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The deep holes of a linear code are the vectors that achieve the maximum
error distance to the code. There has been extensive research on the topic of
deep holes in Reed-Solomon codes. As a generalization of Reed-Solomon codes, we
investigate the problem of deep holes of a class of twisted Reed-Solomon codes
in this paper. The covering radius and a standard class of deep holes of
twisted Reed-Solomon codes ${\rm TRS}_k(\mathcal{A}, \theta)$ are obtained for
a general evaluation set $\mathcal{A} \subseteq \mathbb{F}_q$. Furthermore, we
consider the problem of determining all deep holes of the full-length twisted
Reed-Solomon codes ${\rm TRS}_k(\mathbb{F}_q, \theta)$. Specifically, we prove
that there are no other deep holes of ${\rm TRS}_k(\mathbb{F}_q, \theta)$ for
$\frac{3q-8}{4} \leq k\leq q-4$ when $q$ is even, and $\frac{3q+2\sqrt{q}-7}{4}
\leq k\leq q-4$ when $q$ is odd. We also completely determine their deep holes
for $q-3 \leq k \leq q-1$.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Measuring Quantum Information Leakage Under Detection Threat 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11433v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11433v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Farhad Farokhi, Sejeong Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gentle quantum leakage is proposed as a measure of information leakage to
arbitrary eavesdroppers that aim to avoid detection. Gentle (also sometimes
referred to as weak or non-demolition) measurements are used to encode the
desire of the eavesdropper to evade detection. The gentle quantum leakage meets
important axioms proposed for measures of information leakage including
positivity, independence, and unitary invariance. Global depolarizing noise, an
important family of physical noise in quantum devices, is shown to reduce
gentle quantum leakage (and hence can be used as a mechanism to ensure privacy
or security). A lower bound for the gentle quantum leakage based on asymmetric
approximate cloning is presented. This lower bound relates information leakage
to mutual incompatibility of quantum states. A numerical example, based on the
encoding in the celebrated BB84 quantum key distribution algorithm, is used to
demonstrate the results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Causal Model for Quantifying Multipartite Classical and Quantum
  Correlations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.16414v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.16414v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuchan Wang, Gerhard Wunder
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We give an operational definition of information-theoretic resources within a
given multipartite classical or quantum correlation. We present our causal
model that serves as the source coding side of this correlation and introduce a
novel concept of resource rate. We argue that, beyond classical secrecy,
additional resources exist that are useful for the security of distributed
computing problems, which can be captured by the resource rate. Furthermore, we
establish a relationship between resource rate and an extension of Shannon's
logarithmic information measure, namely, total correlation. Subsequently, we
present a novel quantum secrecy monotone and investigate a quantum hybrid key
distribution system as an extension of our causal model. Finally, we discuss
some connections to optimal transport (OT) problem.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 13 figures. Changes in v2: Appendix B added; some notations
  changed</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Operating Systems
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLM as a System Service on Mobile Devices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11805v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11805v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wangsong Yin, Mengwei Xu, Yuanchun Li, Xuanzhe Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Being more powerful and intrusive into user-device interactions, LLMs are
eager for on-device execution to better preserve user privacy. In this work, we
propose a new paradigm of mobile AI: LLM as a system service on mobile devices
(LLMaaS). Unlike traditional DNNs that execute in a stateless manner, such a
system service is stateful: LLMs execution often needs to maintain persistent
states (mainly KV cache) across multiple invocations. To minimize the LLM
context switching overhead under tight device memory budget, this work presents
LLMS, which decouples the memory management of app and LLM contexts with a key
idea of fine-grained, chunk-wise, globally-optimized KV cache compression and
swapping. By fully leveraging KV cache's unique characteristics, it proposes
three novel techniques: (1) Tolerance-Aware Compression: it compresses chunks
based on their measured accuracy tolerance to compression. (2) IO-Recompute
Pipelined Loading: it introduces recompute to swapping-in for acceleration. (3)
Chunk Lifecycle Management: it optimizes the memory activities of chunks with
an ahead-of-time swapping-out and an LCTRU (Least Compression-Tolerable and
Recently-Used) queue based eviction. In evaluations conducted on
well-established traces and various edge devices, \sys reduces context
switching latency by up to 2 orders of magnitude when compared to competitive
baseline solutions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical Report</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-17T00:00:00Z">2024-03-17</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Distributed, Parallel, and Cluster Computing
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ JORA: JAX Tensor-Parallel LoRA Library for Retrieval Augmented
  Fine-Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11366v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11366v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anique Tahir, Lu Cheng, Huan Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The scaling of Large Language Models (LLMs) for retrieval-based tasks,
particularly in Retrieval Augmented Generation (RAG), faces significant memory
constraints, especially when fine-tuning extensive prompt sequences. Current
open-source libraries support full-model inference and fine-tuning across
multiple GPUs but fall short of accommodating the efficient parameter
distribution required for retrieved context. Addressing this gap, we introduce
a novel framework for PEFT-compatible fine-tuning of Llama-2 models, leveraging
distributed training. Our framework uniquely utilizes JAX's just-in-time (JIT)
compilation and tensor-sharding for efficient resource management, thereby
enabling accelerated fine-tuning with reduced memory requirements. This
advancement significantly improves the scalability and feasibility of
fine-tuning LLMs for complex RAG applications, even on systems with limited GPU
resources. Our experiments show more than 12x improvement in runtime compared
to Hugging Face/DeepSpeed implementation with four GPUs while consuming less
than half the VRAM per GPU. Our library will be open-sourced in due course.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A learning-based solution approach to the application placement problem
  in mobile edge computing under uncertainty 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11259v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11259v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taha-Hossein Hejazi, Zahra Ghadimkhani, Arezoo Borji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Placing applications in mobile edge computing servers presents a complex
challenge involving many servers, users, and their requests. Existing
algorithms take a long time to solve high-dimensional problems with significant
uncertainty scenarios. Therefore, an efficient approach is required to maximize
the quality of service while considering all technical constraints. One of
these approaches is machine learning, which emulates optimal solutions for
application placement in edge servers. Machine learning models are expected to
learn how to allocate user requests to servers based on the spatial positions
of users and servers. In this study, the problem is formulated as a two-stage
stochastic programming. A sufficient amount of training records is generated by
varying parameters such as user locations, their request rates, and solving the
optimization model. Then, based on the distance features of each user from the
available servers and their request rates, machine learning models generate
decision variables for the first stage of the stochastic optimization model,
which is the user-to-server request allocation, and are employed as independent
decision agents that reliably mimic the optimization model. Support Vector
Machines (SVM) and Multi-layer Perceptron (MLP) are used in this research to
achieve practical decisions from the stochastic optimization models. The
performance of each model has shown an execution effectiveness of over 80%.
This research aims to provide a more efficient approach for tackling
high-dimensional problems and scenarios with uncertainties in mobile edge
computing by leveraging machine learning models for optimal decision-making in
request allocation to edge servers. These results suggest that machine-learning
models can significantly improve solution times compared to conventional
approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ JUMBO: Fully Asynchronous BFT Consensus Made Truly Scalable 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11238v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11238v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Cheng, Yuan Lu, Zhenliang Lu, Qiang Tang, Yuxuan Zhang, Zhenfeng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent progresses in asynchronous Byzantine fault-tolerant (BFT) consensus,
e.g. Dumbo-NG (CCS' 22) and Tusk (EuroSys' 22), show promising performance
through decoupling transaction dissemination and block agreement. However, when
executed with a larger number $n$ of nodes, like several hundreds, they would
suffer from significant degradation in performance. Their dominating
scalability bottleneck is the huge authenticator complexity: each node has to
multicast $\bigO(n)$ quorum certificates (QCs) and subsequently verify them for
each block.
  This paper systematically investigates and resolves the above scalability
issue. We first propose a signature-free asynchronous BFT consensus FIN-NG that
adapts a recent signature-free asynchronous common subset protocol FIN (CCS'
23) into the state-of-the-art framework of concurrent broadcast and agreement.
The liveness of FIN-NG relies on our non-trivial redesign of FIN's multi-valued
validated Byzantine agreement towards achieving optimal quality. FIN-NG greatly
improves the performance of FIN and already outperforms Dumbo-NG in most
deployment settings. To further overcome the scalability limit of FIN-NG due to
$\bigO(n^3)$ messages, we propose JUMBO, a scalable instantiation of Dumbo-NG,
with only $\bigO(n^2)$ complexities for both authenticators and messages. We
use various aggregation and dispersal techniques for QCs to significantly
reduce the authenticator complexity of original Dumbo-NG implementations by up
to $\bigO(n^2)$ orders. We also propose a ``fairness'' patch for JUMBO, thus
preventing a flooding adversary from controlling an overwhelming portion of
transactions in its output.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lion: Minimizing Distributed Transactions through Adaptive Replica
  Provision (Extended Version) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11221v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11221v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiushi Zheng, Zhanhao Zhao, Wei Lu, Chang Yao, Yuxing Chen, Anqun Pan, Xiaoyong Du
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Distributed transaction processing often involves multiple rounds of
cross-node communications, and therefore tends to be slow. To improve
performance, existing approaches convert distributed transactions into
single-node transactions by either migrating co-accessed partitions onto the
same nodes or establishing a super node housing replicas of the entire
database. However, migration-based methods might cause transactions to be
blocked due to waiting for data migration, while the super node can become a
bottleneck. In this paper, we present Lion, a novel transaction processing
protocol that utilizes partition-based replication to reduce the occurrence of
distributed transactions. Lion aims to assign a node with one replica from each
partition involved in a given transaction's read or write operations. To ensure
such a node is available, we propose an adaptive replica provision mechanism,
enhanced with an LSTM-based workload prediction algorithm, to determine the
appropriate node for locating replicas of co-accessed partitions. The
adaptation of replica placement is conducted preemptively and asynchronously,
thereby minimizing its impact on performance. By employing this adaptive
replica placement strategy, we ensure that the majority of transactions can be
efficiently processed on a single node without additional overhead. Only a
small fraction of transactions will need to be treated as regular distributed
transactions when such a node is unavailable. Consequently, Lion effectively
minimizes distributed transactions while avoiding any disruption caused by data
migration or the creation of a super node. We conduct extensive experiments to
compare Lion against various transaction processing protocols. The results show
that Lion achieves up to 2.7x higher throughput and 76.4% better scalability
against these state-of-the-art approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Partitioned Neural Network Training via Synthetic Intermediate Labels 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11204v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11204v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cevat Volkan Karadağ, Nezih Topaloğlu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The proliferation of extensive neural network architectures, particularly
deep learning models, presents a challenge in terms of resource-intensive
training. GPU memory constraints have become a notable bottleneck in training
such sizable models. Existing strategies, including data parallelism, model
parallelism, pipeline parallelism, and fully sharded data parallelism, offer
partial solutions. Model parallelism, in particular, enables the distribution
of the entire model across multiple GPUs, yet the ensuing data communication
between these partitions slows down training. Additionally, the substantial
memory overhead required to store auxiliary parameters on each GPU compounds
computational demands. Instead of using the entire model for training, this
study advocates partitioning the model across GPUs and generating synthetic
intermediate labels to train individual segments. These labels, produced
through a random process, mitigate memory overhead and computational load. This
approach results in a more efficient training process that minimizes data
communication while maintaining model accuracy. To validate this method, a
6-layer fully connected neural network is partitioned into two parts and its
performance is assessed on the extended MNIST dataset. Experimental results
indicate that the proposed approach achieves similar testing accuracies to
conventional training methods, while significantly reducing memory and
computational requirements. This work contributes to mitigating the
resource-intensive nature of training large neural networks, paving the way for
more efficient deep learning model development.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Performance
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Secrecy Outage Probability Analysis for Downlink RIS-NOMA Networks with
  On-Off Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11097v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11097v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingjie Pei, Xinwei Yue, Wenqiang Yi, Yuanwei Liu, Xuehua Li, Zhiguo Ding
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reconfigurable intelligent surface (RIS) has been regarded as a promising
technology since it has ability to create the favorable channel conditions.
This paper investigates the secure communications of RIS assisted
non-orthogonal multiple access (NOMA) networks, where both external and
internal eavesdropping scenarios are taken into consideration. More
specifically, novel approximate and asymptotic expressions of secrecy outage
probability (SOP) for the k-th legitimate user (LU) are derived by invoking
imperfect successive interference cancellation (ipSIC) and perfect successive
interference cancellation (pSIC). To characterize the secrecy performance of
RIS-NOMA networks, the diversity order of the k-th LU with ipSIC/pSIC is
obtained in the high signal-to-noise ratio region. The secrecy system
throughput of RIS-NOMA networks is discussed in delay-limited transmission
mode. Numerical results are presented to verify theoretical analysis that: i)
The SOP of RIS-NOMA networks is superior to that of RIS assisted orthogonal
multiple access (OMA) and conventional cooperative communication schemes; ii)
As the number of reflecting elements increases, the RIS-NOMA networks are
capable of achieving the enhanced secrecy performance; and iii) The RIS-NOMA
networks have better secrecy system throughput than that of RIS-OMA networks
and conventional cooperative communication schemes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been published in IEEE Transactions on Vehicular
  Technology</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Databases
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Graph Theory for Consent Management: A New Approach for Complex Data
  Flows 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11361v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11361v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dorota Filipczuk, Enrico H. Gerding, George Konstantinidis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Through legislation and technical advances users gain more control over how
their data is processed, and they expect online services to respect their
privacy choices and preferences. However, data may be processed for many
different purposes by several layers of algorithms that create complex data
workflows. To date, there is no existing approach to automatically satisfy
fine-grained privacy constraints of a user in a way which optimises the service
provider's gains from processing. In this article, we propose a solution to
this problem by modelling a data flow as a graph. User constraints and
processing purposes are pairs of vertices which need to be disconnected in this
graph. In general, this problem is NP-hard, thus, we propose several heuristics
and algorithms. We discuss the optimality versus efficiency of our algorithms
and evaluate them using synthetically generated data. On the practical side,
our algorithms can provide nearly optimal solutions for tens of constraints and
graphs of thousands of nodes, in a few seconds.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Distance Query Processing in Edge Computing Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11246v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11246v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiubo Zhang, Yujie He, Ye Li, Yan Li, Zijie Zhou, Dongyao Wei,  Ryan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the context of changing travel behaviors and the expanding user base of
Geographic Information System (GIS) services, conventional centralized
architectures responsible for handling shortest distance queries are facing
increasing challenges, such as heightened load pressure and longer response
times. To mitigate these concerns, this study is the first to develop an edge
computing framework specially tailored for processing distance queries. In
conjunction with this innovative system, we have developed a straightforward,
yet effective, labeling technique termed Border Labeling. Furthermore, we have
devised and implemented a range of query strategies intended to capitalize on
the capabilities of the edge computing infrastructure. Our experiments
demonstrate that our solution surpasses other methods in terms of both indexing
time and query speed across various road network datasets. The empirical
evidence from our experiments supports the claim that our edge computing
architecture significantly reduces the latency encountered by end-users, thus
markedly decreasing their waiting times.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15pages, 18(sub)pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lion: Minimizing Distributed Transactions through Adaptive Replica
  Provision (Extended Version) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11221v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11221v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiushi Zheng, Zhanhao Zhao, Wei Lu, Chang Yao, Yuxing Chen, Anqun Pan, Xiaoyong Du
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Distributed transaction processing often involves multiple rounds of
cross-node communications, and therefore tends to be slow. To improve
performance, existing approaches convert distributed transactions into
single-node transactions by either migrating co-accessed partitions onto the
same nodes or establishing a super node housing replicas of the entire
database. However, migration-based methods might cause transactions to be
blocked due to waiting for data migration, while the super node can become a
bottleneck. In this paper, we present Lion, a novel transaction processing
protocol that utilizes partition-based replication to reduce the occurrence of
distributed transactions. Lion aims to assign a node with one replica from each
partition involved in a given transaction's read or write operations. To ensure
such a node is available, we propose an adaptive replica provision mechanism,
enhanced with an LSTM-based workload prediction algorithm, to determine the
appropriate node for locating replicas of co-accessed partitions. The
adaptation of replica placement is conducted preemptively and asynchronously,
thereby minimizing its impact on performance. By employing this adaptive
replica placement strategy, we ensure that the majority of transactions can be
efficiently processed on a single node without additional overhead. Only a
small fraction of transactions will need to be treated as regular distributed
transactions when such a node is unavailable. Consequently, Lion effectively
minimizes distributed transactions while avoiding any disruption caused by data
migration or the creation of a super node. We conduct extensive experiments to
compare Lion against various transaction processing protocols. The results show
that Lion achieves up to 2.7x higher throughput and 76.4% better scalability
against these state-of-the-art approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Wait to be Faster: a Smart Pooling Framework for Dynamic Ridesharing <span class="chip">ICDE 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11099v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11099v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoyao Zhong, Jiabao Jin, Peng Cheng, Wangze Ni, Libin Zheng, Lei Chen, Xuemin Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ridesharing services, such as Uber or Didi, have attracted considerable
attention in recent years due to their positive impact on environmental
protection and the economy. Existing studies require quick responses to orders,
which lack the flexibility to accommodate longer wait times for better grouping
opportunities. In this paper, we address a NP-hard ridesharing problem, called
Minimal Extra Time RideSharing (METRS), which balances waiting time and group
quality (i.e., detour time) to improve riders' satisfaction. To tackle this
problem, we propose a novel approach called WATTER (WAit To be fasTER), which
leverages an order pooling management algorithm allowing orders to wait until
they can be matched with suitable groups. The key challenge is to customize the
extra time threshold for each order by reducing the original optimization
objective into a convex function of threshold, thus offering a theoretical
guarantee to be optimized efficiently. We model the dispatch process using a
Markov Decision Process (MDP) with a carefully designed value function to learn
the threshold. Through extensive experiments on three real datasets, we
demonstrate the efficiency and effectiveness of our proposed approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE ICDE 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Programming Frameworks for Differential Privacy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11088v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11088v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marco Gaboardi, Michael Hay, Salil Vadhan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many programming frameworks have been introduced to support the development
of differentially private software applications. In this chapter, we survey
some of the conceptual ideas underlying these frameworks in a way that we hope
will be helpful for both practitioners and researchers. For practitioners, the
survey can provide a starting point for understanding what features may be
valuable when selecting a programming framework. For researchers, it can help
organize existing work in a unified way and provide context for understanding
new features in future frameworks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear as a chapter in the book "Differential Privacy for
  Artificial Intelligence," edited by Ferdinando Fioretto and Pascal van
  Hentenryck and to be published by now publishers</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Science and Game Theory
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Independent RL for Cooperative-Competitive Agents: A Mean-Field
  Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11345v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11345v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Aneeq uz Zaman, Alec Koppel, Mathieu Laurière, Tamer Başar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We address in this paper Reinforcement Learning (RL) among agents that are
grouped into teams such that there is cooperation within each team but
general-sum (non-zero sum) competition across different teams. To develop an RL
method that provably achieves a Nash equilibrium, we focus on a
linear-quadratic structure. Moreover, to tackle the non-stationarity induced by
multi-agent interactions in the finite population setting, we consider the case
where the number of agents within each team is infinite, i.e., the mean-field
setting. This results in a General-Sum LQ Mean-Field Type Game (GS-MFTGs). We
characterize the Nash equilibrium (NE) of the GS-MFTG, under a standard
invertibility condition. This MFTG NE is then shown to be $\mathcal{O}(1/M)$-NE
for the finite population game where $M$ is a lower bound on the number of
agents in each team. These structural results motivate an algorithm called
Multi-player Receding-horizon Natural Policy Gradient (MRPG), where each team
minimizes its cumulative cost independently in a receding-horizon manner.
Despite the non-convexity of the problem, we establish that the resulting
algorithm converges to a global NE through a novel problem decomposition into
sub-problems using backward recursive discrete-time Hamilton-Jacobi-Isaacs
(HJI) equations, in which independent natural policy gradient is shown to
exhibit linear convergence under time-independent diagonal dominance.
Experiments illuminate the merits of this approach in practice.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An upper bound of the mutation probability in the genetic algorithm for
  general 0-1 knapsack problem 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11307v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11307v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As an important part of genetic algorithms (GAs), mutation operators is
widely used in evolutionary algorithms to solve $\mathcal{NP}$-hard problems
because it can increase the population diversity of individual. Due to
limitations in mathematical tools, the mutation probability of the mutation
operator is primarily empirically set in practical applications.
  In this paper, we propose a novel reduction method for the 0-1 knapsack
problem(0-1 KP) and an improved mutation operator (IMO) based on the assumption
$\mathcal{NP}\neq\mathcal{P}$, along with the utilization of linear relaxation
techniques and a recent result by Dey et al. (Math. Prog., pp 569-587, 2022).
We employ this method to calculate an upper bound of the mutation probability
in general instances of the 0-1 KP, and construct an instance where the
mutation probability does not tend towards 0 as the problem size increases.
Finally, we prove that the probability of the IMO hitting the optimal solution
within only a single iteration in large-scale instances is superior to that of
the traditional mutation operator.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Barely Random Algorithms for Metrical Task Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11267v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11267v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Romain Cosson, Laurent Massoulié
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider metrical task systems on general metric spaces with $n$ points,
and show that any fully randomized algorithm can be turned into a randomized
algorithm that uses only $2\log n$ random bits, and achieves the same
competitive ratio up to a factor $2$. This provides the first order-optimal
barely random algorithms for metrical task systems, i.e. which use a number of
random bits that does not depend on the number of requests addressed to the
system. We put forward an equivalent view that we call collective metrical task
systems where $k$ agents in a metrical task system team up, and suffer the
average cost paid by each agent. Our results imply that such team can be
$O(\log n^2)$-competitive, as soon as $k\geq n^2$ (in comparison, a single
agent is $\Omega(n)$-competitive at best). We discuss implications on various
aspects of online decision making such as: distributed systems, transaction
costs, and advice complexity, suggesting broad applicability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Novel Mutual Insurance Model for Hedging Against Cyber Risks in Power
  Systems Deploying Smart Technologies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11054v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11054v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pikkin Lau, Lingfeng Wang, Wei Wei, Zhaoxi Liu, Chee-Wooi Ten
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, a novel cyber-insurance model design is proposed based on
system risk evaluation with smart technology applications. The cyber insurance
policy for power systems is tailored via cyber risk modeling, reliability
impact analysis, and insurance premium calculation. A stochastic Epidemic
Network Model is developed to evaluate the cyber risk by propagating
cyberattacks among graphical vulnerabilities. Smart technologies deployed in
risk modeling include smart monitoring and job thread assignment. Smart
monitoring boosts the substation availability against cyberattacks with
preventive and corrective measures. The job thread assignment solution reduces
the execution failures by distributing the control and monitoring tasks to
multiple threads. Reliability assessment is deployed to estimate load losses
convertible to monetary losses. These monetary losses would be shared through a
mutual insurance plan. To ensure a fair distribution of indemnity, a new
Shapley mutual insurance principle is devised. Effectiveness of the proposed
Shapley mutual insurance design is validated via case studies. The Shapley
premium is compared with existent premium designs. It is shown that the Shapley
premium has high indemnity levels closer to those of Tail Conditional
Expectation premium. Meanwhile, the Shapley premium is nearly as affordable as
the coalitional premium and keeps a relatively low insolvency probability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Power system reliability, cyber-insurance, power system security,
  cyber-physical systems, cyber risk modeling, actuarial design, tail risk</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Strategic Bidding Wars in On-chain Auctions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.14510v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.14510v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fei Wu, Thomas Thiery, Stefanos Leonardos, Carmine Ventre
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Ethereum block-building process has changed significantly since the
emergence of Proposer-Builder Separation. Validators access blocks through a
marketplace, where block builders bid for the right to construct the block and
earn MEV (Maximal Extractable Value) rewards in an on-chain competition, known
as the MEV-boost auction. While more than 90% of blocks are currently built via
MEV-Boost, trade-offs between builders' strategic behaviors and auction design
remain poorly understood. In this paper we address this gap. We introduce a
game-theoretic model for MEV-Boost auctions and use simulations to study
different builders' bidding strategies observed in practice. We study various
strategic interactions and auction setups and evaluate how the interplay
between critical elements such as access to MEV opportunities and improved
connectivity to relays impact bidding performance. Our results demonstrate the
importance of latency on the effectiveness of builders' strategies and the
overall auction outcome from the proposer's perspective.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Theory
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interference Cancellation for OTFS-Based Over-the-Air Computation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11272v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11272v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyu Huang, Henrik Hellstrom, Carlo Fischione
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates over-the-air computation (AirComp) in the context of
multiple-access time-varying multipath channels. We focus on a scenario where
devices with high mobility transmit their sensing data to a fusion center (FC)
for averaging. To combat the time-varying channel and Doppler effect, each
device adopts orthogonal time frequency space (OTFS) modulation. After signals
are received by the FC, the aggregated data undergoes demodulation and
estimation within the delay-Doppler domain. We leverage the mean squared error
(MSE) as a metric for the computational error of OTFS-based AirComp. We then
derive the optimal transmit power at each device and signal scaling factor at
FC for minimizing MSE. Notably, the performance of OTFS-based AirComp is not
only affected by the noise but also by the inter-symbol interference and
inter-link interference arising from the multipath channel. To counteract the
interference-induced computational errors, we incorporate zero-padding
(ZP)-assisted OTFS into AirComp and propose algorithms for interference
cancellation. Numerical results underscore the enhanced performance of
ZP-assisted OTFS-based AirComp over naive OTFS-based AirComp.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 4 figures. Accepted by IEEE ICC 2024 Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Task-Based Quantizer Design for Sensing With Random Signals 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11187v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11187v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hang Ruan, Fan Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In integrated sensing and communication (ISAC) systems, random signaling is
used to convey useful information as well as sense the environment. Such
randomness poses challenges in various components in sensing signal processing.
In this paper, we investigate quantizer design for sensing in ISAC systems.
Unlike quantizers for channel estimation in massive multiple-input-multiple-out
(MIMO) communication systems, sensing in ISAC systems needs to deal with random
nonorthogonal transmitted signals rather than a fixed orthogonal pilot.
Considering sensing performance and hardware implementation, we focus on
task-based hardware-limited quantization with spatial analog combining. We
propose two strategies of quantizer optimization, i.e., data-dependent (DD) and
data-independent (DI). The former achieves optimized sensing performance with
high implementation overhead. To reduce hardware complexity, the latter
optimizes the quantizer with respect to the random signal from a stochastic
perspective. We derive the optimal quantizers for both strategies and formulate
an algorithm based on sample average approximation (SAA) to solve the
optimization in the DI strategy. Numerical results show that the optimized
quantizers outperform digital-only quantizers in terms of sensing performance.
Additionally, the DI strategy, despite its lower computational complexity
compared to the DD strategy, achieves near-optimal sensing performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Prior-dependent analysis of posterior sampling reinforcement learning
  with function approximation <span class="chip">AISTATS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11175v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11175v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingru Li, Zhi-Quan Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work advances randomized exploration in reinforcement learning (RL) with
function approximation modeled by linear mixture MDPs. We establish the first
prior-dependent Bayesian regret bound for RL with function approximation; and
refine the Bayesian regret analysis for posterior sampling reinforcement
learning (PSRL), presenting an upper bound of ${\mathcal{O}}(d\sqrt{H^3 T \log
T})$, where $d$ represents the dimensionality of the transition kernel, $H$ the
planning horizon, and $T$ the total number of interactions. This signifies a
methodological enhancement by optimizing the $\mathcal{O}(\sqrt{\log T})$
factor over the previous benchmark (Osband and Van Roy, 2014) specified to
linear mixture MDPs. Our approach, leveraging a value-targeted model learning
perspective, introduces a decoupling argument and a variance reduction
technique, moving beyond traditional analyses reliant on confidence sets and
concentration inequalities to formalize Bayesian regret bounds more
effectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in the 27th International Conference on Artificial
  Intelligence and Statistics (AISTATS)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The mean and variance of the reciprocal merit factor of four classes of
  binary sequences 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/1911.11246v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/1911.11246v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonathan Jedwab
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The merit factor of a $\{-1, 1\}$ binary sequence measures the collective
smallness of its non-trivial aperiodic autocorrelations. Binary sequences with
large merit factor are important in digital communications because they allow
the efficient separation of signals from noise. It is a longstanding open
question whether the maximum merit factor is asymptotically unbounded and, if
so, what is its limiting value. Attempts to answer this question over almost
sixty years have identified certain classes of binary sequences as particularly
important: skew-symmetric sequences, symmetric sequences, and anti-symmetric
sequences. Using only elementary methods, we find an exact formula for the mean
and variance of the reciprocal merit factor of sequences in each of these
classes, and in the class of all binary sequences. This provides a much deeper
understanding of the distribution of the merit factor in these four classes
than was previously available. A consequence is that, for each of the four
classes, the merit factor of a sequence drawn uniformly at random from the
class converges in probability to a constant as the sequence length increases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Removed discussion of $L_4$ norm to focus on reciprocal merit factor.
  The underlying calculations have not changed but the title, abstract,
  terminology, and introduction have</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-16T00:00:00Z">2024-03-16</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Distributed, Parallel, and Cluster Computing
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Open-Source Experimentation Framework for the Edge Cloud Continuum 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10977v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10977v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Georgios Koukis, Sotiris Skaperas, Ioanna Angeliki Kapetanidou, Vassilis Tsaoussidis, Lefteris Mamatas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The CODECO Experimentation Framework is an open-source solution designed for
the rapid experimentation of Kubernetes-based edge cloud deployments. It adopts
a microservice-based architecture and introduces innovative abstractions for
(i) the holistic deployment of Kubernetes clusters and associated applications,
starting from the VM allocation level; (ii) declarative cross-layer experiment
configuration; and (iii) automation features covering the entire experimental
process, from the configuration up to the results visualization. We present
proof-of-concept results that demonstrate the above capabilities in three
distinct contexts: (i) a comparative evaluation of various network fabrics
across different edge-oriented Kubernetes distributions; (ii) the automated
deployment of EdgeNet, which is a complex edge cloud orchestration system; and
(iii) an assessment of anomaly detection (AD) workflows tailored for edge
environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be published in IEEE INFOCOM CNERT Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ClusterSlice: A Zero-touch Deployment Platform for the Edge Cloud
  Continuum 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10954v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10954v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lefteris Mamatas, Sotiris Skaperas, Ilias Sakellariou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We demonstrate ClusterSlice, an open-source solution for automated
Kubernetes-center deployments for the edge continuum. ClusterSlice is an
infrastructure-as-a-service, platform-as-a-service, and
application-as-a-service solution, supporting: (i) declarative deployment slice
definitions; (ii) infrastructure-on-demand capabilities over multiple
heterogeneous domains; (iii) composable Kubernetes deployments, supporting
multi-clustering as well as various Kubernetes flavors and
intra-cluster/inter-cluster network plugins; (iv) configurable application
deployment; and (v) experimentation automation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Federated Learning based on Pruning and Recovery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15439v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15439v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengjie Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A novel federated learning training framework for heterogeneous environments
is presented, taking into account the diverse network speeds of clients in
realistic settings. This framework integrates asynchronous learning algorithms
and pruning techniques, effectively addressing the inefficiencies of
traditional federated learning algorithms in scenarios involving heterogeneous
devices, as well as tackling the staleness issue and inadequate training of
certain clients in asynchronous algorithms. Through the incremental restoration
of model size during training, the framework expedites model training while
preserving model accuracy. Furthermore, enhancements to the federated learning
aggregation process are introduced, incorporating a buffering mechanism to
enable asynchronous federated learning to operate akin to synchronous learning.
Additionally, optimizations in the process of the server transmitting the
global model to clients reduce communication overhead. Our experiments across
various datasets demonstrate that: (i) significant reductions in training time
and improvements in convergence accuracy are achieved compared to conventional
asynchronous FL and HeteroFL; (ii) the advantages of our approach are more
pronounced in scenarios with heterogeneous clients and non-IID client data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Lumiere: Making Optimal BFT for Partial Synchrony Practical 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.08091v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.08091v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrew Lewis-Pye, Dahlia Malkhi, Oded Naor, Kartik Nayak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The view synchronization problem lies at the heart of many Byzantine Fault
Tolerant (BFT) State Machine Replication (SMR) protocols in the partial
synchrony model, since these protocols are usually based on views. Liveness is
guaranteed if honest processors spend a sufficiently long time in the same view
during periods of synchrony, and if the leader of the view is honest. Ensuring
that these conditions occur, known as Byzantine View Synchronization (BVS), has
turned out to be the performance bottleneck of many BFT SMR protocols.
  A recent line of work has shown that, by using an appropriate view
synchronization protocol, BFT SMR protocols can achieve $O(n^2)$ communication
complexity in the worst case after GST, thereby finally matching the lower
bound established by Dolev and Reischuk in 1985. However, these protocols
suffer from two major issues:
  (1) When implemented so as to be optimistically responsive, even a single
Byzantine processor may infinitely often cause $\Omega(n\Delta)$ latency
between consecutive consensus decisions.
  (2) Even in the absence of Byzantine action, infinitely many views require
honest processors to send $\Omega(n^2)$ messages.
  Here, we present Lumiere, an optimistically responsive BVS protocol which
maintains optimal worst-case communication complexity while simultaneously
addressing the two issues above: for the first time, Lumiere enables BFT
consensus solutions in the partial synchrony setting that have $O(n^2)$
worst-case communication complexity, and that eventually always (i.e., except
for a small constant number of "warmup" decisions) have communication
complexity and latency which is linear in the number of actual faults in the
execution.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EdgeOL: Efficient in-situ Online Learning on Edge Devices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.16694v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.16694v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sheng Li, Geng Yuan, Yawen Wu, Yue Dai, Chao Wu, Alex K. Jones, Jingtong Hu, Yanzhi Wang, Xulong Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Emerging applications, such as robot-assisted eldercare and object
recognition, generally employ deep learning neural networks (DNNs) and
naturally require: i) handling streaming-in inference requests and ii) adapting
to possible deployment scenario changes. Online model fine-tuning is widely
adopted to satisfy these needs. However, an inappropriate fine-tuning scheme
could involve significant energy consumption, making it challenging to deploy
on edge devices. In this paper, we propose EdgeOL, an edge online learning
framework that optimizes inference accuracy, fine-tuning execution time, and
energy efficiency through both inter-tuning and intra-tuning optimizations.
Experimental results show that, on average, EdgeOL reduces overall fine-tuning
execution time by 64%, energy consumption by 52%, and improves average
inference accuracy by 1.75% over the immediate online learning strategy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Design, Configuration, Implementation, and Performance of a Simple 32
  Core Raspberry Pi Cluster 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/1708.05264v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/1708.05264v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vincent A. Cicirello
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this report, I describe the design and implementation of an inexpensive,
eight node, 32 core, cluster of raspberry pi single board computers, as well as
the performance of this cluster on two computational tasks, one that requires
significant data transfer relative to computational time requirements, and one
that does not. We have two use-cases for the cluster: (a) as an educational
tool for classroom usage, such as covering parallel algorithms in an algorithms
course; and (b) as a test system for use during the development of parallel
metaheuristics, essentially serving as a personal desktop parallel computing
cluster. Our preliminary results show that the slow 100 Mbps networking of the
raspberry pi significantly limits such clusters to parallel computational tasks
that are either long running relative to data communications requirements, or
that which requires very little internode communications. Additionally,
although the raspberry pi 3 has a quad-core processor, parallel speedup
degrades during attempts to utilize all four cores of all cluster nodes for a
parallel computation, likely due to resource contention with operating system
level processes. However, distributing a task across three cores of each
cluster node does enable linear (or near linear) speedup.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Minor edits to the text (e.g., fixing typos, etc)</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Databases
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Vector search with small radiuses 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10746v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10746v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gergely Szilvasy, Pierre-Emmanuel Mazaré, Matthijs Douze
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, the dominant accuracy metric for vector search is the recall
of a result list of fixed size (top-k retrieval), considering as ground truth
the exact vector retrieval results. Although convenient to compute, this metric
is distantly related to the end-to-end accuracy of a full system that
integrates vector search. In this paper we focus on the common case where a
hard decision needs to be taken depending on the vector retrieval results, for
example, deciding whether a query image matches a database image or not. We
solve this as a range search task, where all vectors within a certain radius
from the query are returned.
  We show that the value of a range search result can be modeled rigorously
based on the query-to-vector distance. This yields a metric for range search,
RSM, that is both principled and easy to compute without running an end-to-end
evaluation. We apply this metric to the case of image retrieval. We show that
indexing methods that are adapted for top-k retrieval do not necessarily
maximize the RSM. In particular, for inverted file based indexes, we show that
visiting a limited set of clusters and encoding vectors compactly yields near
optimal results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rule based Complex Event Processing for an Air Quality Monitoring System
  in Smart City 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14701v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14701v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shashi Shekhar Kumar, Ritesh Chandra, Sonali Agarwal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, smart city-based development has gained momentum due to its
versatile nature in architecture and planning for the systematic habitation of
human beings. According to World Health Organization (WHO) report, air
pollution causes serious respiratory diseases. Hence, it becomes necessary to
real-time monitoring of air quality to minimize effect by taking time-bound
decisions by the stakeholders. The air pollution comprises various compositions
such as NH3, O3, SO2, NO2, etc., and their concentrations vary from location to
location.The research work proposes an integrated framework for monitoring air
quality using rule-based Complex Event Processing (CEP) and SPARQL queries. CEP
works with the data stream based on predefined rules to detect the complex
pattern, which helps in decision support for stakeholders. Initially, the
dataset was collected from the Central Pollution Control Board (CPCB) of India
and this data was then preprocessed and passed through Apache Kafka. Then a
knowledge graph developed based on the air quality paradigm. Consequently,
convert preprocessed data into Resource Description Framework (RDF) data, and
integrate with Knowledge graph which is ingested to CEP engine using Apache
Jena for enhancing the decision support . Simultaneously, rules are extracted
using a decision tree, and some ground truth parameters of CPCB are added and
ingested to the CEP engine to determine the complex patterns. Consequently, the
SPARQL query is used on real-time RDF dataset for fetching the condition of air
quality as good, poor, severe, hazardous etc based on complex events detection.
For validating the proposed approach various chunks of RDF are used for the
deployment of events to the CEP engine, and its performance is examined over
time while performing simple and complex queries.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Spatialyze: A Geospatial Video Analytics System with Spatial-Aware
  Optimizations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.03276v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.03276v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chanwut Kittivorawong, Yongming Ge, Yousef Helal, Alvin Cheung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Videos that are shot using commodity hardware such as phones and surveillance
cameras record various metadata such as time and location. We encounter such
geospatial videos on a daily basis and such videos have been growing in volume
significantly. Yet, we do not have data management systems that allow users to
interact with such data effectively.
  In this paper, we describe Spatialyze, a new framework for end-to-end
querying of geospatial videos. Spatialyze comes with a domain-specific language
where users can construct geospatial video analytic workflows using a 3-step,
declarative, build-filter-observe paradigm. Internally, Spatialyze leverages
the declarative nature of such workflows, the temporal-spatial metadata stored
with videos, and physical behavior of real-world objects to optimize the
execution of workflows. Our results using real-world videos and workflows show
that Spatialyze can reduce execution time by up to 5.3x, while maintaining up
to 97.1% accuracy compared to unoptimized execution.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>GitHub Repository: https://github.com/apperception-db/spatialyze</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Polystore Architecture Using Knowledge Graphs to Support Queries on
  Heterogeneous Data Stores 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.03584v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.03584v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leonardo Guerreiro Azevedo, Renan Francisco Santos Souza, Elton F. de S. Soares, Raphael M. Thiago, Julio Cesar Cardoso Tesolin, Ann C. Oliveira, Marcio Ferreira Moreno
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern applications commonly need to manage dataset types composed of
heterogeneous data and schemas, making it difficult to access them in an
integrated way. A single data store to manage heterogeneous data using a common
data model is not effective in such a scenario, which results in the domain
data being fragmented in the data stores that best fit their storage and access
requirements (e.g., NoSQL, relational DBMS, or HDFS). Besides, organization
workflows independently consume these fragments, and usually, there is no
explicit link among the fragments that would be useful to support an integrated
view. The research challenge tackled by this work is to provide the means to
query heterogeneous data residing on distinct data repositories that are not
explicitly connected. We propose a federated database architecture by providing
a single abstract global conceptual schema to users, allowing them to write
their queries, encapsulating data heterogeneity, location, and linkage by
employing: (i) meta-models to represent the global conceptual schema, the
remote data local conceptual schemas, and mappings among them; (ii) provenance
to create explicit links among the consumed and generated data residing in
separate datasets. We evaluated the architecture through its implementation as
a polystore service, following a microservice architecture approach, in a
scenario that simulates a real case in Oil \& Gas industry. Also, we compared
the proposed architecture to a relational multidatabase system based on foreign
data wrappers, measuring the user's cognitive load to write a query (or query
complexity) and the query processing time. The results demonstrated that the
proposed architecture allows query writing two times less complex than the one
written for the relational multidatabase system, adding an excess of no more
than 30% in query processing time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Reference the paper as L. G. Azevedo, R. Souza, E. F. de S. Soares,
  R. M. Thiago, J. C. D. Tesolin, A. C. Oliveira, M. F. Moreno, A Polystore
  Architecture Using Knowledge Graphs to Support Queries on Heterogeneous Data
  Stores. Proceedings of 20th Brazilian Symposium in Information Systems, 2024
  (to be published)</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Science and Game Theory
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Auctions with Dynamic Scoring 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11022v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11022v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Martino Banchio, Aranyak Mehta, Andres Perlroth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the design of auctions with dynamic scoring, which allocate a single
item according to a given scoring rule. We are motivated by online advertising
auctions when users interact with a platform over the course of a session. The
platform ranks ads based on a combination of bids and quality scores, and
updates the quality scores throughout the session based on the user's online
activity. The platform must decide when to show an ad during the session. By
delaying the auction, the auctioneer acquires information about an ad's
quality, improving her chances of selecting a high quality ad. However
information is costly, because delay reduces market thickness and in turn
revenue. When should the auctioneer allocate the impression to balance these
forces?
  We develop a theoretical model to study the effect of market design on the
trade-off between market thickness and information. In particular, we focus on
first- and second-price auctions. The auctioneer can commit to the auction
format, but not to its timing: her decision can thus be cast as a real options
problem. We show that under optimal stopping the first-price auction allocates
efficiently but with delay. Instead, the second-price auction generates more
revenue by avoiding delay. The auctioneer benefits from introducing reserve
prices, more so in a first-price auction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Inverse learning of black-box aggregator for robust Nash equilibrium 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10980v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10980v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guanpu Chen, Gehui Xu, Fengxiang He, Dacheng Tao, Thomas Parisini, Karl Henrik Johansson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this note, we investigate the robustness of Nash equilibria (NE) in
multi-player aggregative games with coupling constraints. There are many
algorithms for computing an NE of an aggregative game given a known aggregator.
When the coupling parameters are affected by uncertainty, robust NE need to be
computed. We consider a scenario where players' weight in the aggregator is
unknown, making the aggregator kind of "a black box". We pursue a suitable
learning approach to estimate the unknown aggregator by proposing an inverse
variational inequality-based relationship. We then utilize the counterpart to
reconstruct the game and obtain first-order conditions for robust NE in the
worst case. Furthermore, we characterize the generalization property of the
learning methodology via an upper bound on the violation probability.
Simulation experiments show the effectiveness of the proposed inverse learning
approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mediator Interpretation and Faster Learning Algorithms for Linear
  Correlated Equilibria in General Extensive-Form Games 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.15935v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.15935v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brian Hu Zhang, Gabriele Farina, Tuomas Sandholm
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A recent paper by Farina & Pipis (2023) established the existence of
uncoupled no-linear-swap regret dynamics with polynomial-time iterations in
extensive-form games. The equilibrium points reached by these dynamics, known
as linear correlated equilibria, are currently the tightest known relaxation of
correlated equilibrium that can be learned in polynomial time in any finite
extensive-form game. However, their properties remain vastly unexplored, and
their computation is onerous. In this paper, we provide several contributions
shedding light on the fundamental nature of linear-swap regret. First, we show
a connection between linear deviations and a generalization of communication
deviations in which the player can make queries to a "mediator" who replies
with action recommendations, and, critically, the player is not constrained to
match the timing of the game as would be the case for communication deviations.
We coin this latter set the untimed communication (UTC) deviations. We show
that the UTC deviations coincide precisely with the linear deviations, and
therefore that any player minimizing UTC regret also minimizes linear-swap
regret. We then leverage this connection to develop state-of-the-art no-regret
algorithms for computing linear correlated equilibria, both in theory and in
practice. In theory, our algorithms achieve polynomially better per-iteration
runtimes; in practice, our algorithms represent the state of the art by several
orders of magnitude.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-15T00:00:00Z">2024-03-15</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Distributed, Parallel, and Cluster Computing
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Strict Partitioning for Sporadic Rigid Gang Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10726v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10726v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Binqi Sun, Tomasz Kloda, Marco Caccamo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rigid gang task model is based on the idea of executing multiple threads
simultaneously on a fixed number of processors to increase efficiency and
performance. Although there is extensive literature on global rigid gang
scheduling, partitioned approaches have several practical advantages (e.g.,
task isolation and reduced scheduling overheads). In this paper, we propose a
new partitioned scheduling strategy for rigid gang tasks, named strict
partitioning. The method creates disjoint partitions of tasks and processors to
avoid inter-partition interference. Moreover, it tries to assign tasks with
similar volumes (i.e., parallelisms) to the same partition so that the
intra-partition interference can be reduced. Within each partition, the tasks
can be scheduled using any type of scheduler, which allows the use of a less
pessimistic schedulability test. Extensive synthetic experiments and a case
study based on Edge TPU benchmarks show that strict partitioning achieves
better schedulability performance than state-of-the-art global gang
schedulability analyses for both preemptive and non-preemptive rigid gang task
sets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>to be published in IEEE Real-Time and Embedded Technology and
  Applications Symposium (RTAS 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ATOM: Asynchronous Training of Massive Models for Deep Learning in a
  Decentralized Environment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10504v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10504v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaofeng Wu, Jia Rao, Wei Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of the Transformer architecture has propelled the growth of
natural language processing (NLP) models, leading to remarkable achievements in
numerous NLP tasks. Yet, the absence of specialized hardware like expansive GPU
memory and high-speed interconnects poses challenges for training large-scale
models. This makes it daunting for many users to experiment with pre-training
and fine-tuning large language models (LLMs). In this study, we introduce
\atom, a resilient distributed training framework designed for asynchronous
training of vast models in a decentralized setting using cost-effective
hardware, including consumer-grade GPUs and Ethernet. Unlike conventional model
partitioning methods that distribute sub-models across GPUs, \atom aims to
accommodate a complete LLM on one host (peer) through seamlessly model swapping
and concurrently trains multiple copies across various peers to optimize
training throughput. Through static analysis, \atom identifies the best model
partitioning strategy and flawlessly merges model execution with swapping. Key
benefits of \atom include: Avoiding the central point of failure found in
pipeline parallelism methods. Demonstrating superior performance and
scalability compared to closely-integrated pipeline parallelism in slower
networks. Our experiments using different GPT-3 model configurations reveal
that, in scenarios with suboptimal network connections, \atom can enhance
training efficiency up to $20 \times$ when juxtaposed with the state-of-the-art
decentralized pipeline parallelism approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluation of Quantum and Hybrid Solvers for Combinatorial Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10455v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10455v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amedeo Bertuzzi, Davide Ferrari, Antonio Manzalini, Michele Amoretti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Academic and industrial sectors have been engaged in a fierce competition to
develop quantum technologies, fueled by the explosive advancements in quantum
hardware. While universal quantum computers have been shown to support up to
hundreds of qubits, the scale of quantum annealers has reached three orders of
magnitude (i.e., thousands of qubits). Therefore, quantum algorithms are
becoming increasingly popular in a variety of fields, with optimization being
one of the most prominent. This work aims to explore the topic of quantum
optimization by comprehensively evaluating the technologies provided by D-Wave
Systems. To do so, a model for the energy optimization of data centers is
proposed as a benchmark. D-Wave quantum and hybrid solvers are compared, in
order to identify the most suitable one for the considered application. To
highlight its advantageous performance capabilities and associated solving
potential, the selected D-Wave hybrid solver is then contrasted with CPLEX, a
highly efficient classical solver.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GreedyML: A Parallel Algorithm for Maximizing Submodular Functions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10332v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10332v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shivaram Gopal, S M Ferdous, Hemanta K. Maji, Alex Pothen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We describe a parallel approximation algorithm for maximizing monotone
submodular functions subject to hereditary constraints on distributed memory
multiprocessors. Our work is motivated by the need to solve submodular
optimization problems on massive data sets, for practical applications in areas
such as data summarization, machine learning, and graph sparsification. Our
work builds on the randomized distributed RandGreedI algorithm, proposed by
Barbosa, Ene, Nguyen, and Ward (2015). This algorithm computes a distributed
solution by randomly partitioning the data among all the processors and then
employing a single accumulation step in which all processors send their partial
solutions to one processor. However, for large problems, the accumulation step
could exceed the memory available on a processor, and the processor which
performs the accumulation could become a computational bottleneck.
  Here, we propose a generalization of the RandGreedI algorithm that employs
multiple accumulation steps to reduce the memory required. We analyze the
approximation ratio and the time complexity of the algorithm (in the BSP
model). We also evaluate the new GreedyML algorithm on three classes of
problems, and report results from massive data sets with millions of elements.
The results show that the GreedyML algorithm can solve problems where the
sequential Greedy and distributed RandGreedI algorithms fail due to memory
constraints. For certain computationally intensive problems, the GreedyML
algorithm can be faster than the RandGreedI algorithm. The observed
approximation quality of the solutions computed by the GreedyML algorithm
closely matches those obtained by the RandGreedI algorithm on these problems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DSP: Dynamic Sequence Parallelism for Multi-Dimensional <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10266v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10266v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuanlei Zhao, Shenggan Cheng, Zangwei Zheng, Zheming Yang, Ziming Liu, Yang You
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scaling large models with long sequences across applications like language
generation, video generation and multimodal tasks requires efficient sequence
parallelism. However, existing sequence parallelism methods all assume a single
sequence dimension and fail to adapt to multi-dimensional transformer
architectures that perform attention calculations across different dimensions.
This paper introduces Dynamic Sequence Parallelism (DSP), a novel approach to
enable efficient sequence parallelism for multi-dimensional transformer models.
The key idea is to dynamically switch the parallelism dimension according to
the current computation stage, leveraging the potential characteristics of
multi-dimensional attention. This dynamic dimension switching allows sequence
parallelism with minimal communication overhead compared to applying
traditional single-dimension parallelism to multi-dimensional models.
Experiments show DSP improves end-to-end throughput by 42.0% to 216.8% over
prior sequence parallelism methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A2CI: A Cloud-based, Service-oriented Geospatial Cyberinfrastructure to
  Support Atmospheric Research 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14693v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14693v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenwen Li, Hu Shao, Sizhe Wang, Xiran Zhou, Sheng Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Big earth science data offers the scientific community great opportunities.
Many more studies at large-scales, over long-terms and at high resolution can
now be conducted using the rich information collected by remote sensing
satellites, ground-based sensor networks, and even social media input. However,
the hundreds of terabytes of information collected and compiled on an hourly
basis by NASA and other government agencies present a significant challenge for
atmospheric scientists seeking to improve the understanding of the Earth
atmospheric system. These challenges include effective discovery, organization,
analysis and visualization of large amounts of data. This paper reports the
outcomes of an NSF-funded project that developed a geospatial
cyberinfrastructure -- the A2CI (Atmospheric Analysis Cyberinfrastructure) --
to support atmospheric research. We first introduce the service-oriented system
framework then describe in detail the implementation of the data discovery
module, data management module, data integration module, data analysis and
visualization modules following the cloud computing
principles-Data-as-a-Service, Software-as-a-Service, Platform-as-a-Service and
Infrastructure-as-a-Service. We demonstrate the graphic user interface by
performing an analysis between Sea Surface Temperature and the intensity of
tropical storms in the North Atlantic and Pacific oceans. We expect this work
to contribute to the technical advancement of cyberinfrastructure research as
well as to the development of an online, collaborative scientific analysis
system for atmospheric science.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Harnessing Deep Learning and HPC Kernels via High-Level Loop and Tensor
  Abstractions on CPU Architectures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.12576v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.12576v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Evangelos Georganas, Dhiraj Kalamkar, Kirill Voronin, Abhisek Kundu, Antonio Noack, Hans Pabst, Alexander Breuer, Alexander Heinecke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  During the past decade, Deep Learning (DL) algorithms, programming systems
and hardware have converged with the High Performance Computing (HPC)
counterparts. Nevertheless, the programming methodology of DL and HPC systems
is stagnant, relying on highly-optimized, yet platform-specific and inflexible
vendor-optimized libraries. Such libraries provide close-to-peak performance on
specific platforms, kernels and shapes thereof that vendors have dedicated
optimizations efforts, while they underperform in the remaining use-cases,
yielding non-portable codes with performance glass-jaws. This work introduces a
framework to develop efficient, portable DL and HPC kernels for modern CPU
architectures. We decompose the kernel development in two steps: 1) Expressing
the computational core using Tensor Processing Primitives (TPPs): a compact,
versatile set of 2D-tensor operators, 2) Expressing the logical loops around
TPPs in a high-level, declarative fashion whereas the exact instantiation
(ordering, tiling, parallelization) is determined via simple knobs. We
demonstrate the efficacy of our approach using standalone kernels and
end-to-end workloads that outperform state-of-the-art implementations on
diverse CPU platforms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Ensuring Data Privacy in AC Optimal Power Flow with a Distributed
  Co-Simulation Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.01001v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.01001v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinliang Dai, Alexander Kocher, Jovana Kovačević, Burak Dindar, Yuning Jiang, Colin N. Jones, Hüseyin Çakmak, Veit Hagenmeyer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  During the energy transition, the significance of collaborative management
among institutions is rising, confronting challenges posed by data privacy
concerns. Prevailing research on distributed approaches, as an alternative to
centralized management, often lacks numerical convergence guarantees or is
limited to single-machine numerical simulation. To address this, we present a
distributed approach for solving AC Optimal Power Flow (OPF) problems within a
geographically distributed environment. This involves integrating the energy
system Co-Simulation (eCoSim) module in the eASiMOV framework with the
convergence-guaranteed distributed optimization algorithm, i.e., the Augmented
Lagrangian based Alternating Direction Inexact Newton method (ALADIN).
Comprehensive evaluations across multiple system scenarios reveal a marginal
performance slowdown compared to the centralized approach and the distributed
approach executed on single machines -- a justified trade-off for enhanced data
privacy. This investigation serves as empirical validation of the successful
execution of distributed AC OPF within a geographically distributed
environment, highlighting potential directions for future research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MMO: Meta Multi-Objectivization for Software Configuration Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.07303v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.07303v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengzhou Chen, Tao Chen, Miqing Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Software configuration tuning is essential for optimizing a given performance
objective (e.g., minimizing latency). Yet, due to the software's intrinsically
complex configuration landscape and expensive measurement, there has been a
rather mild success, particularly in preventing the search from being trapped
in local optima. To address this issue, in this paper we take a different
perspective. Instead of focusing on improving the optimizer, we work on the
level of optimization model and propose a meta multi-objectivization (MMO)
model that considers an auxiliary performance objective (e.g., throughput in
addition to latency). What makes this model distinct is that we do not optimize
the auxiliary performance objective, but rather use it to make
similarly-performing while different configurations less comparable (i.e.
Pareto nondominated to each other), thus preventing the search from being
trapped in local optima. Importantly, by designing a new normalization method,
we show how to effectively use the MMO model without worrying about its weight
-- the only yet highly sensitive parameter that can affect its effectiveness.
Experiments on 22 cases from 11 real-world software systems/environments
confirm that our MMO model with the new normalization performs better than its
state-of-the-art single-objective counterparts on 82% cases while achieving up
to 2.09x speedup. For 68% of the cases, the new normalization also enables the
MMO model to outperform the instance when using it with the normalization from
our prior FSE work under pre-tuned best weights, saving a great amount of
resources which would be otherwise necessary to find a good weight. We also
demonstrate that the MMO model with the new normalization can consolidate
recent model-based tuning tools on 68% of the cases with up to 1.22x speedup in
general.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 figures, 4 tables. journal extension at TSE. arXiv admin note:
  text overlap with arXiv:2106.01331</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Real-Time Scheduling for 802.1Qbv Time-Sensitive Networking (TSN): A
  Systematic <span class="highlight-title">Review</span> and Experimental Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.16772v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.16772v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuanyu Xue, Tianyu Zhang, Yuanbin Zhou, Mark Nixon, Andrew Loveless, Song Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Time-Sensitive Networking (TSN) has been recognized as one of the key
enabling technologies for Industry 4.0 and has been deployed in many mission-
and safety-critical applications e.g., automotive and aerospace systems. Given
the stringent real-time requirements of these applications, the Time-Aware
Shaper (TAS) draws special attention among TSN's many traffic shapers due to
its ability to achieve deterministic timing guarantees. Many scheduling methods
for TAS shapers have been recently developed that claim to improve system
schedulability. However, these scheduling methods have yet to be thoroughly
evaluated, especially through experimental comparisons, to provide a
systematical understanding of their performance using different evaluation
metrics in diverse application scenarios. In this paper, we fill this gap by
presenting a systematic review and experimental study on existing TAS-based
scheduling methods for TSN. We first categorize the system models employed in
these works along with the specific problems they aim to solve, and outline the
fundamental considerations in the designs of TAS-based scheduling methods. We
then perform an extensive evaluation on 17 representative solutions using both
high-fidelity simulations and a real-life TSN testbed, and compare their
performance under both synthetic scenarios and real-life industrial use cases.
Through these experimental studies, we identify the limitations of individual
scheduling methods and highlight several important findings. We expect this
work will provide foundational knowledge and performance benchmarks needed for
future studies on real-time TSN scheduling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 6 authors, RTAS24 tech report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Quick Order Fairness: Implementation and Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.13107v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.13107v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christian Cachin, Jovana Micic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Decentralized finance revolutionizes traditional financial systems by
leveraging blockchain technology to reduce trust. However, some vulnerabilities
persist, notably front-running by malicious actors who exploit transaction
information to gain financial advantage. Consensus with a fair order aims at
preventing such attacks, and in particular, the differential order fairness
property addresses this problem and connects fair ordering to the validity of
consensus. The notion is implemented by the Quick Order-Fair Atomic Broadcast
(QOF) protocol (Cachin et al., FC '22). This paper revisits the QOF protocol
and describes a modular implementation that uses a generic consensus component.
Moreover, an empirical evaluation is performed to compare the performance of
QOF to a consensus protocol without fairness. Measurements show that the
increased complexity comes at a cost, throughput decreases by at most 5%, and
latency increases by roughly 50ms, using an emulated ideal network. This paper
contributes to a comprehensive understanding of practical aspects regarding
differential order fairness with the QOF protocol and also connects this with
similar fairness-imposing protocols like Themis and Pompe.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Performance
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluation of Quantum and Hybrid Solvers for Combinatorial Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10455v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10455v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amedeo Bertuzzi, Davide Ferrari, Antonio Manzalini, Michele Amoretti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Academic and industrial sectors have been engaged in a fierce competition to
develop quantum technologies, fueled by the explosive advancements in quantum
hardware. While universal quantum computers have been shown to support up to
hundreds of qubits, the scale of quantum annealers has reached three orders of
magnitude (i.e., thousands of qubits). Therefore, quantum algorithms are
becoming increasingly popular in a variety of fields, with optimization being
one of the most prominent. This work aims to explore the topic of quantum
optimization by comprehensively evaluating the technologies provided by D-Wave
Systems. To do so, a model for the energy optimization of data centers is
proposed as a benchmark. D-Wave quantum and hybrid solvers are compared, in
order to identify the most suitable one for the considered application. To
highlight its advantageous performance capabilities and associated solving
potential, the selected D-Wave hybrid solver is then contrasted with CPLEX, a
highly efficient classical solver.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Power of Training: How Different Neural Network Setups Influence the
  Energy Demand 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01851v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01851v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Geißler, Bo Zhou, Mengxi Liu, Sungho Suh, Paul Lukowicz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work offers a heuristic evaluation of the effects of variations in
machine learning training regimes and learning paradigms on the energy
consumption of computing, especially HPC hardware with a life-cycle aware
perspective. While increasing data availability and innovation in
high-performance hardware fuels the training of sophisticated models, it also
fosters the fading perception of energy consumption and carbon emission.
Therefore, the goal of this work is to raise awareness about the energy impact
of general training parameters and processes, from learning rate over batch
size to knowledge transfer. Multiple setups with different hyperparameter
configurations are evaluated on three different hardware systems. Among many
results, we have found out that even with the same model and hardware to reach
the same accuracy, improperly set training hyperparameters consume up to 5
times the energy of the optimal setup. We also extensively examined the
energy-saving benefits of learning paradigms including recycling knowledge
through pretraining and sharing knowledge through multitask training.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Databases
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interactive Trimming against Evasive Online Data Manipulation Attacks: A
  Game-Theoretic Approach <span class="chip">ICDE '24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10313v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10313v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Fu, Qingqing Ye, Rong Du, Haibo Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the exponential growth of data and its crucial impact on our lives and
decision-making, the integrity of data has become a significant concern.
Malicious data poisoning attacks, where false values are injected into the
data, can disrupt machine learning processes and lead to severe consequences.
To mitigate these attacks, distance-based defenses, such as trimming, have been
proposed, but they can be easily evaded by white-box attackers. The evasiveness
and effectiveness of poisoning attack strategies are two sides of the same
coin, making game theory a promising approach. However, existing
game-theoretical models often overlook the complexities of online data
poisoning attacks, where strategies must adapt to the dynamic process of data
collection.
  In this paper, we present an interactive game-theoretical model to defend
online data manipulation attacks using the trimming strategy. Our model
accommodates a complete strategy space, making it applicable to strong evasive
and colluding adversaries. Leveraging the principle of least action and the
Euler-Lagrange equation from theoretical physics, we derive an analytical model
for the game-theoretic process. To demonstrate its practical usage, we present
a case study in a privacy-preserving data collection system under local
differential privacy where a non-deterministic utility function is adopted. Two
strategies are devised from this analytical model, namely, Tit-for-tat and
Elastic. We conduct extensive experiments on real-world datasets, which
showcase the effectiveness and accuracy of these two strategies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This manuscript is accepted by ICDE '24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ KIF: A Framework for Virtual Integration of Heterogeneous Knowledge
  Bases using Wikidata 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10304v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10304v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guilherme Lima, Marcelo Machado, Elton Soares, Sandro R. Fiorini, Raphael Thiago, Leonardo G. Azevedo, Viviane T. da Silva, Renato Cerqueira
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a knowledge integration framework (called KIF) that uses Wikidata
as a lingua franca to integrate heterogeneous knowledge bases. These can be
triplestores, relational databases, CSV files, etc., which may or may not use
the Wikidata dialect of RDF. KIF leverages Wikidata's data model and vocabulary
plus user-defined mappings to expose a unified view of the integrated bases
while keeping track of the context and provenance of their statements. The
result is a virtual knowledge base which behaves like an "extended Wikidata"
and which can be queried either through an efficient filter interface or using
SPARQL. We present the design and implementation of KIF, discuss how we have
used it to solve a real integration problem in the domain of chemistry
(involving Wikidata, PubChem, and IBM CIRCA), and present experimental results
on the performance and overhead of KIF.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Accelerating Regular Path Queries over Graph Database with
  Processing-in-Memory 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10051v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10051v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruoyan Ma, Shengan Zheng, Guifeng Wang, Jin Pu, Yifan Hua, Wentao Wang, Linpeng Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Regular path queries (RPQs) in graph databases are bottlenecked by the memory
wall. Emerging processing-in-memory (PIM) technologies offer a promising
solution to dispatch and execute path matching tasks in parallel within PIM
modules. We present Moctopus, a PIM-based data management system for graph
databases that supports efficient batch RPQs and graph updates. Moctopus
employs a PIM-friendly dynamic graph partitioning algorithm, which tackles
graph skewness and preserves graph locality with low overhead for RPQ
processing. Moctopus enables efficient graph update by amortizing the host
CPU's update overhead to PIM modules. Evaluation of Moctopus demonstrates
superiority over the state-of-the-art traditional graph database.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VeriEQL: Bounded Equivalence Verification for Complex SQL Queries with
  Integrity Constraints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.03193v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.03193v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang He, Pinhan Zhao, Xinyu Wang, Yuepeng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The task of SQL query equivalence checking is important in various real-world
applications (including query rewriting and automated grading) that involve
complex queries with integrity constraints; yet, state-of-the-art techniques
are very limited in their capability of reasoning about complex features (e.g.,
those that involve sorting, case statement, rich integrity constraints, etc.)
in real-life queries. To the best of our knowledge, we propose the first
SMT-based approach and its implementation, VeriEQL, capable of proving and
disproving bounded equivalence of complex SQL queries. VeriEQL is based on a
new logical encoding that models query semantics over symbolic tuples using the
theory of integers with uninterpreted functions. It is simple yet highly
practical -- our comprehensive evaluation on over 20,000 benchmarks shows that
VeriEQL outperforms all state-of-the-art techniques by more than one order of
magnitude in terms of the number of benchmarks that can be proved or disproved.
VeriEQL can also generate counterexamples that facilitate many downstream tasks
(such as finding serious bugs in systems like MySQL and Apache Calcite).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>OOPSLA 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Finding Smallest Witnesses for Conjunctive Queries 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.18157v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.18157v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiao Hu, Stavros Sintos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A witness is a sub-database that preserves the query results of the original
database but of much smaller size. It has wide applications in query rewriting
and debugging, query explanation, IoT analytics, multi-layer network routing,
etc. In this paper, we study the smallest witness problem (SWP) for the class
of conjunctive queries (CQs) without self-joins.
  We first establish the dichotomy that SWP for a CQ can be computed in
polynomial time if and only if it has {\em head-cluster property}, unless
$\texttt{P} = \texttt{NP}$. We next turn to the approximated version by
relaxing the size of a witness from being minimum. We surprisingly find that
the {\em head-domination} property - that has been identified for the deletion
propagation problem \cite{kimelfeld2012maximizing} - can also precisely capture
the hardness of the approximated smallest witness problem. In polynomial time,
SWP for any CQ with head-domination property can be approximated within a
constant factor, while SWP for any CQ without such a property cannot be
approximated within a logarithmic factor, unless $\texttt{P} = \texttt{NP}$.
  We further explore efficient approximation algorithms for CQs without
head-domination property: (1) we show a trivial algorithm which achieves a
polynomially large approximation ratio for general CQs; (2) for any CQ with
only one non-output attribute, such as star CQs, we show a greedy algorithm
with a logarithmic approximation ratio; (3) for line CQs, which contain at
least two non-output attributes, we relate SWP problem to the directed steiner
forest problem, whose algorithms can be applied to line CQs directly.
Meanwhile, we establish a much higher lower bound, exponentially larger than
the logarithmic lower bound obtained above. It remains open to close the gap
between the lower and upper bound of the approximated SWP for CQs without
head-domination property.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-14T00:00:00Z">2024-03-14</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Distributed, Parallel, and Cluster Computing
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Benchmarking Distributed Coordination Systems: A <span class="highlight-title">Survey</span> and Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09445v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09445v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bekir Turkkan, Tevfik Kosar, Aleksey Charapko, Ailidani Ailijiang, Murat Demirbas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Coordination services and protocols are critical components of distributed
systems and are essential for providing consistency, fault tolerance, and
scalability. However, due to lack of a standard benchmarking tool for
distributed coordination services, coordination service developers/researchers
either use a NoSQL standard benchmark and omit evaluating consistency,
distribution, and fault-tolerance; or create their own ad-hoc microbenchmarks
and skip comparability with other services. In this paper, we analyze and
compare known and widely used distributed coordination services, their
evaluations, and the tools used to benchmark those systems. We identify
important requirements of distributed coordination service benchmarking, like
the metrics and parameters that need to be evaluated and their evaluation
setups and tools.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BurstAttention: An Efficient Distributed Attention Framework for
  Extremely Long Sequences 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09347v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09347v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sun Ao, Weilin Zhao, Xu Han, Cheng Yang, Zhiyuan Liu, Chuan Shi, Maosong Sun, Shengnan Wang, Teng Su
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Effective attention modules have played a crucial role in the success of
Transformer-based large language models (LLMs), but the quadratic time and
memory complexities of these attention modules also pose a challenge when
processing long sequences. One potential solution for the long sequence problem
is to utilize distributed clusters to parallelize the computation of attention
modules across multiple devices (e.g., GPUs). However, adopting a distributed
approach inevitably introduces extra memory overheads to store local attention
results and incurs additional communication costs to aggregate local results
into global ones. In this paper, we propose a distributed attention framework
named ``BurstAttention'' to optimize memory access and communication operations
at both the global cluster and local device levels. In our experiments, we
compare BurstAttention with other competitive distributed attention solutions
for long sequence processing. The experimental results under different length
settings demonstrate that BurstAttention offers significant advantages for
processing long sequences compared with these competitive baselines, reducing
40% communication overheads and achieving 2 X speedup during training 32K
sequence length on 8 X A100.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DA-PFL: Dynamic Affinity Aggregation for Personalized Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09284v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09284v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xu Yang, Jiyuan Feng, Songyue Guo, Ye Wang, Ye Ding, Binxing Fang, Qing Liao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Personalized federated learning becomes a hot research topic that can learn a
personalized learning model for each client. Existing personalized federated
learning models prefer to aggregate similar clients with similar data
distribution to improve the performance of learning models. However,
similaritybased personalized federated learning methods may exacerbate the
class imbalanced problem. In this paper, we propose a novel Dynamic
Affinity-based Personalized Federated Learning model (DA-PFL) to alleviate the
class imbalanced problem during federated learning. Specifically, we build an
affinity metric from a complementary perspective to guide which clients should
be aggregated. Then we design a dynamic aggregation strategy to dynamically
aggregate clients based on the affinity metric in each round to reduce the
class imbalanced risk. Extensive experiments show that the proposed DA-PFL
model can significantly improve the accuracy of each client in three real-world
datasets with state-of-the-art comparison methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uncertainty Estimation in Multi-Agent Distributed Learning for
  AI-Enabled Edge Devices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09141v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09141v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gleb Radchenko, Victoria Andrea Fill
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Initially considered as low-power units with limited autonomous processing,
Edge IoT devices have seen a paradigm shift with the introduction of FPGAs and
AI accelerators. This advancement has vastly amplified their computational
capabilities, emphasizing the practicality of edge AI. Such progress introduces
new challenges of optimizing AI tasks for the limitations of energy and network
resources typical in Edge computing environments. Our study explores methods
that enable distributed data processing through AI-enabled edge devices,
enhancing collaborative learning capabilities. A key focus of our research is
the challenge of determining confidence levels in learning outcomes,
considering the spatial and temporal variability of data sets encountered by
independent agents. To address this issue, we investigate the application of
Bayesian neural networks, proposing a novel approach to manage uncertainty in
distributed learning environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FedComLoc: Communication-Efficient Distributed Training of Sparse and
  Quantized Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09904v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09904v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Yi, Georg Meinhardt, Laurent Condat, Peter Richtárik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) has garnered increasing attention due to its unique
characteristic of allowing heterogeneous clients to process their private data
locally and interact with a central server, while being respectful of privacy.
A critical bottleneck in FL is the communication cost. A pivotal strategy to
mitigate this burden is \emph{Local Training}, which involves running multiple
local stochastic gradient descent iterations between communication phases. Our
work is inspired by the innovative \emph{Scaffnew} algorithm, which has
considerably advanced the reduction of communication complexity in FL. We
introduce FedComLoc (Federated Compressed and Local Training), integrating
practical and effective compression into \emph{Scaffnew} to further enhance
communication efficiency. Extensive experiments, using the popular TopK
compressor and quantization, demonstrate its prowess in substantially reducing
communication overheads in heterogeneous settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DeepVM: Integrating Spot and On-Demand VMs for Cost-Efficient Deep
  Learning Clusters in the Cloud 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.05861v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.05861v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yoochan Kim, Kihyun Kim, Yonghyeon Cho, Jinwoo Kim, Awais Khan, Ki-Dong Kang, Baik-Song An, Myung-Hoon Cha, Hong-Yeon Kim, Youngjae Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Distributed Deep Learning (DDL), as a paradigm, dictates the use of GPU-based
clusters as the optimal infrastructure for training large-scale Deep Neural
Networks (DNNs). However, the high cost of such resources makes them
inaccessible to many users. Public cloud services, particularly Spot Virtual
Machines (VMs), offer a cost-effective alternative, but their unpredictable
availability poses a significant challenge to the crucial checkpointing process
in DDL. To address this, we introduce DeepVM, a novel solution that recommends
cost-effective cluster configurations by intelligently balancing the use of
Spot and On-Demand VMs. DeepVM leverages a four-stage process that analyzes
instance performance using the FLOPP (FLoating-point Operations Per Price)
metric, performs architecture-level analysis with linear programming, and
identifies the optimal configuration for the user-specific needs. Extensive
simulations and real-world deployments in the AWS environment demonstrate that
DeepVM consistently outperforms other policies, reducing training costs and
overall makespan. By enabling cost-effective checkpointing with Spot VMs,
DeepVM opens up DDL to a wider range of users and facilitates a more efficient
training of complex DNNs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FedImpro: Measuring and Improving Client Update in Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.07011v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.07011v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenheng Tang, Yonggang Zhang, Shaohuai Shi, Xinmei Tian, Tongliang Liu, Bo Han, Xiaowen Chu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) models often experience client drift caused by
heterogeneous data, where the distribution of data differs across clients. To
address this issue, advanced research primarily focuses on manipulating the
existing gradients to achieve more consistent client models. In this paper, we
present an alternative perspective on client drift and aim to mitigate it by
generating improved local models. First, we analyze the generalization
contribution of local training and conclude that this generalization
contribution is bounded by the conditional Wasserstein distance between the
data distribution of different clients. Then, we propose FedImpro, to construct
similar conditional distributions for local training. Specifically, FedImpro
decouples the model into high-level and low-level components, and trains the
high-level portion on reconstructed feature distributions. This approach
enhances the generalization contribution and reduces the dissimilarity of
gradients in FL. Experimental results show that FedImpro can help FL defend
against data heterogeneity and enhance the generalization performance of the
model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ COMET: A Comprehensive Cluster Design Methodology for Distributed Deep
  Learning Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.16648v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.16648v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Divya Kiran Kadiyala, Saeed Rashidi, Taekyung Heo, Abhimanyu Rajeshkumar Bambhaniya, Tushar Krishna, Alexandros Daglis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern Deep Learning (DL) models have grown to sizes requiring massive
clusters of specialized, high-end nodes to train. Designing such clusters to
maximize both performance and utilization--to amortize their steep cost--is a
challenging task requiring careful balance of compute, memory, and network
resources. Moreover, a plethora of each model's tuning knobs drastically affect
the performance, with optimal values often depending on the underlying
cluster's characteristics, which necessitates a complex cluster-workload
co-design process. To facilitate the design space exploration of such massive
DL training clusters, we introduce COMET, a holistic cluster design methodology
and workflow to jointly study the impact of parallelization strategies and key
cluster resource provisioning on the performance of distributed DL training. We
develop a step-by-step process to establish a reusable and flexible
methodology, and demonstrate its application with case studies of training
large models on cluster configurations of variable compute, memory, and network
resources. Our case studies demonstrate COMET's utility in identifying
promising architectural optimization directions and guiding system designers in
configuring key model and cluster parameters. To illustrate, cluster
configuration comparisons identify performance differences of up to 7.7x and
highlight performance optimization opportunities of up to 1.4x when employing
memory expansion as an optimization technique.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Long-Term or Temporary? Hybrid Worker Recruitment for Mobile Crowd
  Sensing and Computing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.04354v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.04354v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minghui Liwang, Zhibin Gao, Seyyedali Hosseinalipour, Zhipeng Cheng, Xianbin Wang, Zhenzhen Jiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates a novel hybrid worker recruitment problem where the
mobile crowd sensing and computing (MCSC) platform employs workers to serve
MCSC tasks with diverse quality requirements and budget constraints, under
uncertainties in workers' participation and their local workloads.We propose a
hybrid worker recruitment framework consisting of offline and online trading
modes. The former enables the platform to overbook long-term workers (services)
to cope with dynamic service supply via signing contracts in advance, which is
formulated as 0-1 integer linear programming (ILP) with probabilistic
constraints of service quality and budget.Besides, motivated by the existing
uncertainties which may render long-term workers fail to meet the service
quality requirement of each task, we augment our methodology with an online
temporary worker recruitment scheme as a backup Plan B to support seamless
service provisioning for MCSC tasks, which also represents a 0-1 ILP problem.
To tackle these problems which are proved to be NP-hard, we develop three
algorithms, namely, i) exhaustive searching, ii) unique index-based stochastic
searching with risk-aware filter constraint, iii) geometric programming-based
successive convex algorithm, which achieve the optimal or sub-optimal
solutions. Experimental results demonstrate our effectiveness in terms of
service quality, time efficiency, etc.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Artificial Intelligence
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Transformer</span>s Get Stable: An End-to-End Signal Propagation Theory for
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09635v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09635v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akhil Kedia, Mohd Abbas Zaidi, Sushil Khyalia, Jungho Jung, Harshith Goka, Haejun Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In spite of their huge success, transformer models remain difficult to scale
in depth. In this work, we develop a unified signal propagation theory and
provide formulae that govern the moments of the forward and backward signal
through the transformer model. Our framework can be used to understand and
mitigate vanishing/exploding gradients, rank collapse, and instability
associated with high attention scores. We also propose DeepScaleLM, an
initialization and scaling scheme that conserves unit output/gradient moments
throughout the model, enabling the training of very deep models with 100s of
layers. We find that transformer models could be much deeper - our deep models
with fewer parameters outperform shallow models in Language Modeling, Speech
Translation, and Image Classification, across Encoder-only, Decoder-only and
Encoder-Decoder variants, for both Pre-LN and Post-LN transformers, for
multiple datasets and model sizes. These improvements also translate into
improved performance on downstream Question Answering tasks and improved
robustness for image classification.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Akhil Kedia, Mohd Abbas Zaidi, Sushil Khyalia equal contribution.
  Source code is available at
  https://github.com/akhilkedia/TranformersGetStable</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 3D-VLA: A 3D Vision-Language-Action Generative World Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09631v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09631v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoyu Zhen, Xiaowen Qiu, Peihao Chen, Jincheng Yang, Xin Yan, Yilun Du, Yining Hong, Chuang Gan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent vision-language-action (VLA) models rely on 2D inputs, lacking
integration with the broader realm of the 3D physical world. Furthermore, they
perform action prediction by learning a direct mapping from perception to
action, neglecting the vast dynamics of the world and the relations between
actions and dynamics. In contrast, human beings are endowed with world models
that depict imagination about future scenarios to plan actions accordingly. To
this end, we propose 3D-VLA by introducing a new family of embodied foundation
models that seamlessly link 3D perception, reasoning, and action through a
generative world model. Specifically, 3D-VLA is built on top of a 3D-based
large language model (LLM), and a set of interaction tokens is introduced to
engage with the embodied environment. Furthermore, to inject generation
abilities into the model, we train a series of embodied diffusion models and
align them into the LLM for predicting the goal images and point clouds. To
train our 3D-VLA, we curate a large-scale 3D embodied instruction dataset by
extracting vast 3D-related information from existing robotics datasets. Our
experiments on held-in datasets demonstrate that 3D-VLA significantly improves
the reasoning, multimodal generation, and planning capabilities in embodied
environments, showcasing its potential in real-world applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://vis-www.cs.umass.edu/3dvla/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quiet-STaR: Language Models Can Teach Themselves to Think Before
  Speaking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09629v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09629v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eric Zelikman, Georges Harik, Yijia Shao, Varuna Jayasiri, Nick Haber, Noah D. Goodman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When writing and talking, people sometimes pause to think. Although
reasoning-focused works have often framed reasoning as a method of answering
questions or completing agentic tasks, reasoning is implicit in almost all
written text. For example, this applies to the steps not stated between the
lines of a proof or to the theory of mind underlying a conversation. In the
Self-Taught Reasoner (STaR, Zelikman et al. 2022), useful thinking is learned
by inferring rationales from few-shot examples in question-answering and
learning from those that lead to a correct answer. This is a highly constrained
setting -- ideally, a language model could instead learn to infer unstated
rationales in arbitrary text. We present Quiet-STaR, a generalization of STaR
in which LMs learn to generate rationales at each token to explain future text,
improving their predictions. We address key challenges, including 1) the
computational cost of generating continuations, 2) the fact that the LM does
not initially know how to generate or use internal thoughts, and 3) the need to
predict beyond individual next tokens. To resolve these, we propose a tokenwise
parallel sampling algorithm, using learnable tokens indicating a thought's
start and end, and an extended teacher-forcing technique. Encouragingly,
generated rationales disproportionately help model difficult-to-predict tokens
and improve the LM's ability to directly answer difficult questions. In
particular, after continued pretraining of an LM on a corpus of internet text
with Quiet-STaR, we find zero-shot improvements on GSM8K
(5.9%$\rightarrow$10.9%) and CommonsenseQA (36.3%$\rightarrow$47.2%) and
observe a perplexity improvement of difficult tokens in natural text.
Crucially, these improvements require no fine-tuning on these tasks. Quiet-STaR
marks a step towards LMs that can learn to reason in a more general and
scalable way.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Minimax Optimal and Computationally Efficient Algorithms for
  Distributionally Robust Offline Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09621v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09621v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhishuai Liu, Pan Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Distributionally robust offline reinforcement learning (RL), which seeks
robust policy training against environment perturbation by modeling dynamics
uncertainty, calls for function approximations when facing large state-action
spaces. However, the consideration of dynamics uncertainty introduces essential
nonlinearity and computational burden, posing unique challenges for analyzing
and practically employing function approximation. Focusing on a basic setting
where the nominal model and perturbed models are linearly parameterized, we
propose minimax optimal and computationally efficient algorithms realizing
function approximation and initiate the study on instance-dependent
suboptimality analysis in the context of robust offline RL. Our results uncover
that function approximation in robust offline RL is essentially distinct from
and probably harder than that in standard offline RL. Our algorithms and
theoretical results crucially depend on a variety of new techniques, involving
a novel function approximation mechanism incorporating variance information, a
new procedure of suboptimality and estimation uncertainty decomposition, a
quantification of the robust value function shrinkage, and a meticulously
designed family of hard instances, which might be of independent interest.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>53 pages, 1 figure, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Language Models and Causal Inference in Collaboration: A
  Comprehensive <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09606v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09606v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoyu Liu, Paiheng Xu, Junda Wu, Jiaxin Yuan, Yifan Yang, Yuhang Zhou, Fuxiao Liu, Tianrui Guan, Haoliang Wang, Tong Yu, Julian McAuley, Wei Ai, Furong Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Causal inference has shown potential in enhancing the predictive accuracy,
fairness, robustness, and explainability of Natural Language Processing (NLP)
models by capturing causal relationships among variables. The emergence of
generative Large Language Models (LLMs) has significantly impacted various NLP
domains, particularly through their advanced reasoning capabilities. This
survey focuses on evaluating and improving LLMs from a causal view in the
following areas: understanding and improving the LLMs' reasoning capacity,
addressing fairness and safety issues in LLMs, complementing LLMs with
explanations, and handling multimodality. Meanwhile, LLMs' strong reasoning
capacities can in turn contribute to the field of causal inference by aiding
causal relationship discovery and causal effect estimations. This review
explores the interplay between causal inference frameworks and LLMs from both
perspectives, emphasizing their collective potential to further the development
of more advanced and equitable artificial intelligence systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Counterfactual contrastive learning: robust representations via causal
  image synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09605v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09605v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Melanie Roschewitz, Fabio De Sousa Ribeiro, Tian Xia, Galvin Khara, Ben Glocker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive pretraining is well-known to improve downstream task performance
and model generalisation, especially in limited label settings. However, it is
sensitive to the choice of augmentation pipeline. Positive pairs should
preserve semantic information while destroying domain-specific information.
Standard augmentation pipelines emulate domain-specific changes with
pre-defined photometric transformations, but what if we could simulate
realistic domain changes instead? In this work, we show how to utilise recent
progress in counterfactual image generation to this effect. We propose
CF-SimCLR, a counterfactual contrastive learning approach which leverages
approximate counterfactual inference for positive pair creation. Comprehensive
evaluation across five datasets, on chest radiography and mammography,
demonstrates that CF-SimCLR substantially improves robustness to acquisition
shift with higher downstream performance on both in- and out-of-distribution
data, particularly for domains which are under-represented during training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code available at
  https://github.com/biomedia-mira/counterfactual-contrastive</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimistic Verifiable Training by Controlling Hardware Nondeterminism 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09603v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09603v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Megha Srivastava, Simran Arora, Dan Boneh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing compute demands of AI systems has led to the emergence of
services that train models on behalf of clients lacking necessary resources.
However, ensuring correctness of training and guarding against potential
training-time attacks, such as data poisoning, poses challenges. Existing works
on verifiable training largely fall into two classes: proof-based systems,
which struggle to scale due to requiring cryptographic techniques, and
"optimistic" methods that consider a trusted third-party auditor who replicates
the training process. A key challenge with the latter is that hardware
nondeterminism between GPU types during training prevents an auditor from
replicating the training process exactly, and such schemes are therefore
non-robust. We propose a method that combines training in a higher precision
than the target model, rounding after intermediate computation steps, and
storing rounding decisions based on an adaptive thresholding procedure, to
successfully control for nondeterminism. Across three different NVIDIA GPUs
(A40, Titan XP, RTX 2080 Ti), we achieve exact training replication at FP32
precision for both full-training and fine-tuning of ResNet-50 (23M) and GPT-2
(117M) models. Our verifiable training scheme significantly decreases the
storage and time costs compared to proof-based systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 5 figures, preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Algorithmic syntactic causal identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09580v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09580v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dhurim Cakiqi, Max A. Little
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Causal identification in causal Bayes nets (CBNs) is an important tool in
causal inference allowing the derivation of interventional distributions from
observational distributions where this is possible in principle. However, most
existing formulations of causal identification using techniques such as
d-separation and do-calculus are expressed within the mathematical language of
classical probability theory on CBNs. However, there are many causal settings
where probability theory and hence current causal identification techniques are
inapplicable such as relational databases, dataflow programs such as hardware
description languages, distributed systems and most modern machine learning
algorithms. We show that this restriction can be lifted by replacing the use of
classical probability theory with the alternative axiomatic foundation of
symmetric monoidal categories. In this alternative axiomatization, we show how
an unambiguous and clean distinction can be drawn between the general syntax of
causal models and any specific semantic implementation of that causal model.
This allows a purely syntactic algorithmic description of general causal
identification by a translation of recent formulations of the general ID
algorithm through fixing. Our description is given entirely in terms of the
non-parametric ADMG structure specifying a causal model and the algebraic
signature of the corresponding monoidal category, to which a sequence of
manipulations is then applied so as to arrive at a modified monoidal category
in which the desired, purely syntactic interventional causal model, is
obtained. We use this idea to derive purely syntactic analogues of classical
back-door and front-door causal adjustment, and illustrate an application to a
more complex causal model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 2 TikZ figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Trust in Autonomous Agents: An Architecture for Accountability
  and Explainability through Blockchain and Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09567v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09567v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Laura Fernández-Becerra, Miguel Ángel González-Santamarta, Ángel Manuel Guerrero-Higueras, Francisco Javier Rodríguez-Lera, Vicente Matellán Olivera
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The deployment of autonomous agents in environments involving human
interaction has increasingly raised security concerns. Consequently,
understanding the circumstances behind an event becomes critical, requiring the
development of capabilities to justify their behaviors to non-expert users.
Such explanations are essential in enhancing trustworthiness and safety, acting
as a preventive measure against failures, errors, and misunderstandings.
Additionally, they contribute to improving communication, bridging the gap
between the agent and the user, thereby improving the effectiveness of their
interactions. This work presents an accountability and explainability
architecture implemented for ROS-based mobile robots. The proposed solution
consists of two main components. Firstly, a black box-like element to provide
accountability, featuring anti-tampering properties achieved through blockchain
technology. Secondly, a component in charge of generating natural language
explanations by harnessing the capabilities of Large Language Models (LLMs)
over the data contained within the previously mentioned black box. The study
evaluates the performance of our solution in three different scenarios, each
involving autonomous agent navigation functionalities. This evaluation includes
a thorough examination of accountability and explainability metrics,
demonstrating the effectiveness of our approach in using accountable data from
robot actions to obtain coherent, accurate and understandable explanations,
even when facing challenges inherent in the use of autonomous agents in
real-world scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Welcome Your New AI Teammate: On Safety Analysis by Leashing Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09565v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09565v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Nouri, Beatriz Cabrero-Daniel, Fredrik Törner, Hȧkan Sivencrona, Christian Berger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  DevOps is a necessity in many industries, including the development of
Autonomous Vehicles. In those settings, there are iterative activities that
reduce the speed of SafetyOps cycles. One of these activities is "Hazard
Analysis & Risk Assessment" (HARA), which is an essential step to start the
safety requirements specification. As a potential approach to increase the
speed of this step in SafetyOps, we have delved into the capabilities of Large
Language Models (LLMs).
  Our objective is to systematically assess their potential for application in
the field of safety engineering. To that end, we propose a framework to support
a higher degree of automation of HARA with LLMs. Despite our endeavors to
automate as much of the process as possible, expert review remains crucial to
ensure the validity and correctness of the analysis results, with necessary
modifications made accordingly.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in CAIN 2024, 6 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generalizing Denoising to Non-Equilibrium Structures Improves
  Equivariant Force Fields 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09549v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09549v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi-Lun Liao, Tess Smidt, Abhishek Das
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding the interactions of atoms such as forces in 3D atomistic
systems is fundamental to many applications like molecular dynamics and
catalyst design. However, simulating these interactions requires
compute-intensive ab initio calculations and thus results in limited data for
training neural networks. In this paper, we propose to use denoising
non-equilibrium structures (DeNS) as an auxiliary task to better leverage
training data and improve performance. For training with DeNS, we first corrupt
a 3D structure by adding noise to its 3D coordinates and then predict the
noise. Different from previous works on denoising, which are limited to
equilibrium structures, the proposed method generalizes denoising to a much
larger set of non-equilibrium structures. The main difference is that a
non-equilibrium structure does not correspond to local energy minima and has
non-zero forces, and therefore it can have many possible atomic positions
compared to an equilibrium structure. This makes denoising non-equilibrium
structures an ill-posed problem since the target of denoising is not uniquely
defined. Our key insight is to additionally encode the forces of the original
non-equilibrium structure to specify which non-equilibrium structure we are
denoising. Concretely, given a corrupted non-equilibrium structure and the
forces of the original one, we predict the non-equilibrium structure satisfying
the input forces instead of any arbitrary structures. Since DeNS requires
encoding forces, DeNS favors equivariant networks, which can easily incorporate
forces and other higher-order tensors in node embeddings. We study the
effectiveness of training equivariant networks with DeNS on OC20, OC22 and MD17
datasets and demonstrate that DeNS can achieve new state-of-the-art results on
OC20 and OC22 and significantly improve training efficiency on MD17.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Logits of API-Protected LLMs Leak Proprietary Information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09539v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09539v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthew Finlayson, Swabha Swayamdipta, Xiang Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The commercialization of large language models (LLMs) has led to the common
practice of high-level API-only access to proprietary models. In this work, we
show that even with a conservative assumption about the model architecture, it
is possible to learn a surprisingly large amount of non-public information
about an API-protected LLM from a relatively small number of API queries (e.g.,
costing under $1,000 for OpenAI's gpt-3.5-turbo). Our findings are centered on
one key observation: most modern LLMs suffer from a softmax bottleneck, which
restricts the model outputs to a linear subspace of the full output space. We
show that this lends itself to a model image or a model signature which unlocks
several capabilities with affordable cost: efficiently discovering the LLM's
hidden size, obtaining full-vocabulary outputs, detecting and disambiguating
different model updates, identifying the source LLM given a single full LLM
output, and even estimating the output layer parameters. Our empirical
investigations show the effectiveness of our methods, which allow us to
estimate the embedding size of OpenAI's gpt-3.5-turbo to be about 4,096.
Lastly, we discuss ways that LLM providers can guard against these attacks, as
well as how these capabilities can be viewed as a feature (rather than a bug)
by allowing for greater transparency and accountability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Vision<span class="highlight-title">GPT</span>-3D: A Generalized Multimodal Agent for Enhanced 3D Vision
  Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09530v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09530v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chris Kelly, Luhui Hu, Jiayin Hu, Yu Tian, Deshun Yang, Bang Yang, Cindy Yang, Zihao Li, Zaoshan Huang, Yuexian Zou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The evolution of text to visual components facilitates people's daily lives,
such as generating image, videos from text and identifying the desired elements
within the images. Computer vision models involving the multimodal abilities in
the previous days are focused on image detection, classification based on
well-defined objects. Large language models (LLMs) introduces the
transformation from nature language to visual objects, which present the visual
layout for text contexts. OpenAI GPT-4 has emerged as the pinnacle in LLMs,
while the computer vision (CV) domain boasts a plethora of state-of-the-art
(SOTA) models and algorithms to convert 2D images to their 3D representations.
However, the mismatching between the algorithms with the problem could lead to
undesired results. In response to this challenge, we propose an unified
VisionGPT-3D framework to consolidate the state-of-the-art vision models,
thereby facilitating the development of vision-oriented AI. VisionGPT-3D
provides a versatile multimodal framework building upon the strengths of
multimodal foundation models. It seamlessly integrates various SOTA vision
models and brings the automation in the selection of SOTA vision models,
identifies the suitable 3D mesh creation algorithms corresponding to 2D depth
maps analysis, generates optimal results based on diverse multimodal inputs
such as text prompts.
  Keywords: VisionGPT-3D, 3D vision understanding, Multimodal agent
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 7 figures, pending conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AdaShield: Safeguarding Multimodal Large Language Models from
  Structure-based Attack via Adaptive Shield <span class="highlight-title">Prompt</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09513v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09513v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Wang, Xiaogeng Liu, Yu Li, Muhao Chen, Chaowei Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the advent and widespread deployment of Multimodal Large Language Models
(MLLMs), the imperative to ensure their safety has become increasingly
pronounced. However, with the integration of additional modalities, MLLMs are
exposed to new vulnerabilities, rendering them prone to structured-based
jailbreak attacks, where semantic content (e.g., "harmful text") has been
injected into the images to mislead MLLMs. In this work, we aim to defend
against such threats. Specifically, we propose \textbf{Ada}ptive
\textbf{Shield} Prompting (\textbf{AdaShield}), which prepends inputs with
defense prompts to defend MLLMs against structure-based jailbreak attacks
without fine-tuning MLLMs or training additional modules (e.g., post-stage
content detector). Initially, we present a manually designed static defense
prompt, which thoroughly examines the image and instruction content step by
step and specifies response methods to malicious queries. Furthermore, we
introduce an adaptive auto-refinement framework, consisting of a target MLLM
and a LLM-based defense prompt generator (Defender). These components
collaboratively and iteratively communicate to generate a defense prompt.
Extensive experiments on the popular structure-based jailbreak attacks and
benign datasets show that our methods can consistently improve MLLMs'
robustness against structure-based jailbreak attacks without compromising the
model's general capabilities evaluated on standard benign tasks. Our code is
available at https://github.com/rain305f/AdaShield.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Multimodal Large Language Models Defense, 25 Pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Trust AI Regulation? Discerning users are vital to build trust and
  effective AI regulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09510v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09510v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zainab Alalawi, Paolo Bova, Theodor Cimpeanu, Alessandro Di Stefano, Manh Hong Duong, Elias Fernandez Domingos, The Anh Han, Marcus Krellner, Bianca Ogbo, Simon T. Powers, Filippo Zimmaro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There is general agreement that some form of regulation is necessary both for
AI creators to be incentivised to develop trustworthy systems, and for users to
actually trust those systems. But there is much debate about what form these
regulations should take and how they should be implemented. Most work in this
area has been qualitative, and has not been able to make formal predictions.
Here, we propose that evolutionary game theory can be used to quantitatively
model the dilemmas faced by users, AI creators, and regulators, and provide
insights into the possible effects of different regulatory regimes. We show
that creating trustworthy AI and user trust requires regulators to be
incentivised to regulate effectively. We demonstrate the effectiveness of two
mechanisms that can achieve this. The first is where governments can recognise
and reward regulators that do a good job. In that case, if the AI system is not
too risky for users then some level of trustworthy development and user trust
evolves. We then consider an alternative solution, where users can condition
their trust decision on the effectiveness of the regulators. This leads to
effective regulation, and consequently the development of trustworthy AI and
user trust, provided that the cost of implementing regulations is not too high.
Our findings highlight the importance of considering the effect of different
regulatory regimes from an evolutionary game theoretic perspective.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Don't Judge by the Look: A Motion Coherent Augmentation for Video
  Recognition <span class="chip">ICLR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09506v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09506v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yitian Zhang, Yue Bai, Huan Wang, Yizhou Wang, Yun Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current training pipelines in object recognition neglect Hue Jittering when
doing data augmentation as it not only brings appearance changes that are
detrimental to classification, but also the implementation is inefficient in
practice. In this study, we investigate the effect of hue variance in the
context of video recognition and find this variance to be beneficial since
static appearances are less important in videos that contain motion
information. Based on this observation, we propose a data augmentation method
for video recognition, named Motion Coherent Augmentation (MCA), that
introduces appearance variation in videos and implicitly encourages the model
to prioritize motion patterns, rather than static appearances. Concretely, we
propose an operation SwapMix to efficiently modify the appearance of video
samples, and introduce Variation Alignment (VA) to resolve the distribution
shift caused by SwapMix, enforcing the model to learn appearance invariant
representations. Comprehensive empirical evaluation across various
architectures and different datasets solidly validates the effectiveness and
generalization ability of MCA, and the application of VA in other augmentation
methods. Code is available at https://github.com/BeSpontaneous/MCA-pytorch.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EquiAV: Leveraging Equivariance for Audio-Visual Contrastive Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09502v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09502v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jongsuk Kim, Hyeongkeun Lee, Kyeongha Rho, Junmo Kim, Joon Son Chung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in self-supervised audio-visual representation learning
have demonstrated its potential to capture rich and comprehensive
representations. However, despite the advantages of data augmentation verified
in many learning methods, audio-visual learning has struggled to fully harness
these benefits, as augmentations can easily disrupt the correspondence between
input pairs. To address this limitation, we introduce EquiAV, a novel framework
that leverages equivariance for audio-visual contrastive learning. Our approach
begins with extending equivariance to audio-visual learning, facilitated by a
shared attention-based transformation predictor. It enables the aggregation of
features from diverse augmentations into a representative embedding, providing
robust supervision. Notably, this is achieved with minimal computational
overhead. Extensive ablation studies and qualitative results verify the
effectiveness of our method. EquiAV outperforms previous works across various
audio-visual benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Reinforcement Learning Approach to Dairy Farm Battery Management using
  Q Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09499v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09499v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nawazish Ali, Abdul Wahid, Rachael Shaw, Karl Mason
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dairy farming consumes a significant amount of energy, making it an
energy-intensive sector within agriculture. Integrating renewable energy
generation into dairy farming could help address this challenge. Effective
battery management is important for integrating renewable energy generation.
Managing battery charging and discharging poses significant challenges because
of fluctuations in electrical consumption, the intermittent nature of renewable
energy generation, and fluctuations in energy prices. Artificial Intelligence
(AI) has the potential to significantly improve the use of renewable energy in
dairy farming, however, there is limited research conducted in this particular
domain. This research considers Ireland as a case study as it works towards
attaining its 2030 energy strategy centered on the utilization of renewable
sources. This study proposes a Q-learning-based algorithm for scheduling
battery charging and discharging in a dairy farm setting. This research also
explores the effect of the proposed algorithm by adding wind generation data
and considering additional case studies. The proposed algorithm reduces the
cost of imported electricity from the grid by 13.41\%, peak demand by 2\%, and
24.49\% when utilizing wind generation. These results underline how
reinforcement learning is highly effective in managing batteries in the dairy
farming sector.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Skepticism to Acceptance: Simulating the Attitude Dynamics Toward
  Fake News 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09498v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09498v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhan Liu, Xiuying Chen, Xiaoqing Zhang, Xing Gao, Ji Zhang, Rui Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the digital era, the rapid propagation of fake news and rumors via social
networks brings notable societal challenges and impacts public opinion
regulation. Traditional fake news modeling typically forecasts the general
popularity trends of different groups or numerically represents opinions shift.
However, these methods often oversimplify real-world complexities and overlook
the rich semantic information of news text. The advent of large language models
(LLMs) provides the possibility of modeling subtle dynamics of opinion.
Consequently, in this work, we introduce a Fake news Propagation Simulation
framework (FPS) based on LLM, which studies the trends and control of fake news
propagation in detail. Specifically, each agent in the simulation represents an
individual with a distinct personality. They are equipped with both short-term
and long-term memory, as well as a reflective mechanism to mimic human-like
thinking. Every day, they engage in random opinion exchanges, reflect on their
thinking, and update their opinions. Our simulation results uncover patterns in
fake news propagation related to topic relevance, and individual traits,
aligning with real-world observations. Additionally, we evaluate various
intervention strategies and demonstrate that early and appropriately frequent
interventions strike a balance between governance cost and effectiveness,
offering valuable insights for practical applications. Our study underscores
the significant utility and potential of LLMs in combating fake news.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rectifying Demonstration Shortcut in In-Context Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09488v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09488v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joonwon Jang, Sanghwan Jang, Wonbin Kweon, Minjin Jeon, Hwanjo Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are able to solve various tasks with only a few
demonstrations utilizing their in-context learning (ICL) abilities. However,
LLMs often rely on their pre-trained semantic priors of demonstrations rather
than on the input-label relationships to proceed with ICL prediction. In this
work, we term this phenomenon as the `Demonstration Shortcut'. While previous
works have primarily focused on improving ICL prediction results for predefined
tasks, we aim to rectify the Demonstration Shortcut, thereby enabling the LLM
to effectively learn new input-label relationships from demonstrations. To
achieve this, we introduce In-Context Calibration, a demonstration-aware
calibration method. We evaluate the effectiveness of the proposed method in two
settings: (1) the Original ICL Task using the standard label space and (2) the
Task Learning setting, where the label space is replaced with semantically
unrelated tokens. In both settings, In-Context Calibration demonstrates
substantial improvements, with results generalized across three LLM families
(OPT, GPT, and Llama2) under various configurations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Clinical Reasoning over Tabular Data and Text with Bayesian Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09481v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09481v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paloma Rabaey, Johannes Deleu, Stefan Heytens, Thomas Demeester
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bayesian networks are well-suited for clinical reasoning on tabular data, but
are less compatible with natural language data, for which neural networks
provide a successful framework. This paper compares and discusses strategies to
augment Bayesian networks with neural text representations, both in a
generative and discriminative manner. This is illustrated with simulation
results for a primary care use case (diagnosis of pneumonia) and discussed in a
broader clinical context.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ What Sketch Explainability Really Means for Downstream Tasks <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09480v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09480v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hmrishav Bandyopadhyay, Pinaki Nath Chowdhury, Ayan Kumar Bhunia, Aneeshan Sain, Tao Xiang, Yi-Zhe Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we explore the unique modality of sketch for explainability,
emphasising the profound impact of human strokes compared to conventional
pixel-oriented studies. Beyond explanations of network behavior, we discern the
genuine implications of explainability across diverse downstream sketch-related
tasks. We propose a lightweight and portable explainability solution -- a
seamless plugin that integrates effortlessly with any pre-trained model,
eliminating the need for re-training. Demonstrating its adaptability, we
present four applications: highly studied retrieval and generation, and
completely novel assisted drawing and sketch adversarial attacks. The
centrepiece to our solution is a stroke-level attribution map that takes
different forms when linked with downstream tasks. By addressing the inherent
non-differentiability of rasterisation, we enable explanations at both coarse
stroke level (SLA) and partial stroke level (P-SLA), each with its advantages
for specific downstream tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLM-based agents for automating the enhancement of user story quality:
  An early report 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09442v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09442v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheying Zhang, Maruf Rayhan, Tomas Herda, Manuel Goisauf, Pekka Abrahamsson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In agile software development, maintaining high-quality user stories is
crucial, but also challenging. This study explores the use of large language
models to automatically improve the user story quality in Austrian Post Group
IT agile teams. We developed a reference model for an Autonomous LLM-based
Agent System and implemented it at the company. The quality of user stories in
the study and the effectiveness of these agents for user story quality
improvement was assessed by 11 participants across six agile teams. Our
findings demonstrate the potential of LLMs in improving user story quality,
contributing to the research on AI role in agile development, and providing a
practical example of the transformative impact of AI in an industry setting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 5 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 3D-SceneDreamer: Text-Driven 3D-Consistent Scene Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09439v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09439v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Frank Zhang, Yibo Zhang, Quan Zheng, Rui Ma, Wei Hua, Hujun Bao, Weiwei Xu, Changqing Zou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-driven 3D scene generation techniques have made rapid progress in recent
years. Their success is mainly attributed to using existing generative models
to iteratively perform image warping and inpainting to generate 3D scenes.
However, these methods heavily rely on the outputs of existing models, leading
to error accumulation in geometry and appearance that prevent the models from
being used in various scenarios (e.g., outdoor and unreal scenarios). To
address this limitation, we generatively refine the newly generated local views
by querying and aggregating global 3D information, and then progressively
generate the 3D scene. Specifically, we employ a tri-plane features-based NeRF
as a unified representation of the 3D scene to constrain global 3D consistency,
and propose a generative refinement network to synthesize new contents with
higher quality by exploiting the natural image prior from 2D diffusion model as
well as the global 3D information of the current scene. Our extensive
experiments demonstrate that, in comparison to previous methods, our approach
supports wide variety of scene generation and arbitrary camera trajectories
with improved visual quality and 3D consistency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mitigating attribute amplification in counterfactual image generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09422v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09422v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tian Xia, Mélanie Roschewitz, Fabio De Sousa Ribeiro, Charles Jones, Ben Glocker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Causal generative modelling is gaining interest in medical imaging due to its
ability to answer interventional and counterfactual queries. Most work focuses
on generating counterfactual images that look plausible, using auxiliary
classifiers to enforce effectiveness of simulated interventions. We investigate
pitfalls in this approach, discovering the issue of attribute amplification,
where unrelated attributes are spuriously affected during interventions,
leading to biases across protected characteristics and disease status. We show
that attribute amplification is caused by the use of hard labels in the
counterfactual training process and propose soft counterfactual fine-tuning to
mitigate this issue. Our method substantially reduces the amplification effect
while maintaining effectiveness of generated images, demonstrated on a large
chest X-ray dataset. Our work makes an important advancement towards more
faithful and unbiased causal modelling in medical imaging.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OpenGraph: Open-Vocabulary Hierarchical 3D Graph Representation in
  Large-Scale Outdoor Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09412v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09412v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinan Deng, Jiahui Wang, Jingyu Zhao, Xinyu Tian, Guangyan Chen, Yi Yang, Yufeng Yue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Environment maps endowed with sophisticated semantics are pivotal for
facilitating seamless interaction between robots and humans, enabling them to
effectively carry out various tasks. Open-vocabulary maps, powered by
Visual-Language models (VLMs), possess inherent advantages, including
multimodal retrieval and open-set classes. However, existing open-vocabulary
maps are constrained to closed indoor scenarios and VLM features, thereby
diminishing their usability and inference capabilities. Moreover, the absence
of topological relationships further complicates the accurate querying of
specific instances. In this work, we propose OpenGraph, a representation of
open-vocabulary hierarchical graph structure designed for large-scale outdoor
environments. OpenGraph initially extracts instances and their captions from
visual images using 2D foundation models, encoding the captions with features
to enhance textual reasoning. Subsequently, 3D incremental panoramic mapping
with feature embedding is achieved by projecting images onto LiDAR point
clouds. Finally, the environment is segmented based on lane graph connectivity
to construct a hierarchical graph. Validation results from real public dataset
SemanticKITTI demonstrate that, even without fine-tuning the models, OpenGraph
exhibits the ability to generalize to novel semantic classes and achieve the
highest segmentation and query accuracy. The source code of OpenGraph is
publicly available at https://github.com/BIT-DYN/OpenGraph.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ XCoOp: Explainable <span class="highlight-title">Prompt</span> Learning for Computer-Aided Diagnosis via
  Concept-guided Context Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09410v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09410v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yequan Bie, Luyang Luo, Zhixuan Chen, Hao Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Utilizing potent representations of the large vision-language models (VLMs)
to accomplish various downstream tasks has attracted increasing attention.
Within this research field, soft prompt learning has become a representative
approach for efficiently adapting VLMs such as CLIP, to tasks like image
classification. However, most existing prompt learning methods learn text
tokens that are unexplainable, which cannot satisfy the stringent
interpretability requirements of Explainable Artificial Intelligence (XAI) in
high-stakes scenarios like healthcare. To address this issue, we propose a
novel explainable prompt learning framework that leverages medical knowledge by
aligning the semantics of images, learnable prompts, and clinical
concept-driven prompts at multiple granularities. Moreover, our framework
addresses the lack of valuable concept annotations by eliciting knowledge from
large language models and offers both visual and textual explanations for the
prompts. Extensive experiments and explainability analyses conducted on various
datasets, with and without concept labels, demonstrate that our method
simultaneously achieves superior diagnostic performance, flexibility, and
interpretability, shedding light on the effectiveness of foundation models in
facilitating XAI. The code will be made publically available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ "Like a Nesting Doll": Analyzing Recursion Analogies Generated by CS
  Students using Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09409v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09409v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seth Bernstein, Paul Denny, Juho Leinonen, Lauren Kan, Arto Hellas, Matt Littlefield Sami Sarsa, Stephen MacNeil
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Grasping complex computing concepts often poses a challenge for students who
struggle to anchor these new ideas to familiar experiences and understandings.
To help with this, a good analogy can bridge the gap between unfamiliar
concepts and familiar ones, providing an engaging way to aid understanding.
However, creating effective educational analogies is difficult even for
experienced instructors. We investigate to what extent large language models
(LLMs), specifically ChatGPT, can provide access to personally relevant
analogies on demand. Focusing on recursion, a challenging threshold concept, we
conducted an investigation analyzing the analogies generated by more than 350
first-year computing students. They were provided with a code snippet and
tasked to generate their own recursion-based analogies using ChatGPT,
optionally including personally relevant topics in their prompts. We observed a
great deal of diversity in the analogies produced with student-prescribed
topics, in contrast to the otherwise generic analogies, highlighting the value
of student creativity when working with LLMs. Not only did students enjoy the
activity and report an improved understanding of recursion, but they described
more easily remembering analogies that were personally and culturally relevant.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 2 figures, ITiCSE 2024 preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GaussianImage: 1000 FPS Image Representation and Compression by 2D
  Gaussian Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08551v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08551v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinjie Zhang, Xingtong Ge, Tongda Xu, Dailan He, Yan Wang, Hongwei Qin, Guo Lu, Jing Geng, Jun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Implicit neural representations (INRs) recently achieved great success in
image representation and compression, offering high visual quality and fast
rendering speeds with 10-1000 FPS, assuming sufficient GPU resources are
available. However, this requirement often hinders their use on low-end devices
with limited memory. In response, we propose a groundbreaking paradigm of image
representation and compression by 2D Gaussian Splatting, named GaussianImage.
We first introduce 2D Gaussian to represent the image, where each Gaussian has
8 parameters including position, covariance and color. Subsequently, we unveil
a novel rendering algorithm based on accumulated summation. Remarkably, our
method with a minimum of 3$\times$ lower GPU memory usage and 5$\times$ faster
fitting time not only rivals INRs (e.g., WIRE, I-NGP) in representation
performance, but also delivers a faster rendering speed of 1500-2000 FPS
regardless of parameter size. Furthermore, we integrate existing vector
quantization technique to build an image codec. Experimental results
demonstrate that our codec attains rate-distortion performance comparable to
compression-based INRs such as COIN and COIN++, while facilitating decoding
speeds of approximately 1000 FPS. Additionally, preliminary proof of concept
shows that our codec surpasses COIN and COIN++ in performance when using
partial bits-back coding.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Era Splitting -- Invariant Learning for Decision Trees 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.14496v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.14496v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Timothy DeLise
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-life machine learning problems exhibit distributional shifts in the data
from one time to another or from one place to another. This behavior is beyond
the scope of the traditional empirical risk minimization paradigm, which
assumes i.i.d. distribution of data over time and across locations. The
emerging field of out-of-distribution (OOD) generalization addresses this
reality with new theory and algorithms which incorporate environmental, or
era-wise information into the algorithms. So far, most research has been
focused on linear models and/or neural networks. In this research we develop
two new splitting criteria for decision trees, which allow us to apply ideas
from OOD generalization research to decision tree models, namely, gradient
boosting decision trees (GBDT). The new splitting criteria use era-wise
information associated with the data to grow tree-based models that are optimal
across all disjoint eras in the data, instead of optimal over the entire data
set pooled together, which is the default setting. In this paper, two new
splitting criteria are defined and analyzed theoretically. Effectiveness is
tested on four experiments, ranging from simple, synthetic to complex,
real-world applications. In particular we cast the OOD domain-adaptation
problem in the context of financial markets, where the new models out-perform
state-of-the-art GBDT models on the Numerai data set. The new criteria are
incorporated into the Scikit-Learn code base and made freely available online.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SNAP: Semantic Stories for Next Activity Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.15621v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.15621v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alon Oved, Segev Shlomov, Sergey Zeltyn, Nir Mashkif, Avi Yaeli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Predicting the next activity in an ongoing process is one of the most common
classification tasks in the business process management (BPM) domain. It allows
businesses to optimize resource allocation, enhance operational efficiency, and
aids in risk mitigation and strategic decision-making. This provides a
competitive edge in the rapidly evolving confluence of BPM and AI. Existing
state-of-the-art AI models for business process prediction do not fully
capitalize on available semantic information within process event logs. As
current advanced AI-BPM systems provide semantically-richer textual data, the
need for novel adequate models grows. To address this gap, we propose the novel
SNAP method that leverages language foundation models by constructing semantic
contextual stories from the process historical event logs and using them for
the next activity prediction. We compared the SNAP algorithm with nine
state-of-the-art models on six benchmark datasets and show that SNAP
significantly outperforms them, especially for datasets with high levels of
semantic content.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Zero Coordinate Shift: Whetted Automatic Differentiation for
  Physics-informed Operator Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.00860v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.00860v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kuangdai Leng, Mallikarjun Shankar, Jeyan Thiyagalingam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic differentiation (AD) is a critical step in physics-informed machine
learning, required for computing the high-order derivatives of network output
w.r.t. coordinates of collocation points. In this paper, we present a novel and
lightweight algorithm to conduct AD for physics-informed operator learning,
which we call the trick of Zero Coordinate Shift (ZCS). Instead of making all
sampled coordinates as leaf variables, ZCS introduces only one scalar-valued
leaf variable for each spatial or temporal dimension, simplifying the wanted
derivatives from "many-roots-many-leaves" to "one-root-many-leaves" whereby
reverse-mode AD becomes directly utilisable. It has led to an outstanding
performance leap by avoiding the duplication of the computational graph along
the dimension of functions (physical parameters). ZCS is easy to implement with
current deep learning libraries; our own implementation is achieved by
extending the DeepXDE package. We carry out a comprehensive benchmark analysis
and several case studies, training physics-informed DeepONets to solve partial
differential equations (PDEs) without data. The results show that ZCS has
persistently reduced GPU memory consumption and wall time for training by an
order of magnitude, and such reduction factor scales with the number of
functions. As a low-level optimisation technique, ZCS imposes no restrictions
on data, physics (PDE) or network architecture and does not compromise training
results from any aspect.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in Journal of Computational Physics.
  https://doi.org/10.1016/j.jcp.2024.112904</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Transforming Competition into Collaboration: The Revolutionary Role of
  Multi-Agent Systems and Language Models in Modern Organizations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07769v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07769v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Carlos Jose Xavier Cruz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This article explores the dynamic influence of computational entities based
on multi-agent systems theory (SMA) combined with large language models (LLM),
which are characterized by their ability to simulate complex human
interactions, as a possibility to revolutionize human user interaction from the
use of specialized artificial agents to support everything from operational
organizational processes to strategic decision making based on applied
knowledge and human orchestration. Previous investigations reveal that there
are limitations, particularly in the autonomous approach of artificial agents,
especially when dealing with new challenges and pragmatic tasks such as
inducing logical reasoning and problem solving. It is also considered that
traditional techniques, such as the stimulation of chains of thoughts, require
explicit human guidance. In our approach we employ agents developed from large
language models (LLM), each with distinct prototyping that considers behavioral
elements, driven by strategies that stimulate the generation of knowledge based
on the use case proposed in the scenario (role-play) business, using a
discussion approach between agents (guided conversation). We demonstrate the
potential of developing agents useful for organizational strategies, based on
multi-agent system theories (SMA) and innovative uses based on large language
models (LLM based), offering a differentiated and adaptable experiment to
different applications, complexities, domains, and capabilities from LLM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Impact of Explanations on Fairness in Human-AI Decision-Making:
  Protected vs Proxy Features 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08617v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08617v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Navita Goyal, Connor Baumler, Tin Nguyen, Hal Daumé III
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  AI systems have been known to amplify biases in real-world data. Explanations
may help human-AI teams address these biases for fairer decision-making.
Typically, explanations focus on salient input features. If a model is biased
against some protected group, explanations may include features that
demonstrate this bias, but when biases are realized through proxy features, the
relationship between this proxy feature and the protected one may be less clear
to a human. In this work, we study the effect of the presence of protected and
proxy features on participants' perception of model fairness and their ability
to improve demographic parity over an AI alone. Further, we examine how
different treatments -- explanations, model bias disclosure and proxy
correlation disclosure -- affect fairness perception and parity. We find that
explanations help people detect direct but not indirect biases. Additionally,
regardless of bias type, explanations tend to increase agreement with model
biases. Disclosures can help mitigate this effect for indirect biases,
improving both unfairness recognition and decision-making fairness. We hope
that our findings can help guide further research into advancing explanations
in support of fair human-AI decision-making.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IUI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring Safety Generalization Challenges of Large Language Models via
  Code 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07865v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07865v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qibing Ren, Chang Gao, Jing Shao, Junchi Yan, Xin Tan, Yu Qiao, Wai Lam, Lizhuang Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancement of Large Language Models (LLMs) has brought about
remarkable capabilities in natural language processing but also raised concerns
about their potential misuse. While strategies like supervised fine-tuning and
reinforcement learning from human feedback have enhanced their safety, these
methods primarily focus on natural languages, which may not generalize to other
domains. This paper introduces CodeAttack, a framework that transforms natural
language inputs into code inputs, presenting a novel environment for testing
the safety generalization of LLMs. Our comprehensive studies on
state-of-the-art LLMs including GPT-4, Claude-2, and Llama-2 series reveal a
common safety vulnerability of these models against code input: CodeAttack
consistently bypasses the safety guardrails of all models more than 80% of the
time. Furthermore, we find that a larger distribution gap between CodeAttack
and natural language leads to weaker safety generalization, such as encoding
natural language input with data structures or using less popular programming
languages. These findings highlight new safety risks in the code domain and the
need for more robust safety alignment algorithms to match the code capabilities
of LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VBART: The Turkish LLM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.01308v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.01308v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Meliksah Turker, Mehmet Erdi Ari, Aydin Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present VBART, the first Turkish sequence-to-sequence Large Language
Models (LLMs) pre-trained on a large corpus from scratch. VBART are compact
LLMs based on good ideas leveraged from BART and mBART models and come in two
sizes, Large and XLarge. Fine-tuned VBART models surpass the prior
state-of-the-art results in abstractive text summarization, title generation,
text paraphrasing, question answering and question generation tasks. They allow
fine-tuning for future text generation tasks and datasets, carving a new path
for Turkish Natural Language Processing (NLP) research. Our work shows that
having a pre-trained LLM for Turkish outperforms up to 3x multilingual models,
improving existing results and providing efficient models for training and
inference. Moreover, we show that our monolingual tokenizer is up to 11x more
efficient than multilingual tokenizers. Last but not least, we introduce a
method to enlarge an existing pre-trained LLM and question the relevancy of
Chinchilla Scaling Law to sequence-to-sequence masked language models. Our
fine-tuned models, tokenizer and cleaned vngrs-web-corpus of 135 GB are
publicly available at huggingface.co/vngrs-ai.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TTA-Nav: Test-time Adaptive Reconstruction for Point-Goal Navigation
  under Visual Corruptions <span class="chip">IROS2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.01977v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.01977v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maytus Piriyajitakonkij, Mingfei Sun, Mengmi Zhang, Wei Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robot navigation under visual corruption presents a formidable challenge. To
address this, we propose a Test-time Adaptation (TTA) method, named as TTA-Nav,
for point-goal navigation under visual corruptions. Our "plug-and-play" method
incorporates a top-down decoder to a pre-trained navigation model. Firstly, the
pre-trained navigation model gets a corrupted image and extracts features.
Secondly, the top-down decoder produces the reconstruction given the high-level
features extracted by the pre-trained model. Then, it feeds the reconstruction
of a corrupted image back to the pre-trained model. Finally, the pre-trained
model does forward pass again to output action. Despite being trained solely on
clean images, the top-down decoder can reconstruct cleaner images from
corrupted ones without the need for gradient-based adaptation. The pre-trained
navigation model with our top-down decoder significantly enhances navigation
performance across almost all visual corruptions in our benchmarks. Our method
improves the success rate of point-goal navigation from the state-of-the-art
result of 46% to 94% on the most severe corruption. This suggests its potential
for broader application in robotic visual navigation. Project page:
https://sites.google.com/view/tta-nav
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IROS2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Geometric structure of Deep Learning networks and construction of global
  ${\mathcal L}^2$ minimizers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.10639v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.10639v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Chen, Patricia Muñoz Ewald
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we explicitly determine local and global minimizers of the
$\mathcal{L}^2$ cost function in underparametrized Deep Learning (DL) networks;
our main goal is to shed light on their geometric structure and properties. We
accomplish this by a direct construction, without invoking the gradient descent
flow at any point of this work. We specifically consider $L$ hidden layers, a
ReLU ramp activation function, an $\mathcal{L}^2$ Schatten class (or
Hilbert-Schmidt) cost function, input and output spaces $\mathbb{R}^Q$ with
equal dimension $Q\geq1$, and hidden layers also defined on $\mathbb{R}^{Q}$;
the training inputs are assumed to be sufficiently clustered. The training
input size $N$ can be arbitrarily large - thus, we are considering the
underparametrized regime. More general settings are left to future work. We
construct an explicit family of minimizers for the global minimum of the cost
function in the case $L\geq Q$, which we show to be degenerate. Moreover, we
determine a set of $2^Q-1$ distinct degenerate local minima of the cost
function. In the context presented here, the concatenation of hidden layers of
the DL network is reinterpreted as a recursive application of a {\em truncation
map} which "curates" the training inputs by minimizing their noise to signal
ratio.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AMS Latex, 22 pages. Typos corrected, slightly extended</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Me LLaMA: Foundation Large Language Models for Medical Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.12749v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.12749v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qianqian Xie, Qingyu Chen, Aokun Chen, Cheng Peng, Yan Hu, Fongci Lin, Xueqing Peng, Jimin Huang, Jeffrey Zhang, Vipina Keloth, Xingyu Zhou, Huan He, Lucila Ohno-Machado, Yonghui Wu, Hua Xu, Jiang Bian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent large language models (LLMs) such as ChatGPT and LLaMA have shown
great promise in many AI applications. However, their performance on medical
tasks is suboptimal and can be improved by training on extensive
domain-specific datasets. This study introduces Me LLaMA, a medical LLM family
that includes foundation models - Me LLaMA 13/70B, along with their
chat-enhanced versions - Me LLaMA 13/70B-chat, developed through continual
pre-training and instruction tuning of LLaMA2 using large medical datasets. Our
domain-specific data suite for training and evaluation includes a large-scale,
continual pre-training dataset with 129B tokens, an instruction tuning dataset
with 214k samples, and a new medical evaluation benchmark (MIBE) across six
tasks with 12 datasets. Our extensive evaluation using the MIBE shows that Me
LLaMA models achieve overall better performance than existing open-source
medical LLMs in zero-shot, few-shot and supervised learning abilities. Their
zero-shot performance is comparable with ChatGPT across 7 out of 8 datasets,
with a slight variance of within 3%, and yet falls short when compared to
GPT-4. In addition, we investigated the catastrophic forgetting problem, and
our results show that Me LLaMA models outperform other open-source medical LLMs
in mitigating this issue. Me LLaMA is one of the largest open-source medical
foundation LLMs that use both biomedical and clinical data. It exhibits
superior performance across both general and medical tasks compared to other
open-source medical LLMs, rendering it an attractive choice for medical AI
applications. We release our models, datasets, and evaluation scripts at:
https://github.com/BIDS-Xu-Lab/Me-LLaMA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 3 figures, 8 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FedImpro: Measuring and Improving Client Update in Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.07011v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.07011v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenheng Tang, Yonggang Zhang, Shaohuai Shi, Xinmei Tian, Tongliang Liu, Bo Han, Xiaowen Chu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) models often experience client drift caused by
heterogeneous data, where the distribution of data differs across clients. To
address this issue, advanced research primarily focuses on manipulating the
existing gradients to achieve more consistent client models. In this paper, we
present an alternative perspective on client drift and aim to mitigate it by
generating improved local models. First, we analyze the generalization
contribution of local training and conclude that this generalization
contribution is bounded by the conditional Wasserstein distance between the
data distribution of different clients. Then, we propose FedImpro, to construct
similar conditional distributions for local training. Specifically, FedImpro
decouples the model into high-level and low-level components, and trains the
high-level portion on reconstructed feature distributions. This approach
enhances the generalization contribution and reduces the dissimilarity of
gradients in FL. Experimental results show that FedImpro can help FL defend
against data heterogeneity and enhance the generalization performance of the
model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Most discriminative stimuli for functional cell type clustering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.05342v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.05342v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Max F. Burg, Thomas Zenkel, Michaela Vystrčilová, Jonathan Oesterle, Larissa Höfling, Konstantin F. Willeke, Jan Lause, Sarah Müller, Paul G. Fahey, Zhiwei Ding, Kelli Restivo, Shashwat Sridhar, Tim Gollisch, Philipp Berens, Andreas S. Tolias, Thomas Euler, Matthias Bethge, Alexander S. Ecker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Identifying cell types and understanding their functional properties is
crucial for unraveling the mechanisms underlying perception and cognition. In
the retina, functional types can be identified by carefully selected stimuli,
but this requires expert domain knowledge and biases the procedure towards
previously known cell types. In the visual cortex, it is still unknown what
functional types exist and how to identify them. Thus, for unbiased
identification of the functional cell types in retina and visual cortex, new
approaches are needed. Here we propose an optimization-based clustering
approach using deep predictive models to obtain functional clusters of neurons
using Most Discriminative Stimuli (MDS). Our approach alternates between
stimulus optimization with cluster reassignment akin to an
expectation-maximization algorithm. The algorithm recovers functional clusters
in mouse retina, marmoset retina and macaque visual area V4. This demonstrates
that our approach can successfully find discriminative stimuli across species,
stages of the visual system and recording techniques. The resulting most
discriminative stimuli can be used to assign functional cell types fast and on
the fly, without the need to train complex predictive models or show a large
natural scene dataset, paving the way for experiments that were previously
limited by experimental time. Crucially, MDS are interpretable: they visualize
the distinctive stimulus patterns that most unambiguously identify a specific
type of neuron.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AllSpark: Reborn Labeled Features from Unlabeled in <span class="highlight-title">Transformer</span> for
  Semi-Supervised Semantic Segmentation <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.01818v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.01818v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haonan Wang, Qixiang Zhang, Yi Li, Xiaomeng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semi-supervised semantic segmentation (SSSS) has been proposed to alleviate
the burden of time-consuming pixel-level manual labeling, which leverages
limited labeled data along with larger amounts of unlabeled data. Current
state-of-the-art methods train the labeled data with ground truths and
unlabeled data with pseudo labels. However, the two training flows are
separate, which allows labeled data to dominate the training process, resulting
in low-quality pseudo labels and, consequently, sub-optimal results. To
alleviate this issue, we present AllSpark, which reborns the labeled features
from unlabeled ones with the channel-wise cross-attention mechanism. We further
introduce a Semantic Memory along with a Channel Semantic Grouping strategy to
ensure that unlabeled features adequately represent labeled features. The
AllSpark shed new light on the architecture level designs of SSSS rather than
framework level, which avoids increasingly complicated training pipeline
designs. It can also be regarded as a flexible bottleneck module that can be
seamlessly integrated into a general transformer-based segmentation model. The
proposed AllSpark outperforms existing methods across all evaluation protocols
on Pascal, Cityscapes and COCO benchmarks without bells-and-whistles. Code and
model weights are available at: https://github.com/xmed-lab/AllSpark.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2024; correct typos; this is not the camera-ready
  version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Typology for Exploring the Mitigation of Shortcut Behavior 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.03668v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.03668v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Felix Friedrich, Wolfgang Stammer, Patrick Schramowski, Kristian Kersting
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As machine learning models become increasingly larger, trained weakly
supervised on large, possibly uncurated data sets, it becomes increasingly
important to establish mechanisms for inspecting, interacting, and revising
models to mitigate learning shortcuts and guarantee their learned knowledge is
aligned with human knowledge. The recently proposed XIL framework was developed
for this purpose, and several such methods have been introduced, each with
individual motivations and methodological details. In this work, we provide a
unification of various XIL methods into a single typology by establishing a
common set of basic modules. In doing so, we pave the way for a principled
comparison of existing, but, importantly, also future XIL approaches. In
addition, we discuss existing and introduce novel measures and benchmarks for
evaluating the overall abilities of a XIL method. Given this extensive toolbox,
including our typology, measures, and benchmarks, we finally compare several
recent XIL methods methodologically and quantitatively. In our evaluations, all
methods prove to revise a model successfully. However, we found remarkable
differences in individual benchmark tasks, revealing valuable
application-relevant aspects for integrating these benchmarks in developing
future methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rethinking Class-incremental Learning in the Era of Large <span class="highlight-title">Pre-train</span>ed
  Models via Test-Time Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.11482v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.11482v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Imad Eddine Marouf, Subhankar Roy, Enzo Tartaglione, Stéphane Lathuilière
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Class-incremental learning (CIL) is a challenging task that involves
sequentially learning to categorize classes from new tasks without forgetting
previously learned information. The advent of large pre-trained models (PTMs)
has fast-tracked the progress in CIL due to the highly transferable PTM
representations, where tuning a small set of parameters leads to
state-of-the-art performance when compared with the traditional CIL methods
that are trained from scratch. However, repeated fine-tuning on each task
destroys the rich representations of the PTMs and further leads to forgetting
previous tasks. To strike a balance between the stability and plasticity of
PTMs for CIL, we propose a novel perspective of eliminating training on every
new task and instead train PTM only on the first task, and then refine its
representation at inference time using test-time adaptation (TTA). Concretely,
we propose Test-Time Adaptation for Class-Incremental Learning (TTACIL) that
first fine-tunes PTMs using Adapters on the first task, then adjusts Layer Norm
parameters of the PTM on each test instance for learning task-specific
features, and finally resets them back to the adapted model to preserve
stability. As a consequence, our TTACIL does not undergo any forgetting, while
benefiting each task with the rich PTM features. Additionally, by design, our
TTACIL is robust to common data corruptions. Our method outperforms several
state-of-the-art CIL methods when evaluated on multiple CIL benchmarks under
both clean and corrupted data. Code is available at:
https://github.com/IemProg/TTACIL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages,5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ COMET: A Comprehensive Cluster Design Methodology for Distributed Deep
  Learning Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.16648v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.16648v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Divya Kiran Kadiyala, Saeed Rashidi, Taekyung Heo, Abhimanyu Rajeshkumar Bambhaniya, Tushar Krishna, Alexandros Daglis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern Deep Learning (DL) models have grown to sizes requiring massive
clusters of specialized, high-end nodes to train. Designing such clusters to
maximize both performance and utilization--to amortize their steep cost--is a
challenging task requiring careful balance of compute, memory, and network
resources. Moreover, a plethora of each model's tuning knobs drastically affect
the performance, with optimal values often depending on the underlying
cluster's characteristics, which necessitates a complex cluster-workload
co-design process. To facilitate the design space exploration of such massive
DL training clusters, we introduce COMET, a holistic cluster design methodology
and workflow to jointly study the impact of parallelization strategies and key
cluster resource provisioning on the performance of distributed DL training. We
develop a step-by-step process to establish a reusable and flexible
methodology, and demonstrate its application with case studies of training
large models on cluster configurations of variable compute, memory, and network
resources. Our case studies demonstrate COMET's utility in identifying
promising architectural optimization directions and guiding system designers in
configuring key model and cluster parameters. To illustrate, cluster
configuration comparisons identify performance differences of up to 7.7x and
highlight performance optimization opportunities of up to 1.4x when employing
memory expansion as an optimization technique.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LILO: Learning Interpretable Libraries by Compressing and Documenting
  Code <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.19791v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.19791v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gabriel Grand, Lionel Wong, Matthew Bowers, Theo X. Olausson, Muxin Liu, Joshua B. Tenenbaum, Jacob Andreas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While large language models (LLMs) now excel at code generation, a key aspect
of software development is the art of refactoring: consolidating code into
libraries of reusable and readable programs. In this paper, we introduce LILO,
a neurosymbolic framework that iteratively synthesizes, compresses, and
documents code to build libraries tailored to particular problem domains. LILO
combines LLM-guided program synthesis with recent algorithmic advances in
automated refactoring from Stitch: a symbolic compression system that
efficiently identifies optimal lambda abstractions across large code corpora.
To make these abstractions interpretable, we introduce an auto-documentation
(AutoDoc) procedure that infers natural language names and docstrings based on
contextual examples of usage. In addition to improving human readability, we
find that AutoDoc boosts performance by helping LILO's synthesizer to interpret
and deploy learned abstractions. We evaluate LILO on three inductive program
synthesis benchmarks for string editing, scene reasoning, and graphics
composition. Compared to existing neural and symbolic methods - including the
state-of-the-art library learning algorithm DreamCoder - LILO solves more
complex tasks and learns richer libraries that are grounded in linguistic
knowledge.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024 camera-ready</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Smooth Tchebycheff Scalarization for Multi-Objective Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.19078v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.19078v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xi Lin, Xiaoyuan Zhang, Zhiyuan Yang, Fei Liu, Zhenkun Wang, Qingfu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-objective optimization problems can be found in many real-world
applications, where the objectives often conflict each other and cannot be
optimized by a single solution. In the past few decades, numerous methods have
been proposed to find Pareto solutions that represent different optimal
trade-offs among the objectives for a given problem. However, these existing
methods could have high computational complexity or may not have good
theoretical properties for solving a general differentiable multi-objective
optimization problem. In this work, by leveraging the smooth optimization
technique, we propose a novel and lightweight smooth Tchebycheff scalarization
approach for gradient-based multi-objective optimization. It has good
theoretical properties for finding all Pareto solutions with valid trade-off
preferences, while enjoying significantly lower computational complexity
compared to other methods. Experimental results on various real-world
application problems fully demonstrate the effectiveness of our proposed
method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>fix some typos</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CorNav: Autonomous Agent with Self-Corrected Planning for Zero-Shot
  Vision-and-Language Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.10322v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.10322v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiwen Liang, Liang Ma, Shanshan Guo, Jianhua Han, Hang Xu, Shikui Ma, Xiaodan Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding and following natural language instructions while navigating
through complex, real-world environments poses a significant challenge for
general-purpose robots. These environments often include obstacles and
pedestrians, making it essential for autonomous agents to possess the
capability of self-corrected planning to adjust their actions based on feedback
from the surroundings. However, the majority of existing vision-and-language
navigation (VLN) methods primarily operate in less realistic simulator settings
and do not incorporate environmental feedback into their decision-making
processes. To address this gap, we introduce a novel zero-shot framework called
CorNav, utilizing a large language model for decision-making and comprising two
key components: 1) incorporating environmental feedback for refining future
plans and adjusting its actions, and 2) multiple domain experts for parsing
instructions, scene understanding, and refining predicted actions. In addition
to the framework, we develop a 3D simulator that renders realistic scenarios
using Unreal Engine 5. To evaluate the effectiveness and generalization of
navigation agents in a zero-shot multi-task setting, we create a benchmark
called NavBench. Extensive experiments demonstrate that CorNav consistently
outperforms all baselines by a significant margin across all tasks. On average,
CorNav achieves a success rate of 28.1\%, surpassing the best baseline's
performance of 20.5\%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VisLingInstruct: Elevating Zero-Shot Learning in Multi-Modal Language
  Models with Autonomous Instruction Optimization <span class="chip">NAACL2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.07398v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.07398v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongsheng Zhu, Xunzhu Tang, Weidong Han, Jinghui Lu, Yukun Zhao, Guoliang Xing, Junfeng Wang, Dawei Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents VisLingInstruct, a novel approach to advancing
Multi-Modal Language Models (MMLMs) in zero-shot learning. Current MMLMs show
impressive zero-shot abilities in multi-modal tasks, but their performance
depends heavily on the quality of instructions. VisLingInstruct tackles this by
autonomously evaluating and optimizing instructional texts through In-Context
Learning, improving the synergy between visual perception and linguistic
expression in MMLMs. Alongside this instructional advancement, we have also
optimized the visual feature extraction modules in MMLMs, further augmenting
their responsiveness to textual cues. Our comprehensive experiments on MMLMs,
based on FlanT5 and Vicuna, show that VisLingInstruct significantly improves
zero-shot performance in visual multi-modal tasks. Notably, it achieves a 13.1%
and 9% increase in accuracy over the prior state-of-the-art on the TextVQA and
HatefulMemes datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NAACL2024 main conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Executing Natural Language-Described Algorithms with Large Language
  Models: An Investigation <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.00795v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.00795v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Zheng, Qiming Zhu, Hongyu Lin, Yaojie Lu, Xianpei Han, Le Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Executing computer programs described in natural language has long been a
pursuit of computer science. With the advent of enhanced natural language
understanding capabilities exhibited by large language models (LLMs), the path
toward this goal has been illuminated. In this paper, we seek to examine the
capacity of present-day LLMs to comprehend and execute algorithms outlined in
natural language. We established an algorithm test set sourced from
Introduction to Algorithm, a well-known textbook that contains many
representative widely-used algorithms. To systematically assess LLMs' code
execution abilities, we selected 30 algorithms, generated 300 random-sampled
instances in total, and evaluated whether popular LLMs can understand and
execute these algorithms. Our findings reveal that LLMs, notably GPT-4, can
effectively execute programs described in natural language, as long as no heavy
numeric computation is involved. We believe our findings contribute to
evaluating LLMs' code execution abilities and would encourage further
investigation and application for the computation power of LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Divide-and-Conquer Attack: Harnessing the Power of LLM to Bypass Safety
  Filters of Text-to-Image Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.07130v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.07130v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yimo Deng, Huangxun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-image (TTI) models offer many innovative services but also raise
ethical concerns due to their potential to generate unethical images. Most
public TTI services employ safety filters to prevent unintended images. In this
work, we introduce the Divide-and-Conquer Attack to circumvent the safety
filters of state-of the-art TTI models, including DALL-E 3 and Midjourney. Our
attack leverages LLMs as text transformation agents to create adversarial
prompts. We design attack helper prompts that effectively guide LLMs to break
down an unethical drawing intent into multiple benign descriptions of
individual image elements, allowing them to bypass safety filters while still
generating unethical images. Because the latent harmful meaning only becomes
apparent when all individual elements are drawn together. Our evaluation
demonstrates that our attack successfully circumvents multiple strong
closed-box safety filters. The comprehensive success rate of DACA bypassing the
safety filters of the state-of-the-art TTI engine DALL-E 3 is above 85%, while
the success rate for bypassing Midjourney V6 exceeds 75%. Our findings have
more severe security implications than methods of manual crafting or iterative
TTI model querying due to lower attack barrier, enhanced interpretability , and
better adaptation to defense. Our prototype is available at:
https://github.com/researchcode001/Divide-and-Conquer-Attack
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 11 figures, under review</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Transformer</span>s Get Stable: An End-to-End Signal Propagation Theory for
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09635v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09635v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akhil Kedia, Mohd Abbas Zaidi, Sushil Khyalia, Jungho Jung, Harshith Goka, Haejun Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In spite of their huge success, transformer models remain difficult to scale
in depth. In this work, we develop a unified signal propagation theory and
provide formulae that govern the moments of the forward and backward signal
through the transformer model. Our framework can be used to understand and
mitigate vanishing/exploding gradients, rank collapse, and instability
associated with high attention scores. We also propose DeepScaleLM, an
initialization and scaling scheme that conserves unit output/gradient moments
throughout the model, enabling the training of very deep models with 100s of
layers. We find that transformer models could be much deeper - our deep models
with fewer parameters outperform shallow models in Language Modeling, Speech
Translation, and Image Classification, across Encoder-only, Decoder-only and
Encoder-Decoder variants, for both Pre-LN and Post-LN transformers, for
multiple datasets and model sizes. These improvements also translate into
improved performance on downstream Question Answering tasks and improved
robustness for image classification.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Akhil Kedia, Mohd Abbas Zaidi, Sushil Khyalia equal contribution.
  Source code is available at
  https://github.com/akhilkedia/TranformersGetStable</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quiet-STaR: Language Models Can Teach Themselves to Think Before
  Speaking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09629v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09629v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eric Zelikman, Georges Harik, Yijia Shao, Varuna Jayasiri, Nick Haber, Noah D. Goodman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When writing and talking, people sometimes pause to think. Although
reasoning-focused works have often framed reasoning as a method of answering
questions or completing agentic tasks, reasoning is implicit in almost all
written text. For example, this applies to the steps not stated between the
lines of a proof or to the theory of mind underlying a conversation. In the
Self-Taught Reasoner (STaR, Zelikman et al. 2022), useful thinking is learned
by inferring rationales from few-shot examples in question-answering and
learning from those that lead to a correct answer. This is a highly constrained
setting -- ideally, a language model could instead learn to infer unstated
rationales in arbitrary text. We present Quiet-STaR, a generalization of STaR
in which LMs learn to generate rationales at each token to explain future text,
improving their predictions. We address key challenges, including 1) the
computational cost of generating continuations, 2) the fact that the LM does
not initially know how to generate or use internal thoughts, and 3) the need to
predict beyond individual next tokens. To resolve these, we propose a tokenwise
parallel sampling algorithm, using learnable tokens indicating a thought's
start and end, and an extended teacher-forcing technique. Encouragingly,
generated rationales disproportionately help model difficult-to-predict tokens
and improve the LM's ability to directly answer difficult questions. In
particular, after continued pretraining of an LM on a corpus of internet text
with Quiet-STaR, we find zero-shot improvements on GSM8K
(5.9%$\rightarrow$10.9%) and CommonsenseQA (36.3%$\rightarrow$47.2%) and
observe a perplexity improvement of difficult tokens in natural text.
Crucially, these improvements require no fine-tuning on these tasks. Quiet-STaR
marks a step towards LMs that can learn to reason in a more general and
scalable way.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Make-Your-3D: Fast and Consistent Subject-Driven 3D Content Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09625v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09625v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fangfu Liu, Hanyang Wang, Weiliang Chen, Haowen Sun, Yueqi Duan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent years have witnessed the strong power of 3D generation models, which
offer a new level of creative flexibility by allowing users to guide the 3D
content generation process through a single image or natural language. However,
it remains challenging for existing 3D generation methods to create
subject-driven 3D content across diverse prompts. In this paper, we introduce a
novel 3D customization method, dubbed Make-Your-3D that can personalize
high-fidelity and consistent 3D content from only a single image of a subject
with text description within 5 minutes. Our key insight is to harmonize the
distributions of a multi-view diffusion model and an identity-specific 2D
generative model, aligning them with the distribution of the desired 3D
subject. Specifically, we design a co-evolution framework to reduce the
variance of distributions, where each model undergoes a process of learning
from the other through identity-aware optimization and subject-prior
optimization, respectively. Extensive experiments demonstrate that our method
can produce high-quality, consistent, and subject-specific 3D content with
text-driven modifications that are unseen in subject image.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://liuff19.github.io/Make-Your-3D</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Minimax Optimal and Computationally Efficient Algorithms for
  Distributionally Robust Offline Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09621v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09621v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhishuai Liu, Pan Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Distributionally robust offline reinforcement learning (RL), which seeks
robust policy training against environment perturbation by modeling dynamics
uncertainty, calls for function approximations when facing large state-action
spaces. However, the consideration of dynamics uncertainty introduces essential
nonlinearity and computational burden, posing unique challenges for analyzing
and practically employing function approximation. Focusing on a basic setting
where the nominal model and perturbed models are linearly parameterized, we
propose minimax optimal and computationally efficient algorithms realizing
function approximation and initiate the study on instance-dependent
suboptimality analysis in the context of robust offline RL. Our results uncover
that function approximation in robust offline RL is essentially distinct from
and probably harder than that in standard offline RL. Our algorithms and
theoretical results crucially depend on a variety of new techniques, involving
a novel function approximation mechanism incorporating variance information, a
new procedure of suboptimality and estimation uncertainty decomposition, a
quantification of the robust value function shrinkage, and a meticulously
designed family of hard instances, which might be of independent interest.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>53 pages, 1 figure, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reawakening knowledge: Anticipatory recovery from catastrophic
  interference via structured training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09613v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09613v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanlai Yang, Matt Jones, Michael C. Mozer, Mengye Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We explore the training dynamics of neural networks in a structured non-IID
setting where documents are presented cyclically in a fixed, repeated sequence.
Typically, networks suffer from catastrophic interference when training on a
sequence of documents; however, we discover a curious and remarkable property
of LLMs fine-tuned sequentially in this setting: they exhibit anticipatory
behavior, recovering from the forgetting on documents before encountering them
again. The behavior emerges and becomes more robust as the architecture scales
up its number of parameters. Through comprehensive experiments and
visualizations, we uncover new insights into training over-parameterized
networks in structured environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 18 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Compute-first optical detection for noise-resilient visual perception 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09612v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09612v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jungmin Kim, Nanfang Yu, Zongfu Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the context of visual perception, the optical signal from a scene is
transferred into the electronic domain by detectors in the form of image data,
which are then processed for the extraction of visual information. In noisy and
weak-signal environments such as thermal imaging for night vision applications,
however, the performance of neural computing tasks faces a significant
bottleneck due to the inherent degradation of data quality upon noisy
detection. Here, we propose a concept of optical signal processing before
detection to address this issue. We demonstrate that spatially redistributing
optical signals through a properly designed linear transformer can enhance the
detection noise resilience of visual perception tasks, as benchmarked with the
MNIST classification. Our idea is supported by a quantitative analysis
detailing the relationship between signal concentration and noise robustness,
as well as its practical implementation in an incoherent imaging system. This
compute-first detection scheme can pave the way for advancing infrared machine
vision technologies widely used for industrial and defense applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Main 9 pages, 5 figures, Supplementary information 5 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MM1: Methods, Analysis & Insights from Multimodal LLM <span class="highlight-title">Pre-train</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09611v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09611v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brandon McKinzie, Zhe Gan, Jean-Philippe Fauconnier, Sam Dodge, Bowen Zhang, Philipp Dufter, Dhruti Shah, Xianzhi Du, Futang Peng, Floris Weers, Anton Belyi, Haotian Zhang, Karanjeet Singh, Doug Kang, Hongyu Hè, Max Schwarzer, Tom Gunter, Xiang Kong, Aonan Zhang, Jianyu Wang, Chong Wang, Nan Du, Tao Lei, Sam Wiseman, Mark Lee, Zirui Wang, Ruoming Pang, Peter Grasch, Alexander Toshev, Yinfei Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we discuss building performant Multimodal Large Language Models
(MLLMs). In particular, we study the importance of various architecture
components and data choices. Through careful and comprehensive ablations of the
image encoder, the vision language connector, and various pre-training data
choices, we identified several crucial design lessons. For example, we
demonstrate that for large-scale multimodal pre-training using a careful mix of
image-caption, interleaved image-text, and text-only data is crucial for
achieving state-of-the-art (SOTA) few-shot results across multiple benchmarks,
compared to other published pre-training results. Further, we show that the
image encoder together with image resolution and the image token count has
substantial impact, while the vision-language connector design is of
comparatively negligible importance. By scaling up the presented recipe, we
build MM1, a family of multimodal models up to 30B parameters, consisting of
both dense models and mixture-of-experts (MoE) variants, that are SOTA in
pre-training metrics and achieve competitive performance after supervised
fine-tuning on a range of established multimodal benchmarks. Thanks to
large-scale pre-training, MM1 enjoys appealing properties such as enhanced
in-context learning, and multi-image reasoning, enabling few-shot
chain-of-thought prompting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimistic Verifiable Training by Controlling Hardware Nondeterminism 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09603v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09603v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Megha Srivastava, Simran Arora, Dan Boneh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing compute demands of AI systems has led to the emergence of
services that train models on behalf of clients lacking necessary resources.
However, ensuring correctness of training and guarding against potential
training-time attacks, such as data poisoning, poses challenges. Existing works
on verifiable training largely fall into two classes: proof-based systems,
which struggle to scale due to requiring cryptographic techniques, and
"optimistic" methods that consider a trusted third-party auditor who replicates
the training process. A key challenge with the latter is that hardware
nondeterminism between GPU types during training prevents an auditor from
replicating the training process exactly, and such schemes are therefore
non-robust. We propose a method that combines training in a higher precision
than the target model, rounding after intermediate computation steps, and
storing rounding decisions based on an adaptive thresholding procedure, to
successfully control for nondeterminism. Across three different NVIDIA GPUs
(A40, Titan XP, RTX 2080 Ti), we achieve exact training replication at FP32
precision for both full-training and fine-tuning of ResNet-50 (23M) and GPT-2
(117M) models. Our verifiable training scheme significantly decreases the
storage and time costs compared to proof-based systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 5 figures, preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mixture of Mixups for Multi-label Classification of Rare Anuran Sounds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09598v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09598v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ilyass Moummad, Nicolas Farrugia, Romain Serizel, Jeremy Froidevaux, Vincent Lostanlen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-label imbalanced classification poses a significant challenge in
machine learning, particularly evident in bioacoustics where animal sounds
often co-occur, and certain sounds are much less frequent than others. This
paper focuses on the specific case of classifying anuran species sounds using
the dataset AnuraSet, that contains both class imbalance and multi-label
examples. To address these challenges, we introduce Mixture of Mixups (Mix2), a
framework that leverages mixing regularization methods Mixup, Manifold Mixup,
and MultiMix. Experimental results show that these methods, individually, may
lead to suboptimal results; however, when applied randomly, with one selected
at each training iteration, they prove effective in addressing the mentioned
challenges, particularly for rare classes with few occurrences. Further
analysis reveals that Mix2 is also proficient in classifying sounds across
various levels of class co-occurrences.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Iterative Forgetting: Online Data Stream Regression Using
  Database-Inspired Adaptive Granulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09588v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09588v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Niket Kathiriya, Hossein Haeri, Cindy Chen, Kshitij Jerath
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many modern systems, such as financial, transportation, and
telecommunications systems, are time-sensitive in the sense that they demand
low-latency predictions for real-time decision-making. Such systems often have
to contend with continuous unbounded data streams as well as concept drift,
which are challenging requirements that traditional regression techniques are
unable to cater to. There exists a need to create novel data stream regression
methods that can handle these scenarios. We present a database-inspired
datastream regression model that (a) uses inspiration from R*-trees to create
granules from incoming datastreams such that relevant information is retained,
(b) iteratively forgets granules whose information is deemed to be outdated,
thus maintaining a list of only recent, relevant granules, and (c) uses the
recent data and granules to provide low-latency predictions. The
R*-tree-inspired approach also makes the algorithm amenable to integration with
database systems. Our experiments demonstrate that the ability of this method
to discard data produces a significant order-of-magnitude improvement in
latency and training time when evaluated against the most accurate
state-of-the-art algorithms, while the R*-tree-inspired granulation technique
provides competitively accurate predictions
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Algorithmic syntactic causal identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09580v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09580v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dhurim Cakiqi, Max A. Little
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Causal identification in causal Bayes nets (CBNs) is an important tool in
causal inference allowing the derivation of interventional distributions from
observational distributions where this is possible in principle. However, most
existing formulations of causal identification using techniques such as
d-separation and do-calculus are expressed within the mathematical language of
classical probability theory on CBNs. However, there are many causal settings
where probability theory and hence current causal identification techniques are
inapplicable such as relational databases, dataflow programs such as hardware
description languages, distributed systems and most modern machine learning
algorithms. We show that this restriction can be lifted by replacing the use of
classical probability theory with the alternative axiomatic foundation of
symmetric monoidal categories. In this alternative axiomatization, we show how
an unambiguous and clean distinction can be drawn between the general syntax of
causal models and any specific semantic implementation of that causal model.
This allows a purely syntactic algorithmic description of general causal
identification by a translation of recent formulations of the general ID
algorithm through fixing. Our description is given entirely in terms of the
non-parametric ADMG structure specifying a causal model and the algebraic
signature of the corresponding monoidal category, to which a sequence of
manipulations is then applied so as to arrive at a modified monoidal category
in which the desired, purely syntactic interventional causal model, is
obtained. We use this idea to derive purely syntactic analogues of classical
back-door and front-door causal adjustment, and illustrate an application to a
more complex causal model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 2 TikZ figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ uaMix-MAE: Efficient Tuning of <span class="highlight-title">Pretrain</span>ed Audio <span class="highlight-title">Transformer</span>s with
  Unsupervised Audio Mixtures <span class="chip">ICASSP'2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09579v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09579v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Afrina Tabassum, Dung Tran, Trung Dang, Ismini Lourentzou, Kazuhito Koishida
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Masked Autoencoders (MAEs) learn rich low-level representations from
unlabeled data but require substantial labeled data to effectively adapt to
downstream tasks. Conversely, Instance Discrimination (ID) emphasizes
high-level semantics, offering a potential solution to alleviate annotation
requirements in MAEs. Although combining these two approaches can address
downstream tasks with limited labeled data, naively integrating ID into MAEs
leads to extended training times and high computational costs. To address this
challenge, we introduce uaMix-MAE, an efficient ID tuning strategy that
leverages unsupervised audio mixtures. Utilizing contrastive tuning, uaMix-MAE
aligns the representations of pretrained MAEs, thereby facilitating effective
adaptation to task-specific semantics. To optimize the model with small amounts
of unlabeled data, we propose an audio mixing technique that manipulates audio
samples in both input and virtual label spaces. Experiments in low/few-shot
settings demonstrate that \modelname achieves 4-6% accuracy improvements over
various benchmarks when tuned with limited unlabeled data, such as
AudioSet-20K. Code is available at https://github.com/PLAN-Lab/uamix-MAE
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 6 figures, 4 tables. To appear in ICASSP'2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Are you a robot? Detecting Autonomous Vehicles from Behavior Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09571v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09571v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fabio Maresca, Filippo Grazioli, Antonio Albanese, Vincenzo Sciancalepore, Gianpiero Negri, Xavier Costa-Perez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The tremendous hype around autonomous driving is eagerly calling for emerging
and novel technologies to support advanced mobility use cases. As car
manufactures keep developing SAE level 3+ systems to improve the safety and
comfort of passengers, traffic authorities need to establish new procedures to
manage the transition from human-driven to fully-autonomous vehicles while
providing a feedback-loop mechanism to fine-tune envisioned autonomous systems.
Thus, a way to automatically profile autonomous vehicles and differentiate
those from human-driven ones is a must. In this paper, we present a
fully-fledged framework that monitors active vehicles using camera images and
state information in order to determine whether vehicles are autonomous,
without requiring any active notification from the vehicles themselves.
Essentially, it builds on the cooperation among vehicles, which share their
data acquired on the road feeding a machine learning model to identify
autonomous cars. We extensively tested our solution and created the NexusStreet
dataset, by means of the CARLA simulator, employing an autonomous driving
control agent and a steering wheel maneuvered by licensed drivers. Experiments
show it is possible to discriminate the two behaviors by analyzing video clips
with an accuracy of 80%, which improves up to 93% when the target state
information is available. Lastly, we deliberately degraded the state to observe
how the framework performs under non-ideal data collection conditions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Fidelity Bayesian Optimization With Across-Task Transferable
  Max-Value Entropy Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09570v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09570v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunchuan Zhang, Sangwoo Park, Osvaldo Simeone
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In many applications, ranging from logistics to engineering, a designer is
faced with a sequence of optimization tasks for which the objectives are in the
form of black-box functions that are costly to evaluate. For example, the
designer may need to tune the hyperparameters of neural network models for
different learning tasks over time. Rather than evaluating the objective
function for each candidate solution, the designer may have access to
approximations of the objective functions, for which higher-fidelity
evaluations entail a larger cost. Existing multi-fidelity black-box
optimization strategies select candidate solutions and fidelity levels with the
goal of maximizing the information accrued about the optimal value or solution
for the current task. Assuming that successive optimization tasks are related,
this paper introduces a novel information-theoretic acquisition function that
balances the need to acquire information about the current task with the goal
of collecting information transferable to future tasks. The proposed method
includes shared inter-task latent variables, which are transferred across tasks
by implementing particle-based variational Bayesian updates. Experimental
results across synthetic and real-world examples reveal that the proposed
provident acquisition strategy that caters to future tasks can significantly
improve the optimization efficiency as soon as a sufficient number of tasks is
processed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to IEEE for review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self-Consistency Training for Hamiltonian Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09560v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09560v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        He Zhang, Chang Liu, Zun Wang, Xinran Wei, Siyuan Liu, Nanning Zheng, Bin Shao, Tie-Yan Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hamiltonian prediction is a versatile formulation to leverage machine
learning for solving molecular science problems. Yet, its applicability is
limited by insufficient labeled data for training. In this work, we highlight
that Hamiltonian prediction possesses a self-consistency principle, based on
which we propose an exact training method that does not require labeled data.
This merit addresses the data scarcity difficulty, and distinguishes the task
from other property prediction formulations with unique benefits: (1)
self-consistency training enables the model to be trained on a large amount of
unlabeled data, hence substantially enhances generalization; (2)
self-consistency training is more efficient than labeling data with DFT for
supervised training, since it is an amortization of DFT calculation over a set
of molecular structures. We empirically demonstrate the better generalization
in data-scarce and out-of-distribution scenarios, and the better efficiency
from the amortization. These benefits push forward the applicability of
Hamiltonian prediction to an ever larger scale.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generalizing Denoising to Non-Equilibrium Structures Improves
  Equivariant Force Fields 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09549v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09549v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi-Lun Liao, Tess Smidt, Abhishek Das
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding the interactions of atoms such as forces in 3D atomistic
systems is fundamental to many applications like molecular dynamics and
catalyst design. However, simulating these interactions requires
compute-intensive ab initio calculations and thus results in limited data for
training neural networks. In this paper, we propose to use denoising
non-equilibrium structures (DeNS) as an auxiliary task to better leverage
training data and improve performance. For training with DeNS, we first corrupt
a 3D structure by adding noise to its 3D coordinates and then predict the
noise. Different from previous works on denoising, which are limited to
equilibrium structures, the proposed method generalizes denoising to a much
larger set of non-equilibrium structures. The main difference is that a
non-equilibrium structure does not correspond to local energy minima and has
non-zero forces, and therefore it can have many possible atomic positions
compared to an equilibrium structure. This makes denoising non-equilibrium
structures an ill-posed problem since the target of denoising is not uniquely
defined. Our key insight is to additionally encode the forces of the original
non-equilibrium structure to specify which non-equilibrium structure we are
denoising. Concretely, given a corrupted non-equilibrium structure and the
forces of the original one, we predict the non-equilibrium structure satisfying
the input forces instead of any arbitrary structures. Since DeNS requires
encoding forces, DeNS favors equivariant networks, which can easily incorporate
forces and other higher-order tensors in node embeddings. We study the
effectiveness of training equivariant networks with DeNS on OC20, OC22 and MD17
datasets and demonstrate that DeNS can achieve new state-of-the-art results on
OC20 and OC22 and significantly improve training efficiency on MD17.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Breast Cancer Classification Using Gradient Boosting Algorithms Focusing
  on Reducing the False Negative and SHAP for Explainability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09548v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09548v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        João Manoel Herrera Pinheiro, Marcelo Becker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cancer is one of the diseases that kill the most women in the world, with
breast cancer being responsible for the highest number of cancer cases and
consequently deaths. However, it can be prevented by early detection and,
consequently, early treatment. Any development for detection or perdition this
kind of cancer is important for a better healthy life. Many studies focus on a
model with high accuracy in cancer prediction, but sometimes accuracy alone may
not always be a reliable metric. This study implies an investigative approach
to studying the performance of different machine learning algorithms based on
boosting to predict breast cancer focusing on the recall metric. Boosting
machine learning algorithms has been proven to be an effective tool for
detecting medical diseases. The dataset of the University of California, Irvine
(UCI) repository has been utilized to train and test the model classifier that
contains their attributes. The main objective of this study is to use
state-of-the-art boosting algorithms such as AdaBoost, XGBoost, CatBoost and
LightGBM to predict and diagnose breast cancer and to find the most effective
metric regarding recall, ROC-AUC, and confusion matrix. Furthermore, our study
is the first to use these four boosting algorithms with Optuna, a library for
hyperparameter optimization, and the SHAP method to improve the
interpretability of our model, which can be used as a support to identify and
predict breast cancer. We were able to improve AUC or recall for all the models
and reduce the False Negative for AdaBoost and LigthGBM the final AUC were more
than 99.41\% for all models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 16 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How do Machine Learning Projects use Continuous Integration Practices?
  An Empirical Study on GitHub Actions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09547v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09547v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        João Helis Bernardo, Daniel Alencar da Costa, Sérgio Queiroz de Medeiros, Uirá Kulesza
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Continuous Integration (CI) is a well-established practice in traditional
software development, but its nuances in the domain of Machine Learning (ML)
projects remain relatively unexplored. Given the distinctive nature of ML
development, understanding how CI practices are adopted in this context is
crucial for tailoring effective approaches. In this study, we conduct a
comprehensive analysis of 185 open-source projects on GitHub (93 ML and 92
non-ML projects). Our investigation comprises both quantitative and qualitative
dimensions, aiming to uncover differences in CI adoption between ML and non-ML
projects. Our findings indicate that ML projects often require longer build
durations, and medium-sized ML projects exhibit lower test coverage compared to
non-ML projects. Moreover, small and medium-sized ML projects show a higher
prevalence of increasing build duration trends compared to their non-ML
counterparts. Additionally, our qualitative analysis illuminates the
discussions around CI in both ML and non-ML projects, encompassing themes like
CI Build Execution and Status, CI Testing, and CI Infrastructure. These
insights shed light on the unique challenges faced by ML projects in adopting
CI practices effectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, Mining Software Repositories, MSR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Explorations in Texture Learning <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09543v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09543v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Blaine Hoak, Patrick McDaniel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we investigate \textit{texture learning}: the identification of
textures learned by object classification models, and the extent to which they
rely on these textures. We build texture-object associations that uncover new
insights about the relationships between texture and object classes in CNNs and
find three classes of results: associations that are strong and expected,
strong and not expected, and expected but not present. Our analysis
demonstrates that investigations in texture learning enable new methods for
interpretability and have the potential to uncover unexpected biases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICLR 2024, Tiny Papers Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Logits of API-Protected LLMs Leak Proprietary Information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09539v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09539v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthew Finlayson, Swabha Swayamdipta, Xiang Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The commercialization of large language models (LLMs) has led to the common
practice of high-level API-only access to proprietary models. In this work, we
show that even with a conservative assumption about the model architecture, it
is possible to learn a surprisingly large amount of non-public information
about an API-protected LLM from a relatively small number of API queries (e.g.,
costing under $1,000 for OpenAI's gpt-3.5-turbo). Our findings are centered on
one key observation: most modern LLMs suffer from a softmax bottleneck, which
restricts the model outputs to a linear subspace of the full output space. We
show that this lends itself to a model image or a model signature which unlocks
several capabilities with affordable cost: efficiently discovering the LLM's
hidden size, obtaining full-vocabulary outputs, detecting and disambiguating
different model updates, identifying the source LLM given a single full LLM
output, and even estimating the output layer parameters. Our empirical
investigations show the effectiveness of our methods, which allow us to
estimate the embedding size of OpenAI's gpt-3.5-turbo to be about 4,096.
Lastly, we discuss ways that LLM providers can guard against these attacks, as
well as how these capabilities can be viewed as a feature (rather than a bug)
by allowing for greater transparency and accountability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Prototypical Representations for Mitigating Social Bias
  without Demographic Information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09516v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09516v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shadi Iskander, Kira Radinsky, Yonatan Belinkov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mitigating social biases typically requires identifying the social groups
associated with each data sample. In this paper, we present DAFair, a novel
approach to address social bias in language models. Unlike traditional methods
that rely on explicit demographic labels, our approach does not require any
such information. Instead, we leverage predefined prototypical demographic
texts and incorporate a regularization term during the fine-tuning process to
mitigate bias in the model's representations. Our empirical results across two
tasks and two models demonstrate the effectiveness of our method compared to
previous approaches that do not rely on labeled data. Moreover, with limited
demographic-annotated data, our approach outperforms common debiasing
approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On STPA for Distributed Development of Safe Autonomous Driving: An
  Interview Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09509v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09509v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Nouri, Christian Berger, Fredrik Törner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Safety analysis is used to identify hazards and build knowledge during the
design phase of safety-relevant functions. This is especially true for complex
AI-enabled and software intensive systems such as Autonomous Drive (AD).
System-Theoretic Process Analysis (STPA) is a novel method applied in
safety-related fields like defense and aerospace, which is also becoming
popular in the automotive industry. However, STPA assumes prerequisites that
are not fully valid in the automotive system engineering with distributed
system development and multi-abstraction design levels. This would inhibit
software developers from using STPA to analyze their software as part of a
bigger system, resulting in a lack of traceability. This can be seen as a
maintainability challenge in continuous development and deployment (DevOps). In
this paper, STPA's different guidelines for the automotive industry, e.g.
J31887/ISO21448/STPA handbook, are firstly compared to assess their
applicability to the distributed development of complex AI-enabled systems like
AD. Further, an approach to overcome the challenges of using STPA in a
multi-level design context is proposed. By conducting an interview study with
automotive industry experts for the development of AD, the challenges are
validated and the effectiveness of the proposed approach is evaluated.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at SEAA. 8 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Don't Judge by the Look: A Motion Coherent Augmentation for Video
  Recognition <span class="chip">ICLR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09506v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09506v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yitian Zhang, Yue Bai, Huan Wang, Yizhou Wang, Yun Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current training pipelines in object recognition neglect Hue Jittering when
doing data augmentation as it not only brings appearance changes that are
detrimental to classification, but also the implementation is inefficient in
practice. In this study, we investigate the effect of hue variance in the
context of video recognition and find this variance to be beneficial since
static appearances are less important in videos that contain motion
information. Based on this observation, we propose a data augmentation method
for video recognition, named Motion Coherent Augmentation (MCA), that
introduces appearance variation in videos and implicitly encourages the model
to prioritize motion patterns, rather than static appearances. Concretely, we
propose an operation SwapMix to efficiently modify the appearance of video
samples, and introduce Variation Alignment (VA) to resolve the distribution
shift caused by SwapMix, enforcing the model to learn appearance invariant
representations. Comprehensive empirical evaluation across various
architectures and different datasets solidly validates the effectiveness and
generalization ability of MCA, and the application of VA in other augmentation
methods. Code is available at https://github.com/BeSpontaneous/MCA-pytorch.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EquiAV: Leveraging Equivariance for Audio-Visual Contrastive Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09502v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09502v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jongsuk Kim, Hyeongkeun Lee, Kyeongha Rho, Junmo Kim, Joon Son Chung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in self-supervised audio-visual representation learning
have demonstrated its potential to capture rich and comprehensive
representations. However, despite the advantages of data augmentation verified
in many learning methods, audio-visual learning has struggled to fully harness
these benefits, as augmentations can easily disrupt the correspondence between
input pairs. To address this limitation, we introduce EquiAV, a novel framework
that leverages equivariance for audio-visual contrastive learning. Our approach
begins with extending equivariance to audio-visual learning, facilitated by a
shared attention-based transformation predictor. It enables the aggregation of
features from diverse augmentations into a representative embedding, providing
robust supervision. Notably, this is achieved with minimal computational
overhead. Extensive ablation studies and qualitative results verify the
effectiveness of our method. EquiAV outperforms previous works across various
audio-visual benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Reinforcement Learning Approach to Dairy Farm Battery Management using
  Q Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09499v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09499v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nawazish Ali, Abdul Wahid, Rachael Shaw, Karl Mason
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dairy farming consumes a significant amount of energy, making it an
energy-intensive sector within agriculture. Integrating renewable energy
generation into dairy farming could help address this challenge. Effective
battery management is important for integrating renewable energy generation.
Managing battery charging and discharging poses significant challenges because
of fluctuations in electrical consumption, the intermittent nature of renewable
energy generation, and fluctuations in energy prices. Artificial Intelligence
(AI) has the potential to significantly improve the use of renewable energy in
dairy farming, however, there is limited research conducted in this particular
domain. This research considers Ireland as a case study as it works towards
attaining its 2030 energy strategy centered on the utilization of renewable
sources. This study proposes a Q-learning-based algorithm for scheduling
battery charging and discharging in a dairy farm setting. This research also
explores the effect of the proposed algorithm by adding wind generation data
and considering additional case studies. The proposed algorithm reduces the
cost of imported electricity from the grid by 13.41\%, peak demand by 2\%, and
24.49\% when utilizing wind generation. These results underline how
reinforcement learning is highly effective in managing batteries in the dairy
farming sector.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On using Machine Learning Algorithms for Motorcycle Collision Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09491v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09491v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philipp Rodegast, Steffen Maier, Jonas Kneifl, Jörg Fehr
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Globally, motorcycles attract vast and varied users. However, since the rate
of severe injury and fatality in motorcycle accidents far exceeds passenger car
accidents, efforts have been directed toward increasing passive safety systems.
Impact simulations show that the risk of severe injury or death in the event of
a motorcycle-to-car impact can be greatly reduced if the motorcycle is equipped
with passive safety measures such as airbags and seat belts. For the passive
safety systems to be activated, a collision must be detected within
milliseconds for a wide variety of impact configurations, but under no
circumstances may it be falsely triggered. For the challenge of reliably
detecting impending collisions, this paper presents an investigation towards
the applicability of machine learning algorithms. First, a series of
simulations of accidents and driving operation is introduced to collect data to
train machine learning classification models. Their performance is henceforth
assessed and compared via multiple representative and application-oriented
criteria.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Laying the Foundation First? Investigating the Generalization from
  Atomic Skills to Complex Reasoning Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09479v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09479v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuncheng Huang, Qianyu He, Yipei Xu, Jiaqing Liang, Yanghua Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current language models have demonstrated their capability to develop basic
reasoning, but struggle in more complicated reasoning tasks that require a
combination of atomic skills, such as math word problem requiring skills like
arithmetic and unit conversion. Previous methods either do not improve the
inherent atomic skills of models or not attempt to generalize the atomic skills
to complex reasoning tasks. In this paper, we first propose a probing framework
to investigate whether the atomic skill can spontaneously generalize to complex
reasoning tasks. Then, we introduce a hierarchical curriculum learning training
strategy to achieve better skill generalization. In our experiments, we find
that atomic skills can not spontaneously generalize to compositional tasks. By
leveraging hierarchical curriculum learning, we successfully induce
generalization, significantly improve the performance of open-source LMs on
complex reasoning tasks. Promisingly, the skill generalization exhibit
effective in cross-dataset and cross-domain scenarios. Complex reasoning can
also help enhance atomic skills. Our findings offer valuable guidance for
designing better training strategies for complex reasoning tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VIRUS-NeRF -- Vision, InfraRed and UltraSonic based Neural Radiance
  Fields 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09477v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09477v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicolaj Schmid, Cornelius von Einem, Cesar Cadena, Roland Siegwart, Lorenz Hruby, Florian Tschopp
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous mobile robots are an increasingly integral part of modern factory
and warehouse operations. Obstacle detection, avoidance and path planning are
critical safety-relevant tasks, which are often solved using expensive LiDAR
sensors and depth cameras. We propose to use cost-effective low-resolution
ranging sensors, such as ultrasonic and infrared time-of-flight sensors by
developing VIRUS-NeRF - Vision, InfraRed, and UltraSonic based Neural Radiance
Fields. Building upon Instant Neural Graphics Primitives with a Multiresolution
Hash Encoding (Instant-NGP), VIRUS-NeRF incorporates depth measurements from
ultrasonic and infrared sensors and utilizes them to update the occupancy grid
used for ray marching. Experimental evaluation in 2D demonstrates that
VIRUS-NeRF achieves comparable mapping performance to LiDAR point clouds
regarding coverage. Notably, in small environments, its accuracy aligns with
that of LiDAR measurements, while in larger ones, it is bounded by the utilized
ultrasonic sensors. An in-depth ablation study reveals that adding ultrasonic
and infrared sensors is highly effective when dealing with sparse data and low
view variation. Further, the proposed occupancy grid of VIRUS-NeRF improves the
mapping capabilities and increases the training speed by 46% compared to
Instant-NGP. Overall, VIRUS-NeRF presents a promising approach for
cost-effective local mapping in mobile robotics, with potential applications in
safety and navigation tasks. The code can be found at
https://github.com/ethz-asl/virus nerf.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Convergence of Locally Adaptive and Scalable Diffusion-Based
  Sampling Methods for Deep Bayesian Neural Network Posteriors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08609v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08609v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tim Rensmeyer, Oliver Niggemann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Achieving robust uncertainty quantification for deep neural networks
represents an important requirement in many real-world applications of deep
learning such as medical imaging where it is necessary to assess the
reliability of a neural network's prediction. Bayesian neural networks are a
promising approach for modeling uncertainties in deep neural networks.
Unfortunately, generating samples from the posterior distribution of neural
networks is a major challenge. One significant advance in that direction would
be the incorporation of adaptive step sizes, similar to modern neural network
optimizers, into Monte Carlo Markov chain sampling algorithms without
significantly increasing computational demand. Over the past years, several
papers have introduced sampling algorithms with claims that they achieve this
property. However, do they indeed converge to the correct distribution? In this
paper, we demonstrate that these methods can have a substantial bias in the
distribution they sample, even in the limit of vanishing step sizes and at full
batch size.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Consistent <span class="highlight-title">Prompt</span>ing for Rehearsal-Free Continual Learning <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08568v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08568v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhanxin Gao, Jun Cen, Xiaobin Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Continual learning empowers models to adapt autonomously to the ever-changing
environment or data streams without forgetting old knowledge. Prompt-based
approaches are built on frozen pre-trained models to learn the task-specific
prompts and classifiers efficiently. Existing prompt-based methods are
inconsistent between training and testing, limiting their effectiveness. Two
types of inconsistency are revealed. Test predictions are made from all
classifiers while training only focuses on the current task classifier without
holistic alignment, leading to Classifier inconsistency. Prompt inconsistency
indicates that the prompt selected during testing may not correspond to the one
associated with this task during training. In this paper, we propose a novel
prompt-based method, Consistent Prompting (CPrompt), for more aligned training
and testing. Specifically, all existing classifiers are exposed to prompt
training, resulting in classifier consistency learning. In addition, prompt
consistency learning is proposed to enhance prediction robustness and boost
prompt selection accuracy. Our Consistent Prompting surpasses its prompt-based
counterparts and achieves state-of-the-art performance on multiple continual
learning benchmarks. Detailed analysis shows that improvements come from more
consistent training and testing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CacheGen: Fast Context Loading for Language Model Applications via KV
  Cache Streaming 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07240v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07240v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhan Liu, Hanchen Li, Yihua Cheng, Siddhant Ray, Yuyang Huang, Qizheng Zhang, Kuntai Du, Jiayi Yao, Shan Lu, Ganesh Ananthanarayanan, Michael Maire, Henry Hoffmann, Ari Holtzman, Junchen Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As large language models (LLMs) take on complex tasks, their inputs are
supplemented with longer contexts that incorporate domain knowledge or
user-specific information. Yet using long contexts poses a challenge for
responsive LLM systems, as nothing can be generated until the whole context is
processed by the LLM. While the context-processing delay can be reduced by
reusing the KV cache of a context across different inputs, fetching the KV
cache, which contains large tensors, over the network can cause extra network
delays.
  CacheGen is a fast context-loading module for LLM systems. First, CacheGen
uses a custom tensor encoder, which embraces KV cache's distributional
properties, to encode a KV cache into more compact bitstream representations
with negligible encoding/decoding overhead. This reduces the bandwidth demand
to fetch the KV cache. Second, to maintain low context-loading delay and high
generation quality, CacheGen adapts the streaming strategies to cope with
changes in available bandwidth. When available bandwidth drops, CacheGen may
raise the compression level for a part of the context or choose to recompute
its KV cache on the fly. We test CacheGen on four popular LLMs of various sizes
and four datasets (662 contexts in total). Compared to the recent systems that
reuse the KV cache, CacheGen reduces the KV cache size by 3.7-4.3x and the
total delay in fetching and processing contexts by 2.7-3.2x while having
negligible impact on the LLM response quality in accuracy or perplexity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The statistical thermodynamics of generative diffusion models: Phase
  transitions, symmetry breaking and critical instability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.17467v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.17467v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luca Ambrogioni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative diffusion models have achieved spectacular performance in many
areas of generative modeling. While the fundamental ideas behind these models
come from non-equilibrium physics, variational inference and stochastic
calculus, in this paper we show that many aspects of these models can be
understood using the tools of equilibrium statistical mechanics. Using this
reformulation, we show that generative diffusion models undergo second-order
phase transitions corresponding to symmetry breaking phenomena. We show that
these phase-transitions are always in a mean-field universality class, as they
are the result of a self-consistency condition in the generative dynamics. We
argue that the critical instability that arises from the phase transitions lies
at the heart of their generative capabilities, which are characterized by a set
of mean field critical exponents. Furthermore, using the statistical physics of
disordered systems, we show that memorization can be understood as a form of
critical condensation corresponding to a disordered phase transition. Finally,
we show that the dynamic equation of the generative process can be interpreted
as a stochastic adiabatic transformation that minimizes the free energy while
keeping the system in thermal equilibrium.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Era Splitting -- Invariant Learning for Decision Trees 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.14496v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.14496v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Timothy DeLise
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-life machine learning problems exhibit distributional shifts in the data
from one time to another or from one place to another. This behavior is beyond
the scope of the traditional empirical risk minimization paradigm, which
assumes i.i.d. distribution of data over time and across locations. The
emerging field of out-of-distribution (OOD) generalization addresses this
reality with new theory and algorithms which incorporate environmental, or
era-wise information into the algorithms. So far, most research has been
focused on linear models and/or neural networks. In this research we develop
two new splitting criteria for decision trees, which allow us to apply ideas
from OOD generalization research to decision tree models, namely, gradient
boosting decision trees (GBDT). The new splitting criteria use era-wise
information associated with the data to grow tree-based models that are optimal
across all disjoint eras in the data, instead of optimal over the entire data
set pooled together, which is the default setting. In this paper, two new
splitting criteria are defined and analyzed theoretically. Effectiveness is
tested on four experiments, ranging from simple, synthetic to complex,
real-world applications. In particular we cast the OOD domain-adaptation
problem in the context of financial markets, where the new models out-perform
state-of-the-art GBDT models on the Numerai data set. The new criteria are
incorporated into the Scikit-Learn code base and made freely available online.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Zero Coordinate Shift: Whetted Automatic Differentiation for
  Physics-informed Operator Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.00860v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.00860v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kuangdai Leng, Mallikarjun Shankar, Jeyan Thiyagalingam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic differentiation (AD) is a critical step in physics-informed machine
learning, required for computing the high-order derivatives of network output
w.r.t. coordinates of collocation points. In this paper, we present a novel and
lightweight algorithm to conduct AD for physics-informed operator learning,
which we call the trick of Zero Coordinate Shift (ZCS). Instead of making all
sampled coordinates as leaf variables, ZCS introduces only one scalar-valued
leaf variable for each spatial or temporal dimension, simplifying the wanted
derivatives from "many-roots-many-leaves" to "one-root-many-leaves" whereby
reverse-mode AD becomes directly utilisable. It has led to an outstanding
performance leap by avoiding the duplication of the computational graph along
the dimension of functions (physical parameters). ZCS is easy to implement with
current deep learning libraries; our own implementation is achieved by
extending the DeepXDE package. We carry out a comprehensive benchmark analysis
and several case studies, training physics-informed DeepONets to solve partial
differential equations (PDEs) without data. The results show that ZCS has
persistently reduced GPU memory consumption and wall time for training by an
order of magnitude, and such reduction factor scales with the number of
functions. As a low-level optimisation technique, ZCS imposes no restrictions
on data, physics (PDE) or network architecture and does not compromise training
results from any aspect.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in Journal of Computational Physics.
  https://doi.org/10.1016/j.jcp.2024.112904</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Koopman operators with intrinsic observables in rigged reproducing
  kernel Hil<span class="highlight-title">bert</span> spaces 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.02524v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.02524v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Isao Ishikawa, Yuka Hashimoto, Masahiro Ikeda, Yoshinobu Kawahara
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a novel approach for estimating the Koopman operator
defined on a reproducing kernel Hilbert space (RKHS) and its spectra. We
propose an estimation method, what we call Jet Dynamic Mode Decomposition
(JetDMD), leveraging the intrinsic structure of RKHS and the geometric notion
known as jets to enhance the estimation of the Koopman operator. This method
refines the traditional Extended Dynamic Mode Decomposition (EDMD) in accuracy,
especially in the numerical estimation of eigenvalues. This paper proves
JetDMD's superiority through explicit error bounds and convergence rate for
special positive definite kernels, offering a solid theoretical foundation for
its performance. We also delve into the spectral analysis of the Koopman
operator, proposing the notion of extended Koopman operator within a framework
of rigged Hilbert space. This notion leads to a deeper understanding of
estimated Koopman eigenfunctions and capturing them outside the original
function space. Through the theory of rigged Hilbert space, our study provides
a principled methodology to analyze the estimated spectrum and eigenfunctions
of Koopman operators, and enables eigendecomposition within a rigged RKHS. We
also propose a new effective method for reconstructing the dynamical system
from temporally-sampled trajectory data of the dynamical system with solid
theoretical guarantee. We conduct several numerical simulations using the van
der Pol oscillator, the Duffing oscillator, the H\'enon map, and the Lorenz
attractor, and illustrate the performance of JetDMD with clear numerical
computations of eigenvalues and accurate predictions of the dynamical systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>We correct several typos. We have released the code for the numerical
  simulation at https://github.com/1sa014kawa/JetDMD</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring Safety Generalization Challenges of Large Language Models via
  Code 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07865v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07865v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qibing Ren, Chang Gao, Jing Shao, Junchi Yan, Xin Tan, Yu Qiao, Wai Lam, Lizhuang Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancement of Large Language Models (LLMs) has brought about
remarkable capabilities in natural language processing but also raised concerns
about their potential misuse. While strategies like supervised fine-tuning and
reinforcement learning from human feedback have enhanced their safety, these
methods primarily focus on natural languages, which may not generalize to other
domains. This paper introduces CodeAttack, a framework that transforms natural
language inputs into code inputs, presenting a novel environment for testing
the safety generalization of LLMs. Our comprehensive studies on
state-of-the-art LLMs including GPT-4, Claude-2, and Llama-2 series reveal a
common safety vulnerability of these models against code input: CodeAttack
consistently bypasses the safety guardrails of all models more than 80% of the
time. Furthermore, we find that a larger distribution gap between CodeAttack
and natural language leads to weaker safety generalization, such as encoding
natural language input with data structures or using less popular programming
languages. These findings highlight new safety risks in the code domain and the
need for more robust safety alignment algorithms to match the code capabilities
of LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Assessing the Impact of Sequence Length Learning on Classification Tasks
  for <span class="highlight-title">Transformer</span> Encoder Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08399v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08399v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jean-Thomas Baillargeon, Luc Lamontagne
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Classification algorithms using Transformer architectures can be affected by
the sequence length learning problem whenever observations from different
classes have a different length distribution. This problem causes models to use
sequence length as a predictive feature instead of relying on important textual
information. Although most public datasets are not affected by this problem,
privately owned corpora for fields such as medicine and insurance may carry
this data bias. The exploitation of this sequence length feature poses
challenges throughout the value chain as these machine learning models can be
used in critical applications. In this paper, we empirically expose this
problem and present approaches to minimize its impacts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Combinatorial Optimization via Heat Diffusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08757v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08757v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hengyuan Ma, Wenlian Lu, Jianfeng Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Combinatorial optimization problems are widespread but inherently challenging
due to their discrete nature.The primary limitation of existing methods is that
they can only access a small fraction of the solution space at each iteration,
resulting in limited efficiency for searching the global optimal. To overcome
this challenge, diverging from conventional efforts of expanding the solver's
search scope, we focus on enabling information to actively propagate to the
solver through heat diffusion. By transforming the target function while
preserving its optima, heat diffusion facilitates information flow from distant
regions to the solver, providing more efficient navigation. Utilizing heat
diffusion, we propose a framework for solving general combinatorial
optimization problems. The proposed methodology demonstrates superior
performance across a range of the most challenging and widely encountered
combinatorial optimizations. Echoing recent advancements in harnessing
thermodynamics for generative artificial intelligence, our study further
reveals its significant potential in advancing combinatorial optimization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code is available in https://github.com/AwakerMhy/HeO</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ZeroFlow: Scalable Scene Flow via Distillation <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.10424v8">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.10424v8.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kyle Vedder, Neehar Peri, Nathaniel Chodosh, Ishan Khatri, Eric Eaton, Dinesh Jayaraman, Yang Liu, Deva Ramanan, James Hays
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scene flow estimation is the task of describing the 3D motion field between
temporally successive point clouds. State-of-the-art methods use strong priors
and test-time optimization techniques, but require on the order of tens of
seconds to process full-size point clouds, making them unusable as computer
vision primitives for real-time applications such as open world object
detection. Feedforward methods are considerably faster, running on the order of
tens to hundreds of milliseconds for full-size point clouds, but require
expensive human supervision. To address both limitations, we propose Scene Flow
via Distillation, a simple, scalable distillation framework that uses a
label-free optimization method to produce pseudo-labels to supervise a
feedforward model. Our instantiation of this framework, ZeroFlow, achieves
state-of-the-art performance on the Argoverse 2 Self-Supervised Scene Flow
Challenge while using zero human labels by simply training on large-scale,
diverse unlabeled data. At test-time, ZeroFlow is over 1000x faster than
label-free state-of-the-art optimization-based methods on full-size point
clouds (34 FPS vs 0.028 FPS) and over 1000x cheaper to train on unlabeled data
compared to the cost of human annotation (\$394 vs ~\$750,000). To facilitate
further research, we release our code, trained model weights, and high quality
pseudo-labels for the Argoverse 2 and Waymo Open datasets at
https://vedder.io/zeroflow.html
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICLR 2024. 9 pages, 4 pages of citations, 6 pages of
  Supplemental. Project page with data releases is at
  http://vedder.io/zeroflow.html</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VBART: The Turkish LLM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.01308v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.01308v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Meliksah Turker, Mehmet Erdi Ari, Aydin Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present VBART, the first Turkish sequence-to-sequence Large Language
Models (LLMs) pre-trained on a large corpus from scratch. VBART are compact
LLMs based on good ideas leveraged from BART and mBART models and come in two
sizes, Large and XLarge. Fine-tuned VBART models surpass the prior
state-of-the-art results in abstractive text summarization, title generation,
text paraphrasing, question answering and question generation tasks. They allow
fine-tuning for future text generation tasks and datasets, carving a new path
for Turkish Natural Language Processing (NLP) research. Our work shows that
having a pre-trained LLM for Turkish outperforms up to 3x multilingual models,
improving existing results and providing efficient models for training and
inference. Moreover, we show that our monolingual tokenizer is up to 11x more
efficient than multilingual tokenizers. Last but not least, we introduce a
method to enlarge an existing pre-trained LLM and question the relevancy of
Chinchilla Scaling Law to sequence-to-sequence masked language models. Our
fine-tuned models, tokenizer and cleaned vngrs-web-corpus of 135 GB are
publicly available at huggingface.co/vngrs-ai.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Probabilistic Contrastive Learning for Long-Tailed Visual Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06726v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06726v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chaoqun Du, Yulin Wang, Shiji Song, Gao Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Long-tailed distributions frequently emerge in real-world data, where a large
number of minority categories contain a limited number of samples. Such
imbalance issue considerably impairs the performance of standard supervised
learning algorithms, which are mainly designed for balanced training sets.
Recent investigations have revealed that supervised contrastive learning
exhibits promising potential in alleviating the data imbalance. However, the
performance of supervised contrastive learning is plagued by an inherent
challenge: it necessitates sufficiently large batches of training data to
construct contrastive pairs that cover all categories, yet this requirement is
difficult to meet in the context of class-imbalanced data. To overcome this
obstacle, we propose a novel probabilistic contrastive (ProCo) learning
algorithm that estimates the data distribution of the samples from each class
in the feature space, and samples contrastive pairs accordingly. In fact,
estimating the distributions of all classes using features in a small batch,
particularly for imbalanced data, is not feasible. Our key idea is to introduce
a reasonable and simple assumption that the normalized features in contrastive
learning follow a mixture of von Mises-Fisher (vMF) distributions on unit
space, which brings two-fold benefits. First, the distribution parameters can
be estimated using only the first sample moment, which can be efficiently
computed in an online manner across different batches. Second, based on the
estimated distribution, the vMF distribution allows us to sample an infinite
number of contrastive pairs and derive a closed form of the expected
contrastive loss for efficient optimization. Our code is available at
https://github.com/LeapLabTHU/ProCo.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE Transactions on Pattern Analysis and Machine
  Intelligence (T-PAMI)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Geometric structure of Deep Learning networks and construction of global
  ${\mathcal L}^2$ minimizers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.10639v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.10639v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Chen, Patricia Muñoz Ewald
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we explicitly determine local and global minimizers of the
$\mathcal{L}^2$ cost function in underparametrized Deep Learning (DL) networks;
our main goal is to shed light on their geometric structure and properties. We
accomplish this by a direct construction, without invoking the gradient descent
flow at any point of this work. We specifically consider $L$ hidden layers, a
ReLU ramp activation function, an $\mathcal{L}^2$ Schatten class (or
Hilbert-Schmidt) cost function, input and output spaces $\mathbb{R}^Q$ with
equal dimension $Q\geq1$, and hidden layers also defined on $\mathbb{R}^{Q}$;
the training inputs are assumed to be sufficiently clustered. The training
input size $N$ can be arbitrarily large - thus, we are considering the
underparametrized regime. More general settings are left to future work. We
construct an explicit family of minimizers for the global minimum of the cost
function in the case $L\geq Q$, which we show to be degenerate. Moreover, we
determine a set of $2^Q-1$ distinct degenerate local minima of the cost
function. In the context presented here, the concatenation of hidden layers of
the DL network is reinterpreted as a recursive application of a {\em truncation
map} which "curates" the training inputs by minimizing their noise to signal
ratio.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AMS Latex, 22 pages. Typos corrected, slightly extended</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Expressive Losses for Verified Robustness via Convex Combinations <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.13991v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.13991v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alessandro De Palma, Rudy Bunel, Krishnamurthy Dvijotham, M. Pawan Kumar, Robert Stanforth, Alessio Lomuscio
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In order to train networks for verified adversarial robustness, it is common
to over-approximate the worst-case loss over perturbation regions, resulting in
networks that attain verifiability at the expense of standard performance. As
shown in recent work, better trade-offs between accuracy and robustness can be
obtained by carefully coupling adversarial training with over-approximations.
We hypothesize that the expressivity of a loss function, which we formalize as
the ability to span a range of trade-offs between lower and upper bounds to the
worst-case loss through a single parameter (the over-approximation
coefficient), is key to attaining state-of-the-art performance. To support our
hypothesis, we show that trivial expressive losses, obtained via convex
combinations between adversarial attacks and IBP bounds, yield state-of-the-art
results across a variety of settings in spite of their conceptual simplicity.
We provide a detailed analysis of the relationship between the
over-approximation coefficient and performance profiles across different
expressive losses, showing that, while expressivity is essential, better
approximations of the worst-case loss are not necessarily linked to superior
robustness-accuracy trade-offs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Zero-shot and Few-shot Generation Strategies for Artificial Clinical
  Records 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08664v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08664v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Erlend Frayling, Jake Lever, Graham McDonald
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The challenge of accessing historical patient data for clinical research,
while adhering to privacy regulations, is a significant obstacle in medical
science. An innovative approach to circumvent this issue involves utilising
synthetic medical records that mirror real patient data without compromising
individual privacy. The creation of these synthetic datasets, particularly
without using actual patient data to train Large Language Models (LLMs),
presents a novel solution as gaining access to sensitive patient information to
train models is also a challenge. This study assesses the capability of the
Llama 2 LLM to create synthetic medical records that accurately reflect real
patient information, employing zero-shot and few-shot prompting strategies for
comparison against fine-tuned methodologies that do require sensitive patient
data during training. We focus on generating synthetic narratives for the
History of Present Illness section, utilising data from the MIMIC-IV dataset
for comparison. In this work introduce a novel prompting technique that
leverages a chain-of-thought approach, enhancing the model's ability to
generate more accurate and contextually relevant medical narratives without
prior fine-tuning. Our findings suggest that this chain-of-thought prompted
approach allows the zero-shot model to achieve results on par with those of
fine-tuned models, based on Rouge metrics evaluation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Model-free Reinforcement Learning of Semantic Communication by
  Stochastic Policy Gradient <span class="chip">ICML</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.03571v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.03571v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Edgar Beck, Carsten Bockelmann, Armin Dekorsy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Following the recent success of Machine Learning tools in wireless
communications, the idea of semantic communication by Weaver from 1949 has
gained attention. It breaks with Shannon's classic design paradigm by aiming to
transmit the meaning, i.e., semantics, of a message instead of its exact
version, allowing for information rate savings. In this work, we apply the
Stochastic Policy Gradient (SPG) to design a semantic communication system by
reinforcement learning, separating transmitter and receiver, and not requiring
a known or differentiable channel model -- a crucial step towards deployment in
practice. Further, we derive the use of SPG for both classic and semantic
communication from the maximization of the mutual information between received
and target variables. Numerical results show that our approach achieves
comparable performance to a model-aware approach based on the reparametrization
trick, albeit with a decreased convergence rate.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in IEEE International Conference on Machine
  Learning for Communication and Networking (ICMLCN 2024), Source Code:
  https://github.com/ant-uni-bremen/SINFONY</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Stable Nonconvex-Nonconcave Training via Linear Interpolation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.13459v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.13459v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Pethick, Wanyun Xie, Volkan Cevher
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a theoretical analysis of linear interpolation as a
principled method for stabilizing (large-scale) neural network training. We
argue that instabilities in the optimization process are often caused by the
nonmonotonicity of the loss landscape and show how linear interpolation can
help by leveraging the theory of nonexpansive operators. We construct a new
optimization scheme called relaxed approximate proximal point (RAPP), which is
the first explicit method without anchoring to achieve last iterate convergence
rates for $\rho$-comonotone problems while only requiring $\rho >
-\tfrac{1}{2L}$. The construction extends to constrained and regularized
settings. By replacing the inner optimizer in RAPP we rediscover the family of
Lookahead algorithms for which we establish convergence in cohypomonotone
problems even when the base optimizer is taken to be gradient descent ascent.
The range of cohypomonotone problems in which Lookahead converges is further
expanded by exploiting that Lookahead inherits the properties of the base
optimizer. We corroborate the results with experiments on generative
adversarial networks which demonstrates the benefits of the linear
interpolation present in both RAPP and Lookahead.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FedImpro: Measuring and Improving Client Update in Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.07011v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.07011v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenheng Tang, Yonggang Zhang, Shaohuai Shi, Xinmei Tian, Tongliang Liu, Bo Han, Xiaowen Chu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) models often experience client drift caused by
heterogeneous data, where the distribution of data differs across clients. To
address this issue, advanced research primarily focuses on manipulating the
existing gradients to achieve more consistent client models. In this paper, we
present an alternative perspective on client drift and aim to mitigate it by
generating improved local models. First, we analyze the generalization
contribution of local training and conclude that this generalization
contribution is bounded by the conditional Wasserstein distance between the
data distribution of different clients. Then, we propose FedImpro, to construct
similar conditional distributions for local training. Specifically, FedImpro
decouples the model into high-level and low-level components, and trains the
high-level portion on reconstructed feature distributions. This approach
enhances the generalization contribution and reduces the dissimilarity of
gradients in FL. Experimental results show that FedImpro can help FL defend
against data heterogeneity and enhance the generalization performance of the
model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Most discriminative stimuli for functional cell type clustering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.05342v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.05342v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Max F. Burg, Thomas Zenkel, Michaela Vystrčilová, Jonathan Oesterle, Larissa Höfling, Konstantin F. Willeke, Jan Lause, Sarah Müller, Paul G. Fahey, Zhiwei Ding, Kelli Restivo, Shashwat Sridhar, Tim Gollisch, Philipp Berens, Andreas S. Tolias, Thomas Euler, Matthias Bethge, Alexander S. Ecker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Identifying cell types and understanding their functional properties is
crucial for unraveling the mechanisms underlying perception and cognition. In
the retina, functional types can be identified by carefully selected stimuli,
but this requires expert domain knowledge and biases the procedure towards
previously known cell types. In the visual cortex, it is still unknown what
functional types exist and how to identify them. Thus, for unbiased
identification of the functional cell types in retina and visual cortex, new
approaches are needed. Here we propose an optimization-based clustering
approach using deep predictive models to obtain functional clusters of neurons
using Most Discriminative Stimuli (MDS). Our approach alternates between
stimulus optimization with cluster reassignment akin to an
expectation-maximization algorithm. The algorithm recovers functional clusters
in mouse retina, marmoset retina and macaque visual area V4. This demonstrates
that our approach can successfully find discriminative stimuli across species,
stages of the visual system and recording techniques. The resulting most
discriminative stimuli can be used to assign functional cell types fast and on
the fly, without the need to train complex predictive models or show a large
natural scene dataset, paving the way for experiments that were previously
limited by experimental time. Crucially, MDS are interpretable: they visualize
the distinctive stimulus patterns that most unambiguously identify a specific
type of neuron.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Comprehensive <span class="highlight-title">Dataset</span> and Automated Pipeline for Nailfold Capillary
  Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.05930v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.05930v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linxi Zhao, Jiankai Tang, Dongyu Chen, Xiaohong Liu, Yong Zhou, Yuanchun Shi, Guangyu Wang, Yuntao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nailfold capillaroscopy is widely used in assessing health conditions,
highlighting the pressing need for an automated nailfold capillary analysis
system. In this study, we present a pioneering effort in constructing a
comprehensive nailfold capillary dataset-321 images, 219 videos from 68
subjects, with clinic reports and expert annotations-that serves as a crucial
resource for training deep-learning models. Leveraging this dataset, we
finetuned three deep learning models with expert annotations as supervised
labels and integrated them into a novel end-to-end nailfold capillary analysis
pipeline. This pipeline excels in automatically detecting and measuring a wide
range of size factors, morphological features, and dynamic aspects of nailfold
capillaries. We compared our outcomes with clinical reports. Experiment results
showed that our automated pipeline achieves an average of sub-pixel level
precision in measurements and 89.9% accuracy in identifying morphological
abnormalities. These results underscore its potential for advancing
quantitative medical research and enabling pervasive computing in healthcare.
Our data and code are available at
https://github.com/THU-CS-PI-LAB/ANFC-Automated-Nailfold-Capillary.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Dataset, code, pretrained models:
  https://github.com/THU-CS-PI-LAB/ANFC-Automated-Nailfold-Capillary</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ K-pop Lyric Translation: <span class="highlight-title">Dataset</span>, Analysis, and Neural-Modelling <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.11093v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.11093v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haven Kim, Jongmin Jung, Dasaem Jeong, Juhan Nam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lyric translation, a field studied for over a century, is now attracting
computational linguistics researchers. We identified two limitations in
previous studies. Firstly, lyric translation studies have predominantly focused
on Western genres and languages, with no previous study centering on K-pop
despite its popularity. Second, the field of lyric translation suffers from a
lack of publicly available datasets; to the best of our knowledge, no such
dataset exists. To broaden the scope of genres and languages in lyric
translation studies, we introduce a novel singable lyric translation dataset,
approximately 89\% of which consists of K-pop song lyrics. This dataset aligns
Korean and English lyrics line-by-line and section-by-section. We leveraged
this dataset to unveil unique characteristics of K-pop lyric translation,
distinguishing it from other extensively studied genres, and to construct a
neural lyric translation model, thereby underscoring the importance of a
dedicated dataset for singable lyric translations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Plug and Play Active Learning for Object Detection <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.11612v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.11612v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenhongyi Yang, Lichao Huang, Elliot J. Crowley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Annotating datasets for object detection is an expensive and time-consuming
endeavor. To minimize this burden, active learning (AL) techniques are employed
to select the most informative samples for annotation within a constrained
"annotation budget". Traditional AL strategies typically rely on model
uncertainty or sample diversity for query sampling, while more advanced methods
have focused on developing AL-specific object detector architectures to enhance
performance. However, these specialized approaches are not readily adaptable to
different object detectors due to the significant engineering effort required
for integration. To overcome this challenge, we introduce Plug and Play Active
Learning (PPAL), a simple and effective AL strategy for object detection. PPAL
is a two-stage method comprising uncertainty-based and diversity-based sampling
phases. In the first stage, our Difficulty Calibrated Uncertainty Sampling
leverage a category-wise difficulty coefficient that combines both
classification and localisation difficulties to re-weight instance
uncertainties, from which we sample a candidate pool for the subsequent
diversity-based sampling. In the second stage, we propose Category Conditioned
Matching Similarity to better compute the similarities of multi-instance images
as ensembles of their instance similarities, which is used by the k-Means++
algorithm to sample the final AL queries. PPAL makes no change to model
architectures or detector training pipelines; hence it can be easily
generalized to different object detectors. We benchmark PPAL on the MS-COCO and
Pascal VOC datasets using different detector architectures and show that our
method outperforms prior work by a large margin. Code is available at
https://github.com/ChenhongyiYang/PPAL
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Typology for Exploring the Mitigation of Shortcut Behavior 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.03668v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.03668v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Felix Friedrich, Wolfgang Stammer, Patrick Schramowski, Kristian Kersting
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As machine learning models become increasingly larger, trained weakly
supervised on large, possibly uncurated data sets, it becomes increasingly
important to establish mechanisms for inspecting, interacting, and revising
models to mitigate learning shortcuts and guarantee their learned knowledge is
aligned with human knowledge. The recently proposed XIL framework was developed
for this purpose, and several such methods have been introduced, each with
individual motivations and methodological details. In this work, we provide a
unification of various XIL methods into a single typology by establishing a
common set of basic modules. In doing so, we pave the way for a principled
comparison of existing, but, importantly, also future XIL approaches. In
addition, we discuss existing and introduce novel measures and benchmarks for
evaluating the overall abilities of a XIL method. Given this extensive toolbox,
including our typology, measures, and benchmarks, we finally compare several
recent XIL methods methodologically and quantitatively. In our evaluations, all
methods prove to revise a model successfully. However, we found remarkable
differences in individual benchmark tasks, revealing valuable
application-relevant aspects for integrating these benchmarks in developing
future methods.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Science and Game Theory
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sequential Contracts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09545v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09545v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tomer Ezra, Michal Feldman, Maya Schlesinger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the principal-agent setting, where a principal delegates the
execution of a costly project to an agent. In the classical model, the agent
chooses an action among a set of available actions. Every action is associated
with some cost, and leads to a stochastic outcome for the project. The agent's
action is hidden from the principal, who only observes the outcome. The
principal incentivizes the agent through a payment scheme (a contract) that
maps outcomes to payments, with the objective of finding the optimal contract -
the contract maximizing the principal's expected utility.
  In this work, we introduce a sequential variant of the model, capturing many
real-life settings, where the agent engages in multiple attempts, incurring the
sum of costs of the actions taken and being compensated for the best realized
outcome. We study the contract design problem in this new setting. We first
observe that the agent's problem - finding the sequential set of actions that
maximizes his utility for a given contract - is equivalent to the well-known
Pandora's Box problem. With this insight at hand, we provide algorithms and
hardness results for the (principal's) contract design problem, under both
independent and correlated actions. For independent actions, we show that the
optimal linear contract can be computed in polynomial time. Furthermore, this
result extends to the optimal arbitrary contract when the number of outcomes is
a constant. For correlated actions we find that approximating the optimal
contract within any constant ratio is NP-hard.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Trust AI Regulation? Discerning users are vital to build trust and
  effective AI regulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09510v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09510v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zainab Alalawi, Paolo Bova, Theodor Cimpeanu, Alessandro Di Stefano, Manh Hong Duong, Elias Fernandez Domingos, The Anh Han, Marcus Krellner, Bianca Ogbo, Simon T. Powers, Filippo Zimmaro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There is general agreement that some form of regulation is necessary both for
AI creators to be incentivised to develop trustworthy systems, and for users to
actually trust those systems. But there is much debate about what form these
regulations should take and how they should be implemented. Most work in this
area has been qualitative, and has not been able to make formal predictions.
Here, we propose that evolutionary game theory can be used to quantitatively
model the dilemmas faced by users, AI creators, and regulators, and provide
insights into the possible effects of different regulatory regimes. We show
that creating trustworthy AI and user trust requires regulators to be
incentivised to regulate effectively. We demonstrate the effectiveness of two
mechanisms that can achieve this. The first is where governments can recognise
and reward regulators that do a good job. In that case, if the AI system is not
too risky for users then some level of trustworthy development and user trust
evolves. We then consider an alternative solution, where users can condition
their trust decision on the effectiveness of the regulators. This leads to
effective regulation, and consequently the development of trustworthy AI and
user trust, provided that the cost of implementing regulations is not too high.
Our findings highlight the importance of considering the effect of different
regulatory regimes from an evolutionary game theoretic perspective.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ All-pay Auction Based Profit Maximization in End-to-End Computation
  Offloading System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09129v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09129v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hai Xue, Yun Xia, Di Zhang, Honghua Wei, Xiaolong Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pricing is an important issue in mobile edge computing. How to appropriately
determine the bid of end user (EU) is an incentive factor for edge cloud (EC)
to offer service. In this letter, we propose an equilibrium pricing scheme
based on the all-pay auction model in end-to-end collaboration environment,
wherein all EUs can acquire the service at a lower price than the own value of
the required resource. In addition, we propose a set allocation algorithm to
divide all the bidders into different sets according to the price, and the EUs
in each set get the service, which averts the case of getting no service due to
the low price. Extensive simulation results demonstrate that the proposed
scheme can effectively maximize the total profit of the edge offloading system,
and guarantee all EUs can access the service.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Data Monetization Pathways and Complex Dynamic Game Equilibrium Analysis
  in the Energy Industry 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08082v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08082v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zongxian Wang, Jie Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the most critical production factor in the era of the digital economy,
data will have a significant impact on social production and development.
Energy enterprises possess data that is interconnected with multiple
industries, characterized by diverse needs, sensitivity, and long-term nature.
The path to monetizing energy enterprises' data is challenging yet crucial.
This paper explores the game-theoretic aspects of the data monetization process
in energy enterprises by considering the relationships between enterprises and
trading platforms. We construct a class of game decision models and study their
equilibrium strategies. Our analysis shows that enterprises and platforms can
adjust respective benefits by regulating the wholesale price of data and the
intensity of data value mining to form a benign equilibrium state. Furthermore,
by integrating nonlinear dynamical theory, we discuss the dynamic
characteristics present in multi-period repeated game processes. We find that
decision-makers should keep the adjustment parameters and initial states within
reasonable ranges in multi-period dynamic decision-making to avoid market
failure. Finally, based on the theoretical and numerical analysis, we provide
decision insights and recommendations for enterprise decision-making to
facilitate data monetization through strategic interactions with trading
platforms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Prosumers Participation in Markets: A Scalar-Parameterized Function
  Bidding Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.15423v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.15423v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdullah Alawad, Muhammad Aneeq uz Zaman, Khaled Alshehri, Tamer Başar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In uniform-price markets, suppliers compete to supply a resource to
consumers, resulting in a single market price determined by their competition.
For sufficient flexibility, producers and consumers prefer to commit to a
function as their strategies, indicating their preferred quantity at any given
market price. Producers and consumers may wish to act as both, i.e., prosumers.
In this paper, we examine the behavior of profit-maximizing prosumers in a
uniform-price market for resource allocation with the objective of maximizing
the social welfare. We propose a scalar-parameterized function bidding
mechanism for the prosumers, in which we establish the existence and uniqueness
of Nash equilibrium. Furthermore, we provide an efficient way to compute the
Nash equilibrium through the computation of the market allocation at the Nash
equilibrium. Finally, we present a case study to illustrate the welfare loss
under different variations of market parameters, such as the market's supply
capacity and inelastic demand.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Corrected typos in the figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Approximating Nash Equilibria in Normal-Form Games via Stochastic
  Optimization <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06689v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06689v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ian Gemp, Luke Marris, Georgios Piliouras
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose the first loss function for approximate Nash equilibria of
normal-form games that is amenable to unbiased Monte Carlo estimation. This
construction allows us to deploy standard non-convex stochastic optimization
techniques for approximating Nash equilibria, resulting in novel algorithms
with provable guarantees. We complement our theoretical analysis with
experiments demonstrating that stochastic gradient descent can outperform
previous state-of-the-art approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Diversity-seeking Jump Games in Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.17757v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.17757v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lata Narayanan, Yasaman Sabbagh, Alexandros A. Voudouris
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, strategic games inspired by Schelling's influential model of
residential segregation have been studied in the TCS and AI literature. In
these games, agents of k different types occupy the nodes of a network topology
aiming to maximize their utility, which is a function of the fraction of
same-type agents they are adjacent to in the network. As such, the agents
exhibit similarity seeking strategic behavior. In this paper, we introduce a
class of strategic jump games in which the agents are diversity-seeking: The
utility of an agent is defined as the fraction of its neighbors that are of
different type than itself. We show that in general it is computationally hard
to determine the existence of an equilibrium in such games. However, when the
network is a tree, diversity-seeking jump games always admit an equilibrium
assignment. For regular graphs and spider graphs with a single empty node, we
prove a stronger result: The game is potential, that is, the improving response
dynamics always converge to an equilibrium from any initial placement of the
agents. We also show (nearly tight) bounds on the price of anarchy and price of
stability in terms of the social welfare (the total utility of the agents).
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Theory
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Localization in Digital Twin MIMO Networks: A Case for Massive
  Fingerprinting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09614v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09614v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        João Morais, Ahmed Alkhateeb
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Localization in outdoor wireless systems typically requires transmitting
specific reference signals to estimate distance (trilateration methods) or
angle (triangulation methods). These cause overhead on communication, need a
LoS link to work well, and require multiple base stations, often imposing
synchronization or specific hardware requirements. Fingerprinting has none of
these drawbacks, but building its database requires high human effort to
collect real-world measurements. For a long time, this issue limited the size
of databases and thus their performance. This work proposes significantly
reducing human effort in building fingerprinting databases by populating them
with \textit{digital twin RF maps}. These RF maps are built from ray-tracing
simulations on a digital replica of the environment across several frequency
bands and beamforming configurations. Online user fingerprints are then matched
against this spatial database. The approach was evaluated with practical
simulations using realistic propagation models and user measurements. Our
experiments show sub-meter localization errors on a NLoS location 95\% of the
time using sensible user measurement report sizes. Results highlight the
promising potential of the proposed digital twin approach for ubiquitous
wide-area 6G localization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in ICC 2024. The Dataset and code will be available soon on
  the DeepMIMO and DeepVerse websites https://www.deepmimo.net/
  https://www.deepverse6g.net/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Fidelity Bayesian Optimization With Across-Task Transferable
  Max-Value Entropy Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09570v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09570v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunchuan Zhang, Sangwoo Park, Osvaldo Simeone
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In many applications, ranging from logistics to engineering, a designer is
faced with a sequence of optimization tasks for which the objectives are in the
form of black-box functions that are costly to evaluate. For example, the
designer may need to tune the hyperparameters of neural network models for
different learning tasks over time. Rather than evaluating the objective
function for each candidate solution, the designer may have access to
approximations of the objective functions, for which higher-fidelity
evaluations entail a larger cost. Existing multi-fidelity black-box
optimization strategies select candidate solutions and fidelity levels with the
goal of maximizing the information accrued about the optimal value or solution
for the current task. Assuming that successive optimization tasks are related,
this paper introduces a novel information-theoretic acquisition function that
balances the need to acquire information about the current task with the goal
of collecting information transferable to future tasks. The proposed method
includes shared inter-task latent variables, which are transferred across tasks
by implementing particle-based variational Bayesian updates. Experimental
results across synthetic and real-world examples reveal that the proposed
provident acquisition strategy that caters to future tasks can significantly
improve the optimization efficiency as soon as a sufficient number of tasks is
processed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to IEEE for review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Near-Field Channel Modeling for Holographic MIMO Communications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09411v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09411v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tierui Gong, Li Wei, Chongwen Huang, George C. Alexandropoulos, Mérouane Debbah, Chau Yuen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Empowered by the latest progress on innovative metamaterials/metasurfaces and
advanced antenna technologies, holographic multiple-input multiple-output
(H-MIMO) emerges as a promising technology to fulfill the extreme goals of the
sixth-generation (6G) wireless networks. The antenna arrays utilized in H-MIMO
comprise massive (possibly to extreme extent) numbers of antenna elements,
densely spaced less than half-a-wavelength and integrated into a compact space,
realizing an almost continuous aperture. Thanks to the expected low cost, size,
weight, and power consumption, such apertures are expected to be largely
fabricated for near-field communications. In addition, the physical features of
H-MIMO enable manipulations directly on the electromagnetic (EM) wave domain
and spatial multiplexing. To fully leverage this potential, near-field H-MIMO
channel modeling, especially from the EM perspective, is of paramount
significance. In this article, we overview near-field H-MIMO channel models
elaborating on the various modeling categories and respective features, as well
as their challenges and evaluation criteria. We also present EM-domain channel
models that address the inherit computational and measurement complexities.
Finally, the article is concluded with a set of future research directions on
the topic.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>double column, 9 pages, 3 figures, 2 tables, accepted by IEEE
  Wireless Communications Magazine</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Joint Port Selection and Beamforming Design for Fluid Antenna Assisted
  Integrated Data and Energy Transfer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09357v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09357v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Long Zhang, Halvin Yang, Yizhe Zhao, Jie Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Integrated data and energy transfer (IDET) has been of fundamental importance
for providing both wireless data transfer (WDT) and wireless energy transfer
(WET) services towards low-power devices. Fluid antenna (FA) is capable of
exploiting the huge spatial diversity of the wireless channel to enhance the
receive signal strength, which is more suitable for the tiny-size low-power
devices having the IDET requirements. In this letter, a multiuser FA assisted
IDET system is studied and the weighted energy harvesting power at energy
receivers (ERs) is maximized by jointly optimizing the port selection and
transmit beamforming design under imperfect channel state information (CSI),
while the signal-to-interference-plus-noise ratio (SINR) constraint for each
data receiver (DR) is satisfied. An efficient algorithm is proposed to obtain
the suboptimal solutions for the non-convex problem. Simulation results
evaluate the performance of the FA-IDET system, while also demonstrate that FA
outperforms the multi-input-multi-output (MIMO) counterpart in terms of the
IDET performance, as long as the port number is large enough.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Static Grouping Strategy Design for Beyond Diagonal Reconfigurable
  Intelligent Surfaces 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09275v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09275v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matteo Nerini, Shanpu Shen, Bruno Clerckx
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Beyond diagonal reconfigurable intelligent surface (BD-RIS) extends
conventional RIS through novel architectures, such as group-connected RIS, with
scattering matrix not restricted to being diagonal. However, it remains
unexplored how to optimally group the elements in group-connected RISs to
maximize the performance while maintaining a low-complexity circuit. In this
study, we propose and model BD-RIS with a static grouping strategy optimized
based on the channel statistics. After formulating the corresponding problems,
we design the grouping in single- and multi-user systems. Numerical results
reveal the benefits of grouping optimization, i.e., up to 60% sum rate
improvement, especially in highly correlated channels.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE for publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Deep Reinforcement Learning Approach for Autonomous Reconfigurable
  Intelligent Surfaces 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09270v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09270v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyuckjin Choi, Ly V. Nguyen, Junil Choi, A. Lee Swindlehurst
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A reconfigurable intelligent surface (RIS) is a prospective wireless
technology that enhances wireless channel quality. An RIS is often equipped
with passive array of elements and provides cost and power-efficient solutions
for coverage extension of wireless communication systems. Without any radio
frequency (RF) chains or computing resources, however, the RIS requires control
information to be sent to it from an external unit, e.g., a base station (BS).
The control information can be delivered by wired or wireless channels, and the
BS must be aware of the RIS and the RIS-related channel conditions in order to
effectively configure its behavior. Recent works have introduced hybrid RIS
structures possessing a few active elements that can sense and digitally
process received data. Here, we propose the operation of an entirely autonomous
RIS that operates without a control link between the RIS and BS. Using a few
sensing elements, the autonomous RIS employs a deep Q network (DQN) based on
reinforcement learning in order to enhance the sum rate of the network. Our
results illustrate the potential of deploying autonomous RISs in wireless
networks with essentially no network overhead.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Assembly Theory is an approximation to algorithmic complexity based on
  LZ compression that does not explain selection or evolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06629v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06629v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Felipe S. Abrahão, Santiago Hernández-Orozco, Narsis A. Kiani, Jesper Tegnér, Hector Zenil
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We demonstrate that Assembly Theory, pathway complexity, the assembly index,
and the assembly number are subsumed and constitute a weak version of
algorithmic (Kolmogorov-Solomonoff-Chaitin) complexity reliant on an
approximation method based upon statistical compression, their results obtained
due to the use of methods strictly equivalent to the LZ family of compression
algorithms used in compressing algorithms such as ZIP, GZIP, or JPEG. Such
popular algorithms have been shown to empirically reproduce the results of AT
that were reported before in successful application to separating organic from
non-organic molecules and in the context of the study of selection and
evolution. We prove the connections and full equivalence of Assembly Theory to
Shannon Entropy and statistical compression, and AT's disconnection as a
statistical approach from causality. We demonstrate that formulating a
traditional statistically compressed description of molecules, or the theory
underlying it, does not imply an explanation or quantification of biases in
generative (physical or biological) processes, including those brought about by
selection and evolution, when lacking in logical consistency and empirical
evidence. We argue that in their basic arguments, the authors of AT conflate
how objects may assemble with causal directionality, and conclude that Assembly
Theory does not explain selection or evolution beyond known and previously
established connections, some of which are reviewed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages + appendix, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Low-Complexity Beam Training for Multi-RIS-Assisted Multi-User
  Communications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08339v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08339v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuan Xu, Chongwen Huang, Wei Li, Zhaohui Yang, Xiaoming Chen, Zhaoyang Zhang, Chau Yuen, Mérouane Debbah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we investigate the beam training problem in the multi-user
millimeter wave (mmWave) communication system, where multiple reconfigurable
intelligent surfaces (RISs) are deployed to improve the coverage and the
achievable rate. However, existing beam training techniques in mmWave systems
suffer from the high complexity (i.e., exponential order) and low
identification accuracy. To address these problems, we propose a novel hashing
multi-arm beam (HMB) training scheme that reduces the training complexity to
the logarithmic order with the high accuracy. Specifically, we first design a
generation mechanism for HMB codebooks. Then, we propose a demultiplexing
algorithm based on the soft decision to distinguish signals from different RIS
reflective links. Finally, we utilize a multi-round voting mechanism to align
the beams. Simulation results show that the proposed HMB training scheme
enables simultaneous training for multiple RISs and multiple users, and reduces
the beam training overhead to the logarithmic level. Moreover, it also shows
that our proposed scheme can significantly improve the identification accuracy
by at least 20% compared to existing beam training techniques.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Model-free Reinforcement Learning of Semantic Communication by
  Stochastic Policy Gradient <span class="chip">ICML</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.03571v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.03571v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Edgar Beck, Carsten Bockelmann, Armin Dekorsy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Following the recent success of Machine Learning tools in wireless
communications, the idea of semantic communication by Weaver from 1949 has
gained attention. It breaks with Shannon's classic design paradigm by aiming to
transmit the meaning, i.e., semantics, of a message instead of its exact
version, allowing for information rate savings. In this work, we apply the
Stochastic Policy Gradient (SPG) to design a semantic communication system by
reinforcement learning, separating transmitter and receiver, and not requiring
a known or differentiable channel model -- a crucial step towards deployment in
practice. Further, we derive the use of SPG for both classic and semantic
communication from the maximization of the mutual information between received
and target variables. Numerical results show that our approach achieves
comparable performance to a model-aware approach based on the reparametrization
trick, albeit with a decreased convergence rate.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in IEEE International Conference on Machine
  Learning for Communication and Networking (ICMLCN 2024), Source Code:
  https://github.com/ant-uni-bremen/SINFONY</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A lower bound on the space overhead of fault-tolerant quantum
  computation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.00119v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.00119v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Omar Fawzi, Alexander Müller-Hermes, Ala Shayeghi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The threshold theorem is a fundamental result in the theory of fault-tolerant
quantum computation stating that arbitrarily long quantum computations can be
performed with a polylogarithmic overhead provided the noise level is below a
constant level. A recent work by Fawzi, Grospellier and Leverrier (FOCS 2018)
building on a result by Gottesman (QIC 2013) has shown that the space overhead
can be asymptotically reduced to a constant independent of the circuit provided
we only consider circuits with a length bounded by a polynomial in the width.
In this work, using a minimal model for quantum fault tolerance, we establish a
general lower bound on the space overhead required to achieve fault tolerance.
  For any non-unitary qubit channel $\mathcal{N}$ and any quantum fault
tolerance schemes against $\mathrm{i.i.d.}$ noise modeled by $\mathcal{N}$, we
prove a lower bound of
$\max\left\{\mathrm{Q}(\mathcal{N})^{-1}n,\alpha_\mathcal{N} \log T\right\}$ on
the number of physical qubits, for circuits of length $T$ and width $n$. Here,
$\mathrm{Q}(\mathcal{N})$ denotes the quantum capacity of $\mathcal{N}$ and
$\alpha_\mathcal{N}>0$ is a constant only depending on the channel
$\mathcal{N}$. In our model, we allow for qubits to be replaced by fresh ones
during the execution of the circuit and we allow classical computation to be
free and perfect. This improves upon results that assumed classical
computations to be also affected by noise, and that sometimes did not allow for
fresh qubits to be added. Along the way, we prove an exponential upper bound on
the maximal length of fault-tolerant quantum computation with amplitude damping
noise resolving a conjecture by Ben-Or, Gottesman, and Hassidim (2013).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 2 figures, an earlier version of this paper appeared in
  proceedings of ITCS 2022. In the current version, Lemma 10 has been
  simplified and some bounds are improved</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Uncertainty-Aware and Reliable Neural MIMO Receivers via Modular
  Bayesian Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.02436v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.02436v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tomer Raviv, Sangwoo Park, Osvaldo Simeone, Nir Shlezinger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning is envisioned to play a key role in the design of future
wireless receivers. A popular approach to design learning-aided receivers
combines deep neural networks (DNNs) with traditional model-based receiver
algorithms, realizing hybrid model-based data-driven architectures. Such
architectures typically include multiple modules, each carrying out a different
functionality dictated by the model-based receiver workflow. Conventionally
trained DNN-based modules are known to produce poorly calibrated, typically
overconfident, decisions. Consequently, incorrect decisions may propagate
through the architecture without any indication of their insufficient accuracy.
To address this problem, we present a novel combination of Bayesian deep
learning with hybrid model-based data-driven architectures for wireless
receiver design. The proposed methodology, referred to as modular Bayesian deep
learning, is designed to yield calibrated modules, which in turn improves both
accuracy and calibration of the overall receiver. We specialize this approach
for two fundamental tasks in multiple-input multiple-output (MIMO) receivers -
equalization and decoding. In the presence of scarce data, the ability of
modular Bayesian deep learning to produce reliable uncertainty measures is
consistently shown to directly translate into improved performance of the
overall MIMO receiver chain.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fractal spatio-temporal scale-free messaging: amplitude modulation of
  self-executable carriers given by the Weierstrass function's components 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06633v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06633v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hector Zenil, Luan Carlos de Sena Monteiro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In many communication contexts, the capabilities of the involved actors
cannot be known beforehand, whether it is a cell, a plant, an insect, or even a
life form unknown to Earth. Regardless of the recipient, the message space and
time scale could be too fast, too slow, too large, or too small and may never
be decoded. Therefore, it pays to devise a way to encode messages agnostic of
space and time scales. We propose the use of fractal functions as
self-executable infinite-frequency carriers for sending messages, given their
properties of structural self-similarity and scale invariance. We call it
`fractal messaging'. Starting from a spatial embedding, we introduce a
framework for a space-time scale-free messaging approach to this challenge.
When considering a space and time-agnostic framework for message transmission,
it would be interesting to encode a message such that it could be decoded at
several spatio-temporal scales. Hence, the core idea of the framework proposed
herein is to encode a binary message as waves along infinitely many frequencies
(in power-like distributions) and amplitudes, transmit such a message, and then
decode and reproduce it. To do so, the components of the Weierstrass function,
a known fractal, are used as carriers of the message. Each component will have
its amplitude modulated to embed the binary stream, allowing for a
space-time-agnostic approach to messaging.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Databases
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Iterative Forgetting: Online Data Stream Regression Using
  Database-Inspired Adaptive Granulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09588v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09588v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Niket Kathiriya, Hossein Haeri, Cindy Chen, Kshitij Jerath
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many modern systems, such as financial, transportation, and
telecommunications systems, are time-sensitive in the sense that they demand
low-latency predictions for real-time decision-making. Such systems often have
to contend with continuous unbounded data streams as well as concept drift,
which are challenging requirements that traditional regression techniques are
unable to cater to. There exists a need to create novel data stream regression
methods that can handle these scenarios. We present a database-inspired
datastream regression model that (a) uses inspiration from R*-trees to create
granules from incoming datastreams such that relevant information is retained,
(b) iteratively forgets granules whose information is deemed to be outdated,
thus maintaining a list of only recent, relevant granules, and (c) uses the
recent data and granules to provide low-latency predictions. The
R*-tree-inspired approach also makes the algorithm amenable to integration with
database systems. Our experiments demonstrate that the ability of this method
to discard data produces a significant order-of-magnitude improvement in
latency and training time when evaluated against the most accurate
state-of-the-art algorithms, while the R*-tree-inspired granulation technique
provides competitively accurate predictions
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Query Rewriting via Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09060v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09060v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jie Liu, Barzan Mozafari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Query rewriting is one of the most effective techniques for coping with
poorly written queries before passing them down to the query optimizer. Manual
rewriting is not scalable, as it is error-prone and requires deep expertise.
Similarly, traditional query rewriting algorithms can only handle a small
subset of queries: rule-based techniques do not generalize to new query
patterns and synthesis-based techniques cannot handle complex queries.
Fortunately, the rise of Large Language Models (LLMs), equipped with broad
general knowledge and advanced reasoning capabilities, has created hopes for
solving some of these previously open problems.
  In this paper, we present GenRewrite, the first holistic system that
leverages LLMs for query rewriting. We introduce the notion of Natural Language
Rewrite Rules (NLR2s), and use them as hints to the LLM but also a means for
transferring knowledge from rewriting one query to another, and thus becoming
smarter and more effective over time. We present a novel counterexample-guided
technique that iteratively corrects the syntactic and semantic errors in the
rewritten query, significantly reducing the LLM costs and the manual effort
required for verification. GenRewrite speeds up 22 out of 99 TPC queries (the
most complex public benchmark) by more than 2x, which is 2.5x--3.2x higher
coverage than state-of-the-art traditional query rewriting and 2.1x higher than
the out-of-the-box LLM baseline.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving overlay maps of science: combining <span class="highlight-title">overview</span> and detail 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.07917v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.07917v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peter Sjögårde
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Overlay maps of science are global base maps over which subsets of
publications can be projected. Such maps can be used to monitor, explore, and
study research through its publication output. Most maps of science, including
overlay maps, are flat in the sense that they visualize research fields at one
single level. Such maps generally fail to provide both overview and detail
about the research being analyzed. The aim of this study is to improve overlay
maps of science to provide both features in a single visualization. I created a
map based on a hierarchical classification of publications, including broad
disciplines for overview and more granular levels to incorporate detailed
information. The classification was obtained by clustering articles in a
citation network of about 17 million publication records in PubMed from 1995
onwards. The map emphasizes the hierarchical structure of the classification by
visualizing both disciplines and the underlying specialties. To show how the
visualization methodology can help getting both overview of research and
detailed information about its topical structure, I projected two overlay maps
onto the base map: (1) open access publishing and (2) coronavirus/Covid-19
research.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-13T00:00:00Z">2024-03-13</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Distributed, Parallel, and Cluster Computing
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Plotinus: A Satellite Internet Digital Twin System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08515v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08515v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Gao, Kun Qiu, Zhe Chen, Wenjun Zhu, Qi Zhang, Handong Luo, Quanwei Lin, Ziheng Yang, Wenhao Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The development of integrated space-air-ground network (SAGIN) requires
sophisticated satellite Internet emulation tools that can handle complex,
dynamic topologies and offer in-depth analysis. Existing emulation platforms
struggle with challenges like the need for detailed implementation across all
network layers, real-time response times, and the ability to scale. Plotinus, a
new digital twin system based on microservices for satellite Internet
emulation, aims to solve these problems. It features a modular design, allowing
for easy replacement of the physical layer to emulate different aerial vehicles
and analyze channel interference. It also enables the replacement of path
computation methods to simplify testing and deploying algorithms. In
particular, Plotinus allows for real-time emulation with live network traffic,
enhancing the realism of network models. Evaluation result shows that
Plotinus's effective emulation of dynamic satellite networks with real-world
devices. Its adaptability for various communication models and algorithm
testing highlights Plotinus's role as a vital tool for developing and analyzing
SAGIN systems, offering a scalable, real-time response, and flexible digital
twin system.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ COSTREAM: Learned Cost Models for Operator Placement in Edge-Cloud
  Environments <span class="chip">ICDE 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08444v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08444v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Roman Heinrich, Carsten Binnig, Harald Kornmayer, Manisha Luthra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we present COSTREAM, a novel learned cost model for Distributed
Stream Processing Systems that provides accurate predictions of the execution
costs of a streaming query in an edge-cloud environment. The cost model can be
used to find an initial placement of operators across heterogeneous hardware,
which is particularly important in these environments. In our evaluation, we
demonstrate that COSTREAM can produce highly accurate cost estimates for the
initial operator placement and even generalize to unseen placements, queries,
and hardware. When using COSTREAM to optimize the placements of streaming
operators, a median speed-up of around 21x can be achieved compared to
baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted by IEEE ICDE 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Error-Free Near-Optimal Validated Agreement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08374v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08374v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pierre Civit, Muhammad Ayaz Dzulfikar, Seth Gilbert, Rachid Guerraoui, Jovan Komatovic, Manuel Vidigueira, Igor Zablotchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Byzantine agreement enables n processes to agree on a common L-bit value,
despite t > 0 arbitrary failures. A long line of work has been dedicated to
improving the worst-case bit complexity of Byzantine agreement in synchrony.
This has culminated in COOL, an error-free (deterministically secure against a
computationally unbounded adversary) algorithm that achieves a near-optimal bit
complexity of O(nL + n^2 log n). COOL satisfies strong validity: if all correct
processes propose the same value, only that value can be decided. Thus,
whenever correct processes do not a priori agree, COOL might decide on
"bottom", thus limiting its application in today's state machine replication
(SMR) and blockchain protocols. In this work, we focus on the aforementioned
limitation. Can we design an error-free near-optimal Byzantine agreement
algorithm applicable in today's SMR and blockchain protocols? Can we design an
error-free near-optimal agreement algorithm with external validity (a.k.a.
validated agreement) stipulating that only values valid according to a
predetermined predicate can be decided?
  This paper answers the question affirmatively. Namely, we present EXT, an
error-free synchronous Byzantine agreement algorithm that satisfies external
(along with strong) validity while exchanging O(n log n L + n^2 log n) bits in
the worst case. Importantly, EXT is optimally resilient (tolerates t < n / 3
failures) and terminates in optimal O(n) rounds. Perhaps surprisingly, we
construct EXT by exploiting existing concepts: (1) the recursive framework
proposed by Berman, Garay and Perry and Coan and Welch and recently restated by
Momose and Ren, (2) the aforementioned COOL algorithm introduced by Chen, and
(3) the data dissemination primitive introduced by Das, Xiang and Ren.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scattered Mixture-of-Experts Implementation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08245v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08245v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shawn Tan, Yikang Shen, Rameswar Panda, Aaron Courville
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present ScatterMoE, an implementation of Sparse Mixture-of-Experts (SMoE)
on GPUs. ScatterMoE builds upon existing implementations, and overcoming some
of the limitations to improve inference and training speed, and memory
footprint. This implementation achieves this by avoiding padding and making
excessive copies of the input. We introduce ParallelLinear, the main component
we use to build our implementation and the various kernels used to speed up the
operation. We benchmark our implementation against Megablocks, and show that it
enables a higher throughput and lower memory footprint. We also show how
ParallelLinear enables extension of the Mixture-of-Experts concept by
demonstrating with an implementation of Mixture of Attention.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Virtual Environment for Collaborative Inspection in Additive
  Manufacturing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08940v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08940v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vuthea Chheang, Brian Thomas Weston, Robert William Cerda, Brian Au, Brian Giera, Peer-Timo Bremer, Haichao Miao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Additive manufacturing (AM) techniques have been used to enhance the design
and fabrication of complex components for various applications in the medical,
aerospace, energy, and consumer products industries. A defining feature for
many AM parts is the complex internal geometry enabled by the printing process.
However, inspecting these internal structures requires volumetric imaging,
i.e., X-ray CT, leading to the well-known challenge of visualizing complex 3D
geometries using 2D desktop interfaces. Furthermore, existing tools are limited
to single-user systems making it difficult to jointly discuss or share findings
with a larger team, i.e., the designers, manufacturing experts, and evaluation
team. In this work, we present a collaborative virtual reality (VR) for the
exploration and inspection of AM parts. Geographically separated experts can
virtually inspect and jointly discuss data. It also supports VR and non-VR
users, who can be spectators in the VR environment. Various features for data
exploration and inspection are developed and enhanced via real-time
synchronization. We followed usability and interface verification guidelines
using Nielsen's heuristics approach. Furthermore, we conducted exploratory and
semi-structured interviews with domain experts to collect qualitative feedback.
Results reveal potential benefits, applicability, and current limitations. The
proposed collaborative VR environment provides a new basis and opens new
research directions for virtual inspection and team collaboration in AM
settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Conditionally Accepted - CHI LBW 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Handoffs in User-Centric Cell-Free MIMO Networks: A POMDP Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08900v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08900v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hussein A. Ammar, Raviraj Adve, Shahram Shahbazpanahi, Gary Boudreau, Kothapalli Venkata Srinivas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the problem of managing handoffs (HOs) in user-centric cell-free
massive MIMO (UC-mMIMO) networks. Motivated by the importance of controlling
the number of HOs and by the correlation between efficient HO decisions and the
temporal evolution of the channel conditions, we formulate a partially
observable Markov decision process (POMDP) with the state space representing
the discrete versions of the large-scale fading and the action space
representing the association decisions of the user with the access points
(APs). We develop a novel algorithm that employs this model to derive a HO
policy for a mobile user based on current and future rewards. To alleviate the
high complexity of our POMDP, we follow a divide-and-conquer approach by
breaking down the POMDP formulation into sub-problems, each solved separately.
Then, the policy and the candidate pool of APs for the sub-problem that
produced the best total expected reward are used to perform HOs within a
specific time horizon. We then introduce modifications to our algorithm to
decrease the number of HOs. The results show that half of the number of HOs in
the UC-mMIMO networks can be eliminated. Namely, our novel solution can control
the number of HOs while maintaining a rate guarantee, where a 47%-70% reduction
of the cumulative number of HOs is observed in networks with a density of 125
APs per km2. Most importantly, our results show that a POMDP-based HO scheme is
promising to control HOs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in IEEE Transactions on Wireless Communications (TWC)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cyclic Data Parallelism for Efficient Parallelism of Deep Neural
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08837v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08837v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Louis Fournier, Edouard Oyallon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training large deep learning models requires parallelization techniques to
scale. In existing methods such as Data Parallelism or ZeRO-DP, micro-batches
of data are processed in parallel, which creates two drawbacks: the total
memory required to store the model's activations peaks at the end of the
forward pass, and gradients must be simultaneously averaged at the end of the
backpropagation step. We propose Cyclic Data Parallelism, a novel paradigm
shifting the execution of the micro-batches from simultaneous to sequential,
with a uniform delay. At the cost of a slight gradient delay, the total memory
taken by activations is constant, and the gradient communications are balanced
during the training step. With Model Parallelism, our technique reduces the
number of GPUs needed, by sharing GPUs across micro-batches. Within the ZeRO-DP
framework, our technique allows communication of the model states with
point-to-point operations rather than a collective broadcast operation. We
illustrate the strength of our approach on the CIFAR-10 and ImageNet datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ One-Shot Averaging for Distributed TD($λ$) Under Markov Sampling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08896v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08896v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoxing Tian, Ioannis Ch. Paschalidis, Alex Olshevsky
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider a distributed setup for reinforcement learning, where each agent
has a copy of the same Markov Decision Process but transitions are sampled from
the corresponding Markov chain independently by each agent. We show that in
this setting, we can achieve a linear speedup for TD($\lambda$), a family of
popular methods for policy evaluation, in the sense that $N$ agents can
evaluate a policy $N$ times faster provided the target accuracy is small
enough. Notably, this speedup is achieved by ``one shot averaging,'' a
procedure where the agents run TD($\lambda$) with Markov sampling independently
and only average their results after the final step. This significantly reduces
the amount of communication required to achieve a linear speedup relative to
previous work.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Polylog-Competitive Deterministic Local Routing and Scheduling <span class="chip">STOC 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07410v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07410v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bernhard Haeupler, Shyamal Patel, Antti Roeyskoe, Cliff Stein, Goran Zuzic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses point-to-point packet routing in undirected networks,
which is the most important communication primitive in most networks. The main
result proves the existence of routing tables that guarantee a
polylog-competitive completion-time $\textbf{deterministically}$: in any
undirected network, it is possible to give each node simple stateless
deterministic local forwarding rules, such that, any adversarially chosen set
of packets are delivered as fast as possible, up to polylog factors.
  All previous routing strategies crucially required randomization for both
route selection and packet scheduling.
  The core technical contribution of this paper is a new local packet
scheduling result of independent interest. This scheduling strategy integrates
well with recent sparse semi-oblivious path selection strategies. Such
strategies deterministically select not one but several candidate paths for
each packet and require a global coordinator to select a single good path from
those candidates for each packet. Another challenge is that, even if a single
path is selected for each packet, no strategy for scheduling packets along
low-congestion paths that is both local and deterministic is known. Our novel
scheduling strategy utilizes the fact that every semi-oblivious routing
strategy uses only a small (polynomial) subset of candidate routes. It
overcomes the issue of global coordination by furthermore being provably robust
to adversarial noise. This avoids the issue of having to choose a single path
per packet because congestion caused by ineffective candidate paths can be
treated as noise.
  Our results imply the first deterministic universally-optimal algorithms in
the distributed supported-CONGEST model for many important global distributed
tasks, including computing minimum spanning trees, approximate shortest paths,
and part-wise aggregates.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear at STOC 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Training Machine Learning models at the Edge: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.02619v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.02619v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aymen Rayane Khouas, Mohamed Reda Bouadjenek, Hakim Hacid, Sunil Aryal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Edge Computing (EC) has gained significant traction in recent years,
promising enhanced efficiency by integrating Artificial Intelligence (AI)
capabilities at the edge. While the focus has primarily been on the deployment
and inference of Machine Learning (ML) models at the edge, the training aspect
remains less explored. This survey delves into Edge Learning (EL), specifically
the optimization of ML model training at the edge. The objective is to
comprehensively explore diverse approaches and methodologies in EL, synthesize
existing knowledge, identify challenges, and highlight future trends. Utilizing
Scopus' advanced search, relevant literature on EL was identified, revealing a
concentration of research efforts in distributed learning methods, particularly
Federated Learning (FL). This survey further provides a guideline for comparing
techniques used to optimize ML for edge learning, along with an exploration of
different frameworks, libraries, and simulation tools available for EL. In
doing so, the paper contributes to a holistic understanding of the current
landscape and future directions in the intersection of edge computing and
machine learning, paving the way for informed comparisons between optimization
methods and techniques designed for edge learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages, 7 figures, submitted to IEEE Communications Surveys &
  Tutorials</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Privacy-Preserving Sharing of Data Analytics Runtime Metrics for
  Performance Modeling <span class="chip">SP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.05692v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.05692v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonathan Will, Dominik Scheinert, Jan Bode, Cedric Kring, Seraphin Zunzer, Lauritz Thamsen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Performance modeling for large-scale data analytics workloads can improve the
efficiency of cluster resource allocations and job scheduling. However, the
performance of these workloads is influenced by numerous factors, such as job
inputs and the assigned cluster resources. As a result, performance models
require significant amounts of training data. This data can be obtained by
exchanging runtime metrics between collaborating organizations. Yet, not all
organizations may be inclined to publicly disclose such metadata.
  We present a privacy-preserving approach for sharing runtime metrics based on
differential privacy and data synthesis. Our evaluation on performance data
from 736 Spark job executions indicates that fully anonymized training data
largely maintains performance prediction accuracy, particularly when there is
minimal original data available. With 30 or fewer available original data
samples, the use of synthetic training data resulted only in a one percent
reduction in performance model accuracy on average.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 4 figures, presented at the WOSP-C workshop at ICPE 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On a Voter Model with Context-Dependent Opinion Adoption 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.07377v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.07377v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luca Becchetti, Vincenzo Bonifaci, Emilio Cruciani, Francesco Pasquale
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Opinion diffusion is a crucial phenomenon in social networks, often
underlying the way in which a collective of agents develops a consensus on
relevant decisions. The voter model is a well-known theoretical model to study
opinion spreading in social networks and structured populations. Its simplest
version assumes that an updating agent will adopt the opinion of a neighboring
agent chosen at random. The model allows us to study, for example, the
probability that a certain opinion will fixate into a consensus opinion, as
well as the expected time it takes for a consensus opinion to emerge.
  Standard voter models are oblivious to the opinions held by the agents
involved in the opinion adoption process. We propose and study a
context-dependent opinion spreading process on an arbitrary social graph, in
which the probability that an agent abandons opinion $a$ in favor of opinion
$b$ depends on both $a$ and $b$. We discuss the relations of the model with
existing voter models and then derive theoretical results for both the fixation
probability and the expected consensus time for two opinions, for both the
synchronous and the asynchronous update models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AMSP: Reducing Communication Overhead of ZeRO for Efficient LLM Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.00257v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.00257v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiaoling Chen, Qinghao Hu, Guoteng Wang, Yingtong Xiong, Ting Huang, Xun Chen, Yang Gao, Hang Yan, Yonggang Wen, Tianwei Zhang, Peng Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training large language models (LLMs) encounters challenges in GPU memory
consumption due to the high memory requirements of model states. The widely
used Zero Redundancy Optimizer (ZeRO) addresses this issue through strategic
sharding but introduces communication challenges at scale. To tackle this
problem, we propose AMSP, a system designed to optimize ZeRO for scalable LLM
training. AMSP incorporates three flexible sharding strategies: Full-Replica,
Full-Sharding, and Partial-Sharding, and allows each component within the model
states (Parameters, Gradients, Optimizer States) to independently choose a
sharding strategy as well as the device mesh. We conduct a thorough analysis of
communication costs, formulating an optimization problem to discover the
optimal sharding strategy. Additionally, AMSP optimizes distributed LLM
training by efficiently overlapping communication with computation. Evaluations
demonstrate up to 52\% Model FLOPs Utilization (MFU) when training the
LLaMA-based model on 1024 GPUs, resulting in a 1.56 times improvement in
training throughput compared to newly proposed systems like MiCS and ZeRO++.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OSMOSIS: Enabling Multi-Tenancy in Datacenter SmartNICs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.03628v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.03628v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mikhail Khalilov, Marcin Chrapek, Siyuan Shen, Alessandro Vezzu, Thomas Benz, Salvatore Di Girolamo, Timo Schneider, Daniele De Sensi, Luca Benini, Torsten Hoefler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-tenancy is essential for unleashing SmartNIC's potential in
datacenters. Our systematic analysis in this work shows that existing on-path
SmartNICs have resource multiplexing limitations. For example, existing
solutions lack multi-tenancy capabilities such as performance isolation and QoS
provisioning for compute and IO resources. Compared to standard NIC data paths
with a well-defined set of offloaded functions, unpredictable execution times
of SmartNIC kernels make conventional approaches for multi-tenancy and QoS
insufficient. We fill this gap with OSMOSIS, a SmartNICs resource manager
co-design. OSMOSIS extends existing OS mechanisms to enable dynamic hardware
resource multiplexing of the on-path packet processing data plane. We integrate
OSMOSIS within an open-source RISC-V-based 400Gbit/s SmartNIC. Our performance
results demonstrate that OSMOSIS fully supports multi-tenancy and enables
broader adoption of SmartNICs in datacenters with low overhead.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 14 figures, 103 references</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Artificial Intelligence
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FastMAC: Stochastic Spectral Sampling of Correspondence Graph <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08770v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08770v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifei Zhang, Hao Zhao, Hongyang Li, Siheng Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D correspondence, i.e., a pair of 3D points, is a fundamental concept in
computer vision. A set of 3D correspondences, when equipped with compatibility
edges, forms a correspondence graph. This graph is a critical component in
several state-of-the-art 3D point cloud registration approaches, e.g., the one
based on maximal cliques (MAC). However, its properties have not been well
understood. So we present the first study that introduces graph signal
processing into the domain of correspondence graph. We exploit the generalized
degree signal on correspondence graph and pursue sampling strategies that
preserve high-frequency components of this signal. To address time-consuming
singular value decomposition in deterministic sampling, we resort to a
stochastic approximate sampling strategy. As such, the core of our method is
the stochastic spectral sampling of correspondence graph. As an application, we
build a complete 3D registration algorithm termed as FastMAC, that reaches
real-time speed while leading to little to none performance drop. Through
extensive experiments, we validate that FastMAC works for both indoor and
outdoor benchmarks. For example, FastMAC can accelerate MAC by 80 times while
maintaining high registration success rate on KITTI. Codes are publicly
available at https://github.com/Forrest-110/FastMAC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024, Code: https://github.com/Forrest-110/FastMAC</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Simple and Scalable Strategies to Continually <span class="highlight-title">Pre-train</span> Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08763v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08763v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adam Ibrahim, Benjamin Thérien, Kshitij Gupta, Mats L. Richter, Quentin Anthony, Timothée Lesort, Eugene Belilovsky, Irina Rish
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are routinely pre-trained on billions of tokens,
only to start the process over again once new data becomes available. A much
more efficient solution is to continually pre-train these models, saving
significant compute compared to re-training. However, the distribution shift
induced by new data typically results in degraded performance on previous data
or poor adaptation to the new data. In this work, we show that a simple and
scalable combination of learning rate (LR) re-warming, LR re-decaying, and
replay of previous data is sufficient to match the performance of fully
re-training from scratch on all available data, as measured by final loss and
language model (LM) evaluation benchmarks. Specifically, we show this for a
weak but realistic distribution shift between two commonly used LLM
pre-training datasets (English$\rightarrow$English) and a stronger distribution
shift (English$\rightarrow$German) at the $405$M parameter model scale with
large dataset sizes (hundreds of billions of tokens). Selecting the weak but
realistic shift for larger-scale experiments, we also find that our continual
learning strategies match the re-training baseline for a 10B parameter LLM. Our
results demonstrate that LLMs can be successfully updated via simple and
scalable continual learning strategies, matching the re-training baseline using
only a fraction of the compute. Finally, inspired by previous work, we propose
alternatives to the cosine learning rate schedule that help circumvent
forgetting induced by LR re-warming and that are not bound to a fixed token
budget.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DAM: Dynamic Adapter Merging for Continual Video QA Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08755v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08755v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Feng Cheng, Ziyang Wang, Yi-Lin Sung, Yan-Bo Lin, Mohit Bansal, Gedas Bertasius
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a parameter-efficient method for continual video
question-answering (VidQA) learning. Our method, named DAM, uses the proposed
Dynamic Adapter Merging to (i) mitigate catastrophic forgetting, (ii) enable
efficient adaptation to continually arriving datasets, (iii) handle inputs from
unknown datasets during inference, and (iv) enable knowledge sharing across
similar dataset domains. Given a set of continually streaming VidQA datasets,
we sequentially train dataset-specific adapters for each dataset while freezing
the parameters of a large pretrained video-language backbone. During inference,
given a video-question sample from an unknown domain, our method first uses the
proposed non-parametric router function to compute a probability for each
adapter, reflecting how relevant that adapter is to the current video-question
input instance. Subsequently, the proposed dynamic adapter merging scheme
aggregates all the adapter weights into a new adapter instance tailored for
that particular test sample to compute the final VidQA prediction, mitigating
the impact of inaccurate router predictions and facilitating knowledge sharing
across domains. Our DAM model outperforms prior state-of-the-art continual
learning approaches by 9.1% while exhibiting 1.9% less forgetting on 6 VidQA
datasets spanning various domains. We further extend DAM to continual image
classification and image QA and outperform prior methods by a large margin. The
code is publicly available at: https://github.com/klauscc/DAM
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The first two authors contribute equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Steering LLMs Towards Unbiased Responses: A Causality-Guided Debiasing
  Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08743v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08743v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingling Li, Zeyu Tang, Xiaoyu Liu, Peter Spirtes, Kun Zhang, Liu Leqi, Yang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) can easily generate biased and discriminative
responses. As LLMs tap into consequential decision-making (e.g., hiring and
healthcare), it is of crucial importance to develop strategies to mitigate
these biases. This paper focuses on social bias, tackling the association
between demographic information and LLM outputs. We propose a causality-guided
debiasing framework that utilizes causal understandings of (1) the
data-generating process of the training corpus fed to LLMs, and (2) the
internal reasoning process of LLM inference, to guide the design of prompts for
debiasing LLM outputs through selection mechanisms. Our framework unifies
existing de-biasing prompting approaches such as inhibitive instructions and
in-context contrastive examples, and sheds light on new ways of debiasing by
encouraging bias-free reasoning. Our strong empirical performance on real-world
datasets demonstrates that our framework provides principled guidelines on
debiasing LLM outputs even with only the black-box access.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Garden of Forking Paths: Observing Dynamic Parameters Distribution
  in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08739v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08739v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Carlo Nicolini, Jacopo Staiano, Bruno Lepri, Raffaele Marino
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A substantial gap persists in understanding the reasons behind the
exceptional performance of the Transformer architecture in NLP. A particularly
unexplored area involves the mechanistic description of how the distribution of
parameters evolves over time during training. In this work we suggest that
looking at the time evolution of the statistic distribution of model
parameters, and specifically at bifurcation effects, can help understanding the
model quality, potentially reducing training costs and evaluation efforts and
empirically showing the reasons behind the effectiveness of weights
sparsification.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ambient Diffusion Posterior Sampling: Solving Inverse Problems with
  Diffusion Models trained on Corrupted Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08728v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08728v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Asad Aali, Giannis Daras, Brett Levac, Sidharth Kumar, Alexandros G. Dimakis, Jonathan I. Tamir
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We provide a framework for solving inverse problems with diffusion models
learned from linearly corrupted data. Our method, Ambient Diffusion Posterior
Sampling (A-DPS), leverages a generative model pre-trained on one type of
corruption (e.g. image inpainting) to perform posterior sampling conditioned on
measurements from a potentially different forward process (e.g. image
blurring). We test the efficacy of our approach on standard natural image
datasets (CelebA, FFHQ, and AFHQ) and we show that A-DPS can sometimes
outperform models trained on clean data for several image restoration tasks in
both speed and performance. We further extend the Ambient Diffusion framework
to train MRI models with access only to Fourier subsampled multi-coil MRI
measurements at various acceleration factors (R=2, 4, 6, 8). We again observe
that models trained on highly subsampled data are better priors for solving
inverse problems in the high acceleration regime than models trained on fully
sampled data. We open-source our code and the trained Ambient Diffusion MRI
models: https://github.com/utcsilab/ambient-diffusion-mri .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Pre-print, work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Implicit Regularization of Gradient Flow on One-Layer Softmax Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08699v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08699v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Heejune Sheen, Siyu Chen, Tianhao Wang, Harrison H. Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study gradient flow on the exponential loss for a classification problem
with a one-layer softmax attention model, where the key and query weight
matrices are trained separately. Under a separability assumption on the data,
we show that when gradient flow achieves the minimal loss value, it further
implicitly minimizes the nuclear norm of the product of the key and query
weight matrices. Such implicit regularization can be described by a Support
Vector Machine (SVM) problem with respect to the attention weights. This
finding contrasts with prior results showing that the gradient descent induces
an implicit regularization on the Frobenius norm on the product weight matrix
when the key and query matrices are combined into a single weight matrix for
training. For diagonal key and query matrices, our analysis builds upon the
reparameterization technique and exploits approximate KKT conditions of the SVM
associated with the classification data. Moreover, the results are extended to
general weights configurations given proper alignment of the weight matrices'
singular spaces with the data features at initialization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>34 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Token Alignment via Character Matching for Subword Completion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08688v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08688v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ben Athiwaratkun, Shiqi Wang, Mingyue Shang, Yuchen Tian, Zijian Wang, Sujan Kumar Gonugondla, Sanjay Krishna Gouda, Rob Kwiatowski, Ramesh Nallapati, Bing Xiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative models, widely utilized in various applications, can often
struggle with prompts corresponding to partial tokens. This struggle stems from
tokenization, where partial tokens fall out of distribution during inference,
leading to incorrect or nonsensical outputs. This paper examines a technique to
alleviate the tokenization artifact on text completion in generative models,
maintaining performance even in regular non-subword cases. The method, termed
token alignment, involves backtracking to the last complete tokens and ensuring
the model's generation aligns with the prompt. This approach showcases marked
improvement across many partial token scenarios, including nuanced cases like
space-prefix and partial indentation, with only a minor time increase. The
technique and analysis detailed in this paper contribute to the continuous
advancement of generative models in handling partial inputs, bearing relevance
for applications like code completion and text autocompletion.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Human Alignment of Large Language Models through Online Preference
  Optimisation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08635v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08635v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniele Calandriello, Daniel Guo, Remi Munos, Mark Rowland, Yunhao Tang, Bernardo Avila Pires, Pierre Harvey Richemond, Charline Le Lan, Michal Valko, Tianqi Liu, Rishabh Joshi, Zeyu Zheng, Bilal Piot
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ensuring alignment of language models' outputs with human preferences is
critical to guarantee a useful, safe, and pleasant user experience. Thus, human
alignment has been extensively studied recently and several methods such as
Reinforcement Learning from Human Feedback (RLHF), Direct Policy Optimisation
(DPO) and Sequence Likelihood Calibration (SLiC) have emerged. In this paper,
our contribution is two-fold. First, we show the equivalence between two recent
alignment methods, namely Identity Policy Optimisation (IPO) and Nash Mirror
Descent (Nash-MD). Second, we introduce a generalisation of IPO, named IPO-MD,
that leverages the regularised sampling approach proposed by Nash-MD.
  This equivalence may seem surprising at first sight, since IPO is an offline
method whereas Nash-MD is an online method using a preference model. However,
this equivalence can be proven when we consider the online version of IPO, that
is when both generations are sampled by the online policy and annotated by a
trained preference model. Optimising the IPO loss with such a stream of data
becomes then equivalent to finding the Nash equilibrium of the preference model
through self-play. Building on this equivalence, we introduce the IPO-MD
algorithm that generates data with a mixture policy (between the online and
reference policy) similarly as the general Nash-MD algorithm. We compare
online-IPO and IPO-MD to different online versions of existing losses on
preference data such as DPO and SLiC on a summarisation task.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Verifix: Post-Training Correction to Improve Label Noise Robustness with
  Verified Samples 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08618v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08618v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sangamesh Kodge, Deepak Ravikumar, Gobinda Saha, Kaushik Roy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Label corruption, where training samples have incorrect labels, can
significantly degrade the performance of machine learning models. This
corruption often arises from non-expert labeling or adversarial attacks.
Acquiring large, perfectly labeled datasets is costly, and retraining large
models from scratch when a clean dataset becomes available is computationally
expensive. To address this challenge, we propose Post-Training Correction, a
new paradigm that adjusts model parameters after initial training to mitigate
label noise, eliminating the need for retraining. We introduce Verifix, a novel
Singular Value Decomposition (SVD) based algorithm that leverages a small,
verified dataset to correct the model weights using a single update. Verifix
uses SVD to estimate a Clean Activation Space and then projects the model's
weights onto this space to suppress activations corresponding to corrupted
data. We demonstrate Verifix's effectiveness on both synthetic and real-world
label noise. Experiments on the CIFAR dataset with 25% synthetic corruption
show 7.36% generalization improvements on average. Additionally, we observe
generalization improvements of up to 2.63% on naturally corrupted datasets like
WebVision1.0 and Clothing1M.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Link Prediction for Social Networks using Representation Learning and
  Heuristic-based Features <span class="chip">IJCAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08613v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08613v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samarth Khanna, Sree Bhattacharyya, Sudipto Ghosh, Kushagra Agarwal, Asit Kumar Das
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The exponential growth in scale and relevance of social networks enable them
to provide expansive insights. Predicting missing links in social networks
efficiently can help in various modern-day business applications ranging from
generating recommendations to influence analysis. Several categories of
solutions exist for the same. Here, we explore various feature extraction
techniques to generate representations of nodes and edges in a social network
that allow us to predict missing links. We compare the results of using ten
feature extraction techniques categorized across Structural embeddings,
Neighborhood-based embeddings, Graph Neural Networks, and Graph Heuristics,
followed by modeling with ensemble classifiers and custom Neural Networks.
Further, we propose combining heuristic-based features and learned
representations that demonstrate improved performance for the link prediction
task on social network datasets. Using this method to generate accurate
recommendations for many applications is a matter of further study that appears
very promising. The code for all the experiments has been made public.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the MAISoN Workshop at IJCAI 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MedInsight: A Multi-Source Context Augmentation Framework for Generating
  Patient-Centric Medical Responses using Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08607v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08607v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Subash Neupane, Shaswata Mitra, Sudip Mittal, Noorbakhsh Amiri Golilarz, Shahram Rahimi, Amin Amirlatifi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have shown impressive capabilities in generating
human-like responses. However, their lack of domain-specific knowledge limits
their applicability in healthcare settings, where contextual and comprehensive
responses are vital. To address this challenge and enable the generation of
patient-centric responses that are contextually relevant and comprehensive, we
propose MedInsight:a novel retrieval augmented framework that augments LLM
inputs (prompts) with relevant background information from multiple sources.
MedInsight extracts pertinent details from the patient's medical record or
consultation transcript. It then integrates information from authoritative
medical textbooks and curated web resources based on the patient's health
history and condition. By constructing an augmented context combining the
patient's record with relevant medical knowledge, MedInsight generates
enriched, patient-specific responses tailored for healthcare applications such
as diagnosis, treatment recommendations, or patient education. Experiments on
the MTSamples dataset validate MedInsight's effectiveness in generating
contextually appropriate medical responses. Quantitative evaluation using the
Ragas metric and TruLens for answer similarity and answer correctness
demonstrates the model's efficacy. Furthermore, human evaluation studies
involving Subject Matter Expert (SMEs) confirm MedInsight's utility, with
moderate inter-rater agreement on the relevance and correctness of the
generated responses.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Call Me When Necessary: LLMs can Efficiently and Faithfully Reason over
  Structured Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08593v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08593v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sitao Cheng, Ziyuan Zhuang, Yong Xu, Fangkai Yang, Chaoyun Zhang, Xiaoting Qin, Xiang Huang, Ling Chen, Qingwei Lin, Dongmei Zhang, Saravan Rajmohan, Qi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have shown potential in reasoning over
structured environments, e.g., knowledge graph and table. Such tasks typically
require multi-hop reasoning, i.e., match natural language utterance with
instances in the environment. Previous methods leverage LLMs to incrementally
build a reasoning path, where the LLMs either invoke tools or pick up schemas
by step-by-step interacting with the environment. We propose
Reasoning-Path-Editing (Readi), a novel framework where LLMs can efficiently
and faithfully reason over structured environments. In Readi, LLMs initially
generate a reasoning path given a query, and edit the path only when necessary.
We instantiate the path on structured environments and provide feedback to edit
the path if anything goes wrong. Experimental results on three KGQA datasets
and two TableQA datasets show the effectiveness of Readi, significantly
surpassing all LLM-based methods (by 9.1% on WebQSP, 12.4% on MQA-3H and 10.9%
on WTQ), comparable with state-of-the-art fine-tuned methods (67% on CWQ and
74.7% on WebQSP) and substantially boosting the vanilla LLMs (by 14.9% on CWQ).
Our code will be available upon publication.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 8 figures, 9 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Non-discrimination Criteria for Generative Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08564v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08564v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sara Sterlie, Nina Weng, Aasa Feragen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Within recent years, generative AI, such as large language models, has
undergone rapid development. As these models become increasingly available to
the public, concerns arise about perpetuating and amplifying harmful biases in
applications. Gender stereotypes can be harmful and limiting for the
individuals they target, whether they consist of misrepresentation or
discrimination. Recognizing gender bias as a pervasive societal construct, this
paper studies how to uncover and quantify the presence of gender biases in
generative language models. In particular, we derive generative AI analogues of
three well-known non-discrimination criteria from classification, namely
independence, separation and sufficiency. To demonstrate these criteria in
action, we design prompts for each of the criteria with a focus on occupational
gender stereotype, specifically utilizing the medical test to introduce the
ground truth in the generative AI context. Our results address the presence of
occupational gender bias within such conversational language models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 5 figures. Submitted to ACM Conference on Fairness,
  Accountability, and Transparency (ACM FAccT 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Structural perspective on constraint-based learning of Markov networks <span class="chip">AISTATS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08562v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08562v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tuukka Korhonen, Fedor V. Fomin, Pekka Parviainen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Markov networks are probabilistic graphical models that employ undirected
graphs to depict conditional independence relationships among variables. Our
focus lies in constraint-based structure learning, which entails learning the
undirected graph from data through the execution of conditional independence
tests. We establish theoretical limits concerning two critical aspects of
constraint-based learning of Markov networks: the number of tests and the sizes
of the conditioning sets. These bounds uncover an exciting interplay between
the structural properties of the graph and the amount of tests required to
learn a Markov network. The starting point of our work is that the graph
parameter maximum pairwise connectivity, $\kappa$, that is, the maximum number
of vertex-disjoint paths connecting a pair of vertices in the graph, is
responsible for the sizes of independence tests required to learn the graph. On
one hand, we show that at least one test with the size of the conditioning set
at least $\kappa$ is always necessary. On the other hand, we prove that any
graph can be learned by performing tests of size at most $\kappa$. This
completely resolves the question of the minimum size of conditioning sets
required to learn the graph. When it comes to the number of tests, our upper
bound on the sizes of conditioning sets implies that every $n$-vertex graph can
be learned by at most $n^{\kappa}$ tests with conditioning sets of sizes at
most $\kappa$. We show that for any upper bound $q$ on the sizes of the
conditioning sets, there exist graphs with $O(n q)$ vertices that require at
least $n^{\Omega(\kappa)}$ tests to learn. This lower bound holds even when the
treewidth and the maximum degree of the graph are at most $\kappa+2$. On the
positive side, we prove that every graph of bounded treewidth can be learned by
a polynomial number of tests with conditioning sets of sizes at most $2\kappa$.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AISTATS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SM4Depth: Seamless Monocular Metric Depth Estimation across Multiple
  Cameras and Scenes by One Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08556v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08556v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yihao Liu, Feng Xue, Anlong Ming
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The generalization of monocular metric depth estimation (MMDE) has been a
longstanding challenge. Recent methods made progress by combining relative and
metric depth or aligning input image focal length. However, they are still
beset by challenges in camera, scene, and data levels: (1) Sensitivity to
different cameras; (2) Inconsistent accuracy across scenes; (3) Reliance on
massive training data. This paper proposes SM4Depth, a seamless MMDE method, to
address all the issues above within a single network. First, we reveal that a
consistent field of view (FOV) is the key to resolve ``metric ambiguity''
across cameras, which guides us to propose a more straightforward preprocessing
unit. Second, to achieve consistently high accuracy across scenes, we
explicitly model the metric scale determination as discretizing the depth
interval into bins and propose variation-based unnormalized depth bins. This
method bridges the depth gap of diverse scenes by reducing the ambiguity of the
conventional metric bin. Third, to reduce the reliance on massive training
data, we propose a ``divide and conquer" solution. Instead of estimating
directly from the vast solution space, the correct metric bins are estimated
from multiple solution sub-spaces for complexity reduction. Finally, with just
150K RGB-D pairs and a consumer-grade GPU for training, SM4Depth achieves
state-of-the-art performance on most previously unseen datasets, especially
surpassing ZoeDepth and Metric3D on mRI$_\theta$. The code can be found at
https://github.com/1hao-Liu/SM4Depth.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: xuefeng-cvr.github.io/SM4Depth</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Federated Knowledge Graph Unlearning via Diffusion Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08554v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08554v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bingchen Liu, Yuanyuan Fang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning (FL) promotes the development and application of
artificial intelligence technologies by enabling model sharing and
collaboration while safeguarding data privacy. Knowledge graph (KG) embedding
representation provides a foundation for knowledge reasoning and applications
by mapping entities and relations into vector space. Federated KG embedding
enables the utilization of knowledge from diverse client sources while
safeguarding the privacy of local data. However, due to demands such as privacy
protection and the need to adapt to dynamic data changes, investigations into
machine unlearning (MU) have been sparked. However, it is challenging to
maintain the performance of KG embedding models while forgetting the influence
of specific forgotten data on the model. In this paper, we propose FedDM, a
novel framework tailored for machine unlearning in federated knowledge graphs.
Leveraging diffusion models, we generate noisy data to sensibly mitigate the
influence of specific knowledge on FL models while preserving the overall
performance concerning the remaining data. We conduct experimental evaluations
on benchmark datasets to assess the efficacy of the proposed model. Extensive
experiments demonstrate that FedDM yields promising results in knowledge
forgetting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HOLMES: HOLonym-MEronym based Semantic inspection for Convolutional
  Image Classifiers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08536v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08536v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francesco Dibitonto, Fabio Garcea, André Panisson, Alan Perotti, Lia Morra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Convolutional Neural Networks (CNNs) are nowadays the model of choice in
Computer Vision, thanks to their ability to automatize the feature extraction
process in visual tasks. However, the knowledge acquired during training is
fully subsymbolic, and hence difficult to understand and explain to end users.
In this paper, we propose a new technique called HOLMES (HOLonym-MEronym based
Semantic inspection) that decomposes a label into a set of related concepts,
and provides component-level explanations for an image classification model.
Specifically, HOLMES leverages ontologies, web scraping and transfer learning
to automatically construct meronym (parts)-based detectors for a given holonym
(class). Then, it produces heatmaps at the meronym level and finally, by
probing the holonym CNN with occluded images, it highlights the importance of
each part on the classification output. Compared to state-of-the-art saliency
methods, HOLMES takes a step further and provides information about both where
and what the holonym CNN is looking at, without relying on densely annotated
datasets and without forcing concepts to be associated to single computational
units. Extensive experimental evaluation on different categories of objects
(animals, tools and vehicles) shows the feasibility of our approach. On
average, HOLMES explanations include at least two meronyms, and the ablation of
a single meronym roughly halves the holonym model confidence. The resulting
heatmaps were quantitatively evaluated using the
deletion/insertion/preservation curves. All metrics were comparable to those
achieved by GradCAM, while offering the advantage of further decomposing the
heatmap in human-understandable concepts, thus highlighting both the relevance
of meronyms to object classification, as well as HOLMES ability to capture it.
The code is available at https://github.com/FrancesC0de/HOLMES.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been accepted to be presented to The 1st World
  Conference on eXplainable Artificial Intelligence (xAI 2023), July 26-28,
  2023 - Lisboa, Portugal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pig aggression classification using CNN, <span class="highlight-title">Transformer</span>s and Recurrent
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08528v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08528v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junior Silva Souza, Eduardo Bedin, Gabriel Toshio Hirokawa Higa, Newton Loebens, Hemerson Pistori
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The development of techniques that can be used to analyze and detect animal
behavior is a crucial activity for the livestock sector, as it is possible to
monitor the stress and animal welfare and contributes to decision making in the
farm. Thus, the development of applications can assist breeders in making
decisions to improve production performance and reduce costs, once the animal
behavior is analyzed by humans and this can lead to susceptible errors and time
consumption. Aggressiveness in pigs is an example of behavior that is studied
to reduce its impact through animal classification and identification. However,
this process is laborious and susceptible to errors, which can be reduced
through automation by visually classifying videos captured in controlled
environment. The captured videos can be used for training and, as a result, for
classification through computer vision and artificial intelligence, employing
neural network techniques. The main techniques utilized in this study are
variants of transformers: STAM, TimeSformer, and ViViT, as well as techniques
using convolutions, such as ResNet3D2, Resnet(2+1)D, and CnnLstm. These
techniques were employed for pig video classification with the objective of
identifying aggressive and non-aggressive behaviors. In this work, various
techniques were compared to analyze the contribution of using transformers, in
addition to the effectiveness of the convolution technique in video
classification. The performance was evaluated using accuracy, precision, and
recall. The TimerSformer technique showed the best results in video
classification, with median accuracy of 0.729.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Content-aware Masked Image Modeling <span class="highlight-title">Transformer</span> for Stereo Image
  Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08505v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08505v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinjie Zhang, Shenyuan Gao, Zhening Liu, Xingtong Ge, Dailan He, Tongda Xu, Yan Wang, Jun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing learning-based stereo image codec adopt sophisticated transformation
with simple entropy models derived from single image codecs to encode latent
representations. However, those entropy models struggle to effectively capture
the spatial-disparity characteristics inherent in stereo images, which leads to
suboptimal rate-distortion results. In this paper, we propose a stereo image
compression framework, named CAMSIC. CAMSIC independently transforms each image
to latent representation and employs a powerful decoder-free Transformer
entropy model to capture both spatial and disparity dependencies, by
introducing a novel content-aware masked image modeling (MIM) technique. Our
content-aware MIM facilitates efficient bidirectional interaction between prior
information and estimated tokens, which naturally obviates the need for an
extra Transformer decoder. Experiments show that our stereo image codec
achieves state-of-the-art rate-distortion performance on two stereo image
datasets Cityscapes and InStereo2K with fast encoding and decoding speed.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Masked Generative Story <span class="highlight-title">Transformer</span> with Character Guidance and Caption
  Augmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08502v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08502v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christos Papadimitriou, Giorgos Filandrianos, Maria Lymperaiou, Giorgos Stamou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Story Visualization (SV) is a challenging generative vision task, that
requires both visual quality and consistency between different frames in
generated image sequences. Previous approaches either employ some kind of
memory mechanism to maintain context throughout an auto-regressive generation
of the image sequence, or model the generation of the characters and their
background separately, to improve the rendering of characters. On the contrary,
we embrace a completely parallel transformer-based approach, exclusively
relying on Cross-Attention with past and future captions to achieve
consistency. Additionally, we propose a Character Guidance technique to focus
on the generation of characters in an implicit manner, by forming a combination
of text-conditional and character-conditional logits in the logit space. We
also employ a caption-augmentation technique, carried out by a Large Language
Model (LLM), to enhance the robustness of our approach. The combination of
these methods culminates into state-of-the-art (SOTA) results over various
metrics in the most prominent SV benchmark (Pororo-SV), attained with
constraint resources while achieving superior computational complexity compared
to previous arts. The validity of our quantitative results is supported by a
human survey.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reproducibility and Geometric Intrinsic Dimensionality: An Investigation
  on Graph Neural Network Research 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08438v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08438v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tobias Hille, Maximilian Stubbemann, Tom Hanika
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Difficulties in replication and reproducibility of empirical evidences in
machine learning research have become a prominent topic in recent years.
Ensuring that machine learning research results are sound and reliable requires
reproducibility, which verifies the reliability of research findings using the
same code and data. This promotes open and accessible research, robust
experimental workflows, and the rapid integration of new findings. Evaluating
the degree to which research publications support these different aspects of
reproducibility is one goal of the present work. For this we introduce an
ontology of reproducibility in machine learning and apply it to methods for
graph neural networks. Building on these efforts we turn towards another
critical challenge in machine learning, namely the curse of dimensionality,
which poses challenges in data collection, representation, and analysis, making
it harder to find representative data and impeding the training and inference
processes. Using the closely linked concept of geometric intrinsic dimension we
investigate to which extend the used machine learning models are influenced by
the intrinsic dimension of the data sets they are trained on.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>39 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Search-based Optimisation of LLM Learning Shots for Story Point
  Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08430v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08430v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vali Tawosi, Salwa Alamir, Xiaomo Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One of the ways Large Language Models (LLMs) are used to perform machine
learning tasks is to provide them with a few examples before asking them to
produce a prediction. This is a meta-learning process known as few-shot
learning. In this paper, we use available Search-Based methods to optimise the
number and combination of examples that can improve an LLM's estimation
performance, when it is used to estimate story points for new agile tasks. Our
preliminary results show that our SBSE technique improves the estimation
performance of the LLM by 59.34% on average (in terms of mean absolute error of
the estimation) over three datasets against a zero-shot setting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, Accepted at SSBSE'23 NIER Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Software Vulnerability and Functionality Assessment using LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08429v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08429v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rasmus Ingemann Tuffveson Jensen, Vali Tawosi, Salwa Alamir
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While code review is central to the software development process, it can be
tedious and expensive to carry out. In this paper, we investigate whether and
how Large Language Models (LLMs) can aid with code reviews. Our investigation
focuses on two tasks that we argue are fundamental to good reviews: (i)
flagging code with security vulnerabilities and (ii) performing software
functionality validation, i.e., ensuring that code meets its intended
functionality. To test performance on both tasks, we use zero-shot and
chain-of-thought prompting to obtain final ``approve or reject''
recommendations. As data, we employ seminal code generation datasets (HumanEval
and MBPP) along with expert-written code snippets with security vulnerabilities
from the Common Weakness Enumeration (CWE). Our experiments consider a mixture
of three proprietary models from OpenAI and smaller open-source LLMs. We find
that the former outperforms the latter by a large margin. Motivated by
promising results, we finally ask our models to provide detailed descriptions
of security vulnerabilities. Results show that 36.7% of LLM-generated
descriptions can be associated with true CWE vulnerabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, accepted to NLBSE'24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Language-Driven Visual Consensus for Zero-Shot Semantic Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08426v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08426v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zicheng Zhang, Tong Zhang, Yi Zhu, Jianzhuang Liu, Xiaodan Liang, QiXiang Ye, Wei Ke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The pre-trained vision-language model, exemplified by CLIP, advances
zero-shot semantic segmentation by aligning visual features with class
embeddings through a transformer decoder to generate semantic masks. Despite
its effectiveness, prevailing methods within this paradigm encounter
challenges, including overfitting on seen classes and small fragmentation in
masks. To mitigate these issues, we propose a Language-Driven Visual Consensus
(LDVC) approach, fostering improved alignment of semantic and visual
information.Specifically, we leverage class embeddings as anchors due to their
discrete and abstract nature, steering vision features toward class embeddings.
Moreover, to circumvent noisy alignments from the vision part due to its
redundant nature, we introduce route attention into self-attention for finding
visual consensus, thereby enhancing semantic consistency within the same
object. Equipped with a vision-language prompting strategy, our approach
significantly boosts the generalization capacity of segmentation models for
unseen classes. Experimental results underscore the effectiveness of our
approach, showcasing mIoU gains of 4.5 on the PASCAL VOC 2012 and 3.6 on the
COCO-Stuff 164k for unseen classes compared with the state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Specification Overfitting in Artificial Intelligence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08425v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08425v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benjamin Roth, Pedro Henrique Luz de Araujo, Yuxi Xia, Saskia Kaltenbrunner, Christoph Korab
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning (ML) and artificial intelligence (AI) approaches are often
criticized for their inherent bias and for their lack of control,
accountability, and transparency. Consequently, regulatory bodies struggle with
containing this technology's potential negative side effects. High-level
requirements such as fairness and robustness need to be formalized into
concrete specification metrics, imperfect proxies that capture isolated aspects
of the underlying requirements. Given possible trade-offs between different
metrics and their vulnerability to over-optimization, integrating specification
metrics in system development processes is not trivial. This paper defines
specification overfitting, a scenario where systems focus excessively on
specified metrics to the detriment of high-level requirements and task
performance. We present an extensive literature survey to categorize how
researchers propose, measure, and optimize specification metrics in several AI
fields (e.g., natural language processing, computer vision, reinforcement
learning). Using a keyword-based search on papers from major AI conferences and
journals between 2018 and mid-2023, we identify and analyze 74 papers that
propose or optimize specification metrics. We find that although most papers
implicitly address specification overfitting (e.g., by reporting more than one
specification metric), they rarely discuss which role specification metrics
should play in system development or explicitly define the scope and
assumptions behind metric formulations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>40 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tastle: Distract Large Language Models for Automatic Jailbreak Attack 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08424v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08424v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeguan Xiao, Yan Yang, Guanhua Chen, Yun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have achieved significant advances in recent
days. Extensive efforts have been made before the public release of LLMs to
align their behaviors with human values. The primary goal of alignment is to
ensure their helpfulness, honesty and harmlessness. However, even meticulously
aligned LLMs remain vulnerable to malicious manipulations such as jailbreaking,
leading to unintended behaviors. The jailbreak is to intentionally develop a
malicious prompt that escapes from the LLM security restrictions to produce
uncensored detrimental contents. Previous works explore different jailbreak
methods for red teaming LLMs, yet they encounter challenges regarding to
effectiveness and scalability. In this work, we propose Tastle, a novel
black-box jailbreak framework for automated red teaming of LLMs. We designed
malicious content concealing and memory reframing with an iterative
optimization algorithm to jailbreak LLMs, motivated by the research about the
distractibility and over-confidence phenomenon of LLMs. Extensive experiments
of jailbreaking both open-source and proprietary LLMs demonstrate the
superiority of our framework in terms of effectiveness, scalability and
transferability. We also evaluate the effectiveness of existing jailbreak
defense methods against our attack and highlight the crucial need to develop
more effective and practical defense strategies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Causal Graph Neural Networks for Wildfire Danger Prediction <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08414v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08414v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shan Zhao, Ioannis Prapas, Ilektra Karasante, Zhitong Xiong, Ioannis Papoutsis, Gustau Camps-Valls, Xiao Xiang Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Wildfire forecasting is notoriously hard due to the complex interplay of
different factors such as weather conditions, vegetation types and human
activities. Deep learning models show promise in dealing with this complexity
by learning directly from data. However, to inform critical decision making, we
argue that we need models that are right for the right reasons; that is, the
implicit rules learned should be grounded by the underlying processes driving
wildfires. In that direction, we propose integrating causality with Graph
Neural Networks (GNNs) that explicitly model the causal mechanism among complex
variables via graph learning. The causal adjacency matrix considers the
synergistic effect among variables and removes the spurious links from highly
correlated impacts. Our methodology's effectiveness is demonstrated through
superior performance forecasting wildfire patterns in the European boreal and
mediterranean biome. The gain is especially prominent in a highly imbalanced
dataset, showcasing an enhanced robustness of the model to adapt to regime
shifts in functional relationships. Furthermore, SHAP values from our trained
model further enhance our understanding of the model's inner workings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR 2024 Machine Learning for Remote Sensing (ML4RS)
  Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Demystifying Embedding Spaces using Large Language Models <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04475v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04475v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guy Tennenholtz, Yinlam Chow, Chih-Wei Hsu, Jihwan Jeong, Lior Shani, Azamat Tulepbergenov, Deepak Ramachandran, Martin Mladenov, Craig Boutilier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Embeddings have become a pivotal means to represent complex, multi-faceted
information about entities, concepts, and relationships in a condensed and
useful format. Nevertheless, they often preclude direct interpretation. While
downstream tasks make use of these compressed representations, meaningful
interpretation usually requires visualization using dimensionality reduction or
specialized machine learning interpretability methods. This paper addresses the
challenge of making such embeddings more interpretable and broadly useful, by
employing Large Language Models (LLMs) to directly interact with embeddings --
transforming abstract vectors into understandable narratives. By injecting
embeddings into LLMs, we enable querying and exploration of complex embedding
data. We demonstrate our approach on a variety of diverse tasks, including:
enhancing concept activation vectors (CAVs), communicating novel embedded
entities, and decoding user preferences in recommender systems. Our work
couples the immense information potential of embeddings with the interpretative
power of LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> A Hitchhiker's Guide to Geometric GNNs for 3D Atomic Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.07511v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.07511v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexandre Duval, Simon V. Mathis, Chaitanya K. Joshi, Victor Schmidt, Santiago Miret, Fragkiskos D. Malliaros, Taco Cohen, Pietro Liò, <span class="highlight-author">Yoshua Bengio</span>, Michael Bronstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in computational modelling of atomic systems, spanning
molecules, proteins, and materials, represent them as geometric graphs with
atoms embedded as nodes in 3D Euclidean space. In these graphs, the geometric
attributes transform according to the inherent physical symmetries of 3D atomic
systems, including rotations and translations in Euclidean space, as well as
node permutations. In recent years, Geometric Graph Neural Networks have
emerged as the preferred machine learning architecture powering applications
ranging from protein structure prediction to molecular simulations and material
generation. Their specificity lies in the inductive biases they leverage - such
as physical symmetries and chemical properties - to learn informative
representations of these geometric graphs.
  In this opinionated paper, we provide a comprehensive and self-contained
overview of the field of Geometric GNNs for 3D atomic systems. We cover
fundamental background material and introduce a pedagogical taxonomy of
Geometric GNN architectures: (1) invariant networks, (2) equivariant networks
in Cartesian basis, (3) equivariant networks in spherical basis, and (4)
unconstrained networks. Additionally, we outline key datasets and application
areas and suggest future research directions. The objective of this work is to
present a structured perspective on the field, making it accessible to
newcomers and aiding practitioners in gaining an intuition for its mathematical
abstractions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GenTKG: Generative Forecasting on Temporal Knowledge Graph <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07793v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07793v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruotong Liao, Xu Jia, Yunpu Ma, Yangzhe Li, Volker Tresp
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancements in large language models (LLMs) have ignited interest
in the temporal knowledge graph (tKG) domain, where conventional
embedding-based and rule-based methods dominate. The question remains open of
whether pre-trained LLMs can understand structured temporal relational data and
replace them as the foundation model for temporal relational forecasting.
Therefore, we bring temporal knowledge forecasting into the generative setting.
However, challenges occur in the huge chasms between complex temporal graph
data structure and sequential natural expressions LLMs can handle, and between
the enormous data sizes of tKGs and heavy computation costs of finetuning LLMs.
To address these challenges, we propose a novel retrieval-augmented generation
framework named GenTKG combining a temporal logical rule-based retrieval
strategy and few-shot parameter-efficient instruction tuning to solve the above
challenges, respectively. Extensive experiments have shown that GenTKG
outperforms conventional methods of temporal relational forecasting with low
computation resources using extremely limited training data as few as 16
samples. GenTKG also highlights remarkable cross-domain generalizability with
outperforming performance on unseen datasets without re-training, and in-domain
generalizability regardless of time split in the same dataset. Our work reveals
the huge potential of LLMs in the tKG domain and opens a new frontier for
generative forecasting on tKGs. Code and data are released here:
https://github.com/mayhugotong/GenTKG.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, Findings of NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ mForms : Multimodal Form-Filling with Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2011.12340v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2011.12340v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Larry Heck, Simon Heck, Anirudh Sundar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a new approach to form-filling by reformulating the task
as multimodal natural language Question Answering (QA). The reformulation is
achieved by first translating the elements on the GUI form (text fields,
buttons, icons, etc.) to natural language questions, where these questions
capture the element's multimodal semantics. After a match is determined between
the form element (Question) and the user utterance (Answer), the form element
is filled through a pre-trained extractive QA system. By leveraging pre-trained
QA models and not requiring form-specific training, this approach to
form-filling is zero-shot. The paper also presents an approach to further
refine the form-filling by using multi-task training to incorporate a
potentially large number of successive tasks. Finally, the paper introduces a
multimodal natural language form-filling dataset Multimodal Forms (mForms), as
well as a multimodal extension of the popular ATIS dataset to support future
research and experimentation. Results show the new approach not only maintains
robust accuracy for sparse training conditions but achieves state-of-the-art F1
of 0.97 on ATIS with approximately 1/10th of the training data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 6 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Linear attention is (maybe) all you need (to understand <span class="highlight-title">transformer</span>
  optimization) <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.01082v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.01082v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kwangjun Ahn, Xiang Cheng, Minhak Song, Chulhee Yun, Ali Jadbabaie, Suvrit Sra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer training is notoriously difficult, requiring a careful design of
optimizers and use of various heuristics. We make progress towards
understanding the subtleties of training Transformers by carefully studying a
simple yet canonical linearized shallow Transformer model. Specifically, we
train linear Transformers to solve regression tasks, inspired by J.~von Oswald
et al.~(ICML 2023), and K.~Ahn et al.~(NeurIPS 2023). Most importantly, we
observe that our proposed linearized models can reproduce several prominent
aspects of Transformer training dynamics. Consequently, the results obtained in
this paper suggest that a simple linearized Transformer model could actually be
a valuable, realistic abstraction for understanding Transformer optimization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AGI: Artificial General Intelligence for Education 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.12479v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.12479v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ehsan Latif, Gengchen Mai, Matthew Nyaaba, Xuansheng Wu, Ninghao Liu, Guoyu Lu, Sheng Li, Tianming Liu, Xiaoming Zhai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial general intelligence (AGI) has gained global recognition as a
future technology due to the emergence of breakthrough large language models
and chatbots such as GPT-4 and ChatGPT, respectively. Compared to conventional
AI models, typically designed for a limited range of tasks, demand significant
amounts of domain-specific data for training and may not always consider
intricate interpersonal dynamics in education. AGI, driven by the recent large
pre-trained models, represents a significant leap in the capability of machines
to perform tasks that require human-level intelligence, such as reasoning,
problem-solving, decision-making, and even understanding human emotions and
social interactions. This position paper reviews AGI's key concepts,
capabilities, scope, and potential within future education, including achieving
future educational goals, designing pedagogy and curriculum, and performing
assessments. It highlights that AGI can significantly improve intelligent
tutoring systems, educational assessment, and evaluation procedures. AGI
systems can adapt to individual student needs, offering tailored learning
experiences. They can also provide comprehensive feedback on student
performance and dynamically adjust teaching methods based on student progress.
The paper emphasizes that AGI's capabilities extend to understanding human
emotions and social interactions, which are critical in educational settings.
The paper discusses that ethical issues in education with AGI include data
bias, fairness, and privacy and emphasizes the need for codes of conduct to
ensure responsible AGI use in academic settings like homework, teaching, and
recruitment. We also conclude that the development of AGI necessitates
interdisciplinary collaborations between educators and AI engineers to advance
research and application efforts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Position Paper on AGI for Education, Submitted to Technology and
  Society</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Referential communication in heterogeneous communities of <span class="highlight-title">pre-train</span>ed
  visual deep networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.08913v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.08913v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matéo Mahaut, Francesca Franzon, Roberto Dessì, Marco Baroni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As large pre-trained image-processing neural networks are being embedded in
autonomous agents such as self-driving cars or robots, the question arises of
how such systems can communicate with each other about the surrounding world,
despite their different architectures and training regimes. As a first step in
this direction, we systematically explore the task of \textit{referential
communication} in a community of heterogeneous state-of-the-art pre-trained
visual networks, showing that they can develop, in a self-supervised way, a
shared protocol to refer to a target object among a set of candidates. This
shared protocol can also be used, to some extent, to communicate about
previously unseen object categories of different granularity. Moreover, a
visual network that was not initially part of an existing community can learn
the community's protocol with remarkable ease. Finally, we study, both
qualitatively and quantitatively, the properties of the emergent protocol,
providing some evidence that it is capturing high-level semantic features of
objects.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MuseGraph: Graph-oriented Instruction Tuning of Large Language Models
  for Generic Graph Mining 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.04780v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.04780v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanchao Tan, Hang Lv, Xinyi Huang, Jiawei Zhang, Shiping Wang, Carl Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graphs with abundant attributes are essential in modeling interconnected
entities and improving predictions in various real-world applications.
Traditional Graph Neural Networks (GNNs), which are commonly used for modeling
attributed graphs, need to be re-trained every time when applied to different
graph tasks and datasets. Although the emergence of Large Language Models
(LLMs) has introduced a new paradigm in natural language processing, the
generative potential of LLMs in graph mining remains largely under-explored. To
this end, we propose a novel framework MuseGraph, which seamlessly integrates
the strengths of GNNs and LLMs and facilitates a more effective and generic
approach for graph mining across different tasks and datasets. Specifically, we
first introduce a compact graph description via the proposed adaptive input
generation to encapsulate key information from the graph under the constraints
of language token limitations. Then, we propose a diverse instruction
generation mechanism, which distills the reasoning capabilities from LLMs
(e.g., GPT-4) to create task-specific Chain-of-Thought-based instruction
packages for different graph tasks. Finally, we propose a graph-aware
instruction tuning with a dynamic instruction package allocation strategy
across tasks and datasets, ensuring the effectiveness and generalization of the
training process. Our experimental results demonstrate significant improvements
in different graph tasks, showcasing the potential of our MuseGraph in
enhancing the accuracy of graph-oriented downstream tasks while keeping the
generation powers of LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ In-Context Learning Learns Label Relationships but Is Not Conventional
  Learning <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.12375v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.12375v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jannik Kossen, Yarin Gal, Tom Rainforth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The predictions of Large Language Models (LLMs) on downstream tasks often
improve significantly when including examples of the input--label relationship
in the context. However, there is currently no consensus about how this
in-context learning (ICL) ability of LLMs works. For example, while Xie et al.
(2021) liken ICL to a general-purpose learning algorithm, Min et al. (2022)
argue ICL does not even learn label relationships from in-context examples. In
this paper, we provide novel insights into how ICL leverages label information,
revealing both capabilities and limitations. To ensure we obtain a
comprehensive picture of ICL behavior, we study probabilistic aspects of ICL
predictions and thoroughly examine the dynamics of ICL as more examples are
provided. Our experiments show that ICL predictions almost always depend on
in-context labels and that ICL can learn truly novel tasks in-context. However,
we also find that ICL struggles to fully overcome prediction preferences
acquired from pre-training data and, further, that ICL does not consider all
in-context information equally.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Imitate the Good and Avoid the Bad: An Incremental Approach to Safe
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.10385v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.10385v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huy Hoang, Tien Mai, Pradeep Varakantham
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A popular framework for enforcing safe actions in Reinforcement Learning (RL)
is Constrained RL, where trajectory based constraints on expected cost (or
other cost measures) are employed to enforce safety and more importantly these
constraints are enforced while maximizing expected reward. Most recent
approaches for solving Constrained RL convert the trajectory based cost
constraint into a surrogate problem that can be solved using minor
modifications to RL methods. A key drawback with such approaches is an over or
underestimation of the cost constraint at each state. Therefore, we provide an
approach that does not modify the trajectory based cost constraint and instead
imitates ``good'' trajectories and avoids ``bad'' trajectories generated from
incrementally improving policies. We employ an oracle that utilizes a reward
threshold (which is varied with learning) and the overall cost constraint to
label trajectories as ``good'' or ``bad''. A key advantage of our approach is
that we are able to work from any starting policy or set of trajectories and
improve on it. In an exhaustive set of experiments, we demonstrate that our
approach is able to outperform top benchmark approaches for solving Constrained
RL problems, with respect to expected cost, CVaR cost, or even unknown cost
constraints.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Class Incremental Learning via Likelihood Ratio Based Task Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.15048v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.15048v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haowei Lin, Yijia Shao, Weinan Qian, Ningxin Pan, Yiduo Guo, Bing Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Class incremental learning (CIL) is a challenging setting of continual
learning, which learns a series of tasks sequentially. Each task consists of a
set of unique classes. The key feature of CIL is that no task identifier (or
task-id) is provided at test time. Predicting the task-id for each test sample
is a challenging problem. An emerging theory-guided approach (called TIL+OOD)
is to train a task-specific model for each task in a shared network for all
tasks based on a task-incremental learning (TIL) method to deal with
catastrophic forgetting. The model for each task is an out-of-distribution
(OOD) detector rather than a conventional classifier. The OOD detector can
perform both within-task (in-distribution (IND)) class prediction and OOD
detection. The OOD detection capability is the key to task-id prediction during
inference. However, this paper argues that using a traditional OOD detector for
task-id prediction is sub-optimal because additional information (e.g., the
replay data and the learned tasks) available in CIL can be exploited to design
a better and principled method for task-id prediction. We call the new method
TPL (Task-id Prediction based on Likelihood Ratio). TPL markedly outperforms
strong CIL baselines and has negligible catastrophic forgetting. The code of
TPL is publicly available at https://github.com/linhaowei1/TPL.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Medical Multimodal-Multitask Foundation Model for Superior Chest CT
  Performance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.02649v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.02649v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuang Niu, Qing Lyu, Christopher D. Carothers, Parisa Kaviani, Josh Tan, Pingkun Yan, Mannudeep K. Kalra, Christopher T. Whitlow, Ge Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Patient management requires multitasking interaction with multimodal data.
While today's AI, particularly large foundation models, promises unprecedented
opportunities, progress remains relatively slow in developing medical
multimodal multitask foundation models. There are two main challenges along
this direction: the data challenge -- the high bar to curate medical multimodal
multitask datasets including 3D medical tomographic images in alignment with
other clinical datasets, and the model challenge -- the unavailability of a
scalable and adaptable foundation model architecture to synergize multimodal
datasets for diverse clinical tasks. Here we propose the first-of-its-kind
medical multimodal-multitask foundation model (M3FM) with an emphasis on lung
cancer screening. To train our M3FM, we first curated a comprehensive
multimodal multitask dataset consisting of 163,725 3D chest CT exams, 48
clinical data types, and 17 medical tasks on lung, heart, and other chest
diseases. Then, we created and applied a multimodal question-answering
framework as a unified training strategy to effectively integrate multimodal
information and naturally perform multiple tasks with free-text prompting.
Extensive experimental results demonstrate that M3FM consistently outperforms
the previous state-of-the-art models. M3FM can identify informative multimodal
data elements that are relevant to specific clinical tasks, being instrumental
in building AI models and gaining insights into correlations among multimodal
data and diseases. M3FM can be adapted to boost the performance of new tasks
with a small out-of-distribution dataset. M3FM has enabled superior volumetric
CT imaging performance for lung cancer screening, cardiac disease prediction,
and other CT-related tasks. M3FM can be extended to incorporate more data types
and improve other medical tasks, towards AI-empowered precise and efficient
medicine.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Spectral Meets Spatial: Harmonising 3D Shape Matching and Interpolation <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.18920v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.18920v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongliang Cao, Marvin Eisenberger, Nafie El Amrani, Daniel Cremers, Florian Bernard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although 3D shape matching and interpolation are highly interrelated, they
are often studied separately and applied sequentially to relate different 3D
shapes, thus resulting in sub-optimal performance. In this work we present a
unified framework to predict both point-wise correspondences and shape
interpolation between 3D shapes. To this end, we combine the deep functional
map framework with classical surface deformation models to map shapes in both
spectral and spatial domains. On the one hand, by incorporating spatial maps,
our method obtains more accurate and smooth point-wise correspondences compared
to previous functional map methods for shape matching. On the other hand, by
introducing spectral maps, our method gets rid of commonly used but
computationally expensive geodesic distance constraints that are only valid for
near-isometric shape deformations. Furthermore, we propose a novel test-time
adaptation scheme to capture both pose-dominant and shape-dominant
deformations. Using different challenging datasets, we demonstrate that our
method outperforms previous state-of-the-art methods for both shape matching
and interpolation, even compared to supervised approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by CVPR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Contrastive Explanations of Centralized Multi-agent Optimization
  Solutions <span class="chip">ICAPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.05984v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.05984v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Parisa Zehtabi, Alberto Pozanco, Ayala Bloch, Daniel Borrajo, Sarit Kraus
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In many real-world scenarios, agents are involved in optimization problems.
Since most of these scenarios are over-constrained, optimal solutions do not
always satisfy all agents. Some agents might be unhappy and ask questions of
the form ``Why does solution $S$ not satisfy property $P$?''. We propose CMAoE,
a domain-independent approach to obtain contrastive explanations by: (i)
generating a new solution $S^\prime$ where property $P$ is enforced, while also
minimizing the differences between $S$ and $S^\prime$; and (ii) highlighting
the differences between the two solutions, with respect to the features of the
objective function of the multi-agent system. Such explanations aim to help
agents understanding why the initial solution is better in the context of the
multi-agent system than what they expected. We have carried out a computational
evaluation that shows that CMAoE can generate contrastive explanations for
large multi-agent optimization problems. We have also performed an extensive
user study in four different domains that shows that: (i) after being presented
with these explanations, humans' satisfaction with the original solution
increases; and (ii) the constrastive explanations generated by CMAoE are
preferred or equally preferred by humans over the ones generated by state of
the art approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper accepted at ICAPS 2024. This is a extended version that
  includes Supplementary Material</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Jellyfish: A Large Language Model for Data Preprocessing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.01678v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.01678v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haochen Zhang, Yuyang Dong, Chuan Xiao, Masafumi Oyamada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper explores the utilization of LLMs for data preprocessing (DP), a
crucial step in the data mining pipeline that transforms raw data into a clean
format conducive to easy processing. Whereas the use of LLMs has sparked
interest in devising universal solutions to DP, recent initiatives in this
domain typically rely on GPT APIs, raising inevitable data breach concerns.
Unlike these approaches, we consider instruction-tuning local LLMs (7 - 13B
models) as universal DP ask solver. We select a collection of datasets across
four representative DP tasks and construct instruction-tuning data using
serialization and knowledge injection techniques tailored to DP. As such, the
instruction-tuned LLMs empower users to manually craft instructions for DP.
Meanwhile, they can operate on a local, single, and low-priced GPU, ensuring
data security and enabling further tuning. Our experiments show that our
dataset constructed for DP instruction tuning, namely Jellyfish, effectively
enhances LLMs' DP performances and barely compromises their abilities in NLP
tasks. By tuning Mistral-7B and OpenOrca-Platypus2-13B with Jellyfish, the
models deliver competitiveness compared to state-of-the-art DP methods and
strong generalizability to unseen tasks. The models' performance rivals that of
GPT series models, and the interpretation offers enhanced reasoning
capabilities compared to GPT-3.5. The 7B and 13B Jellyfish models are available
at Hugging Face: https://huggingface.co/NECOUDBFM/Jellyfish-7B
https://huggingface.co/NECOUDBFM/Jellyfish-13B
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>a.k.a. "Jellyfish: Instruction-Tuning Local Large Language Models for
  Data Preprocessing''</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beyond Memorization: The Challenge of Random Memory Access in Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07805v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07805v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tongyao Zhu, Qian Liu, Liang Pang, Zhengbao Jiang, Min-Yen Kan, Min Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent developments in Language Models (LMs) have shown their effectiveness
in NLP tasks, particularly in knowledge-intensive tasks. However, the
mechanisms underlying knowledge storage and memory access within their
parameters remain elusive. In this paper, we investigate whether a generative
LM (e.g., GPT-2) is able to access its memory sequentially or randomly. Through
carefully-designed synthetic tasks, covering the scenarios of full recitation,
selective recitation and grounded question answering, we reveal that LMs manage
to sequentially access their memory while encountering challenges in randomly
accessing memorized content. We find that techniques including recitation and
permutation improve the random memory access capability of LMs. Furthermore, by
applying this intervention to realistic scenarios of open-domain question
answering, we validate that enhancing random access by recitation leads to
notable improvements in question answering. The code to reproduce our
experiments can be found at https://github.com/sail-sg/lm-random-memory-access.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 figures; fixed typos</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Retentive or Forgetful? Diving into the Knowledge Memorizing Mechanism
  of Language Models <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.09144v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.09144v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boxi Cao, Qiaoyu Tang, Hongyu Lin, Shanshan Jiang, Bin Dong, Xianpei Han, Jiawei Chen, Tianshu Wang, Le Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Memory is one of the most essential cognitive functions serving as a
repository of world knowledge and episodes of activities. In recent years,
large-scale pre-trained language models have shown remarkable memorizing
ability. On the contrary, vanilla neural networks without pre-training have
been long observed suffering from the catastrophic forgetting problem. To
investigate such a retentive-forgetful contradiction and understand the memory
mechanism of language models, we conduct thorough experiments by controlling
the target knowledge types, the learning strategies and the learning schedules.
We find that: 1) Vanilla language models are forgetful; 2) Pre-training leads
to retentive language models; 3) Knowledge relevance and diversification
significantly influence the memory formation. These conclusions are useful for
understanding the abilities of pre-trained language models and shed light on
designing and evaluating new learning and inference algorithms of language
models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SSMG: Spatial-Semantic Map Guided Diffusion Model for Free-form
  Layout-to-Image Generation <span class="chip">AAAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.10156v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.10156v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengyou Jia, Minnan Luo, Zhuohang Dang, Guang Dai, Xiaojun Chang, Mengmeng Wang, Jingdong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite significant progress in Text-to-Image (T2I) generative models, even
lengthy and complex text descriptions still struggle to convey detailed
controls. In contrast, Layout-to-Image (L2I) generation, aiming to generate
realistic and complex scene images from user-specified layouts, has risen to
prominence. However, existing methods transform layout information into tokens
or RGB images for conditional control in the generative process, leading to
insufficient spatial and semantic controllability of individual instances. To
address these limitations, we propose a novel Spatial-Semantic Map Guided
(SSMG) diffusion model that adopts the feature map, derived from the layout, as
guidance. Owing to rich spatial and semantic information encapsulated in
well-designed feature maps, SSMG achieves superior generation quality with
sufficient spatial and semantic controllability compared to previous works.
Additionally, we propose the Relation-Sensitive Attention (RSA) and
Location-Sensitive Attention (LSA) mechanisms. The former aims to model the
relationships among multiple objects within scenes while the latter is designed
to heighten the model's sensitivity to the spatial information embedded in the
guidance. Extensive experiments demonstrate that SSMG achieves highly promising
results, setting a new state-of-the-art across a range of metrics encompassing
fidelity, diversity, and controllability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to AAAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Lowering Detection in Sport Climbing Based on Orientation of the Sensor
  Enhanced Quickdraw 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.10164v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.10164v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sadaf Moaveninejad, Andrea Janes, Camillo Porcaro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tracking climbers' activity to improve services and make the best use of
their infrastructure is a concern for climbing gyms. Each climbing session must
be analyzed from beginning till lowering of the climber. Therefore, spotting
the climbers descending is crucial since it indicates when the ascent has come
to an end. This problem must be addressed while preserving privacy and
convenience of the climbers and the costs of the gyms. To this aim, a hardware
prototype is developed to collect data using accelerometer sensors attached to
a piece of climbing equipment mounted on the wall, called quickdraw, that
connects the climbing rope to the bolt anchors. The corresponding sensors are
configured to be energy-efficient, hence become practical in terms of expenses
and time consumption for replacement when using in large quantity in a climbing
gym. This paper describes hardware specifications, studies data measured by the
sensors in ultra-low power mode, detect sensors' orientation patterns during
lowering different routes, and develop an supervised approach to identify
lowering.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2211.02680</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Equipping Computational Pathology Systems with Artifact Processing
  Pipelines: A Showcase for Computation and Performance Trade-offs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07743v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07743v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Neel Kanwal, Farbod Khoraminia, Umay Kiraz, Andres Mosquera-Zamudio, Carlos Monteagudo, Emiel A. M. Janssen, Tahlita C. M. Zuiverloon, Chunmig Rong, Kjersti Engan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Histopathology is a gold standard for cancer diagnosis under a microscopic
examination. However, histological tissue processing procedures result in
artifacts, which are ultimately transferred to the digitized version of glass
slides, known as whole slide images (WSIs). Artifacts are diagnostically
irrelevant areas and may result in wrong deep learning (DL) algorithms
predictions. Therefore, detecting and excluding artifacts in the computational
pathology (CPATH) system is essential for reliable automated diagnosis. In this
paper, we propose a mixture of experts (MoE) scheme for detecting five notable
artifacts, including damaged tissue, blur, folded tissue, air bubbles, and
histologically irrelevant blood from WSIs. First, we train independent binary
DL models as experts to capture particular artifact morphology. Then, we
ensemble their predictions using a fusion mechanism. We apply probabilistic
thresholding over the final probability distribution to improve the sensitivity
of the MoE. We developed DL pipelines using two MoEs and two multiclass models
of state-of-the-art deep convolutional neural networks (DCNNs) and vision
transformers (ViTs). DCNNs-based MoE and ViTs-based MoE schemes outperformed
simpler multiclass models and were tested on datasets from different hospitals
and cancer types, where MoE using DCNNs yielded the best results. The proposed
MoE yields 86.15% F1 and 97.93% sensitivity scores on unseen data, retaining
less computational cost for inference than MoE using ViTs. This best
performance of MoEs comes with relatively higher computational trade-offs than
multiclass models. The proposed artifact detection pipeline will not only
ensure reliable CPATH predictions but may also provide quality control.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to BMC Medical Informatics and Decision Making Journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beyond Gradient and Priors in Privacy Attacks: Leveraging Pooler Layer
  Inputs of Language Models in Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.05720v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.05720v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianwei Li, Sheng Liu, Qi Lei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models trained via federated learning (FL) demonstrate impressive
capabilities in handling complex tasks while protecting user privacy. Recent
studies indicate that leveraging gradient information and prior knowledge can
potentially reveal training samples within FL setting. However, these
investigations have overlooked the potential privacy risks tied to the
intrinsic architecture of the models. This paper presents a two-stage privacy
attack strategy that targets the vulnerabilities in the architecture of
contemporary language models, significantly enhancing attack performance by
initially recovering certain feature directions as additional supervisory
signals. Our comparative experiments demonstrate superior attack performance
across various datasets and scenarios, highlighting the privacy leakage risk
associated with the increasingly complex architectures of language models. We
call for the community to recognize and address these potential privacy risks
in designing large language models.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Operating Systems
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Physical Memory Attacks and a Memory Safe Management System for Memory
  Defense <span class="chip">SC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08656v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08656v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alon Hillel-Tuch, Aspen Olmstead
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Programming errors, defective hardware components (such as hard disk spindle
defects), and environmental hazards can lead to invalid memory operations. In
addition, less predictable forms of environmental stress, such as radiation,
thermal influence, and energy fluctuations, can induce hardware faults.
Sometimes, a soft error can occur instead of a complete failure, such as a
bit-flip. The 'natural' factors that can cause bit-flips are replicable through
targeted attacks that result in significant compromises, including full
privileged system access. Existing physical defense solutions have consistently
been circumvented shortly after deployment. We will explore the concept of a
novel software-based low-level layer that can protect vulnerable memory
targeted by physical attack vectors related to bit-flip vulnerabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Computer Science, Computer Engineering, and Applied Computing (CSCE)
  Conference 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OSMOSIS: Enabling Multi-Tenancy in Datacenter SmartNICs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.03628v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.03628v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mikhail Khalilov, Marcin Chrapek, Siyuan Shen, Alessandro Vezzu, Thomas Benz, Salvatore Di Girolamo, Timo Schneider, Daniele De Sensi, Luca Benini, Torsten Hoefler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-tenancy is essential for unleashing SmartNIC's potential in
datacenters. Our systematic analysis in this work shows that existing on-path
SmartNICs have resource multiplexing limitations. For example, existing
solutions lack multi-tenancy capabilities such as performance isolation and QoS
provisioning for compute and IO resources. Compared to standard NIC data paths
with a well-defined set of offloaded functions, unpredictable execution times
of SmartNIC kernels make conventional approaches for multi-tenancy and QoS
insufficient. We fill this gap with OSMOSIS, a SmartNICs resource manager
co-design. OSMOSIS extends existing OS mechanisms to enable dynamic hardware
resource multiplexing of the on-path packet processing data plane. We integrate
OSMOSIS within an open-source RISC-V-based 400Gbit/s SmartNIC. Our performance
results demonstrate that OSMOSIS fully supports multi-tenancy and enables
broader adoption of SmartNICs in datacenters with low overhead.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 14 figures, 103 references</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Databases
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ COSTREAM: Learned Cost Models for Operator Placement in Edge-Cloud
  Environments <span class="chip">ICDE 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08444v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08444v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Roman Heinrich, Carsten Binnig, Harald Kornmayer, Manisha Luthra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we present COSTREAM, a novel learned cost model for Distributed
Stream Processing Systems that provides accurate predictions of the execution
costs of a streaming query in an edge-cloud environment. The cost model can be
used to find an initial placement of operators across heterogeneous hardware,
which is particularly important in these environments. In our evaluation, we
demonstrate that COSTREAM can produce highly accurate cost estimates for the
initial operator placement and even generalize to unseen placements, queries,
and hardware. When using COSTREAM to optimize the placements of streaming
operators, a median speed-up of around 21x can be achieved compared to
baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted by IEEE ICDE 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Translating between SQL Dialects for Cloud Migration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08375v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08375v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ran Zmigrod, Salwa Alamir, Xiaomo Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Migrations of systems from on-site premises to the cloud has been a
fundamental endeavor by many industrial institutions. A crucial component of
such cloud migrations is the transition of databases to be hosted online. In
this work, we consider the difficulties of this migration for SQL databases.
While SQL is one of the prominent methods for storing database procedures,
there are a plethora of different SQL dialects (e.g., MySQL, Postgres, etc.)
which can complicate migrations when the on-premise SQL dialect differs to the
dialect hosted on the cloud. Tools exist by common cloud provides such as AWS
and Azure to aid in translating between dialects in order to mitigate the
majority of the difficulties. However, these tools do not successfully
translate $100\%$ of the code. Consequently, software engineers must manually
convert the remainder of the untranslated database. For large organizations,
this task quickly becomes intractable and so more innovative solutions are
required. We consider this challenge a novel yet vital industrial research
problem for any large corporation that is considering cloud migrations.
Furthermore, we introduce potential avenues of research to tackle this
challenge that have yielded promising preliminary results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reconciling Conflicting Data Curation Actions: Transparency Through
  Argumentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08257v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08257v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yilin Xia, Shawn Bowers, Lan Li, Bertram Ludäscher
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a new approach for modeling and reconciling conflicting data
cleaning actions. Such conflicts arise naturally in collaborative data curation
settings where multiple experts work independently and then aim to put their
efforts together to improve and accelerate data cleaning. The key idea of our
approach is to model conflicting updates as a formal \emph{argumentation
framework}(AF). Such argumentation frameworks can be automatically analyzed and
solved by translating them to a logic program $P_{AF}$ whose declarative
semantics yield a transparent solution with many desirable properties, e.g.,
uncontroversial updates are accepted, unjustified ones are rejected, and the
remaining ambiguities are exposed and presented to users for further analysis.
After motivating the problem, we introduce our approach and illustrate it with
a detailed running example introducing both well-founded and stable semantics
to help understand the AF solutions. We have begun to develop open source tools
and Jupyter notebooks that demonstrate the practicality of our approach. In
future work we plan to develop a toolkit for conflict resolution that can be
used in conjunction with OpenRefine, a popular interactive data cleaning tool.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IDCC 2024. Source code is available at
  https://github.com/idaks/Games-and-Argumentation/tree/idcc</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PathFinder: A unified approach for handling paths in graph query
  languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02194v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02194v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benjamín Farías, Wim Martens, Carlos Rojas, Domagoj Vrgoč
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Path queries are a core feature of modern graph query languages such as
Cypher, SQL/PGQ, and GQL. These languages provide a rich set of features for
matching paths, such as restricting to certain path modes (shortest, simple,
trail) and constraining the edge labels along the path by a regular expression.
In this paper we present PathFinder, a unifying approach for dealing with path
queries in all these query languages. PathFinder leverages a compact
representation of the (potentially exponential number of) paths that can match
a given query, extends it with pipelined execution, and supports all commonly
used path modes. In the paper we describe the algorithmic backbone of
PathFinder, provide a reference implementation, and test it over a large set of
real-world queries and datasets. Our results show that PathFinder exhibits very
stable behavior, even on large data and complex queries, and its performance is
an order of magnitude better than that of many modern graph engines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Jellyfish: A Large Language Model for Data Preprocessing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.01678v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.01678v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haochen Zhang, Yuyang Dong, Chuan Xiao, Masafumi Oyamada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper explores the utilization of LLMs for data preprocessing (DP), a
crucial step in the data mining pipeline that transforms raw data into a clean
format conducive to easy processing. Whereas the use of LLMs has sparked
interest in devising universal solutions to DP, recent initiatives in this
domain typically rely on GPT APIs, raising inevitable data breach concerns.
Unlike these approaches, we consider instruction-tuning local LLMs (7 - 13B
models) as universal DP ask solver. We select a collection of datasets across
four representative DP tasks and construct instruction-tuning data using
serialization and knowledge injection techniques tailored to DP. As such, the
instruction-tuned LLMs empower users to manually craft instructions for DP.
Meanwhile, they can operate on a local, single, and low-priced GPU, ensuring
data security and enabling further tuning. Our experiments show that our
dataset constructed for DP instruction tuning, namely Jellyfish, effectively
enhances LLMs' DP performances and barely compromises their abilities in NLP
tasks. By tuning Mistral-7B and OpenOrca-Platypus2-13B with Jellyfish, the
models deliver competitiveness compared to state-of-the-art DP methods and
strong generalizability to unseen tasks. The models' performance rivals that of
GPT series models, and the interpretation offers enhanced reasoning
capabilities compared to GPT-3.5. The 7B and 13B Jellyfish models are available
at Hugging Face: https://huggingface.co/NECOUDBFM/Jellyfish-7B
https://huggingface.co/NECOUDBFM/Jellyfish-13B
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>a.k.a. "Jellyfish: Instruction-Tuning Local Large Language Models for
  Data Preprocessing''</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Simple and Scalable Strategies to Continually <span class="highlight-title">Pre-train</span> Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08763v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08763v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adam Ibrahim, Benjamin Thérien, Kshitij Gupta, Mats L. Richter, Quentin Anthony, Timothée Lesort, Eugene Belilovsky, Irina Rish
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are routinely pre-trained on billions of tokens,
only to start the process over again once new data becomes available. A much
more efficient solution is to continually pre-train these models, saving
significant compute compared to re-training. However, the distribution shift
induced by new data typically results in degraded performance on previous data
or poor adaptation to the new data. In this work, we show that a simple and
scalable combination of learning rate (LR) re-warming, LR re-decaying, and
replay of previous data is sufficient to match the performance of fully
re-training from scratch on all available data, as measured by final loss and
language model (LM) evaluation benchmarks. Specifically, we show this for a
weak but realistic distribution shift between two commonly used LLM
pre-training datasets (English$\rightarrow$English) and a stronger distribution
shift (English$\rightarrow$German) at the $405$M parameter model scale with
large dataset sizes (hundreds of billions of tokens). Selecting the weak but
realistic shift for larger-scale experiments, we also find that our continual
learning strategies match the re-training baseline for a 10B parameter LLM. Our
results demonstrate that LLMs can be successfully updated via simple and
scalable continual learning strategies, matching the re-training baseline using
only a fraction of the compute. Finally, inspired by previous work, we propose
alternatives to the cosine learning rate schedule that help circumvent
forgetting induced by LR re-warming and that are not bound to a fixed token
budget.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Combinatorial Optimization via Heat Diffusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08757v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08757v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hengyuan Ma, Wenlian Lu, Jianfeng Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Combinatorial optimization problems are widespread but inherently challenging
due to their discrete nature.The primary limitation of existing methods is that
they can only access a small fraction of the solution space at each iteration,
resulting in limited efficiency for searching the global optimal. To overcome
this challenge, diverging from conventional efforts of expanding the solver's
search scope, we focus on enabling information to actively propagate to the
solver through heat diffusion. By transforming the target function while
preserving its optima, heat diffusion facilitates information flow from distant
regions to the solver, providing more efficient navigation. Utilizing heat
diffusion, we propose a framework for solving general combinatorial
optimization problems. The proposed methodology demonstrates superior
performance across a range of the most challenging and widely encountered
combinatorial optimizations. Echoing recent advancements in harnessing
thermodynamics for generative artificial intelligence, our study further
reveals its significant potential in advancing combinatorial optimization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DAM: Dynamic Adapter Merging for Continual Video QA Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08755v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08755v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Feng Cheng, Ziyang Wang, Yi-Lin Sung, Yan-Bo Lin, Mohit Bansal, Gedas Bertasius
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a parameter-efficient method for continual video
question-answering (VidQA) learning. Our method, named DAM, uses the proposed
Dynamic Adapter Merging to (i) mitigate catastrophic forgetting, (ii) enable
efficient adaptation to continually arriving datasets, (iii) handle inputs from
unknown datasets during inference, and (iv) enable knowledge sharing across
similar dataset domains. Given a set of continually streaming VidQA datasets,
we sequentially train dataset-specific adapters for each dataset while freezing
the parameters of a large pretrained video-language backbone. During inference,
given a video-question sample from an unknown domain, our method first uses the
proposed non-parametric router function to compute a probability for each
adapter, reflecting how relevant that adapter is to the current video-question
input instance. Subsequently, the proposed dynamic adapter merging scheme
aggregates all the adapter weights into a new adapter instance tailored for
that particular test sample to compute the final VidQA prediction, mitigating
the impact of inaccurate router predictions and facilitating knowledge sharing
across domains. Our DAM model outperforms prior state-of-the-art continual
learning approaches by 9.1% while exhibiting 1.9% less forgetting on 6 VidQA
datasets spanning various domains. We further extend DAM to continual image
classification and image QA and outperform prior methods by a large margin. The
code is publicly available at: https://github.com/klauscc/DAM
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The first two authors contribute equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural reproducing kernel Banach spaces and representer theorems for
  deep networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08750v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08750v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francesca Bartolucci, Ernesto De Vito, Lorenzo Rosasco, Stefano Vigogna
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Studying the function spaces defined by neural networks helps to understand
the corresponding learning models and their inductive bias. While in some
limits neural networks correspond to function spaces that are reproducing
kernel Hilbert spaces, these regimes do not capture the properties of the
networks used in practice. In contrast, in this paper we show that deep neural
networks define suitable reproducing kernel Banach spaces.
  These spaces are equipped with norms that enforce a form of sparsity,
enabling them to adapt to potential latent structures within the input data and
their representations. In particular, leveraging the theory of reproducing
kernel Banach spaces, combined with variational results, we derive representer
theorems that justify the finite architectures commonly employed in
applications. Our study extends analogous results for shallow networks and can
be seen as a step towards considering more practically plausible neural
architectures.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Steering LLMs Towards Unbiased Responses: A Causality-Guided Debiasing
  Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08743v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08743v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingling Li, Zeyu Tang, Xiaoyu Liu, Peter Spirtes, Kun Zhang, Liu Leqi, Yang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) can easily generate biased and discriminative
responses. As LLMs tap into consequential decision-making (e.g., hiring and
healthcare), it is of crucial importance to develop strategies to mitigate
these biases. This paper focuses on social bias, tackling the association
between demographic information and LLM outputs. We propose a causality-guided
debiasing framework that utilizes causal understandings of (1) the
data-generating process of the training corpus fed to LLMs, and (2) the
internal reasoning process of LLM inference, to guide the design of prompts for
debiasing LLM outputs through selection mechanisms. Our framework unifies
existing de-biasing prompting approaches such as inhibitive instructions and
in-context contrastive examples, and sheds light on new ways of debiasing by
encouraging bias-free reasoning. Our strong empirical performance on real-world
datasets demonstrates that our framework provides principled guidelines on
debiasing LLM outputs even with only the black-box access.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning How to Strategically Disclose Information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08741v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08741v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raj Kiriti Velicheti, Melih Bastopcu, S. Rasoul Etesami, Tamer Başar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Strategic information disclosure, in its simplest form, considers a game
between an information provider (sender) who has access to some private
information that an information receiver is interested in. While the receiver
takes an action that affects the utilities of both players, the sender can
design information (or modify beliefs) of the receiver through signal
commitment, hence posing a Stackelberg game. However, obtaining a Stackelberg
equilibrium for this game traditionally requires the sender to have access to
the receiver's objective. In this work, we consider an online version of
information design where a sender interacts with a receiver of an unknown type
who is adversarially chosen at each round. Restricting attention to Gaussian
prior and quadratic costs for the sender and the receiver, we show that
$\mathcal{O}(\sqrt{T})$ regret is achievable with full information feedback,
where $T$ is the total number of interactions between the sender and the
receiver. Further, we propose a novel parametrization that allows the sender to
achieve $\mathcal{O}(\sqrt{T})$ regret for a general convex utility function.
We then consider the Bayesian Persuasion problem with an additional cost term
in the objective function, which penalizes signaling policies that are more
informative and obtain $\mathcal{O}(\log(T))$ regret. Finally, we establish a
sublinear regret bound for the partial information feedback setting and provide
simulations to support our theoretical results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ambient Diffusion Posterior Sampling: Solving Inverse Problems with
  Diffusion Models trained on Corrupted Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08728v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08728v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Asad Aali, Giannis Daras, Brett Levac, Sidharth Kumar, Alexandros G. Dimakis, Jonathan I. Tamir
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We provide a framework for solving inverse problems with diffusion models
learned from linearly corrupted data. Our method, Ambient Diffusion Posterior
Sampling (A-DPS), leverages a generative model pre-trained on one type of
corruption (e.g. image inpainting) to perform posterior sampling conditioned on
measurements from a potentially different forward process (e.g. image
blurring). We test the efficacy of our approach on standard natural image
datasets (CelebA, FFHQ, and AFHQ) and we show that A-DPS can sometimes
outperform models trained on clean data for several image restoration tasks in
both speed and performance. We further extend the Ambient Diffusion framework
to train MRI models with access only to Fourier subsampled multi-coil MRI
measurements at various acceleration factors (R=2, 4, 6, 8). We again observe
that models trained on highly subsampled data are better priors for solving
inverse problems in the high acceleration regime than models trained on fully
sampled data. We open-source our code and the trained Ambient Diffusion MRI
models: https://github.com/utcsilab/ambient-diffusion-mri .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Pre-print, work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diffusion-based Iterative Counterfactual Explanations for Fetal
  Ultrasound Image Quality Assessment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08700v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08700v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paraskevas Pegios, Manxi Lin, Nina Weng, Morten Bo Søndergaard Svendsen, Zahra Bashir, Siavash Bigdeli, Anders Nymark Christensen, Martin Tolsgaard, Aasa Feragen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Obstetric ultrasound image quality is crucial for accurate diagnosis and
monitoring of fetal health. However, producing high-quality standard planes is
difficult, influenced by the sonographer's expertise and factors like the
maternal BMI or the fetus dynamics. In this work, we propose using
diffusion-based counterfactual explainable AI to generate realistic
high-quality standard planes from low-quality non-standard ones. Through
quantitative and qualitative evaluation, we demonstrate the effectiveness of
our method in producing plausible counterfactuals of increased quality. This
shows future promise both for enhancing training of clinicians by providing
visual feedback, as well as for improving image quality and, consequently,
downstream diagnosis and monitoring.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Implicit Regularization of Gradient Flow on One-Layer Softmax Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08699v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08699v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Heejune Sheen, Siyu Chen, Tianhao Wang, Harrison H. Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study gradient flow on the exponential loss for a classification problem
with a one-layer softmax attention model, where the key and query weight
matrices are trained separately. Under a separability assumption on the data,
we show that when gradient flow achieves the minimal loss value, it further
implicitly minimizes the nuclear norm of the product of the key and query
weight matrices. Such implicit regularization can be described by a Support
Vector Machine (SVM) problem with respect to the attention weights. This
finding contrasts with prior results showing that the gradient descent induces
an implicit regularization on the Frobenius norm on the product weight matrix
when the key and query matrices are combined into a single weight matrix for
training. For diagonal key and query matrices, our analysis builds upon the
reparameterization technique and exploits approximate KKT conditions of the SVM
associated with the classification data. Moreover, the results are extended to
general weights configurations given proper alignment of the weight matrices'
singular spaces with the data features at initialization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>34 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Digital Twin-assisted Reinforcement Learning for Resource-aware
  Microservice Offloading in Edge Computing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08687v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08687v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiangchun Chen, Jiannong Cao, Zhixuan Liang, Yuvraj Sahni, Mingjin Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Collaborative edge computing (CEC) has emerged as a promising paradigm,
enabling edge nodes to collaborate and execute microservices from end devices.
Microservice offloading, a fundamentally important problem, decides when and
where microservices are executed upon the arrival of services. However, the
dynamic nature of the real-world CEC environment often leads to inefficient
microservice offloading strategies, resulting in underutilized resources and
network congestion. To address this challenge, we formulate an online joint
microservice offloading and bandwidth allocation problem, JMOBA, to minimize
the average completion time of services. In this paper, we introduce a novel
microservice offloading algorithm, DTDRLMO, which leverages deep reinforcement
learning (DRL) and digital twin technology. Specifically, we employ digital
twin techniques to predict and adapt to changing edge node loads and network
conditions of CEC in real-time. Furthermore, this approach enables the
generation of an efficient offloading plan, selecting the most suitable edge
node for each microservice. Simulation results on real-world and synthetic
datasets demonstrate that DTDRLMO outperforms heuristic and learning-based
methods in average service completion time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ When can we Approximate Wide Contrastive Models with Neural Tangent
  Kernels and Principal Component Analysis? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08673v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08673v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gautham Govind Anil, Pascal Esser, Debarghya Ghoshdastidar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive learning is a paradigm for learning representations from
unlabelled data that has been highly successful for image and text data.
Several recent works have examined contrastive losses to claim that contrastive
models effectively learn spectral embeddings, while few works show relations
between (wide) contrastive models and kernel principal component analysis
(PCA). However, it is not known if trained contrastive models indeed correspond
to kernel methods or PCA. In this work, we analyze the training dynamics of
two-layer contrastive models, with non-linear activation, and answer when these
models are close to PCA or kernel methods. It is well known in the supervised
setting that neural networks are equivalent to neural tangent kernel (NTK)
machines, and that the NTK of infinitely wide networks remains constant during
training. We provide the first convergence results of NTK for contrastive
losses, and present a nuanced picture: NTK of wide networks remains almost
constant for cosine similarity based contrastive losses, but not for losses
based on dot product similarity. We further study the training dynamics of
contrastive models with orthogonality constraints on output layer, which is
implicitly assumed in works relating contrastive learning to spectral
embedding. Our deviation bounds suggest that representations learned by
contrastive models are close to the principal components of a certain matrix
computed from random features. We empirically show that our theoretical results
possibly hold beyond two-layer networks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Zero-shot and Few-shot Generation Strategies for Artificial Clinical
  Records 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08664v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08664v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Erlend Frayling, Jake Lever, Graham McDonald
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The challenge of accessing historical patient data for clinical research,
while adhering to privacy regulations, is a significant obstacle in medical
science. An innovative approach to circumvent this issue involves utilising
synthetic medical records that mirror real patient data without compromising
individual privacy. The creation of these synthetic datasets, particularly
without using actual patient data to train Large Language Models (LLMs),
presents a novel solution as gaining access to sensitive patient information to
train models is also a challenge. This study assesses the capability of the
Llama 2 LLM to create synthetic medical records that accurately reflect real
patient information, employing zero-shot and few-shot prompting strategies for
comparison against fine-tuned methodologies that do require sensitive patient
data during training. We focus on generating synthetic narratives for the
History of Present Illness section, utilising data from the MIMIC-IV dataset
for comparison. In this work introduce a novel prompting technique that
leverages a chain-of-thought approach, enhancing the model's ability to
generate more accurate and contextually relevant medical narratives without
prior fine-tuning. Our findings suggest that this chain-of-thought prompted
approach allows the zero-shot model to achieve results on par with those of
fine-tuned models, based on Rouge metrics evaluation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Self-Supervised</span> Learning for Covariance Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08662v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08662v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tzvi Diskin, Ami Wiesel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the use of deep learning for covariance estimation. We propose to
globally learn a neural network that will then be applied locally at inference
time. Leveraging recent advancements in self-supervised foundational models, we
train the network without any labeling by simply masking different samples and
learning to predict their covariance given their surrounding neighbors. The
architecture is based on the popular attention mechanism. Its main advantage
over classical methods is the automatic exploitation of global characteristics
without any distributional assumptions or regularization. It can be pre-trained
as a foundation model and then be repurposed for various downstream tasks,
e.g., adaptive target detection in radar or hyperspectral imagery.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Extracting Explanations, Justification, and Uncertainty from Black-Box
  Deep Neural Networks <span class="chip">SP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08652v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08652v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paul Ardis, Arjuna Flenner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Neural Networks (DNNs) do not inherently compute or exhibit
empirically-justified task confidence. In mission critical applications, it is
important to both understand associated DNN reasoning and its supporting
evidence. In this paper, we propose a novel Bayesian approach to extract
explanations, justifications, and uncertainty estimates from DNNs. Our approach
is efficient both in terms of memory and computation, and can be applied to any
black box DNN without any retraining, including applications to anomaly
detection and out-of-distribution detection tasks. We validate our approach on
the CIFAR-10 dataset, and show that it can significantly improve the
interpretability and reliability of DNNs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 figures, SPIE DCS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Disparate Effect Of Missing Mediators On Transportability of Causal
  Effects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08638v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08638v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vishwali Mhasawade, Rumi Chunara
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transported mediation effects provide an avenue to understand how upstream
interventions (such as improved neighborhood conditions like green spaces)
would work differently when applied to different populations as a result of
factors that mediate the effects. However, when mediators are missing in the
population where the effect is to be transported, these estimates could be
biased. We study this issue of missing mediators, motivated by challenges in
public health, wherein mediators can be missing, not at random. We propose a
sensitivity analysis framework that quantifies the impact of missing mediator
data on transported mediation effects. This framework enables us to identify
the settings under which the conditional transported mediation effect is
rendered insignificant for the subgroup with missing mediator data.
Specifically, we provide the bounds on the transported mediation effect as a
function of missingness. We then apply the framework to longitudinal data from
the Moving to Opportunity Study, a large-scale housing voucher experiment, to
quantify the effect of missing mediators on transport effect estimates of
voucher receipt, an upstream intervention on living location, in childhood on
subsequent risk of mental health or substance use disorder mediated through
parental health across sites. Our findings provide a tangible understanding of
how much missing data can be withstood for unbiased effect estimates.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Human Alignment of Large Language Models through Online Preference
  Optimisation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08635v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08635v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniele Calandriello, Daniel Guo, Remi Munos, Mark Rowland, Yunhao Tang, Bernardo Avila Pires, Pierre Harvey Richemond, Charline Le Lan, Michal Valko, Tianqi Liu, Rishabh Joshi, Zeyu Zheng, Bilal Piot
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ensuring alignment of language models' outputs with human preferences is
critical to guarantee a useful, safe, and pleasant user experience. Thus, human
alignment has been extensively studied recently and several methods such as
Reinforcement Learning from Human Feedback (RLHF), Direct Policy Optimisation
(DPO) and Sequence Likelihood Calibration (SLiC) have emerged. In this paper,
our contribution is two-fold. First, we show the equivalence between two recent
alignment methods, namely Identity Policy Optimisation (IPO) and Nash Mirror
Descent (Nash-MD). Second, we introduce a generalisation of IPO, named IPO-MD,
that leverages the regularised sampling approach proposed by Nash-MD.
  This equivalence may seem surprising at first sight, since IPO is an offline
method whereas Nash-MD is an online method using a preference model. However,
this equivalence can be proven when we consider the online version of IPO, that
is when both generations are sampled by the online policy and annotated by a
trained preference model. Optimising the IPO loss with such a stream of data
becomes then equivalent to finding the Nash equilibrium of the preference model
through self-play. Building on this equivalence, we introduce the IPO-MD
algorithm that generates data with a mixture policy (between the online and
reference policy) similarly as the general Nash-MD algorithm. We compare
online-IPO and IPO-MD to different online versions of existing losses on
preference data such as DPO and SLiC on a summarisation task.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Decade's Battle on <span class="highlight-title">Dataset</span> Bias: Are We There Yet? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08632v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08632v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuang Liu, Kaiming He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We revisit the "dataset classification" experiment suggested by Torralba and
Efros a decade ago, in the new era with large-scale, diverse, and hopefully
less biased datasets as well as more capable neural network architectures.
Surprisingly, we observe that modern neural networks can achieve excellent
accuracy in classifying which dataset an image is from: e.g., we report 84.7%
accuracy on held-out validation data for the three-way classification problem
consisting of the YFCC, CC, and DataComp datasets. Our further experiments show
that such a dataset classifier could learn semantic features that are
generalizable and transferable, which cannot be simply explained by
memorization. We hope our discovery will inspire the community to rethink the
issue involving dataset bias and model capabilities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Non-Decimated Wavelet Packet Features and <span class="highlight-title">Transformer</span> Models
  for Time Series Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08630v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08630v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guy P Nason, James L. Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This article combines wavelet analysis techniques with machine learning
methods for univariate time series forecasting, focusing on three main
contributions. Firstly, we consider the use of Daubechies wavelets with
different numbers of vanishing moments as input features to both non-temporal
and temporal forecasting methods, by selecting these numbers during the
cross-validation phase. Secondly, we compare the use of both the non-decimated
wavelet transform and the non-decimated wavelet packet transform for computing
these features, the latter providing a much larger set of potentially useful
coefficient vectors. The wavelet coefficients are computed using a shifted
version of the typical pyramidal algorithm to ensure no leakage of future
information into these inputs. Thirdly, we evaluate the use of these wavelet
features on a significantly wider set of forecasting methods than previous
studies, including both temporal and non-temporal models, and both statistical
and deep learning-based methods. The latter include state-of-the-art
transformer-based neural network architectures. Our experiments suggest
significant benefit in replacing higher-order lagged features with wavelet
features across all examined non-temporal methods for one-step-forward
forecasting, and modest benefit when used as inputs for temporal deep
learning-based models for long-horizon forecasting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multifidelity linear regression for scientific machine learning from
  scarce data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08627v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08627v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elizabeth Qian, Anirban Chaudhuri, Dayoung Kang, Vignesh Sella
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning (ML) methods, which fit to data the parameters of a given
parameterized model class, have garnered significant interest as potential
methods for learning surrogate models for complex engineering systems for which
traditional simulation is expensive. However, in many scientific and
engineering settings, generating high-fidelity data on which to train ML models
is expensive, and the available budget for generating training data is limited.
ML models trained on the resulting scarce high-fidelity data have high variance
and are sensitive to vagaries of the training data set. We propose a new
multifidelity training approach for scientific machine learning that exploits
the scientific context where data of varying fidelities and costs are
available; for example high-fidelity data may be generated by an expensive
fully resolved physics simulation whereas lower-fidelity data may arise from a
cheaper model based on simplifying assumptions. We use the multifidelity data
to define new multifidelity Monte Carlo estimators for the unknown parameters
of linear regression models, and provide theoretical analyses that guarantee
the approach's accuracy and improved robustness to small training budgets.
Numerical results verify the theoretical analysis and demonstrate that
multifidelity learned models trained on scarce high-fidelity data and
additional low-fidelity data achieve order-of-magnitude lower model variance
than standard models trained on only high-fidelity data of comparable cost.
This illustrates that in the scarce data regime, our multifidelity training
strategy yields models with lower expected error than standard training
approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Verifix: Post-Training Correction to Improve Label Noise Robustness with
  Verified Samples 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08618v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08618v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sangamesh Kodge, Deepak Ravikumar, Gobinda Saha, Kaushik Roy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Label corruption, where training samples have incorrect labels, can
significantly degrade the performance of machine learning models. This
corruption often arises from non-expert labeling or adversarial attacks.
Acquiring large, perfectly labeled datasets is costly, and retraining large
models from scratch when a clean dataset becomes available is computationally
expensive. To address this challenge, we propose Post-Training Correction, a
new paradigm that adjusts model parameters after initial training to mitigate
label noise, eliminating the need for retraining. We introduce Verifix, a novel
Singular Value Decomposition (SVD) based algorithm that leverages a small,
verified dataset to correct the model weights using a single update. Verifix
uses SVD to estimate a Clean Activation Space and then projects the model's
weights onto this space to suppress activations corresponding to corrupted
data. We demonstrate Verifix's effectiveness on both synthetic and real-world
label noise. Experiments on the CIFAR dataset with 25% synthetic corruption
show 7.36% generalization improvements on average. Additionally, we observe
generalization improvements of up to 2.63% on naturally corrupted datasets like
WebVision1.0 and Clothing1M.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Link Prediction for Social Networks using Representation Learning and
  Heuristic-based Features <span class="chip">IJCAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08613v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08613v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samarth Khanna, Sree Bhattacharyya, Sudipto Ghosh, Kushagra Agarwal, Asit Kumar Das
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The exponential growth in scale and relevance of social networks enable them
to provide expansive insights. Predicting missing links in social networks
efficiently can help in various modern-day business applications ranging from
generating recommendations to influence analysis. Several categories of
solutions exist for the same. Here, we explore various feature extraction
techniques to generate representations of nodes and edges in a social network
that allow us to predict missing links. We compare the results of using ten
feature extraction techniques categorized across Structural embeddings,
Neighborhood-based embeddings, Graph Neural Networks, and Graph Heuristics,
followed by modeling with ensemble classifiers and custom Neural Networks.
Further, we propose combining heuristic-based features and learned
representations that demonstrate improved performance for the link prediction
task on social network datasets. Using this method to generate accurate
recommendations for many applications is a matter of further study that appears
very promising. The code for all the experiments has been made public.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the MAISoN Workshop at IJCAI 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Data-Efficient Sleep Staging with Synthetic Time Series <span class="highlight-title">Pretrain</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08592v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08592v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Niklas Grieger, Siamak Mehrkanoon, Stephan Bialonski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Analyzing electroencephalographic (EEG) time series can be challenging,
especially with deep neural networks, due to the large variability among human
subjects and often small datasets. To address these challenges, various
strategies, such as self-supervised learning, have been suggested, but they
typically rely on extensive empirical datasets. Inspired by recent advances in
computer vision, we propose a pretraining task termed "frequency pretraining"
to pretrain a neural network for sleep staging by predicting the frequency
content of randomly generated synthetic time series. Our experiments
demonstrate that our method surpasses fully supervised learning in scenarios
with limited data and few subjects, and matches its performance in regimes with
many subjects. Furthermore, our results underline the relevance of frequency
information for sleep stage scoring, while also demonstrating that deep neural
networks utilize information beyond frequencies to enhance sleep staging
performance, which is consistent with previous research. We anticipate that our
approach will be advantageous across a broad spectrum of applications where EEG
data is limited or derived from a small number of subjects, including the
domain of brain-computer interfaces.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can physical information aid the generalization ability of Neural
  Networks for hydraulic modeling? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08589v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08589v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gianmarco Guglielmo, Andrea Montessori, Jean-Michel Tucny, Michele La Rocca, Pietro Prestininzi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Application of Neural Networks to river hydraulics is fledgling, despite the
field suffering from data scarcity, a challenge for machine learning
techniques. Consequently, many purely data-driven Neural Networks proved to
lack predictive capabilities. In this work, we propose to mitigate such problem
by introducing physical information into the training phase. The idea is
borrowed from Physics-Informed Neural Networks which have been recently
proposed in other contexts. Physics-Informed Neural Networks embed physical
information in the form of the residual of the Partial Differential Equations
(PDEs) governing the phenomenon and, as such, are conceived as neural solvers,
i.e. an alternative to traditional numerical solvers. Such approach is seldom
suitable for environmental hydraulics, where epistemic uncertainties are large,
and computing residuals of PDEs exhibits difficulties similar to those faced by
classical numerical methods. Instead, we envisaged the employment of Neural
Networks as neural operators, featuring physical constraints formulated without
resorting to PDEs. The proposed novel methodology shares similarities with data
augmentation and regularization. We show that incorporating such soft physical
information can improve predictive capabilities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Implicit Regularization of SGD with Preconditioning for Least
  Square Problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08585v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08585v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junwei Su, Difan Zou, Chuan Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stochastic gradient descent (SGD) exhibits strong algorithmic regularization
effects in practice and plays an important role in the generalization of modern
machine learning. However, prior research has revealed instances where the
generalization performance of SGD is worse than ridge regression due to uneven
optimization along different dimensions. Preconditioning offers a natural
solution to this issue by rebalancing optimization across different directions.
Yet, the extent to which preconditioning can enhance the generalization
performance of SGD and whether it can bridge the existing gap with ridge
regression remains uncertain. In this paper, we study the generalization
performance of SGD with preconditioning for the least squared problem. We make
a comprehensive comparison between preconditioned SGD and (standard \&
preconditioned) ridge regression. Our study makes several key contributions
toward understanding and improving SGD with preconditioning. First, we
establish excess risk bounds (generalization performance) for preconditioned
SGD and ridge regression under an arbitrary preconditions matrix. Second,
leveraging the excessive risk characterization of preconditioned SGD and ridge
regression, we show that (through construction) there exists a simple
preconditioned matrix that can outperform (standard \& preconditioned) ridge
regression. Finally, we show that our proposed preconditioning matrix is
straightforward enough to allow robust estimation from finite samples while
maintaining a theoretical advantage over ridge regression. Our empirical
results align with our theoretical findings, collectively showcasing the
enhanced regularization effect of preconditioned SGD.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Local Binary and Multiclass SVMs Trained on a Quantum Annealer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08584v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08584v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Enrico Zardini, Amer Delilbasic, Enrico Blanzieri, Gabriele Cavallaro, Davide Pastorello
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Support vector machines (SVMs) are widely used machine learning models (e.g.,
in remote sensing), with formulations for both classification and regression
tasks. In the last years, with the advent of working quantum annealers, hybrid
SVM models characterised by quantum training and classical execution have been
introduced. These models have demonstrated comparable performance to their
classical counterparts. However, they are limited in the training set size due
to the restricted connectivity of the current quantum annealers. Hence, to take
advantage of large datasets (like those related to Earth observation), a
strategy is required. In the classical domain, local SVMs, namely, SVMs trained
on the data samples selected by a k-nearest neighbors model, have already
proven successful. Here, the local application of quantum-trained SVM models is
proposed and empirically assessed. In particular, this approach allows
overcoming the constraints on the training set size of the quantum-trained
models while enhancing their performance. In practice, the FaLK-SVM method,
designed for efficient local SVMs, has been combined with quantum-trained SVM
models for binary and multiclass classification. In addition, for comparison,
FaLK-SVM has been interfaced for the first time with a classical single-step
multiclass SVM model (CS SVM). Concerning the empirical evaluation, D-Wave's
quantum annealers and real-world datasets taken from the remote sensing domain
have been employed. The results have shown the effectiveness and scalability of
the proposed approach, but also its practical applicability in a real-world
large-scale scenario.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 1 figure, 11 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Machine Learning Optimized Orthogonal Basis Piecewise Polynomial
  Approximation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08579v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08579v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hannes Waclawek, Stefan Huber
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Piecewise Polynomials (PPs) are utilized in several engineering disciplines,
like trajectory planning, to approximate position profiles given in the form of
a set of points. While the approximation target along with domain-specific
requirements, like Ck -continuity, can be formulated as a system of equations
and a result can be computed directly, such closed-form solutions posses
limited flexibility with respect to polynomial degrees, polynomial bases or
adding further domain-specific requirements. Sufficiently complex optimization
goals soon call for the use of numerical methods, like gradient descent. Since
gradient descent lies at the heart of training Artificial Neural Networks
(ANNs), modern Machine Learning (ML) frameworks like TensorFlow come with a set
of gradient-based optimizers potentially suitable for a wide range of
optimization problems beyond the training task for ANNs. Our approach is to
utilize the versatility of PP models and combine it with the potential of
modern ML optimizers for the use in function approximation in 1D trajectory
planning in the context of electronic cam design. We utilize available
optimizers of the ML framework TensorFlow directly, outside of the scope of
ANNs, to optimize model parameters of our PP model. In this paper, we show how
an orthogonal polynomial basis contributes to improving approximation and
continuity optimization performance. Utilizing Chebyshev polynomials of the
first kind, we develop a novel regularization approach enabling clearly
improved convergence behavior. We show that, using this regularization
approach, Chebyshev basis performs better than power basis for all relevant
optimizers in the combined approximation and continuity optimization setting
and demonstrate usability of the presented approach within the electronic cam
domain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to LION18</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Caformer: Rethinking Time Series Analysis from Causal Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08572v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08572v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kexuan Zhang, Xiaobei Zou, Yang Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Time series analysis is a vital task with broad applications in various
domains. However, effectively capturing cross-dimension and cross-time
dependencies in non-stationary time series poses significant challenges,
particularly in the context of environmental factors. The spurious correlation
induced by the environment confounds the causal relationships between
cross-dimension and cross-time dependencies. In this paper, we introduce a
novel framework called Caformer (\underline{\textbf{Ca}}usal
Trans\underline{\textbf{former}}) for time series analysis from a causal
perspective. Specifically, our framework comprises three components: Dynamic
Learner, Environment Learner, and Dependency Learner. The Dynamic Learner
unveils dynamic interactions among dimensions, the Environment Learner
mitigates spurious correlations caused by environment with a back-door
adjustment, and the Dependency Learner aims to infer robust interactions across
both time and dimensions. Our Caformer demonstrates consistent state-of-the-art
performance across five mainstream time series analysis tasks, including long-
and short-term forecasting, imputation, classification, and anomaly detection,
with proper interpretability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Physics-driven GraphSAGE Method for Physical Process Simulations
  Described by Partial Differential Equations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08569v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08569v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hang Hu, Sidi Wu, Guoxiong Cai, Na Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Physics-informed neural networks (PINNs) have successfully addressed various
computational physics problems based on partial differential equations (PDEs).
However, while tackling issues related to irregularities like singularities and
oscillations, trained solutions usually suffer low accuracy. In addition, most
current works only offer the trained solution for predetermined input
parameters. If any change occurs in input parameters, transfer learning or
retraining is required, and traditional numerical techniques also need an
independent simulation. In this work, a physics-driven GraphSAGE approach
(PD-GraphSAGE) based on the Galerkin method and piecewise polynomial nodal
basis functions is presented to solve computational problems governed by
irregular PDEs and to develop parametric PDE surrogate models. This approach
employs graph representations of physical domains, thereby reducing the demands
for evaluated points due to local refinement. A distance-related edge feature
and a feature mapping strategy are devised to help training and convergence for
singularity and oscillation situations, respectively. The merits of the
proposed method are demonstrated through a couple of cases. Moreover, the
robust PDE surrogate model for heat conduction problems parameterized by the
Gaussian random field source is successfully established, which not only
provides the solution accurately but is several times faster than the finite
element method in our experiments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages,11 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can Direct Latent Model Learning Solve Linear Quadratic Gaussian
  Control? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.14511v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.14511v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Tian, Kaiqing Zhang, Russ Tedrake, Suvrit Sra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the task of learning state representations from potentially
high-dimensional observations, with the goal of controlling an unknown
partially observable system. We pursue a direct latent model learning approach,
where a dynamic model in some latent state space is learned by predicting
quantities directly related to planning (e.g., costs) without reconstructing
the observations. In particular, we focus on an intuitive cost-driven state
representation learning method for solving Linear Quadratic Gaussian (LQG)
control, one of the most fundamental partially observable control problems. As
our main results, we establish finite-sample guarantees of finding a
near-optimal state representation function and a near-optimal controller using
the directly learned latent model. To the best of our knowledge, despite
various empirical successes, prior to this work it was unclear if such a
cost-driven latent model learner enjoys finite-sample guarantees. Our work
underscores the value of predicting multi-step costs, an idea that is key to
our theory, and notably also an idea that is known to be empirically valuable
for learning state representations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>37 pages; Updated structure and proofs</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Demystifying Embedding Spaces using Large Language Models <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04475v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04475v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guy Tennenholtz, Yinlam Chow, Chih-Wei Hsu, Jihwan Jeong, Lior Shani, Azamat Tulepbergenov, Deepak Ramachandran, Martin Mladenov, Craig Boutilier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Embeddings have become a pivotal means to represent complex, multi-faceted
information about entities, concepts, and relationships in a condensed and
useful format. Nevertheless, they often preclude direct interpretation. While
downstream tasks make use of these compressed representations, meaningful
interpretation usually requires visualization using dimensionality reduction or
specialized machine learning interpretability methods. This paper addresses the
challenge of making such embeddings more interpretable and broadly useful, by
employing Large Language Models (LLMs) to directly interact with embeddings --
transforming abstract vectors into understandable narratives. By injecting
embeddings into LLMs, we enable querying and exploration of complex embedding
data. We demonstrate our approach on a variety of diverse tasks, including:
enhancing concept activation vectors (CAVs), communicating novel embedded
entities, and decoding user preferences in recommender systems. Our work
couples the immense information potential of embeddings with the interpretative
power of LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> A Hitchhiker's Guide to Geometric GNNs for 3D Atomic Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.07511v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.07511v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexandre Duval, Simon V. Mathis, Chaitanya K. Joshi, Victor Schmidt, Santiago Miret, Fragkiskos D. Malliaros, Taco Cohen, Pietro Liò, <span class="highlight-author">Yoshua Bengio</span>, Michael Bronstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in computational modelling of atomic systems, spanning
molecules, proteins, and materials, represent them as geometric graphs with
atoms embedded as nodes in 3D Euclidean space. In these graphs, the geometric
attributes transform according to the inherent physical symmetries of 3D atomic
systems, including rotations and translations in Euclidean space, as well as
node permutations. In recent years, Geometric Graph Neural Networks have
emerged as the preferred machine learning architecture powering applications
ranging from protein structure prediction to molecular simulations and material
generation. Their specificity lies in the inductive biases they leverage - such
as physical symmetries and chemical properties - to learn informative
representations of these geometric graphs.
  In this opinionated paper, we provide a comprehensive and self-contained
overview of the field of Geometric GNNs for 3D atomic systems. We cover
fundamental background material and introduce a pedagogical taxonomy of
Geometric GNN architectures: (1) invariant networks, (2) equivariant networks
in Cartesian basis, (3) equivariant networks in spherical basis, and (4)
unconstrained networks. Additionally, we outline key datasets and application
areas and suggest future research directions. The objective of this work is to
present a structured perspective on the field, making it accessible to
newcomers and aiding practitioners in gaining an intuition for its mathematical
abstractions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adaptive Sharpness-Aware Pruning for Robust Sparse Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.14306v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.14306v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anna Bair, Hongxu Yin, Maying Shen, Pavlo Molchanov, Jose Alvarez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robustness and compactness are two essential attributes of deep learning
models that are deployed in the real world. The goals of robustness and
compactness may seem to be at odds, since robustness requires generalization
across domains, while the process of compression exploits specificity in one
domain. We introduce Adaptive Sharpness-Aware Pruning (AdaSAP), which unifies
these goals through the lens of network sharpness. The AdaSAP method produces
sparse networks that are robust to input variations which are unseen at
training time. We achieve this by strategically incorporating weight
perturbations in order to optimize the loss landscape. This allows the model to
be both primed for pruning and regularized for improved robustness. AdaSAP
improves the robust accuracy of pruned models on image classification by up to
+6% on ImageNet C and +4% on ImageNet V2, and on object detection by +4% on a
corrupted Pascal VOC dataset, over a wide range of compression ratios, pruning
criteria, and network architectures, outperforming recent pruning art by large
margins.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Randomized Kaczmarz in Adversarial Distributed Setting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.14615v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.14615v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Longxiu Huang, Xia Li, Deanna Needell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Developing large-scale distributed methods that are robust to the presence of
adversarial or corrupted workers is an important part of making such methods
practical for real-world problems. In this paper, we propose an iterative
approach that is adversary-tolerant for convex optimization problems. By
leveraging simple statistics, our method ensures convergence and is capable of
adapting to adversarial distributions. Additionally, the efficiency of the
proposed methods for solving convex problems is shown in simulations with the
presence of adversaries. Through simulations, we demonstrate the efficiency of
our approach in the presence of adversaries and its ability to identify
adversarial workers with high accuracy and tolerate varying levels of adversary
rates.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GenTKG: Generative Forecasting on Temporal Knowledge Graph <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07793v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07793v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruotong Liao, Xu Jia, Yunpu Ma, Yangzhe Li, Volker Tresp
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancements in large language models (LLMs) have ignited interest
in the temporal knowledge graph (tKG) domain, where conventional
embedding-based and rule-based methods dominate. The question remains open of
whether pre-trained LLMs can understand structured temporal relational data and
replace them as the foundation model for temporal relational forecasting.
Therefore, we bring temporal knowledge forecasting into the generative setting.
However, challenges occur in the huge chasms between complex temporal graph
data structure and sequential natural expressions LLMs can handle, and between
the enormous data sizes of tKGs and heavy computation costs of finetuning LLMs.
To address these challenges, we propose a novel retrieval-augmented generation
framework named GenTKG combining a temporal logical rule-based retrieval
strategy and few-shot parameter-efficient instruction tuning to solve the above
challenges, respectively. Extensive experiments have shown that GenTKG
outperforms conventional methods of temporal relational forecasting with low
computation resources using extremely limited training data as few as 16
samples. GenTKG also highlights remarkable cross-domain generalizability with
outperforming performance on unseen datasets without re-training, and in-domain
generalizability regardless of time split in the same dataset. Our work reveals
the huge potential of LLMs in the tKG domain and opens a new frontier for
generative forecasting on tKGs. Code and data are released here:
https://github.com/mayhugotong/GenTKG.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, Findings of NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fast Dual-Regularized Autoencoder for Sparse Biological Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.16664v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.16664v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aleksandar Poleksic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Relationship inference from sparse data is an important task with
applications ranging from product recommendation to drug discovery. A recently
proposed linear model for sparse matrix completion has demonstrated surprising
advantage in speed and accuracy over more sophisticated recommender systems
algorithms. Here we extend the linear model to develop a shallow autoencoder
for the dual neighborhood-regularized matrix completion problem. We demonstrate
the speed and accuracy advantage of our approach over the existing
state-of-the-art in predicting drug-target interactions and drug-disease
associations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Constructing Variables Using Classifiers as an Aid to Regression: An
  Empirical Assessment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06829v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06829v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Colin Troisemaine, Vincent Lemaire
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a method for the automatic creation of variables (in the
case of regression) that complement the information contained in the initial
input vector. The method works as a pre-processing step in which the continuous
values of the variable to be regressed are discretized into a set of intervals
which are then used to define value thresholds. Then classifiers are trained to
predict whether the value to be regressed is less than or equal to each of
these thresholds. The different outputs of the classifiers are then
concatenated in the form of an additional vector of variables that enriches the
initial vector of the regression problem. The implemented system can thus be
considered as a generic pre-processing tool. We tested the proposed enrichment
method with 5 types of regressors and evaluated it in 33 regression datasets.
Our experimental results confirm the interest of the approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Linear attention is (maybe) all you need (to understand <span class="highlight-title">transformer</span>
  optimization) <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.01082v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.01082v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kwangjun Ahn, Xiang Cheng, Minhak Song, Chulhee Yun, Ali Jadbabaie, Suvrit Sra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer training is notoriously difficult, requiring a careful design of
optimizers and use of various heuristics. We make progress towards
understanding the subtleties of training Transformers by carefully studying a
simple yet canonical linearized shallow Transformer model. Specifically, we
train linear Transformers to solve regression tasks, inspired by J.~von Oswald
et al.~(ICML 2023), and K.~Ahn et al.~(NeurIPS 2023). Most importantly, we
observe that our proposed linearized models can reproduce several prominent
aspects of Transformer training dynamics. Consequently, the results obtained in
this paper suggest that a simple linearized Transformer model could actually be
a valuable, realistic abstraction for understanding Transformer optimization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SkillDiffuser: Interpretable Hierarchical Planning via Skill
  Abstractions in Diffusion-Based Task Execution <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.11598v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.11598v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhixuan Liang, Yao Mu, Hengbo Ma, Masayoshi Tomizuka, Mingyu Ding, Ping Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have demonstrated strong potential for robotic trajectory
planning. However, generating coherent trajectories from high-level
instructions remains challenging, especially for long-range composition tasks
requiring multiple sequential skills. We propose SkillDiffuser, an end-to-end
hierarchical planning framework integrating interpretable skill learning with
conditional diffusion planning to address this problem. At the higher level,
the skill abstraction module learns discrete, human-understandable skill
representations from visual observations and language instructions. These
learned skill embeddings are then used to condition the diffusion model to
generate customized latent trajectories aligned with the skills. This allows
generating diverse state trajectories that adhere to the learnable skills. By
integrating skill learning with conditional trajectory generation,
SkillDiffuser produces coherent behavior following abstract instructions across
diverse tasks. Experiments on multi-task robotic manipulation benchmarks like
Meta-World and LOReL demonstrate state-of-the-art performance and
human-interpretable skill representations from SkillDiffuser. More
visualization results and information could be found on our website.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2024. Camera ready version. Project page:
  https://skilldiffuser.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tight Group-Level DP Guarantees for DP-SGD with Sampling via Mixture of
  Gaussians Mechanisms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.10294v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.10294v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arun Ganesh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We give a procedure for computing group-level $(\epsilon, \delta)$-DP
guarantees for DP-SGD, when using Poisson sampling or fixed batch size
sampling. Up to discretization errors in the implementation, the DP guarantees
computed by this procedure are tight (assuming we release every intermediate
iterate).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>v2: Added links to open-source implementation of PLD accounting for
  MoG mechanisms</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Referential communication in heterogeneous communities of <span class="highlight-title">pre-train</span>ed
  visual deep networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.08913v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.08913v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matéo Mahaut, Francesca Franzon, Roberto Dessì, Marco Baroni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As large pre-trained image-processing neural networks are being embedded in
autonomous agents such as self-driving cars or robots, the question arises of
how such systems can communicate with each other about the surrounding world,
despite their different architectures and training regimes. As a first step in
this direction, we systematically explore the task of \textit{referential
communication} in a community of heterogeneous state-of-the-art pre-trained
visual networks, showing that they can develop, in a self-supervised way, a
shared protocol to refer to a target object among a set of candidates. This
shared protocol can also be used, to some extent, to communicate about
previously unseen object categories of different granularity. Moreover, a
visual network that was not initially part of an existing community can learn
the community's protocol with remarkable ease. Finally, we study, both
qualitatively and quantitatively, the properties of the emergent protocol,
providing some evidence that it is capturing high-level semantic features of
objects.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Continual Adversarial Defense 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.09481v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.09481v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qian Wang, Yaoyao Liu, Hefei Ling, Yingwei Li, Qihao Liu, Ping Li, Jiazhong Chen, Alan Yuille, Ning Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In response to the rapidly evolving nature of adversarial attacks against
visual classifiers on a monthly basis, numerous defenses have been proposed to
generalize against as many known attacks as possible. However, designing a
defense method that generalizes to all types of attacks is not realistic
because the environment in which defense systems operate is dynamic and
comprises various unique attacks that emerge as time goes on. The defense
system must gather online few-shot defense feedback to promptly enhance itself,
leveraging efficient memory utilization. Therefore, we propose the first
continual adversarial defense (CAD) framework that adapts to any attacks in a
dynamic scenario, where various attacks emerge stage by stage. In practice, CAD
is modeled under four principles: (1) continual adaptation to new attacks
without catastrophic forgetting, (2) few-shot adaptation, (3) memory-efficient
adaptation, and (4) high accuracy on both clean and adversarial images. We
explore and integrate cutting-edge continual learning, few-shot learning, and
ensemble learning techniques to qualify the principles. Experiments conducted
on CIFAR-10 and ImageNet-100 validate the effectiveness of our approach against
multiple stages of modern adversarial attacks and demonstrate significant
improvements over numerous baseline methods. In particular, CAD is capable of
quickly adapting with minimal feedback and a low cost of defense failure, while
maintaining good performance against previous attacks. Our research sheds light
on a brand-new paradigm for continual defense adaptation against dynamic and
evolving attacks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ In-Context Learning Learns Label Relationships but Is Not Conventional
  Learning <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.12375v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.12375v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jannik Kossen, Yarin Gal, Tom Rainforth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The predictions of Large Language Models (LLMs) on downstream tasks often
improve significantly when including examples of the input--label relationship
in the context. However, there is currently no consensus about how this
in-context learning (ICL) ability of LLMs works. For example, while Xie et al.
(2021) liken ICL to a general-purpose learning algorithm, Min et al. (2022)
argue ICL does not even learn label relationships from in-context examples. In
this paper, we provide novel insights into how ICL leverages label information,
revealing both capabilities and limitations. To ensure we obtain a
comprehensive picture of ICL behavior, we study probabilistic aspects of ICL
predictions and thoroughly examine the dynamics of ICL as more examples are
provided. Our experiments show that ICL predictions almost always depend on
in-context labels and that ICL can learn truly novel tasks in-context. However,
we also find that ICL struggles to fully overcome prediction preferences
acquired from pre-training data and, further, that ICL does not consider all
in-context information equally.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CoLiDE: Concomitant Linear DAG Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02895v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02895v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seyed Saman Saboksayr, Gonzalo Mateos, Mariano Tepper
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We deal with the combinatorial problem of learning directed acyclic graph
(DAG) structure from observational data adhering to a linear structural
equation model (SEM). Leveraging advances in differentiable, nonconvex
characterizations of acyclicity, recent efforts have advocated a continuous
constrained optimization paradigm to efficiently explore the space of DAGs.
Most existing methods employ lasso-type score functions to guide this search,
which (i) require expensive penalty parameter retuning when the
$\textit{unknown}$ SEM noise variances change across problem instances; and
(ii) implicitly rely on limiting homoscedasticity assumptions. In this work, we
propose a new convex score function for sparsity-aware learning of linear DAGs,
which incorporates concomitant estimation of scale and thus effectively
decouples the sparsity parameter from the exogenous noise levels.
Regularization via a smooth, nonconvex acyclicity penalty term yields CoLiDE
($\textbf{Co}$ncomitant $\textbf{Li}$near $\textbf{D}$AG
$\textbf{E}$stimation), a regression-based criterion amenable to efficient
gradient computation and closed-form estimation of noise variances in
heteroscedastic scenarios. Our algorithm outperforms state-of-the-art methods
without incurring added complexity, especially when the DAGs are larger and the
noise level profile is heterogeneous. We also find CoLiDE exhibits enhanced
stability manifested via reduced standard deviations in several domain-specific
metrics, underscoring the robustness of our novel linear DAG estimator.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dr. Jekyll and Mr. Hyde: Two Faces of LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.03853v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.03853v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matteo Gioele Collu, Tom Janssen-Groesbeek, Stefanos Koffas, Mauro Conti, Stjepan Picek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Only a year ago, we witnessed a rise in the use of Large Language Models
(LLMs), especially when combined with applications like chatbot assistants.
Safety mechanisms and specialized training procedures are implemented to
prevent improper responses from these assistants. In this work, we bypass these
measures for ChatGPT and Bard (and, to some extent, Bing chat) by making them
impersonate complex personas with opposite characteristics as those of the
truthful assistants they are supposed to be. We start by creating elaborate
biographies of these personas, which we then use in a new session with the same
chatbots. Our conversation followed a role-play style to get the response the
assistant was not allowed to provide. By making use of personas, we show that
the response that is prohibited is actually provided, making it possible to
obtain unauthorized, illegal, or harmful information. This work shows that by
using adversarial personas, one can overcome safety mechanisms set out by
ChatGPT and Bard. We also introduce several ways of activating such adversarial
personas, altogether showing that both chatbots are vulnerable to this kind of
attack. With the same principle, we introduce two defenses that push the model
to interpret trustworthy personalities and make it more robust against such
attacks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Imitate the Good and Avoid the Bad: An Incremental Approach to Safe
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.10385v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.10385v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huy Hoang, Tien Mai, Pradeep Varakantham
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A popular framework for enforcing safe actions in Reinforcement Learning (RL)
is Constrained RL, where trajectory based constraints on expected cost (or
other cost measures) are employed to enforce safety and more importantly these
constraints are enforced while maximizing expected reward. Most recent
approaches for solving Constrained RL convert the trajectory based cost
constraint into a surrogate problem that can be solved using minor
modifications to RL methods. A key drawback with such approaches is an over or
underestimation of the cost constraint at each state. Therefore, we provide an
approach that does not modify the trajectory based cost constraint and instead
imitates ``good'' trajectories and avoids ``bad'' trajectories generated from
incrementally improving policies. We employ an oracle that utilizes a reward
threshold (which is varied with learning) and the overall cost constraint to
label trajectories as ``good'' or ``bad''. A key advantage of our approach is
that we are able to work from any starting policy or set of trajectories and
improve on it. In an exhaustive set of experiments, we demonstrate that our
approach is able to outperform top benchmark approaches for solving Constrained
RL problems, with respect to expected cost, CVaR cost, or even unknown cost
constraints.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A New Quantum CNN Model for Image Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11155v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11155v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        X. Q. Zhao, T. L. Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantum density matrix represents all the information of the entire quantum
system, and novel models of meaning employing density matrices naturally model
linguistic phenomena such as hyponymy and linguistic ambiguity, among others in
quantum question answering tasks. Naturally, we argue that the quantum density
matrix can enhance the image feature information and the relationship between
the features for the classical image classification. Specifically, we (i)
combine density matrices and CNN to design a new mechanism; (ii) apply the new
mechanism to some representative classical image classification tasks. A series
of experiments show that the application of quantum density matrix in image
classification has the generalization and high efficiency on different
datasets. The application of quantum density matrix both in classical question
answering tasks and classical image classification tasks show more effective
performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DataInf: Efficiently Estimating Data Influence in LoRA-tuned LLMs and
  Diffusion Models <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.00902v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.00902v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongchan Kwon, Eric Wu, Kevin Wu, James Zou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantifying the impact of training data points is crucial for understanding
the outputs of machine learning models and for improving the transparency of
the AI pipeline. The influence function is a principled and popular data
attribution method, but its computational cost often makes it challenging to
use. This issue becomes more pronounced in the setting of large language models
and text-to-image models. In this work, we propose DataInf, an efficient
influence approximation method that is practical for large-scale generative AI
models. Leveraging an easy-to-compute closed-form expression, DataInf
outperforms existing influence computation algorithms in terms of computational
and memory efficiency. Our theoretical analysis shows that DataInf is
particularly well-suited for parameter-efficient fine-tuning techniques such as
LoRA. Through systematic empirical evaluations, we show that DataInf accurately
approximates influence scores and is orders of magnitude faster than existing
methods. In applications to RoBERTa-large, Llama-2-13B-chat, and
stable-diffusion-v1.5 models, DataInf effectively identifies the most
influential fine-tuning examples better than other approximate influence
scores. Moreover, it can help to identify which data points are mislabeled.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Class Incremental Learning via Likelihood Ratio Based Task Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.15048v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.15048v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haowei Lin, Yijia Shao, Weinan Qian, Ningxin Pan, Yiduo Guo, Bing Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Class incremental learning (CIL) is a challenging setting of continual
learning, which learns a series of tasks sequentially. Each task consists of a
set of unique classes. The key feature of CIL is that no task identifier (or
task-id) is provided at test time. Predicting the task-id for each test sample
is a challenging problem. An emerging theory-guided approach (called TIL+OOD)
is to train a task-specific model for each task in a shared network for all
tasks based on a task-incremental learning (TIL) method to deal with
catastrophic forgetting. The model for each task is an out-of-distribution
(OOD) detector rather than a conventional classifier. The OOD detector can
perform both within-task (in-distribution (IND)) class prediction and OOD
detection. The OOD detection capability is the key to task-id prediction during
inference. However, this paper argues that using a traditional OOD detector for
task-id prediction is sub-optimal because additional information (e.g., the
replay data and the learned tasks) available in CIL can be exploited to design
a better and principled method for task-id prediction. We call the new method
TPL (Task-id Prediction based on Likelihood Ratio). TPL markedly outperforms
strong CIL baselines and has negligible catastrophic forgetting. The code of
TPL is publicly available at https://github.com/linhaowei1/TPL.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Science and Game Theory
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning How to Strategically Disclose Information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08741v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08741v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raj Kiriti Velicheti, Melih Bastopcu, S. Rasoul Etesami, Tamer Başar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Strategic information disclosure, in its simplest form, considers a game
between an information provider (sender) who has access to some private
information that an information receiver is interested in. While the receiver
takes an action that affects the utilities of both players, the sender can
design information (or modify beliefs) of the receiver through signal
commitment, hence posing a Stackelberg game. However, obtaining a Stackelberg
equilibrium for this game traditionally requires the sender to have access to
the receiver's objective. In this work, we consider an online version of
information design where a sender interacts with a receiver of an unknown type
who is adversarially chosen at each round. Restricting attention to Gaussian
prior and quadratic costs for the sender and the receiver, we show that
$\mathcal{O}(\sqrt{T})$ regret is achievable with full information feedback,
where $T$ is the total number of interactions between the sender and the
receiver. Further, we propose a novel parametrization that allows the sender to
achieve $\mathcal{O}(\sqrt{T})$ regret for a general convex utility function.
We then consider the Bayesian Persuasion problem with an additional cost term
in the objective function, which penalizes signaling policies that are more
informative and obtain $\mathcal{O}(\log(T))$ regret. Finally, we establish a
sublinear regret bound for the partial information feedback setting and provide
simulations to support our theoretical results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Algorithmic Theory of Simplicity in Mechanism Design 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08610v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08610v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Diodato Ferraioli, Carmine Ventre
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A growing body of work in economics and computation focuses on the trade-off
between implementability and simplicity in mechanism design. The goal is to
develop a theory that not only allows to design an incentive structure easy to
grasp for imperfectly rational agents, but also understand the ensuing
limitations on the class of mechanisms that enforce it. In this context, the
concept of OSP mechanisms has assumed a prominent role since they provably
account for the absence of contingent reasoning skills, a specific cognitive
limitation. For single-dimensional agents, it is known that OSP mechanisms need
to use certain greedy algorithms.
  In this work, we introduce a notion that interpolates between OSP and SOSP, a
more stringent notion where agents only plan a subset of their own future
moves. We provide an algorithmic characterization of this novel class of
mechanisms for single-dimensional domains and binary allocation problems, that
precisely measures the interplay between simplicity and implementability. We
build on this to show how mechanisms based on reverse greedy algorithms
(a.k.a., deferred acceptance auctions) are algorithmically more robust to
imperfectly rationality than those adopting greedy algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Measures of relevance to the success of streaming platforms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08421v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08421v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juan Carlos Gonçalves-Dosantos, Ricardo Martínez, Joaquín Sánchez-Soriano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Digital streaming platforms, including Twitch, Spotify, Netflix, Disney, and
Kindle, have emerged as one of the main sources of entertainment with
significant growth potential. Many of these platforms distribute royalties
among streamers, artists, producers, or writers based on their impact. In this
paper, we measure the relevance of each of these contributors to the overall
success of the platform, which is information that can play a key role in
revenue allocation. We perform an axiomatic analysis to provide normative
foundations for three relevance metrics: the uniform, the proportional, and the
subscriber-proportional indicators. The last two indicators implement the
so-called pro-rata and user-centric models, which are extensively applied to
distribute revenues in the music streaming market. The axioms we propose
formalize different principles of fairness, stability, and non-manipulability,
and are tailor-made for the streaming context. We complete our analysis with a
case study that measures the influence of the 19 most-followed streamers
worldwide on the Twitch platform.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tractable Local Equilibria in Non-Concave Games 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08171v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08171v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Cai, Constantinos Daskalakis, Haipeng Luo, Chen-Yu Wei, Weiqiang Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While Online Gradient Descent and other no-regret learning procedures are
known to efficiently converge to coarse correlated equilibrium in games where
each agent's utility is concave in their own strategy, this is not the case
when the utilities are non-concave, a situation that is common in machine
learning applications where the agents' strategies are parameterized by deep
neural networks, or the agents' utilities are computed by a neural network, or
both. Indeed, non-concave games present a host of game-theoretic and
optimization challenges: (i) Nash equilibria may fail to exist; (ii) local Nash
equilibria exist but are intractable; and (iii) mixed Nash, correlated, and
coarse correlated equilibria have infinite support in general, and are
intractable. To sidestep these challenges we propose a new solution concept,
termed $(\varepsilon, \Phi(\delta))$-local equilibrium, which generalizes local
Nash equilibrium in non-concave games, as well as (coarse) correlated
equilibrium in concave games. Importantly, we show that two instantiations of
this solution concept capture the convergence guarantees of Online Gradient
Descent and no-regret learning, which we show efficiently converge to this type
of equilibrium in non-concave games with smooth utilities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Algorithmic Information Disclosure in Optimal Auctions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08145v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08145v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Cai, Yingkai Li, Jinzhao Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper studies a joint design problem where a seller can design both the
signal structures for the agents to learn their values, and the allocation and
payment rules for selling the item. In his seminal work, Myerson (1981) shows
how to design the optimal auction with exogenous signals. We show that the
problem becomes NP-hard when the seller also has the ability to design the
signal structures. Our main result is a polynomial-time approximation scheme
(PTAS) for computing the optimal joint design with at most an $\epsilon$
multiplicative loss in expected revenue. Moreover, we show that in our joint
design problem, the seller can significantly reduce the information rent of the
agents by providing partial information, which ensures a revenue that is at
least $1 - \frac{1}{e}$ of the optimal welfare for all valuation distributions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Model-free Resilient Controller Design based on Incentive Feedback
  Stackelberg Game and Q-learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08948v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08948v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiajun Shen, Fengjun Li, Morteza Hashemi, Huazhen Fang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the swift evolution of Cyber-Physical Systems (CPSs) within intelligent
environments, especially in the industrial domain shaped by Industry 4.0, the
surge in development brings forth unprecedented security challenges. This paper
explores the intricate security issues of Industrial CPSs (ICPSs), with a
specific focus on the unique threats presented by intelligent attackers capable
of directly compromising the controller, thereby posing a direct risk to
physical security. Within the framework of hierarchical control and incentive
feedback Stackelberg game, we design a resilient leading controller (leader)
that is adaptive to a compromised following controller (follower) such that the
compromised follower acts cooperatively with the leader, aligning its
strategies with the leader's objective to achieve a team-optimal solution.
First, we provide sufficient conditions for the existence of an incentive
Stackelberg solution when system dynamics are known. Then, we propose a
Q-learning-based Approximate Dynamic Programming (ADP) approach, and
corresponding algorithms for the online resolution of the incentive Stackelberg
solution without requiring prior knowledge of system dynamics. Last but not
least, we prove the convergence of our approach to the optimum.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Language-based game theory in the age of artificial intelligence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08944v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08944v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Valerio Capraro, Roberto Di Paolo, Matjaz Perc, Veronica Pizziol
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding human behaviour in decision problems and strategic interactions
has wide-ranging applications in economics, psychology, and artificial
intelligence. Game theory offers a robust foundation for this understanding,
based on the idea that individuals aim to maximize a utility function. However,
the exact factors influencing strategy choices remain elusive. While
traditional models try to explain human behaviour as a function of the outcomes
of available actions, recent experimental research reveals that linguistic
content significantly impacts decision-making, thus prompting a paradigm shift
from outcome-based to language-based utility functions. This shift is more
urgent than ever, given the advancement of generative AI, which has the
potential to support humans in making critical decisions through language-based
interactions. We propose sentiment analysis as a fundamental tool for this
shift and take an initial step by analyzing 61 experimental instructions from
the dictator game, an economic game capturing the balance between self-interest
and the interest of others, which is at the core of many social interactions.
Our meta-analysis shows that sentiment analysis can explain human behaviour
beyond economic outcomes. We discuss future research directions. We hope this
work sets the stage for a novel game theoretical approach that emphasizes the
importance of language in human decisions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Distributed Nash Equilibrium Seeking over Time-Varying Directed
  Communication Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.02323v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.02323v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Duong Thuy Anh Nguyen, Duong Tung Nguyen, Angelia Nedić
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study distributed algorithms for finding a Nash equilibrium (NE) in a
class of non-cooperative convex games under partial information. Specifically,
each agent has access only to its own smooth local cost function and can
receive information from its neighbors in a time-varying directed communication
network. To this end, we propose a distributed gradient play algorithm to
compute a NE by utilizing local information exchange among the players. In this
algorithm, every agent performs a gradient step to minimize its own cost
function while sharing and retrieving information locally among its neighbors.
The existing methods impose strong assumptions such as balancedness of the
mixing matrices and global knowledge of the network communication structure,
including Perron-Frobenius eigenvector of the adjacency matrix and other graph
connectivity constants. In contrast, our approach relies only on a reasonable
and widely-used assumption of row-stochasticity of the mixing matrices. We
analyze the algorithm for time-varying directed graphs and prove its
convergence to the NE, when the agents' cost functions are strongly convex and
have Lipschitz continuous gradients. Numerical simulations are performed for a
Nash-Cournot game to illustrate the efficacy of the proposed algorithm.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Search and Rescue on a Poset 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.06622v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.06622v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jan-Tino Brethouwer, Robbert Fokkink
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A Search and Rescue game (SR game) is a new type of game on a graph that has
quickly found applications in scheduling, object detection, and adaptive
search. In this paper, we broaden the definition of SR games by putting them
into the context of ordered sets and Bayesian networks, extending known
solutions of these games and opening up the way to further applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Expected flow networks in stochastic environments and two-player
  zero-sum games <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02779v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02779v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marco Jiralerspong, Bilun Sun, Danilo Vucetic, Tianyu Zhang, <span class="highlight-author">Yoshua Bengio</span>, Gauthier Gidel, Nikolay Malkin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative flow networks (GFlowNets) are sequential sampling models trained
to match a given distribution. GFlowNets have been successfully applied to
various structured object generation tasks, sampling a diverse set of
high-reward objects quickly. We propose expected flow networks (EFlowNets),
which extend GFlowNets to stochastic environments. We show that EFlowNets
outperform other GFlowNet formulations in stochastic tasks such as protein
design. We then extend the concept of EFlowNets to adversarial environments,
proposing adversarial flow networks (AFlowNets) for two-player zero-sum games.
We show that AFlowNets learn to find above 80% of optimal moves in Connect-4
via self-play and outperform AlphaZero in tournaments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024; code: https://github.com/GFNOrg/AdversarialFlowNetworks</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Theory
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning How to Strategically Disclose Information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08741v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08741v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raj Kiriti Velicheti, Melih Bastopcu, S. Rasoul Etesami, Tamer Başar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Strategic information disclosure, in its simplest form, considers a game
between an information provider (sender) who has access to some private
information that an information receiver is interested in. While the receiver
takes an action that affects the utilities of both players, the sender can
design information (or modify beliefs) of the receiver through signal
commitment, hence posing a Stackelberg game. However, obtaining a Stackelberg
equilibrium for this game traditionally requires the sender to have access to
the receiver's objective. In this work, we consider an online version of
information design where a sender interacts with a receiver of an unknown type
who is adversarially chosen at each round. Restricting attention to Gaussian
prior and quadratic costs for the sender and the receiver, we show that
$\mathcal{O}(\sqrt{T})$ regret is achievable with full information feedback,
where $T$ is the total number of interactions between the sender and the
receiver. Further, we propose a novel parametrization that allows the sender to
achieve $\mathcal{O}(\sqrt{T})$ regret for a general convex utility function.
We then consider the Bayesian Persuasion problem with an additional cost term
in the objective function, which penalizes signaling policies that are more
informative and obtain $\mathcal{O}(\log(T))$ regret. Finally, we establish a
sublinear regret bound for the partial information feedback setting and provide
simulations to support our theoretical results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The q-ary Gil<span class="highlight-title">bert</span>-Varshamov bound can be improved for all but finitely
  many positive integers q 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08727v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08727v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xue-Bin Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For any positive integer $q\geq 2$ and any real number $\delta\in(0,1)$, let
$\alpha_q(n,\delta n)$ denote the maximum size of a subset of $\mathbb{Z}_q^n$
with minimum Hamming distance at least $\delta n$, where
$\mathbb{Z}_q=\{0,1,\dotsc,q-1\}$ and $n\in\mathbb{N}$. The asymptotic rate
function is defined by $ R_q(\delta) =
\limsup_{n\rightarrow\infty}\frac{1}{n}\log_q\alpha_q(n,\delta n). $ The famous
$q$-ary asymptotic Gilbert-Varshamov bound, obtained in the 1950s, states that
\[ R_q(\delta) \geq 1 -
\delta\log_q(q-1)-\delta\log_q\frac{1}{\delta}-(1-\delta)\log_q\frac{1}{1-\delta}
\stackrel{\mathrm{def}}{=}R_\mathrm{GV}(\delta,q) \] for all positive integers
$q\geq 2$ and $0<\delta<1-q^{-1}$. In the case that $q$ is an even power of a
prime with $q\geq 49$, the $q$-ary Gilbert-Varshamov bound was firstly improved
by using algebraic geometry codes in the works of Tsfasman, Vladut, and Zink
and of Ihara in the 1980s. The further investigation in algebraic geometry
codes has shown that the $q$-ary Gilbert-Varshamov bound can also be improved
in the case that $q$ is an odd power of a prime but not a prime with $q > 125$.
However, it remains a long-standing open problem whether the $q$-ary
Gilbert-Varshamov bound would be tight for those infinitely many integers $q$
which is a prime, except for Fermat primes not less than 257, and which is a
generic positive integer not being a prime power.
  In this paper, we prove that the $q$-ary Gilbert-Varshamov bound can be
improved for all but finitely many positive integers $q\geq 2$. It is shown
that $ R_q(1/2) > R_\mathrm{GV}(1/2,q) $ for all integers $q > \exp(29)$.
Furthermore, we show that the growth of the rate function $R_q(\delta)$ for
$\delta\in(0,1)$ fixed and $q$ growing large has a nontrivial lower bound.
These new lower bounds are achieved by using codes from geometry of numbers
introduced by Lenstra in the 1980s.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>33 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improved Trade-offs Between Amortization and Download Bandwidth for
  Linear HSS 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08719v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08719v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keller Blackwell, Mary Wootters
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A Homomorphic Secret Sharing (HSS) scheme is a secret-sharing scheme that
shares a secret $x$ among $s$ servers, and additionally allows an output client
to reconstruct some function $f(x)$ using information that can be locally
computed by each server. A key parameter in HSS schemes is download rate, which
quantifies how much information the output client needs to download from the
servers. Often, download rate is improved by amortizing over $\ell$ instances
of the problem, making $\ell$ also a key parameter of interest.
  Recent work (Fosli, Ishai, Kolobov, and Wootters 2022) established a limit on
the download rate of linear HSS schemes for computing low-degree polynomials
and constructed schemes that achieve this optimal download rate; their schemes
required amortization over $\ell = \Omega(s \log(s))$ instances of the problem.
Subsequent work (Blackwell and Wootters, 2023) completely characterized linear
HSS schemes that achieve optimal download rate in terms of a coding-theoretic
notion termed optimal labelweight codes. A consequence of this characterization
was that $\ell = \Omega(s \log(s))$ is in fact necessary to achieve optimal
download rate.
  In this paper, we characterize all linear HSS schemes, showing that schemes
of any download rate are equivalent to a generalization of optimal labelweight
codes. This equivalence is constructive and provides a way to obtain an
explicit linear HSS scheme from any linear code. Using this characterization,
we present explicit linear HSS schemes with slightly sub-optimal rate but with
much improved amortization $\ell = O(s)$. Our constructions are based on
algebraic geometry codes (specifically Hermitian codes and Goppa codes).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2311.14842</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Meta Reinforcement Learning for Resource Allocation in Aerial
  Active-RIS-assisted Networks with Rate-Splitting Multiple Access 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08648v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08648v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sajad Faramarzi, Sepideh Javadi, Farshad Zeinali, Hosein Zarini, Mohammad Robat Mili, Mehdi Bennis, Yonghui Li, Kai-Kit Wong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mounting a reconfigurable intelligent surface (RIS) on an unmanned aerial
vehicle (UAV) holds promise for improving traditional terrestrial network
performance. Unlike conventional methods deploying passive RIS on UAVs, this
study delves into the efficacy of an aerial active RIS (AARIS). Specifically,
the downlink transmission of an AARIS network is investigated, where the base
station (BS) leverages rate-splitting multiple access (RSMA) for effective
interference management and benefits from the support of an AARIS for jointly
amplifying and reflecting the BS's transmit signals. Considering both the
non-trivial energy consumption of the active RIS and the limited energy storage
of the UAV, we propose an innovative element selection strategy for optimizing
the on/off status of RIS elements, which adaptively and remarkably manages the
system's power consumption. To this end, a resource management problem is
formulated, aiming to maximize the system energy efficiency (EE) by jointly
optimizing the transmit beamforming at the BS, the element activation, the
phase shift and the amplification factor at the RIS, the RSMA common data rate
at users, as well as the UAV's trajectory. Due to the dynamicity nature of UAV
and user mobility, a deep reinforcement learning (DRL) algorithm is designed
for resource allocation, utilizing meta-learning to adaptively handle fast
time-varying system dynamics. Simulations indicate that incorporating an active
RIS at the UAV leads to substantial EE gain, compared to passive RIS-aided UAV.
We observe the superiority of the RSMA-based AARIS system in terms of EE,
compared to existing approaches adopting non-orthogonal multiple access (NOMA).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust Distributed Compression with Learned Heegard-Berger Scheme 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08411v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08411v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eyyup Tasci, Ezgi Ozyilkan, Oguzhan Kubilay Ulger, Elza Erkip
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider lossy compression of an information source when decoder-only side
information may be absent. This setup, also referred to as the Heegard-Berger
or Kaspi problem, is a special case of robust distributed source coding.
Building upon previous works on neural network-based distributed compressors
developed for the decoder-only side information (Wyner-Ziv) case, we propose
learning-based schemes that are amenable to the availability of side
information. We find that our learned compressors mimic the achievability part
of the Heegard-Berger theorem and yield interpretable results operating close
to information-theoretic bounds. Depending on the availability of the side
information, our neural compressors recover characteristics of the
point-to-point (i.e., with no side information) and the Wyner-Ziv coding
strategies that include binning in the source space, although no structure
exploiting knowledge of the source and side information was imposed into the
design.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Coverage and Rate Analysis for Integrated Sensing and Communication
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08343v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08343v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xu Gan, Chongwen Huang, Zhaohui Yang, Xiaoming Chen, Jiguang He, Zhaoyang Zhang, Chau Yuen, Yong Liang Guan, Mérouane Debbah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Integrated sensing and communication (ISAC) is increasingly recognized as a
pivotal technology for next-generation cellular networks, offering mutual
benefits in both sensing and communication capabilities. This advancement
necessitates a re-examination of the fundamental limits within networks where
these two functions coexist via shared spectrum and infrastructures. However,
traditional stochastic geometry-based performance analyses are confined to
either communication or sensing networks separately. This paper bridges this
gap by introducing a generalized stochastic geometry framework in ISAC
networks. Based on this framework, we define and calculate the coverage and
ergodic rate of sensing and communication performance under resource
constraints. Then, we shed light on the fundamental limits of ISAC networks by
presenting theoretical results for the coverage rate of the unified
performance, taking into account the coupling effects of dual functions in
coexistence networks. Further, we obtain the analytical formulations for
evaluating the ergodic sensing rate constrained by the maximum communication
rate, and the ergodic communication rate constrained by the maximum sensing
rate. Extensive numerical results validate the accuracy of all theoretical
derivations, and also indicate that denser networks significantly enhance ISAC
coverage. Specifically, increasing the base station density from $1$
$\text{km}^{-2}$ to $10$ $\text{km}^{-2}$ can boost the ISAC coverage rate from
$1.4\%$ to $39.8\%$. Further, results also reveal that with the increase of the
constrained sensing rate, the ergodic communication rate improves
significantly, but the reverse is not obvious.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Quadratic Detection in Noncoherent Massive SIMO Systems over Correlated
  Channels 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.15030v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.15030v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marc Vilà-Insa, Aniol Martí, Jaume Riba, Meritxell Lamarca
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the goal of enabling ultrareliable and low-latency wireless
communications for industrial internet of things (IIoT), this paper studies the
use of energy-based modulations in noncoherent massive single-input
multiple-output (SIMO) systems. We consider a one-shot communication over a
channel with correlated Rayleigh fading and colored Gaussian noise, in which
the receiver has statistical channel state information (CSI). We first provide
a theoretical analysis on the limitations of unipolar pulse-amplitude
modulation (PAM) in systems of this kind, based on maximum likelihood
detection. The existence of a fundamental error floor at high signal-to-noise
ratio (SNR) regimes is proved for constellations with more than two energy
levels, when no (statistical) CSI is available at the transmitter. In the main
body of the paper, we present a design framework for quadratic detectors that
generalizes the widely-used energy detector, to better exploit the statistical
knowledge of the channel. This allows us to design receivers optimized
according to information-theoretic criteria that exhibit lower error rates at
moderate and high SNR. We subsequently derive an analytic approximation for the
error probability of a general class of quadratic detectors in the large array
regime. Finally, we numerically validate it and discuss the outage probability
of the system.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BICM-compatible Rate Adaptive Geometric Constellation Shaping Using
  Optimized Many-to-one Labeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.09444v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.09444v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Metodi Plamenov Yankov, Smaranika Swain, Ognjen Jovanovic, Darko Zibar, Francesco Da Ros
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, a rate adaptive geometric constellation shaping (GCS) scheme
which is fully backward-compatible with existing state of the art
bit-interleaved coded modulation (BICM) systems is proposed and experimentally
demonstrated. The system relies on optimization of the positions of the
quadrature amplitude modulation (QAM) points on the I/Q plane for maximized
achievable information rate, while maintaining quantization and fiber nonlinear
noise robustness. Furthermore, `dummy' bits are multiplexed with coded bits
before mapping to symbols. Rate adaptivity is achieved by tuning the ratio of
coded and `dummy' bits, while maintaining a fixed forward error-correction
block and a fixed modulation format size. The points' positions and their
labeling are optimized using automatic differentiation. The proposed GCS scheme
is compared to a time-sharing hybrid (TH) QAM modulation and the now mainstream
probabilistic amplitude shaping (PAS) scheme. The TH without shaping is
outperformed for all studied data rates in a simulated linear channel by up to
0.7 dB. In a linear channel, PAS is shown to outperform the proposed GCS
scheme, while similar performances are reported for PAS and the proposed GCS in
a simulated nonlinear fiber channel. The GCS scheme is experimentally
demonstrated in a multi-span recirculating loop coherent optical fiber
transmission system with a total distance of up to 3000 km. Near-continuous
zero-error flexible throughput is reported as a function of the transmission
distance. Up to 1-2 spans of increased reach gains are achieved at the same net
data rate w.r.t. conventional QAM. At a given distance, up to 0.79 bits/2D
symbol of gain w.r.t. conventional QAM is achieved. In the experiment, similar
performance to PAS is demonstrated.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to Journal of Lightwave Technology as a special extended
  version of a 'top-scored' paper at the European Conference on Optical
  Communications (ECOC) 2023</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>

</body>

<footer>
    <div>
        <time id="build-timestamp" datetime="2024-03-28T00:49:13.511345621Z">
            2024-03-28 00:49:13 UTC
        </time>
    </div>
</footer>
<script src="index.js"></script>
</html>
